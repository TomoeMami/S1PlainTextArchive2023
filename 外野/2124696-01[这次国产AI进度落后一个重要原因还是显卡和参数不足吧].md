
*****

####  freedomkought  
##### 1#       楼主       发表于 2023-3-18 20:00

查了下，GPT用的是A100显卡，大概数万张在运行（也有说1万多张的），但是这个显卡已经去年8月就对我国禁售了，现在老黄特供的只有阉割版的A800了，同样比A100更高端的H800也在禁运名单里

之前听到消息说百度把全公司的A100全部拉去跑AI了数量还是不够，我国拥有1万张A100的只有一家叫幻方的对冲基金。。国内几大巨头手上的专业级能用来跑AI的显卡数量不足，国产的专业显卡还没上市而且初期必然面对产量不足的局面+性能问题

也就是说当前局面是即便抛开国内各大公司的数据壁垒导致参数获取很困难，最基本能先搭建出与之对应的服务器群都有问题，短期内要追上的话要么考虑新模型减少参数和硬件需求，另外现在国内应该不缺设备的钱，几万张A100或H100级的显卡只要量够哪个巨头能搞到并且有靠谱的模型(比如清华的，百度的我觉得不太靠谱，当天看发布会后面技术总监介绍似乎是其他模型），融资分分钟钟就冲进来了，毕竟计算下十万张，售价十万（A100官方报价1万5000刀）也才100亿级别，相对于现在已经能看到未来发展潜力和路已经走通的情况下，这笔投资起码回报在国内是有望的

另外我持有一个观点是，各大厂包括谷歌都那么急的原因是：现在的模型除了硬件和砸钱外，最重要的是要有足够的参数（CPT很多次强调了他们的参数从2到3的指数级增长），先发的在参数数量上有优势，后跑的这个就吃亏了，在没有摸到天花板前只要领先者不停一直也在不断的累计参数那么后来者所需要的投入的资源就需要更多，至少当前看后发不一定有优势反而落后越多越难以追赶

谷歌自己发表的transformerm论文，结果它自己和大家都没想到最后真能大力出奇迹的结果，感觉就是大家都觉得可能有戏，但是谁都不愿意押宝，然后有人SH之后大家看到结果都觉得：艹怎么可能这样。。。

﹍﹍﹍

评分

 参与人数 4战斗力 -3

|昵称|战斗力|理由|
|----|---|---|
| 青蛙叔叔|-2|用过AI吗？|
| iamwar|-1|并不是，计算卡产品很多，百度纯粹没有准备.|
| 大江户战士|-2|不懂装懂|
| bapijun| + 2||

查看全部评分

*****

####  coldhot3  
##### 2#       发表于 2023-3-18 20:10

找到一个好的框架，做专用asic加速卡才是王道。你看挖矿也从显卡变成专用芯片矿机了。

*****

####  dmasdfdmazz  
##### 3#       发表于 2023-3-18 20:11

是钱不够吧》》

看微软，谷歌烧的钱？？

国内除了芯片还没这么大投入的公司。

*****

####  Nanachi  
##### 4#       发表于 2023-3-18 20:12

我记得手机上已经有给ai计算优化过的npu了，啥时候造出专门的计算硬件也不奇怪

—— 来自 HUAWEI JAD-AL50, Android 12上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4

*****

####  whirlpool  
##### 5#       发表于 2023-3-18 20:14

做ASIC意味着你软件方面的投入也要一锤子买卖到位上去，cypto这种应用太单一了所以可以搞，对于ML这种需要不断添加新算子的这个成本迁移起来还是很大的 毕竟你自己搞可是没有老黄帮你搞cuda这种东西

*****

####  嘟帝  
##### 6#       发表于 2023-3-18 20:16

我感觉纯粹是没重视而已，ai百度一直都在搞，结果被openai当头一棒赶紧拿点货出来，跟谷歌行为差不太多。

*****

####  widder  
##### 7#       发表于 2023-3-18 20:16

直接上超算不香么，干嘛还去凑显卡

*****

####  PENTAX-DA  
##### 8#       发表于 2023-3-18 20:18

因为之前的成功都是跟随效应生效吧

就骑车跟车更省力，到终点再超车也来得及

但是AI这种玩意，一个氮气加速，车尾灯都看不到了

其实大数据开始就是先发垄断优势

国内公司知道抢数据，但是不知道怎么用数据

还是慢了

*****

####  tillnight  
##### 9#       发表于 2023-3-18 20:20

整天落后落后的百度答辩的，我寻思chatGPT也不是ai的唯一方向，只是找到一个让大众能wow的展示方式了。不然谷歌为什么也端不出不“答辩”的？

*****

####  orecheng  
##### 10#       发表于 2023-3-18 20:25

百度文心一言是在华为鹏城云脑2上面训练的，E级超算

*****

####  shmilywhale  
##### 11#       发表于 2023-3-18 20:27

是的 所以半导体制裁现在是我们AI的压制器

*****

####  battleship64  
##### 12#       发表于 2023-3-18 20:30

做ai用通用芯片是懒
真想好好做肯定还是专用芯片

[论坛助手,iPhone](https://bbs.saraba1st.com/2b/forum.php?mod=viewthread&amp;tid=2029836)

*****

####  battleship64  
##### 13#       发表于 2023-3-18 20:32

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136218&amp;ptid=2124696" target="_blank">whirlpool 发表于 2023-3-18 20:14</a>
做ASIC意味着你软件方面的投入也要一锤子买卖到位上去，cypto这种应用太单一了所以可以搞，对于ML这种需要 ...</blockquote>
不asic还有fpga呢
不如asic暴打cpu

[论坛助手,iPhone](https://bbs.saraba1st.com/2b/forum.php?mod=viewthread&amp;tid=2029836)

*****

####  freedomkought  
##### 14#         楼主| 发表于 2023-3-18 20:40

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136283&amp;ptid=2124696" target="_blank">tillnight 发表于 2023-3-18 20:20</a>

整天落后落后的百度答辩的，我寻思chatGPT也不是ai的唯一方向，只是找到一个让大众能wow的展示方式了。不然 ...</blockquote>
发布会当天百度技术那位说了，文心一言在降低参数需求方面花了力气

百度现在的文心的确对比GPT很勉强，而且发布会的确砸了，谷歌拿出来的玩意儿翻车主要原因我文里也说了，参数不足，谷歌虽然重视AI但是多方面发展单一方向投入和训练都不足

走其他方向是可以的，但是前提是证明能走通而不是白白烧钱最后什么都烧不出来或者烧出个性能不够的东西，现在摆在面前的问题就是现有情况是要搞出国产的AI，不换赛道就只能跟着微软和OPENAI走大力飞砖路线，要换赛道就要有沉没成本的准备，而且时间可能会很长，OPENAI是15年成立，谷歌的模型论文是18年吧我记得发布的，到2022年底发布耗时了4年，而且这还是对方赌对了方向出来的，谷歌自己都不确定方向对不对

*****

####  qratosone  
##### 15#       发表于 2023-3-18 20:41

GLM6B都出来了，泥潭已经有帖子玩上了，LZ还在复读这套殖人话术呢？
[https://bbs.saraba1st.com/2b/thread-2124067-1-1.html](https://bbs.saraba1st.com/2b/thread-2124067-1-1.html)

事实证明甜点尺寸的模型是存在的，实用化的产品对算力的需求明显被高估了——反过来LLM目前在可控性等方面遇到的短板也几乎不可能靠堆规模解决

*****

####  lvseqiji  
##### 16#       发表于 2023-3-18 20:41

算法和数据本身都有差距，很多人觉得有了硬件硬算就行，但是实际工程上要做到openAI那样还是需要很多试错的

—— 来自 Sony XQ-BQ72, Android 13上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.2-play

*****

####  Ichthys  
##### 17#       发表于 2023-3-18 20:42

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136338&amp;ptid=2124696" target="_blank">orecheng 发表于 2023-3-18 20:25</a>
百度文心一言是在华为鹏城云脑2上面训练的，E级超算</blockquote>
鹏城不是1000pops么，什么时候是e级了。

*****

####  TuzDoDez  
##### 18#       发表于 2023-3-18 20:42

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136512&amp;ptid=2124696" target="_blank">freedomkought 发表于 2023-3-18 20:40</a>

发布会当天百度技术那位说了，文心一言在降低参数需求方面花了力气

百度现在的文心的确对比GPT很勉强，而 ...</blockquote>
谷歌的PaLM-E有五千多亿模型，但它训练的范围太广了。

*****

####  amgryn  
##### 19#       发表于 2023-3-18 20:43

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136238&amp;ptid=2124696" target="_blank">widder 发表于 2023-3-18 20:16</a>

直接上超算不香么，干嘛还去凑显卡</blockquote>
超算的通用性不是那么强，而且超算里边也要用GPU。。。

虽说其实AI这种东西确实不太需要通用性，但是总不可能CPU都抛弃了只用显卡吧

*****

####  究极学霸  
##### 20#       发表于 2023-3-18 20:44

说到这个，前几天听一个播客，主持人提出了一个很有意思的看法，就是说近几年很有可能忽然对国内半导体的制裁放开，光刻机想订就订，如果真出现这个情况，代表AI时代下的全新运算方式取得重要突破了，现有的半导体体系一下子都成废物了，所以要让我国去买单了……

*****

####  freedomkought  
##### 21#         楼主| 发表于 2023-3-18 20:44

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136523&amp;ptid=2124696" target="_blank">qratosone 发表于 2023-3-18 20:41</a>

GLM6B都出来了，泥潭已经有帖子玩上了，LZ还在复读这套殖人话术呢？

https://bbs.saraba1st.com/2b/thread- ...</blockquote>
你是不是没看全我说的？我提过清华的模型可行了，你发的那个主题我已经早看过了。。。并且我也认可清华方向是可行的，说点可能的忧虑就扣殖人的帽子？

*****

####  碳.  
##### 22#       发表于 2023-3-18 20:45

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136174&amp;ptid=2124696" target="_blank">coldhot3 发表于 2023-3-18 20:10</a>

找到一个好的框架，做专用asic加速卡才是王道。你看挖矿也从显卡变成专用芯片矿机了。 ...</blockquote>
问题是去哪流片

所以前一段时间才会有那个《别忽视游戏行业的科技价值》的社论

*****

####  orecheng  
##### 23#       发表于 2023-3-18 20:45

<blockquote>Ichthys 发表于 2023-3-18 20:42
鹏城不是1000pops么，什么时候是e级了。</blockquote>
千p和e不就是一回事

*****

####  Diog  
##### 24#       发表于 2023-3-18 20:48

百度的AI 方向还是多的吧，那天王海峰的slide 下面还有蛋白方向的模型。

*****

####  宏.  
##### 25#       发表于 2023-3-18 20:48

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136523&amp;ptid=2124696" target="_blank">qratosone 发表于 2023-3-18 20:41</a>

GLM6B都出来了，泥潭已经有帖子玩上了，LZ还在复读这套殖人话术呢？

https://bbs.saraba1st.com/2b/thread- ...</blockquote>
对话模型都是小事，目前大家猜的是靠堆参数堆算力就能堆出AGI，那才是大事。而显然在大模型AGI出来之前试图搞一个小模型AGI根本不可能。

*****

####  宏.  
##### 26#       发表于 2023-3-18 20:49

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136576&amp;ptid=2124696" target="_blank">究极学霸 发表于 2023-3-18 20:44</a>

说到这个，前几天听一个播客，主持人提出了一个很有意思的看法，就是说近几年很有可能忽然对国内半导体的制 ...</blockquote>
不可能，硅芯片能用的地方多了去了，何况AGI现在看只能从硅芯片上面堆出来

*****

####  widder  
##### 27#       发表于 2023-3-18 20:50

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136571&amp;ptid=2124696" target="_blank">amgryn 发表于 2023-3-18 20:43</a>

超算的通用性不是那么强，而且超算里边也要用GPU。。。

虽说其实AI这种东西确实不太需要通用性，但是总不 ...</blockquote>
超算就是通用的啊，用显卡搭超算只是方便实施的方案，用专用芯片其实更好，我们不是有申威么

我们AI落后的原因还在于我们是实用主义路线，不会去搞需要大量投入，但实用前景并不怎么明朗的东西，像实用的自动驾驶不是搞得很起劲么，大语言模型这种就投入很少了，即使是百度的文心一言估计原本也没多少投入，只是国外风头起来后发现了各种实用价值，于是就赶鸭子上架了

*****

####  勿徊哉  
##### 28#       发表于 2023-3-18 20:50

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136576&amp;ptid=2124696" target="_blank">究极学霸 发表于 2023-3-18 20:44</a>

说到这个，前几天听一个播客，主持人提出了一个很有意思的看法，就是说近几年很有可能忽然对国内半导体的制 ...</blockquote>
我觉得这种主持人首先会被ChatGPT取代，尤其是在幻想这方面。

*****

####  自旋  
##### 29#       发表于 2023-3-18 20:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136338&amp;ptid=2124696" target="_blank">orecheng 发表于 2023-3-18 20:25</a>

百度文心一言是在华为鹏城云脑2上面训练的，E级超算</blockquote>
百度官方说是自己的计算中心啊，三个计算中心一起算的

*****

####  燕山雪  
##### 30#       发表于 2023-3-18 20:56

可不要妄自菲薄，我大华为规模性能都超越GPT3的盘古大模型已经出来两年了哦，至今还在“即将上线”只是为了给英美鬼畜留口饭吃而已<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">
[https://www.huaweicloud.com/product/modelarts/pangu.html](https://www.huaweicloud.com/product/modelarts/pangu.html)

<img src="https://img.saraba1st.com/forum/202303/18/205620l4deo2fuueduaemo.png" referrerpolicy="no-referrer">

<strong>屏幕截图 2023-03-18 205614.png</strong> (71.3 KB, 下载次数: 0)

下载附件

2023-3-18 20:56 上传


*****

####  Diog  
##### 31#       发表于 2023-3-18 20:59

之前在B站看到有的游戏玩家发现自己对于老黄来说根本可有可无，破防的样子真的好可怜，英伟达真的是目前AI 时代的大赢家，元宇宙也赚了一波。老黄真的赚麻了，希望国产显卡有一天能挑起大梁吧。

*****

####  愿闻其翔  
##### 32#       发表于 2023-3-18 21:02

我倒觉得这种程度的落后还算可以接受，GPT出来还没多久，这么快就拿出一个可以对标的成品了，全世界也就中国做到了吧

反正还是摸着美帝过河，路子已经在那了，接下来就靠卷呗<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">

*****

####  bapijun  
##### 33#       发表于 2023-3-18 21:03

楼上先把推理和训练的概念搞清楚

*****

####  wer5lcy  
##### 34#       发表于 2023-3-18 21:07

能够自主设计深度学习模型的人才还是偏少。

*****

####  shikiki  
##### 35#       发表于 2023-3-18 21:15

<img src="https://static.saraba1st.com/image/smiley/face2017/020.png" referrerpolicy="no-referrer">不就是不赚钱国内就没人搞，看看那个波士顿动力也是一样例子只不过人家还是没实现质的突破

*****

####  chaucerling  
##### 36#       发表于 2023-3-18 21:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136896&amp;ptid=2124696" target="_blank">wer5lcy 发表于 2023-3-18 21:07</a>

能够自主设计深度学习模型的人才还是偏少。</blockquote>
能出top论文的华人大部分都在美国的研究院，国内没这个环境

*****

####  phorcys02  
##### 37#       发表于 2023-3-18 21:16

 本帖最后由 phorcys02 于 2023-3-18 21:19 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136550&amp;ptid=2124696" target="_blank">Ichthys 发表于 2023-3-18 20:42</a>

鹏城不是1000pops么，什么时候是e级了。</blockquote>
1E AI ops，显然和通常所说的  E级超算不是一个赛道...<img src="https://static.saraba1st.com/image/smiley/face2017/003.png" referrerpolicy="no-referrer"> <blockquote>“鹏城云脑II” 包括4096颗昇腾910 AI处理器和2048颗鲲鹏920 CPU处理器，可以提供1E OPS算力</blockquote>
都到这种超大规模模型了，其实用不用cuda不是特别重要

个人自家炼图，当然只能用cuda了，a卡rocm都很少人用

但这种超大规模模型，又不缺码农，用谁家的卡其实差别不是特别大

主要华为造晟腾的路子被掐了，晟腾910 2019年发布的东西了， FP16 256Tops， INT8 512Tops

当年看是比v100强的，可老黄有a100了，华为晟腾凉了。

华为真的自己造了Atlas900集群，还部署到 广州，还提供了全栈的框架MindSpore， cuda的代码改改就能直接跑了。

但国内其他AI芯片厂商就稍差一点了

*****

####  衛藤美彩  
##### 38#       发表于 2023-3-18 21:19

<img src="https://static.saraba1st.com/image/smiley/face2017/047.png" referrerpolicy="no-referrer">openai堆了多少人去搞数据标注或者人工强化学习都没个介绍，这就一锤子确定是算力问题了？ 

*****

####  yesicant  
##### 39#       发表于 2023-3-18 21:21

LLaMA13B能力超越GPT3的175B，这东西要的是可解释性不是盲堆，你不可能造出你无法理解的怪物

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  twy_2000  
##### 40#       发表于 2023-3-18 21:23

楼主没从事过这个领域的科研就不要扯淡了。

*****

####  燕山雪  
##### 41#       发表于 2023-3-18 21:25

<blockquote>衛藤美彩 发表于 2023-3-18 21:19
openai堆了多少人去搞数据标注或者人工强化学习都没个介绍，这就一锤子确定是算力问题了？  ...</blockquote>
因为只有算力问题方便甩锅给美国，总不能宣称学术能力调参技巧还有肯尼亚标记工被禁运了吧……

*****

####  宏.  
##### 42#       发表于 2023-3-18 21:26

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137047&amp;ptid=2124696" target="_blank">yesicant 发表于 2023-3-18 21:21</a>

LLaMA13B能力超越GPT3的175B，这东西要的是可解释性不是盲堆，你不可能造出你无法理解的怪物

—— 来自 S1 ...</blockquote>
然而现实是好像真造出来了，LLM发现了显着的涌现现象，和可解释性没关系

*****

####  freedomkought  
##### 43#         楼主| 发表于 2023-3-18 21:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137029&amp;ptid=2124696" target="_blank">衛藤美彩 发表于 2023-3-18 21:19</a>

openai堆了多少人去搞数据标注或者人工强化学习都没个介绍，这就一锤子确定是算力问题了？  ...</blockquote>
因为在我的理解里，你说的OPENAI雇佣的那些海量劳动力做标记和操作“奖励”机制人员这些都算是参数里的了

*****

####  酸菜泡面  
##### 44#       发表于 2023-3-18 21:28

拉不出来怪地球，
足球
动画
电影
ai----new

—— 来自 samsung SM-G9600, Android 9上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4

*****

####  sfqjsml  
##### 45#       发表于 2023-3-18 21:30

不懂就问，显卡这种东西不能靠走私吗

*****

####  哇库哇库  
##### 46#       发表于 2023-3-18 21:31

<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">按泥潭友的说法，既不是硬件问题，又不是算法问题，那只能是t。。。。。问题了

*****

####  宏.  
##### 47#       发表于 2023-3-18 21:38

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137175&amp;ptid=2124696" target="_blank">sfqjsml 发表于 2023-3-18 21:30</a>

不懂就问，显卡这种东西不能靠走私吗</blockquote>
你买100张显卡可以走私，你买100万张显卡，去哪走私去，沃尔玛？

*****

####  CCauchy  
##### 48#       发表于 2023-3-18 21:55

应该是公司领导层决策问题，不然为什么谷歌没拿出chatgpt？谷歌还有tpu呢，国内也有专用ai芯片

[论坛助手,iPhone](https://bbs.saraba1st.com/2b/forum.php?mod=viewthread&amp;tid=2029836)

*****

####  yesicant  
##### 49#       发表于 2023-3-18 22:13

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137116&amp;ptid=2124696" target="_blank">宏. 发表于 2023-3-18 21:26</a>
然而现实是好像真造出来了，LLM发现了显着的涌现现象，和可解释性没关系 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/245.png" referrerpolicy="no-referrer">那你倒是说说涌现了啥，cot吗，cot也可以移植到小模型，更别说还有错误的cot幻觉了，这就蒙的中和蒙不中的区别，效果好吗，早就有证明无限宽的神经网络可以拟合任何函数了，参数大拟合的精准不是很正常吗

语言模型之所以必须足够大，才能进行类比推理，可以用解决问题需要的规则必须存储在足够多的参数中加以解释。例如需要同时使用三条规则进行类比的问题，就需要模型的参数能同时存储三条规则。然而语言模型的类比能力完全来自于预测人类文本，语言充满了类比，因此准确地预测自然语言可能需要一种能力。但是我们没有理由假设同样的系统，如果没有人类产生的输入,会自发形成类比式的思维能力。在某种程度上，大型语言模型捕获了成年人类的类比能力，它们的类比能力从根本上来说寄生在人类的自然智力上。

<img src="https://img.saraba1st.com/forum/202303/18/221334jfkh46tg4t8eated.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230318-221321.jpg</strong> (308.61 KB, 下载次数: 0)

下载附件

2023-3-18 22:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  大江户战士  
##### 50#       发表于 2023-3-18 22:26

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137178&amp;ptid=2124696" target="_blank">哇库哇库 发表于 2023-3-18 21:31</a>

按泥潭友的说法，既不是硬件问题，又不是算法问题，那只能是t。。。。。问题了 ...</blockquote>
说白了国内的大科技企业不肯为了一个不确定的东西投大钱，别人先搞出来证明可行了才会跟

这点其实和大部分国内高精尖行业都差不多，比如航天现在坐等星舰成功好立项长九呢<img src="https://static.saraba1st.com/image/smiley/face2017/035.png" referrerpolicy="no-referrer">

*****

####  宏.  
##### 51#       发表于 2023-3-18 22:35

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137626&amp;ptid=2124696" target="_blank">yesicant 发表于 2023-3-18 22:13</a>

那你倒是说说涌现了啥，cot吗，cot也可以移植到小模型，更别说还有错误的cot幻觉了，这就蒙的中和 ...</blockquote>
你训练的是语言模型，那当然语言能力强其他能力弱了啊，这不是理所当然的吗？<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">所以多模态大模型是显而易见的前进方向
 <blockquote>但是我们没有理由假设同样的系统，如果没有人类产生的输入,会自发形成类比式的思维能力</blockquote>
有没有理由假设无所谓，做出来看能不能跑就是了，现在从GPT-4来看，能跑。解释性根本不重要。

*****

####  内务部的阿尔金  
##### 52#       发表于 2023-3-18 22:40

是重要原因，但不是根本原因。什么时候不“大炼”各种产业不搞二革崇拜了，什么时候就能有希望冒头

*****

####  少打音游多读书  
##### 53#       发表于 2023-3-18 22:41

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136403&amp;ptid=2124696" target="_blank">battleship64 发表于 2023-3-18 20:30</a>
做ai用通用芯片是懒
真想好好做肯定还是专用芯片</blockquote>
如果你说的是把整个网络硬件化的话那做不到，网络中上百个不同的算子，芯片面积必然放不下。如果指的是将性能热点部分比如矩阵乘法电路化，那现在赛场里的所有玩家都有类似于tensor core的专用硬件

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yesicant  
##### 54#       发表于 2023-3-18 22:57

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137852&amp;ptid=2124696" target="_blank">宏. 发表于 2023-3-18 22:35</a>
你训练的是语言模型，那当然语言能力强其他能力弱了啊，这不是理所当然的吗？所以多模态大模型是 ...</blockquote>
你还没发现吗

【在逻辑推理、因果推理和视觉相关的任务中，具有涌现特征的任务比例最低。】

也许你还不明白

<img src="https://img.saraba1st.com/forum/202303/18/225444rwquzqlp8u9889v8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230315-215432.jpg</strong> (136.83 KB, 下载次数: 0)

下载附件

2023-3-18 22:54 上传

说白了，这玩意只是【规模比较大】的【特化模型】，因为底层结构的能力【不是无所不能】的，你要清楚的理解【本质和功能】，及时的拓展模型的结构与可能性，否则堆一辈子参数都不可能堆出【AGI】

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  HZLJ  
##### 55#       发表于 2023-3-18 23:00

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137029&amp;ptid=2124696" target="_blank">衛藤美彩 发表于 2023-3-18 21:19</a>

openai堆了多少人去搞数据标注或者人工强化学习都没个介绍，这就一锤子确定是算力问题了？  ...</blockquote>
《你有肯尼亚数据奴工，我有黄土高原宝妈》

[https://bbs.saraba1st.com/2b/thread-2122580-1-1.html](https://bbs.saraba1st.com/2b/thread-2122580-1-1.html)

*****

####  大江户战士  
##### 56#       发表于 2023-3-18 23:00

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137921&amp;ptid=2124696" target="_blank">少打音游多读书 发表于 2023-3-18 22:41</a>

如果你说的是把整个网络硬件化的话那做不到，网络中上百个不同的算子，芯片面积必然放不下。如果指的是将 ...</blockquote>
实际上已经有特定模型专用的AI芯片了，用FPGA实现的

*****

####  宏.  
##### 57#       发表于 2023-3-18 23:07

 本帖最后由 宏. 于 2023-3-18 23:17 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60138091&amp;ptid=2124696" target="_blank">yesicant 发表于 2023-3-18 22:57</a>

你还没发现吗

【在逻辑推理、因果推理和视觉相关的任务中，具有涌现特征的任务比例最低。】</blockquote>
我不都说了吗，你拿语料训练当然就是语言逻辑能力特化，但是物理和数学逻辑很差，所以才会连基本数学题都算出，这有什么问题？所以我说了多模态大模型才是方向，不只是语料和图像，甚至接入一个机械臂去反复练习抓东西可能都有必要，如此才能建立符合现实的数学和物理逻辑，这依旧是理所当然的啊

几年前就有的一个例子，游戏物理引擎里的双足机器人跑动问题，反复训练之后确实能跑起来了，这其实也是一种涌现

把多模态大模型接入物理引擎甚至数学引擎的尝试，这个主意显然很容易就能想到。

*****

####  大江户战士  
##### 58#       发表于 2023-3-18 23:13

 本帖最后由 大江户战士 于 2023-3-18 23:15 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60138208&amp;ptid=2124696" target="_blank">宏. 发表于 2023-3-18 23:07</a>

我不都说了吗，你拿语料训练当然就是语言逻辑能力特化，但是物理和数学逻辑很差，所以才会连基本数学题都 ...</blockquote>
LM距离AGI差距还很大

*****

####  東京急行  
##### 59#       发表于 2023-3-18 23:13

是是是，微软谷歌苹果亚马逊都是答辩没钱没卡没技术<img src="https://static.saraba1st.com/image/smiley/face2017/020.png" referrerpolicy="no-referrer">

*****

####  钦念以忱  
##### 60#       发表于 2023-3-18 23:14

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60138138&amp;ptid=2124696" target="_blank">HZLJ 发表于 2023-3-18 23:00</a>

《你有肯尼亚数据奴工，我有黄土高原宝妈》

https://bbs.saraba1st.com/2b/thread-2122580-1-1.html ...</blockquote>
我觉得肯尼亚数据奴工这方面的劣势比其他的更大。现在乃至近未来，以肯尼亚数据奴工为代表的全球最穷英语人群相比美国中产为代表的全球最富英语人群，这个收入财富差距，肯定比黄土高原宝妈为代表的最穷华语人群相比北上广中产乃至港台高华为代表的全球最富华语人群，要大得多。

这就好像以前很多国家，找不出足够大的语言人群，所以发展不好互联网一样，中文虽然有十几亿人口，能和英语人口大体一个水平，但中文世界的内部贫富差距肯定达不到英语世界的水平。


*****

####  宏.  
##### 61#       发表于 2023-3-18 23:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60138274&amp;ptid=2124696" target="_blank">大江户战士 发表于 2023-3-18 23:13</a>

看得出你的确对AI知之甚少… ...</blockquote>
多模态大模型和AGI差距有多大这个问题现在谁都说不准，但是现在这轮军备竞赛显然有人也想到了这个问题。注意看下图谷歌和deepmind的多模态应用
<img src="https://pic2.zhimg.com/80/v2-dae9e63e308ac86f6167126147945291_1440w.webp" referrerpolicy="no-referrer">

*****

####  yesicant  
##### 62#       发表于 2023-3-18 23:17

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60138208&amp;ptid=2124696" target="_blank">宏. 发表于 2023-3-18 23:07</a>
我不都说了吗，你拿语料训练当然就是语言逻辑能力特化，但是物理和数学逻辑很差，所以才会连基本数学题都 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/245.png" referrerpolicy="no-referrer">一个人类只需要24*365*18=157680小时的五感数据就可以成材，你AI需要多少？AI三剑客，算法/数据/算力，算法才是最重要的

<img src="https://img.saraba1st.com/forum/202303/18/231331ng2sk2fggz7a5ttw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230318-231313.jpg</strong> (213.16 KB, 下载次数: 0)

下载附件

2023-3-18 23:13 上传

这玩意进步的方向是仿生学，是把思维的功能抽象出来，再让模型互相特化和协作，你又落入极端思维了

数据集是AI的课本，但AI的结构才能真正决定AI学到什么东西，效率又如何，你这就盯着一个大字了…

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  oyss  
##### 63#       发表于 2023-3-18 23:19

难道真觉得美国AI领先优势都是去年禁运到现在这大概半年时间内搞出来的么?

*****

####  宏.  
##### 64#       发表于 2023-3-18 23:22

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60138316&amp;ptid=2124696" target="_blank">yesicant 发表于 2023-3-18 23:17</a>

一个人类只需要24*365*18=157680小时的五感数据就可以成材，你AI需要多少？AI三剑客，算法/数据/ ...</blockquote>
和人类比效率干嘛？还是那个例子，鸟飞起来只要甚至不到1W的功率，莱特兄弟的飞行者一号需要多少功率？莱特兄弟也没有建立完备的空气动力学，人家直接靠自行车上绑个翼板就试出了可用的翼型截面

所有的问题，能跑才是关键。跑起来再去研究算法也不迟。GPT对NLP领域的冲击已经够明显了。


*****

####  yesicant  
##### 65#       发表于 2023-3-18 23:29

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60138377&amp;ptid=2124696" target="_blank">宏. 发表于 2023-3-18 23:22</a>
和人类比效率干嘛？还是那个例子，鸟飞起来只要甚至不到1W的功率，莱特兄弟的飞行者一号需要多少功率？莱 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/245.png" referrerpolicy="no-referrer">我从来没有说工程学不重要，工程学当然可以领先一阵子

“想象一个古代的王国，他们的技术也在进步，能为士兵造出更好的刀啊剑啊长矛啊，甚至还有可能做出像机关枪那样连发的弓箭呢。”

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Sacross  
##### 66#       发表于 2023-3-18 23:33

讲道理国内ai口论文和科研能力又不弱，只不过chatgpt这种不只是科研问题，也不是ai的全部

不如说这种需要动用大规模显卡阵列来力大飞砖的本身就得是企业口投资试错，核心倒是成本的问题，你不能拿商品来断言科研不行，你比如更加有商业价值的人脸识别这块我就记得国内几个实验室就顶在顶上
或者说现在chatgpt那么火产生了商业影响力迫使政府及几个互联网巨头重视这个领域后应该可能会短期出竞品，百度这波属于赶鸭子上架了

另一块就是目前科研主流用的框架就那么点，这块也得承认，国内科研口哪怕做的改进也是在这些主流框架下的

—— 来自 OnePlus IN2020, Android 12上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4

*****

####  宏.  
##### 67#       发表于 2023-3-18 23:34

 本帖最后由 宏. 于 2023-3-18 23:35 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60138433&amp;ptid=2124696" target="_blank">yesicant 发表于 2023-3-18 23:29</a>

我从来没有说工程学不重要，工程学当然可以领先一阵子

“想象一个古代的王国，他们的技术也在进 ...</blockquote>
人类历史从来就是如此，火绳枪开始大规模列装的时候燃素说都还没退场呢，这有什么可奇怪的？<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">可解释性从来不是工程学的必备要素。


*****

####  鸳鸳相抱  
##### 68#       发表于 2023-3-18 23:55

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136975&amp;ptid=2124696" target="_blank">shikiki 发表于 2023-3-18 21:15</a>

不就是不赚钱国内就没人搞，看看那个波士顿动力也是一样例子只不过人家还是没实现质的突破 ...</blockquote>
明明可以赚得盆满钵满啊，不然资本们在激动啥

我们只是不做高风险的开拓！


*****

####  千本blur  
##### 69#       发表于 2023-3-19 00:42

按照百度的服务器建设计划，大概要把50%的加速器国产化，这部分有他们自己的昆仑卡，还有一部分寒武纪590，将来璧仞也有可能加进来。目前AI算力中心的国产化率是政治指标，国家的策略就是长痛不如短痛，大家都难受一点，才能有一天不难受。

至于人工智能本身，说实话我感觉国内缺少的是国外的那些strong believer. GPT这种撞南墙撞出来的产品只有多几个玻尔兹曼才能做出来。


*****

####  qq8294193  
##### 70#       发表于 2023-3-19 01:34

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137175&amp;ptid=2124696" target="_blank">sfqjsml 发表于 2023-3-18 07:30</a>
不懂就问，显卡这种东西不能靠走私吗</blockquote>
不对个人开放<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">


*****

####  nekomimimode  
##### 71#       发表于 2023-3-19 04:17

指不定在专门搞军用AI呢，就不跟你说<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">


*****

####  profh3  
##### 72#       发表于 2023-3-19 06:43

 本帖最后由 profh3 于 2023-3-19 06:46 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137075&amp;ptid=2124696" target="_blank">twy_2000 发表于 2023-3-18 21:23</a>
楼主没从事过这个领域的科研就不要扯淡了。</blockquote>
现在很多热烈参与讨论chatgpt这in最酷炫的赛博科技的人很多连理工类背景知识都没有<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">


*****

####  深蓝巫妖  
##### 73#       发表于 2023-3-19 08:25

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60136187&amp;ptid=2124696" target="_blank">dmasdfdmazz 发表于 2023-03-18 20:11:12</a>
是钱不够吧》》

看微软，谷歌烧的钱？？

国内除了芯片还没这么大投入的公司。 ...</blockquote>我觉得还是中国当权者痛恨游戏的原因……………

[  -- 来自 能搜索的 Stage1官方 Android客户端](https://www.coolapk.com/apk/140634)


*****

####  深蓝巫妖  
##### 74#       发表于 2023-3-19 08:32

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60137762&amp;ptid=2124696" target="_blank">大江户战士 发表于 2023-03-18 22:26:46</a>
说白了国内的大科技企业不肯为了一个不确定的东西投大钱，别人先搞出来证明可行了才会跟

这点其实和大部 ...</blockquote>问题国企更懒…………

[  -- 来自 能手机投票的 Stage1官方 Android客户端](https://www.coolapk.com/apk/140634)


*****

####  右代宫嘉音  
##### 75#       发表于 2023-3-19 09:09

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60140193&amp;ptid=2124696" target="_blank"> 深蓝巫妖 发表于 2023-3-19 08:25</a> 引用:dmasdfdmazz 发表于 2023-03-18 20:11:12 是钱不够吧》》 看微软，谷歌烧的钱？？ 国内除了芯片还没这么大投入的公司。 ...我觉得还是中国当权者痛恨游戏的原因……………    -- 来自 能搜索的 Stage1官方 Android客户端 </blockquote>
有点三体光速飞船的味道了来自: iPhone客户端


*****

####  mimighost  
##### 76#       发表于 2023-3-19 09:28

能句句都讲错也是不容易


*****

####  兆悲  
##### 77#       发表于 2023-3-19 09:38

国家根本之前根本不重视民企的科学创新导致的


*****

####  mimighost  
##### 78#       发表于 2023-3-19 09:44

<blockquote>東京急行 发表于 2023-3-18 23:13
是是是，微软谷歌苹果亚马逊都是答辩没钱没卡没技术</blockquote>
有卡但是没人

美国大企业有的问题国内肯定一个都不少，比如扯皮，抢资源，放暗箭。特别是谷歌这种地方，一个事情几个组做，分给A组，B组就不干了。没看到这次好多牛批的人都跳去OpenAI了

OpenAI聚集谷歌搞transformer的核心的那批人，而且他们人少，钱和卡却多，执行能力不是大企业能比的。大企业除了有钱，其他哪哪儿都是flag，还真不如直接给小企业投钱让他们负责创新，自己收割成果就行了

OpenAI很天选，牛逼的经理，牛批的技术带头，牛逼的团队，工程能力极强，队伍小，钱多，当然关键是运气太特么好了，押宝押的准，那活该人家成功

