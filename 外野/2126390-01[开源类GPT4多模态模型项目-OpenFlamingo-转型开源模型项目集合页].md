
*****

####  yesicant  
##### 1#       楼主       发表于 2023-3-29 05:13

 本帖最后由 yesicant 于 2023-3-29 16:53 编辑 

最初是半夜突然刷到这条推特

<img src="https://img.saraba1st.com/forum/202303/29/050623clmz9099z8iilbkk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-050230__01.jpg</strong> (241.72 KB, 下载次数: 0)

下载附件

2023-3-29 05:06 上传

一查<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">复现了去年刷榜各种项目的的多模态模型，Flamingo相关的权重与训练代码

关于Flamingo的相关介绍可以看这里:https://zhuanlan.zhihu.com/p/508918171

hugface模型仓库地址:https://huggingface.co/openflamingo/OpenFlamingo-9B

训练代码:https://github.com/mlfoundations/open_flamingo

<img src="https://img.saraba1st.com/forum/202303/29/050936y2wa5pxpliev7a2d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-045154__01.jpg</strong> (57.36 KB, 下载次数: 0)

下载附件

2023-3-29 05:09 上传

模型权重净重5.2G，参数为9B，大部分人的显卡应该都能跑

<img src="https://img.saraba1st.com/forum/202303/29/051136l6kod7tkk977k9kh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-045504__01.jpg</strong> (242.13 KB, 下载次数: 0)

下载附件

2023-3-29 05:11 上传

效果比Flamingo论文中的同参数下性能差一些，可能缺少某些调优吧，不过有这个效果要什么自行车！

在线演示地址:https://7164d2142d11.ngrok.app/

<img src="https://img.saraba1st.com/forum/202303/29/051317wgbpffgt2rc4d7z6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-051239.jpg</strong> (309.45 KB, 下载次数: 0)

下载附件

2023-3-29 05:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

﹍﹍﹍

评分

 参与人数 5战斗力 +7

|昵称|战斗力|理由|
|----|---|---|
| arcanearcher| + 1|好评加鹅|
| 晨曦之下| + 1|高质量|
| Linjiangzhu| + 2||
| lzz| + 2|好评加鹅|
| wujae| + 1|好评加鹅|

查看全部评分

*****

####  yesicant  
##### 2#         楼主| 发表于 2023-3-29 05:15

好像忘了放相关博客地址:https://laion.ai/blog/open-flamingo/

<img src="https://img.saraba1st.com/forum/202303/29/051458wcx9c9sioz9s1e3u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-044650__01.jpg</strong> (378.22 KB, 下载次数: 0)

下载附件

2023-3-29 05:14 上传

<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">开源视频总结AI不远了

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yesicant  
##### 3#         楼主| 发表于 2023-3-29 05:16

大家记得不要用来做坏事哦

<img src="https://img.saraba1st.com/forum/202303/29/051629wb4didi6ty66xi59.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-045341__01.jpg</strong> (44.02 KB, 下载次数: 2)

下载附件

2023-3-29 05:16 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yesicant  
##### 4#         楼主| 发表于 2023-3-29 05:29

We are committed to build fully open-source models, and believe this transparency is essential for fostering collaboration, accelerating progress, and democratizing access to state-of-the-art LMMs. Our release is the first step towards this goal.

<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">非常的欢乐

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  大江户战士  
##### 5#       发表于 2023-3-29 05:33

有意思

*****

####  c月光咖啡  
##### 6#       发表于 2023-3-29 07:18

插眼

—— 来自 HONOR KKG-AN70, Android 11上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4

*****

####  神奇的春日~  
##### 7#       发表于 2023-3-29 07:54

cy

—— 来自 HUAWEI EBG-AN10, Android 10上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4

*****

####  yesicant  
##### 8#         楼主| 发表于 2023-3-29 16:45

草生，最近开源的LLM以及多模态项目一个比一个多，我都想特意开个贴集合了，不过这个帖子就挺好的，待我一个个更新吧<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202303/29/164447gnfca5pxfct4nzpb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-164423.jpg</strong> (262.4 KB, 下载次数: 0)

下载附件

2023-3-29 16:44 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  猫咪挠墙  
##### 9#       发表于 2023-3-29 16:48

给楼主打个标，棒！<img src="https://static.saraba1st.com/image/smiley/face2017/041.png" referrerpolicy="no-referrer">

*****

####  yesicant  
##### 10#         楼主| 发表于 2023-3-29 16:50

首先是Nomic AI发布的gpt4all项目

<img src="https://img.saraba1st.com/forum/202303/29/164721gi5xunu5pci886qn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-164619.jpg</strong> (308.71 KB, 下载次数: 0)

下载附件

2023-3-29 16:47 上传

github地址:https://github.com/nomic-ai/gpt4all

可以跑在笔记本上，使用了800k的gpt3.5 turbo接口生成的合成文本数据集，在自指导等论文形式的帮助下训练出的LLaMA-Lora插件，项目页有Lora，数据集，训练代码等，不过因为LLaMA的学术研究性质，所以并不直接提供对应的7B LLaMA权重，这个得自己准备，性能非常不错

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  astkaasa  
##### 11#       发表于 2023-3-29 16:54

mark

*****

####  darktide  
##### 12#       发表于 2023-3-29 16:58

插眼关注

*****

####  yesicant  
##### 13#         楼主| 发表于 2023-3-29 17:01

然后是Cerebras systems出品的Cerebras-GPT系列

<img src="https://img.saraba1st.com/forum/202303/29/165618jx79em0me2fw2vxr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-165218.jpg</strong> (346.26 KB, 下载次数: 0)

下载附件

2023-3-29 16:56 上传

项目主页说明地址:https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/

项目意义说明:https://www.businesswire.com/news/home/20230328005366/en/Cerebras-Systems-Releases-Seven-New-GPT-Models-Trained-on-CS-2-Wafer-Scale-Systems

arXiv论文还是coming soon
不过hugface已经开源权重了，使用了Chinchilla缩放法则高效训练模型，授权许可为Apache 2.0
一共公开了111m参数-13b参数总共7个版本不同的模型权重

hhugface页面https://huggingface.co/cerebras

<img src="https://img.saraba1st.com/forum/202303/29/165959owwi4f3h477tmoot.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-165828.jpg</strong> (244.52 KB, 下载次数: 0)

下载附件

2023-3-29 16:59 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  格林达姆  
##### 14#       发表于 2023-3-29 17:02

用消费级硬件一键生成小黄文的日子是不是不远了

*****

####  miraclePTSD  
##### 15#       发表于 2023-3-29 17:06

qmpdcy

*****

####  yesicant  
##### 16#         楼主| 发表于 2023-3-29 17:06

 本帖最后由 yesicant 于 2023-3-29 17:07 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60263691&amp;ptid=2126390" target="_blank">格林达姆 发表于 2023-3-29 17:02</a>
用消费级硬件一键生成小黄文的日子是不是不远了</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">项目就是搞这个的，不过没有中文版本，最新版本都堆到6b参数了，目前来说也是开源项目，在hugface上那不是一般的火

<img src="https://img.saraba1st.com/forum/202303/29/170731naq330993k4i2zma.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-170714.jpg</strong> (164.53 KB, 下载次数: 0)

下载附件

2023-3-29 17:07 上传

https://rentry.org/pygmalion-ai

<img src="https://img.saraba1st.com/forum/202303/29/170557k95l3koc3ll32hc5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-170546.jpg</strong> (133.15 KB, 下载次数: 0)

下载附件

2023-3-29 17:05 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Lacsiess  
##### 17#       发表于 2023-3-29 17:07

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60263691&amp;ptid=2126390" target="_blank">格林达姆 发表于 2023-3-29 17:02</a>

用消费级硬件一键生成小黄文的日子是不是不远了</blockquote>
应该是一键续写太监书的时代快来了......前面的文本作为学习材料,后面自己填想看的剧情一键生成

*****

####  摇曳的树影  
##### 18#       发表于 2023-3-29 17:08

收藏比回复多哈哈<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">

*****

####  yesicant  
##### 19#         楼主| 发表于 2023-3-29 17:15

然后就是比较重复造轮子的lit-llama项目了
因为LLaMA代码库的授权许可是GPL，这个项目通过基于nanoGPT的实现构造了一个新的Apache 2.0许可的lit-llama项目，经过这个项目转换后就可以用于商业化等措施了

项目地址:https://github.com/Lightning-AI/lit-llama

<img src="https://img.saraba1st.com/forum/202303/29/171449eznc84699n66zu6c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-171437.jpg</strong> (75.93 KB, 下载次数: 0)

下载附件

2023-3-29 17:14 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  thegodra  
##### 20#       发表于 2023-3-29 17:15

世界在我们看不到的地方在变革！

这要怎样才能跟上时代的潮流呢？

*****

####  yesicant  
##### 21#         楼主| 发表于 2023-3-29 17:27

LLaMA-Adapter项目

一看到Adapter，大部分人应该就懂了，腾讯人工智能实验室在不久前搞过一个名为T2I-Adapter的用于SD微调的附加模型，原理也来自于Adapter，通过微调附加增量参数的方法高效训练大模型的微调，并创造更多可玩性，这个项目与其是类似的，不过是用于LLM(大语言模型)方向的

可以看到相比alpaca(斯坦福LLaMA)的全量微调，使用的参数减少到了1.2M(120万参数)，Lora大小也减少到了仅仅4.7M

github地址:https://github.com/ZrrSkywalker/LLaMA-Adapter

<img src="https://img.saraba1st.com/forum/202303/29/172512mqzbprwuwwykeqfk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-172458.jpg</strong> (221.61 KB, 下载次数: 0)

下载附件

2023-3-29 17:25 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  tk553521  
##### 22#       发表于 2023-3-29 17:31

马之，下班了研究一下

*****

####  Freewolf  
##### 23#       发表于 2023-3-29 17:44

mark，回头autodl租个gpu试试

*****

####  yesicant  
##### 24#         楼主| 发表于 2023-3-29 17:44

 本帖最后由 yesicant 于 2023-3-29 18:42 编辑 

前面搬了那么多LLM与多模态相关的项目，来个不一样的

CVPR 2023论文:panic3d 单张动漫图像生成3D头部风格模型！

github地址页:https://github.com/ShuhongChen/panic3d-anime-reconstruction

<img src="https://img.saraba1st.com/forum/202303/29/174147k88eedkhhekk8xar.png" referrerpolicy="no-referrer">

<strong>schematic.png</strong> (916.29 KB, 下载次数: 0)

下载附件

2023-3-29 17:41 上传

通过使用两个大型新数据集（11.2k Vroid 3D 模型，1k Vtuber 肖像插图）训练出来的根据单张图片(你甚至可以用webui直接生成头像)并转换成3D立体头部模型

提供有全套流程，docker，训练代码等齐全工具和说明，可以配合生成说话动作等(B站应该经常见)

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yesicant  
##### 25#         楼主| 发表于 2023-3-29 18:01

Japanese-Alpaca-LoRA/日语 Alpaca Lora插件
使用日文数据集训练的Alpaca Lora，可以进行日语输出

项目说明页https://note.com/kun1emon/n/n1533345d5d26
github地址:https://github.com/kunishou/Japanese-Alpaca-LoRA

<img src="https://img.saraba1st.com/forum/202303/29/175904insfr12bgb212fcc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-175850.jpg</strong> (282.25 KB, 下载次数: 0)

下载附件

2023-3-29 17:59 上传

这里额外说一下，其实Alpaca微调或者Alpaca Lora微调也是可以支持中文输入与输出的，我了解的不少项目都正在做这个工作，希望能早日完成吧<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  右代宫嘉音  
##### 26#       发表于 2023-3-29 18:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60264292&amp;ptid=2126390" target="_blank"> yesicant 发表于 2023-3-29 17:44</a>  本帖最后由 yesicant 于 2023-3-29 17:46 编辑   前面搬了那么多LLM与多模态相关的项目，来个不一样的  CVPR 2023论文:panic3d 单张动漫图像生成3D头部风格模型！  github地址页:https://github.com/ShuhongChen/panic3d-anime-reconstruction   通过使用个两大型新数据集（11.2k Vro </blockquote>
喔靠这个顶来自: iPhone客户端

*****

####  塔奇克马  
##### 27#       发表于 2023-3-29 18:32

<img src="https://static.saraba1st.com/image/smiley/carton2017/385.png" referrerpolicy="no-referrer">坐等支持中文的免安装版,小白包

*****

####  琉璃苑軒風  
##### 28#       发表于 2023-3-29 18:33

模型要求也越来越亲民了

*****

####  kaildo  
##### 29#       发表于 2023-3-29 18:37

百度狂喜

*****

####  wujae  
##### 30#       发表于 2023-3-29 18:38

多多益善<img src="https://static.saraba1st.com/image/smiley/face2017/026.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Orienser  
##### 31#       发表于 2023-3-29 18:41

好啊<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">非常好

*****

####  Benighted  
##### 32#       发表于 2023-3-29 18:42

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60263878&amp;ptid=2126390" target="_blank">thegodra 发表于 2023-3-29 17:15</a>
世界在我们看不到的地方在变革！

这要怎样才能跟上时代的潮流呢？</blockquote>
先买块4090<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">

—— 来自 Xiaomi 2211133C, Android 13上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4

*****

####  jojog  
##### 33#       发表于 2023-3-29 18:48

<img src="https://static.saraba1st.com/image/smiley/face2017/029.png" referrerpolicy="no-referrer">这是逼微软就范啊 不错

不过真把openai整开源了估计人类也就要寄了吧

*****

####  mahoraga  
##### 34#       发表于 2023-3-29 18:51

 本帖最后由 mahoraga 于 2023-3-29 19:16 编辑 

好啊，现在商用开源的项目也多起来了，赶紧加大力度给我干死closeai!(开玩笑的），我也看到几个，一个是databricks的dolly,还有中文的belle之类的。目前还是太依赖alpaca那套，开源上束手束脚，其实赶紧整一个基础模型跟llama五五开的，再花点小钱标一套数据出来就可以尽情开源了。我回去再把项目地址贴下
[https://github.com/databrickslabs/dolly](https://github.com/databrickslabs/dolly)
[https://github.com/LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)

*****

####  qadfg  
##### 35#       发表于 2023-3-29 18:52

摩多摩多<img src="https://static.saraba1st.com/image/smiley/face2017/033.png" referrerpolicy="no-referrer">

*****

####  阿酷怕苦  
##### 36#       发表于 2023-3-29 18:57

mark，回去了研究

*****

####  yesicant  
##### 37#         楼主| 发表于 2023-3-29 19:03

 本帖最后由 yesicant 于 2023-3-29 19:04 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60265015&amp;ptid=2126390" target="_blank">mahoraga 发表于 2023-3-29 18:51</a>
好啊，现在商用开源的项目也多起来了，赶紧加大力度给我干死closeai!(开玩笑的），我也看到几个，一个是dat ...</blockquote>
甚至chatGLM项目本身也有相关微调项目在进行中，综合来看其实模型最重要的还是训练集，但是这方面现在有了openai主动珠玉在前，越来越好起来了，openflamingo最重要的是开源了多模态的c4数据集<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

https://github.com/mymusise/ChatGLM-Tuning
https://github.com/ssbuild/chatglm_finetuning

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  INDIASH  
##### 38#       发表于 2023-3-29 19:16

 本帖最后由 INDIASH 于 2023-3-29 19:17 编辑 

希望有个能像linux一样和巨硬打擂台的开源AI，但是感觉这玩意最重要的还是数据<img src="https://static.saraba1st.com/image/smiley/face2017/001.png" referrerpolicy="no-referrer">

*****

####  mahoraga  
##### 39#       发表于 2023-3-29 19:22

 本帖最后由 mahoraga 于 2023-3-29 19:26 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60265160&amp;ptid=2126390" target="_blank">yesicant 发表于 2023-3-29 19:03</a>

甚至chatGLM项目本身也有相关微调项目在进行中，综合来看其实模型最重要的还是训练集，但是这方面现在有 ...</blockquote>
我现在还是先不指望开源社区整多模态，要紧先把ChatGPT的效果追平然后开源，就可以疯狂生成凰文了

现在的几个7B~20B左右的模型说实话表面上虽然还行，但是一旦任务变得复杂，出来的效果和ChatGPT就差很远，只要把这块补上就人人有功练<img src="https://static.saraba1st.com/image/smiley/face2017/033.png" referrerpolicy="no-referrer">

lit-llama那个我觉得有希望，目前很多生态都依赖在llama上，如果lit-llama真的能对齐原版llama，那就可以直接把alpaca-lora那套迁上去，后面只要换套真正开源的数据集，就可以彻底实现开源的ChatGPT替代，好时代，来临力<img src="https://static.saraba1st.com/image/smiley/face2017/245.png" referrerpolicy="no-referrer">

*****

####  Fuero  
##### 40#       发表于 2023-3-29 19:27

这些模型是都可以本地部署的吗？

*****

####  yesicant  
##### 41#         楼主| 发表于 2023-3-29 19:36

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60265409&amp;ptid=2126390" target="_blank">Fuero 发表于 2023-3-29 19:27</a>
这些模型是都可以本地部署的吗？</blockquote>
全都是<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)

﹍﹍﹍

评分

 参与人数 1战斗力 +1

|昵称|战斗力|理由|
|----|---|---|
| 巨魔型美羽| + 1|射了|

查看全部评分

*****

####  yesicant  
##### 42#         楼主| 发表于 2023-3-29 19:39

开源的中文语言模型骆驼 (Luotuo)，该项目基于 LLaMA、Stanford Alpaca、Alpaca LoRA、Japanese-Alpaca-LoRA 等完成，单卡就能完成训练部署。

这个模型是在 Meta 开源的 LLaMA 基础上，参考 Alpaca 和 Alpaca-LoRA 两个项目，对中文进行了训练。

项目地址:https://github.com/LC1332/Chinese-alpaca-lora

<img src="https://img.saraba1st.com/forum/202303/29/193902cz0sf8rnnpe3g0nm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-193830.jpg</strong> (66.66 KB, 下载次数: 0)

下载附件

2023-3-29 19:39 上传

<img src="https://img.saraba1st.com/forum/202303/29/193902v7p5kevp4itgflv9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-193843.jpg</strong> (162.98 KB, 下载次数: 0)

下载附件

2023-3-29 19:39 上传

<img src="https://img.saraba1st.com/forum/202303/29/193946m79j6qjx6v7wuvu9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-193934.jpg</strong> (100.08 KB, 下载次数: 0)

下载附件

2023-3-29 19:39 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  羽川易  
##### 43#       发表于 2023-3-29 19:39

 mark

*****

####  yesicant  
##### 44#         楼主| 发表于 2023-3-29 19:44

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60265358&amp;ptid=2126390" target="_blank">mahoraga 发表于 2023-3-29 19:22</a>
我现在还是先不指望开源社区整多模态，要紧先把ChatGPT的效果追平然后开源，就可以疯狂生成凰文了

现在的 ...</blockquote>
背后的本质还是因为本机能跑的参数有限导致微调拟合的性能也比较有限<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">
如果业界可以实现一个比较新颖的构架，比如稀疏激活的专家集群这样的，部署在本地时不需要同时激活太多参数，易用性，效果和可玩性也会得到大量提升

https://arxiv.org/abs/2303.14177

感觉业界可能几个月内就会有大量相关工作发布，openai确实惹了众怒

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  埃罗芒阿.  
##### 45#       发表于 2023-3-29 19:45

mark一下

*****

####  yesicant  
##### 46#         楼主| 发表于 2023-3-29 20:38

 本帖最后由 yesicant 于 2023-3-29 20:43 编辑 

ChatYuan-large-v2，元语chatAI的v2迭代版本，就是之前网上梗图那个微信小程序服务被封的那家的

看描述开启量化之后甚至可以在手机上跑推理，开发者专门写了个gradio的ui方便交互

github项目页:https://github.com/clue-ai/ChatYuan

模型下载与体验地址

https://huggingface.co/ClueAI/ChatYuan-large-v2/

https://modelscope.cn/studios/ClueAI/ChatYuan-large-v2

<img src="https://img.saraba1st.com/forum/202303/29/203546pfswncgzk9sgzsro.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-203457.jpg</strong> (454.54 KB, 下载次数: 0)

下载附件

2023-3-29 20:35 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  为你写诗  
##### 47#       发表于 2023-3-29 21:19

先插眼，等换显卡

*****

####  yukiecho  
##### 48#       发表于 2023-3-29 21:28

谢谢LZ的总结！周末好好研究一下

*****

####  勿徊哉  
##### 49#       发表于 2023-3-29 21:43

现在最大的问题是什么时候显卡能自定义显存大小。

24GB不够啊

*****

####  yesicant  
##### 50#         楼主| 发表于 2023-3-29 22:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60266847&amp;ptid=2126390" target="_blank">勿徊哉 发表于 2023-3-29 21:43</a>
现在最大的问题是什么时候显卡能自定义显存大小。

24GB不够啊</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">主要问题还是内存与显存的IO差距太大了，这个真没啥办法

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  塔奇克马  
##### 51#       发表于 2023-3-29 23:02

 本帖最后由 塔奇克马 于 2023-3-29 23:06 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60267331&amp;ptid=2126390" target="_blank">yesicant 发表于 2023-3-29 22:30</a>

主要问题还是内存与显存的IO差距太大了，这个真没啥办法

—— 来自 S1Fun ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/carton2017/248.png" referrerpolicy="no-referrer">8通道DDR5 OC 应该能达到4090带宽的1/5..应该可以将就吧？（PCIE7.0应该不是瓶颈

*****

####  yesicant  
##### 52#         楼主| 发表于 2023-3-29 23:06

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60267630&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-3-29 23:02</a>
8通道DDR5 OC 应该能达到4090带宽的1/5..应该可以将就吧？（PCIE7.0应该不是瓶颈 ...</blockquote>
确实可以，现在已经有不少用内存代替显存的LLM解决方案了，当然速度慢一些也是没办法的，量化又是另一条路，通过降精度来换运行，不过这些都不能本质上解决门槛比较高的问题，而且还有算力的差距，也只能妥协了<img src="https://static.saraba1st.com/image/smiley/animal2017/002.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yswm  
##### 53#       发表于 2023-3-29 23:11

有什么办法本地运行让它写代码吗？

*****

####  sunbeach  
##### 54#       发表于 2023-3-29 23:14

本地猫娘老婆有希望了吗?

*****

####  yesicant  
##### 55#         楼主| 发表于 2023-3-29 23:18

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60267710&amp;ptid=2126390" target="_blank">yswm 发表于 2023-3-29 23:11</a>
有什么办法本地运行让它写代码吗？</blockquote>
写代码国产有些专门的代码LLM大模型，比如CodeGeeX，不过需要申请才能用，而大多数的LLM，其实都有经历过代码预训练的，但这不代表一定写的好代码

https://huggingface.co/spaces/THUDM/CodeGeeX

如果你真的比较需要这方面的生产力，开个20刀的gpt4 plus或者copilot显然是更好的选择，毕竟就算本地真的跑起来了，差距也比较大

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yesicant  
##### 56#         楼主| 发表于 2023-3-29 23:20

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60267727&amp;ptid=2126390" target="_blank">sunbeach 发表于 2023-3-29 23:14</a>
本地猫娘老婆有希望了吗?</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">本地猫娘一直是有的，就是比较傻，没有chatgpt聪明，毕竟参数和数据集差异摆在这里

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yesicant  
##### 57#         楼主| 发表于 2023-3-29 23:36

 本帖最后由 yesicant 于 2023-3-30 01:09 编辑 

https://www.qbitai.com/2023/03/43246.html

量子位的文章，ColossalChat，又一个搭在LLaMA上的AI，有在线demo，不过

<img src="https://img.saraba1st.com/forum/202303/29/233600cheunn2ez21hzk32.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230329-233529.jpg</strong> (182.68 KB, 下载次数: 0)

下载附件

2023-3-29 23:36 上传

甚至中文能力极强，还有完整的RLHF微调过程，太强了

github地址:https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat

项目说明页:https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  巨魔型美羽  
##### 58#       发表于 2023-3-29 23:36

老黄能不能也捐点机器啊，给"OPEN"AI捐dgx1时可标榜推进AI民主化呢<img src="https://static.saraba1st.com/image/smiley/face2017/047.png" referrerpolicy="no-referrer">

*****

####  ambivalence  
##### 59#       发表于 2023-3-30 00:24

人人有丹炼 好时代来临力

*****

####  蓝泽玲  
##### 60#       发表于 2023-3-30 05:16

人类寄就寄吧。
希望智械能有个多神教信仰，各位在这个领域做过贡献的同行在智械的神话里能混个八百万众神神位。


*****

####  圈量子  
##### 61#       发表于 2023-3-30 07:35

好事，支持大家卷起来<img src="https://static.saraba1st.com/image/smiley/face2017/053.png" referrerpolicy="no-referrer">


*****

####  wszweill  
##### 62#       发表于 2023-3-30 09:22

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60267710&amp;ptid=2126390" target="_blank">yswm 发表于 2023-3-29 10:11</a>

有什么办法本地运行让它写代码吗？</blockquote>
chatgpt跑代码都不一定不出bug，特别是写完bug之后还挺信誓旦旦的狡辩<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer"> 

省心还是copliot方便吧，直接整合进vscode，一样概率性有小问题<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">


*****

####  yesicant  
##### 63#         楼主| 发表于 2023-3-31 05:51

 本帖最后由 yesicant 于 2023-3-31 05:53 编辑 

Vicuna-13B，微调 LLaMA13B版本，使用ShareGPT 收集的数据集进行训练
项目主页:https://vicuna.lmsys.org/

训练集70k，来源是ShareGPT.com，效果非常不错

注:谷歌的Bard最近也被怀疑使用chatgpt的数据进行训练

<img src="https://img.saraba1st.com/forum/202303/31/055134uvqw73uuqi82kywg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230331-055119.jpg</strong> (884 KB, 下载次数: 0)

下载附件

2023-3-31 05:51 上传

openai收集互联网训练集，其他公司又使用openai的训练集，再指令微调模型继续再喂给下一个模型，什么人体蜈蚣<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yesicant  
##### 64#         楼主| 发表于 2023-3-31 06:01

chatGLM版的visual-chatgpt，实现原理也比较类似，readme有演示GIF

https://github.com/visual-openllm/visual-openllm

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  191634  
##### 65#       发表于 2023-3-31 10:43

<blockquote>wszweill 发表于 2023-3-30 09:22
chatgpt跑代码都不一定不出bug，特别是写完bug之后还挺信誓旦旦的狡辩 

省心还是copliot方便吧， ...</blockquote>
那可不是不一定，是除了入门算法题之外的代码大概率出bug。


*****

####  ziyuan008  
##### 66#       发表于 2023-3-31 11:16

等个一键安装包


*****

####  yesicant  
##### 67#         楼主| 发表于 2023-3-31 16:08

https://github.com/manyoso/haltt4llm

一个近期的LLM对比基准测试

<img src="https://img.saraba1st.com/forum/202303/31/160326wnls96tmtnk64654.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230331-153514__01.jpg</strong> (78.82 KB, 下载次数: 0)

下载附件

2023-3-31 16:03 上传

在使用800k的gpt3.5turbo合成数据训练后的gpt4all(LLaMA7b微调项目)在数项测试中超越gpt3.5turbo

没想到RLHF对性能的影响已经严重到了这种程度，以至于在直接拥有微调样本的情况下可以泛化出更强的性能超越原模型

原项目中已经放出量化好的模型
https://github.com/nomic-ai/gpt4all

值得一提的是这里

<img src="https://img.saraba1st.com/forum/202303/31/160822ckz4afgppseop3tp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230331-152214__01__01.jpg</strong> (52.33 KB, 下载次数: 0)

下载附件

2023-3-31 16:08 上传

把所有拒绝回答的微调样本从训练集里删了，AI就不会拒绝回答了<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  yesicant  
##### 68#         楼主| 发表于 2023-3-31 18:00

http://mp.weixin.qq.com/s?__biz=MzkxNjMzMjM3NA==&amp;mid=2247483735&amp;idx=1&amp;sn=af09f2d5414d6771b34d0934ba4facee

ChatGLM出官方微调教程了！<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">不过数据集还是要自己整

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  snowpumpkin  
##### 69#       发表于 2023-3-31 18:13

插眼<img src="https://static.saraba1st.com/image/smiley/face2017/177.png" referrerpolicy="no-referrer">这太强了


*****

####  龙骑士尹志平  
##### 70#       发表于 2023-3-31 18:22

坐等


*****

####  革萌  
##### 71#       发表于 2023-3-31 19:01

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60263878&amp;ptid=2126390" target="_blank">thegodra 发表于 2023-3-29 17:15</a>

世界在我们看不到的地方在变革！

这要怎样才能跟上时代的潮流呢？</blockquote>
git clone


*****

####  sahan  
##### 72#       发表于 2023-4-1 02:03

插眼，显卡大炼丹时代

*****

####  xinleoii  
##### 73#       发表于 2023-4-1 02:15

mark

*****

####  ambivalence  
##### 74#       发表于 2023-4-1 02:16

插眼等一个赛博女友

*****

####  btnooni  
##### 75#       发表于 2023-4-1 02:18

<img src="https://static.saraba1st.com/image/smiley/face2017/018.png" referrerpolicy="no-referrer">gpt4all基本啥都不用干就能部署起来，也就吃4G物理内存，效果还比量化llama 7B好得多，真心轻松愉快


*****

####  yesicant  
##### 76#         楼主| 发表于 2023-4-1 19:19

OpenChatKit出品的GPT-NeoXT-Chat-Base-20B更新到v0.16了

其实这个模型前不久才出的，数据集用了40M也就是4000万指令微调深度优化了，但当时实测起来效果并不是很好，做NLP任务还可以，对话就有点不行了，但是今天更新之后，他们专门根据对话又进行了微调，对话强了很多，虽然外语还比较差，但也有了一定程度的能力，当然最主要的还是英语能力

数据集是和Laion合作的，另外20b的模型可能看着会比较大，不过自带量化int8和cpu推理模式，也不必太过担心

<img src="https://img.saraba1st.com/forum/202304/01/191759a99z353zzure3fw2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230401-183426__01.jpg</strong> (28.88 KB, 下载次数: 0)

下载附件

2023-4-1 19:17 上传

<img src="https://img.saraba1st.com/forum/202304/01/191759oy664n0fpqv0zpy4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230401-183444__01.jpg</strong> (28.23 KB, 下载次数: 0)

下载附件

2023-4-1 19:17 上传

演示demo:https://huggingface.co/spaces/togethercomputer/OpenChatKit

模型仓库地址:https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  yesicant  
##### 77#         楼主| 发表于 2023-4-2 05:58

GPTrillion！
世界上第一个开源的万亿多模态大模型！
仓库地址:https://huggingface.co/banana-dev/GPTrillion

<img src="https://img.saraba1st.com/forum/202304/02/055634tfw4j4kzz7w94zgz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230402-055620.jpg</strong> (253.56 KB, 下载次数: 0)

下载附件

2023-4-2 05:56 上传

GPTrillion 在包含各种文本、图像和音频数据的海量数据集上进行训练。使用 BPE 算法对数据集进行预处理和标记化，并分别处理每个模态。训练过程涉及监督和无监督学习技术的组合，以自我监督的方式训练模型。

<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  yesicant  
##### 78#         楼主| 发表于 2023-4-2 05:58

这谁能跑起来…

<img src="https://img.saraba1st.com/forum/202304/02/055840iayzkampypgkgrn8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230402-055830.jpg</strong> (487.39 KB, 下载次数: 0)

下载附件

2023-4-2 05:58 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  oldttt  
##### 79#       发表于 2023-4-2 06:02

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60302012&amp;ptid=2126390" target="_blank">yesicant 发表于 2023-4-2 05:58</a>
这谁能跑起来…</blockquote>
过于夸张了


*****

####  yesicant  
##### 80#         楼主| 发表于 2023-4-2 06:04

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60302014&amp;ptid=2126390" target="_blank">oldttt 发表于 2023-4-2 06:02</a>
过于夸张了</blockquote>

<img src="https://img.saraba1st.com/forum/202304/02/060406o5pl6erq55g9i65s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230402-060357.jpg</strong> (50.11 KB, 下载次数: 0)

下载附件

2023-4-2 06:04 上传

<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">就当无事发生过

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  jcwatm1  
##### 81#       发表于 2023-4-2 22:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60286739&amp;ptid=2126390" target="_blank">yesicant 发表于 2023-3-31 16:08</a>
https://github.com/manyoso/haltt4llm

一个近期的LLM对比基准测试</blockquote>
gpt4all支持中文吗？


*****

####  yesicant  
##### 82#         楼主| 发表于 2023-4-2 23:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60310780&amp;ptid=2126390" target="_blank">jcwatm1 发表于 2023-4-2 22:28</a>
gpt4all支持中文吗？</blockquote>
不支持吧<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">得训练集有中文训练出来的才会懂中文

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Orienser  
##### 83#       发表于 2023-4-2 23:58

一直在等个日翻中效果比较好的模型，这样就能自己翻点黄油了<img src="https://static.saraba1st.com/image/smiley/face2017/002.png" referrerpolicy="no-referrer">


*****

####  酱徐子  
##### 84#       发表于 2023-4-3 00:13

期待中文版小黄文生成器


*****

####  squallx  
##### 85#       发表于 2023-4-3 04:25

插眼

—— 来自 OnePlus GM1910, Android 12上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4

*****

####  大江户战士  
##### 86#       发表于 2023-4-3 04:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60311797&amp;ptid=2126390" target="_blank">Orienser 发表于 2023-4-2 23:58</a>

一直在等个日翻中效果比较好的模型，这样就能自己翻点黄油了。之前在huggingface上试了几个效果都不 ...</blockquote>
试试RWKV


*****

####  yesicant  
##### 87#         楼主| 发表于 2023-4-3 17:50

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60265015&amp;ptid=2126390" target="_blank">mahoraga 发表于 2023-3-29 18:51</a>
好啊，现在商用开源的项目也多起来了，赶紧加大力度给我干死closeai!(开玩笑的），我也看到几个，一个是dat ...</blockquote>
最近中文LLaMA微调项目也多起来了，配合llama.cpp实装的mmap高速推理，非常劲

<img src="https://img.saraba1st.com/forum/202304/03/175019iru79zp70ruw03rh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230403-174847.jpg</strong> (331.04 KB, 下载次数: 0)

下载附件

2023-4-3 17:50 上传

https://github.com/ymcui/Chinese-LLaMA-Alpaca

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  mahoraga  
##### 88#       发表于 2023-4-3 22:59

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60321218&amp;ptid=2126390" target="_blank">yesicant 发表于 2023-4-3 17:50</a>

最近中文LLaMA微调项目也多起来了，配合llama.cpp实装的mmap高速推理，非常劲</blockquote>
嗯，中文的光llama的微调我感觉就有两三个了，还有BELLE是基于BLOOM的，好像也还可以。

llama.cpp我看它之前说不能支持gpu就没仔细看。好是挺好，我就是在想它作者为什么不直接搞个ggml的库，他反正已经又whisper.cpp和llam.cpp这两个大热项目了，直接维护一个ggml的基础库然后让其他开源老哥做具体实现，说不定以后就成了C++推理部署的通用标准了，这不是瞬间起飞<img src="https://static.saraba1st.com/image/smiley/face2017/265.gif" referrerpolicy="no-referrer">


*****

####  yesicant  
##### 89#         楼主| 发表于 2023-4-4 05:17

 本帖最后由 yesicant 于 2023-4-4 05:19 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60281404&amp;ptid=2126390" target="_blank">yesicant 发表于 2023-3-31 05:51</a>
Vicuna-13B，微调 LLaMA13B版本，使用ShareGPT 收集的数据集进行训练
项目主页:https://vicuna.lmsys.org/
 ...</blockquote>
更新:Vicuna开源了模型权重，这次不止训练集了

项目地址:https://github.com/lm-sys/FastChat/#vicuna-weights

除了控制台界面还有对应的gradio ui

<img src="https://img.saraba1st.com/forum/202304/04/051647odjqgqugupz04g10.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230404-051638.jpg</strong> (165 KB, 下载次数: 0)

下载附件

2023-4-4 05:16 上传

从项目主页的说明来看，未来还会放出7b版本

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  yesicant  
##### 90#         楼主| 发表于 2023-4-4 15:09

新项目Baize(白泽)

demo地址:https://huggingface.co/spaces/project-baize/baize-lora-7B

github地址:https://github.com/project-baize/baize

<img src="https://img.saraba1st.com/forum/202304/04/150751ohhfy8f0mxi5yw5c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230404-150153__01.jpg</strong> (52.71 KB, 下载次数: 0)

下载附件

2023-4-4 15:07 上传

遗憾的是目前还不怎么会中文<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202304/04/150808bg5vbfgg6999vnng.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230404-150630__01.jpg</strong> (46.28 KB, 下载次数: 0)

下载附件

2023-4-4 15:08 上传

好消息是以后可能会

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  zgodel  
##### 91#       发表于 2023-4-4 18:38

Mark


*****

####  oldttt  
##### 92#       发表于 2023-4-5 18:30

楼主被塞了，这收藏了也看不到内容了


*****

####  傲游  
##### 93#       发表于 2023-4-5 18:52

中午互联网就是这样
一夜不见 早已沧海桑田


*****

####  囧Smith  
##### 94#       发表于 2023-4-5 19:27

楼主被封了，新号要三天才能发言，三天后恢复更新

*****

####  red2077  
##### 95#       发表于 2023-4-5 19:29

<img src="https://img.saraba1st.com/forum/202304/05/192917t3ojoat03j5opupj.png" referrerpolicy="no-referrer">

<strong>S30405-19275449.png</strong> (88.41 KB, 下载次数: 0)

下载附件

2023-4-5 19:29 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  stanzgy  
##### 96#       发表于 2023-4-5 19:35

lz是干嘛了<img src="https://static.saraba1st.com/image/smiley/face2017/213.gif" referrerpolicy="no-referrer">

[论坛助手,iPhone](https://bbs.saraba1st.com/2b/forum.php?mod=viewthread&amp;tid=2029836)


*****

####  巨魔型美羽  
##### 97#       发表于 2023-4-5 19:51

往好处想至少管理员没法让人全网消失不是<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">


*****

####  Lucario  
##### 98#       发表于 2023-4-5 20:12

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60344350&amp;ptid=2126390" target="_blank">stanzgy 发表于 2023-4-5 19:35</a>

lz是干嘛了

论坛助手,iPhone</blockquote>
我怀疑是早上讨论ykw进去的贴说了啥

*****

####  勿徊哉  
##### 99#       发表于 2023-4-5 20:14

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60344744&amp;ptid=2126390" target="_blank">Lucario 发表于 2023-4-5 20:12</a>
我怀疑是早上讨论ykw进去的贴说了啥</blockquote>
难道不是因为ykw的帖子就是楼主发的？
并且楼主在回复中扬言：为了发这个贴我进去也值了！

*****

####  Lucario  
##### 100#       发表于 2023-4-5 20:15

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60344767&amp;ptid=2126390" target="_blank">勿徊哉 发表于 2023-4-5 20:14</a>

难道不是因为ykw的帖子就是楼主发的？

并且楼主在回复中扬言：为了发这个贴我进去也值了！ ...</blockquote>
草，lz这是明知故犯啊，就不能开小号键政么，赶着给狗叔送人头


*****

####  伊莉伊莉雅  
##### 101#       发表于 2023-4-6 02:50

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60344767&amp;ptid=2126390" target="_blank">勿徊哉 发表于 2023-4-5 20:14</a>
难道不是因为ykw的帖子就是楼主发的？
并且楼主在回复中扬言：为了发这个贴我进去也值了！ ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">太草了

*****

####  矢吹奈子  
##### 102#       发表于 2023-4-6 03:59

 本帖最后由 矢吹奈子 于 2023-4-6 04:17 编辑 

整合一下lz的亡语

<img src="https://img.saraba1st.com/forum/202304/06/041716qsvsdgssgs2cst9f.jpg" referrerpolicy="no-referrer">

<strong>6d050af1ly1hcokekb88tj20n40yldiz.jpg</strong> (74.44 KB, 下载次数: 0)

下载附件

2023-4-6 04:17 上传

----------------------------

Laion组织(提供SD模型训练集的开源非盈利组织)根据论文用Clip与LLaMA(没想到吧，又是我们)复现了去年刷榜各种项目的的多模态模型，Flamingo相关的权重与训练代码

关于Flamingo的相关介绍可以看这里:[https://zhuanlan.zhihu.com/p/508918171](https://zhuanlan.zhihu.com/p/508918171)

hugface模型仓库地址:[https://huggingface.co/openflamingo/OpenFlamingo-9B](https://huggingface.co/openflamingo/OpenFlamingo-9B)

训练代码:[https://github.com/mlfoundations/open_flamingo](https://github.com/mlfoundations/open_flamingo)

模型权重净重5.2G，参数为9B，大部分人的显卡应该都能跑

效果比Flamingo论文中的同参数下性能差一些，可能缺少某些调优吧，不过有这个效果要什么自行车！

在线演示地址:[https://7164d2142d11.ngrok.app/](https://7164d2142d11.ngrok.app/)

博客地址:[https://laion.ai/blog/open-flamingo/](https://laion.ai/blog/open-flamingo/)

----------------------------

Nomic AI发布的gpt4all项目

github地址:[https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)

可以跑在笔记本上，使用了800k的gpt3.5 turbo接口生成的合成文本数据集，在自指导等论文形式的帮助下训练出的LLaMA-Lora插件，项目页有Lora，数据集，训练代码等，不过因为LLaMA的学术研究性质，所以并不直接提供对应的7B LLaMA权重，这个得自己准备，性能非常不错

----------------------------

Cerebras systems出品的Cerebras-GPT系列

项目主页说明地址:[https://www.cerebras.net/blog/ce ... ge-language-models/](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)

项目意义说明:[https://www.businesswire.com/new ... Wafer-Scale-Systems](https://www.businesswire.com/news/home/20230328005366/en/Cerebras-Systems-Releases-Seven-New-GPT-Models-Trained-on-CS-2-Wafer-Scale-Systems)

arXiv论文还是coming soon

不过hugface已经开源权重了，使用了Chinchilla缩放法则高效训练模型，授权许可为Apache 2.0

一共公开了111m参数-13b参数总共7个版本不同的模型权重

hhugface页面[https://huggingface.co/cerebras](https://huggingface.co/cerebras)

----------------------------

    格林达姆 发表于 2023-3-29 17:02

    用消费级硬件一键生成小黄文的日子是不是不远了

4chan老哥专门有个皮格马利翁(瞧这名字)项目就是搞这个的，不过没有中文版本，最新版本都堆到6b参数了，目前来说也是开源项目，在hugface上那不是一般的火
[https://rentry.org/pygmalion-ai](https://rentry.org/pygmalion-ai)

----------------------------

比较重复造轮子的lit-llama项目了

因为LLaMA代码库的授权许可是GPL，这个项目通过基于nanoGPT的实现构造了一个新的Apache 2.0许可的lit-llama项目，经过这个项目转换后就可以用于商业化等措施了

项目地址:[https://github.com/Lightning-AI/lit-llama](https://github.com/Lightning-AI/lit-llama)

----------------------------

LLaMA-Adapter项目

一看到Adapter，大部分人应该就懂了，腾讯人工智能实验室在不久前搞过一个名为T2I-Adapter的用于SD微调的附加模型，原理也来自于Adapter，通过微调附加增量参数的方法高效训练大模型的微调，并创造更多可玩性，这个项目与其是类似的，不过是用于LLM(大语言模型)方向的

可以看到相比alpaca(斯坦福LLaMA)的全量微调，使用的参数减少到了1.2M(120万参数)，Lora大小也减少到了仅仅4.7M

github地址:[https://github.com/ZrrSkywalker/LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)

----------------------------

前面搬了那么多LLM与多模态相关的项目，来个不一样的

CVPR 2023论文:panic3d 单张动漫图像生成3D头部风格模型！

github地址页:[https://github.com/ShuhongChen/panic3d-anime-reconstruction](https://github.com/ShuhongChen/panic3d-anime-reconstruction)

通过使用两个大型新数据集（11.2k Vroid 3D 模型，1k Vtuber 肖像插图）训练出来的根据单张图片(你甚至可以用webui直接生成头像)并转换成3D立体头部模型

提供有全套流程，docker，训练代码等齐全工具和说明，可以配合生成说话动作等(B站应该经常见)

----------------------------

Japanese-Alpaca-LoRA/日语 Alpaca Lora插件

使用日文数据集训练的Alpaca Lora，可以进行日语输出

项目说明页[https://note.com/kun1emon/n/n1533345d5d26](https://note.com/kun1emon/n/n1533345d5d26)

github地址:[https://github.com/kunishou/Japanese-Alpaca-LoRA](https://github.com/kunishou/Japanese-Alpaca-LoRA)

----------------------------

    mahoraga 发表于 2023-3-29 18:51

    好啊，现在商用开源的项目也多起来了，赶紧加大力度给我干死closeai!(开玩笑的），我也看到几个，一个是dat ...

甚至chatGLM项目本身也有相关微调项目在进行中，综合来看其实模型最重要的还是训练集，但是这方面现在有了openai主动珠玉在前，越来越好起来了，openflamingo最重要的是开源了多模态的c4数据集

[https://github.com/mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)
[https://github.com/ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning)

----------------------------

开源的中文语言模型骆驼 (Luotuo)，该项目基于 LLaMA、Stanford Alpaca、Alpaca LoRA、Japanese-Alpaca-LoRA 等完成，单卡就能完成训练部署。

这个模型是在 Meta 开源的 LLaMA 基础上，参考 Alpaca 和 Alpaca-LoRA 两个项目，对中文进行了训练。

项目地址:[https://github.com/LC1332/Chinese-alpaca-lora](https://github.com/LC1332/Chinese-alpaca-lora)

----------------------------

ChatYuan-large-v2，元语chatAI的v2迭代版本，就是之前网上梗图那个微信小程序服务被封的那家的

看描述开启量化之后甚至可以在手机上跑推理，开发者专门写了个gradio的ui方便交互

github项目页:[https://github.com/clue-ai/ChatYuan](https://github.com/clue-ai/ChatYuan)

模型下载与体验地址

[https://huggingface.co/ClueAI/ChatYuan-large-v2/](https://huggingface.co/ClueAI/ChatYuan-large-v2/)

[https://modelscope.cn/studios/ClueAI/ChatYuan-large-v2](https://modelscope.cn/studios/ClueAI/ChatYuan-large-v2)

----------------------------

    yswm 发表于 2023-3-29 23:11

    有什么办法本地运行让它写代码吗？

写代码国产有些专门的代码LLM大模型，比如CodeGeeX，不过需要申请才能用，而大多数的LLM，其实都有经历过代码预训练的，但这不代表一定写的好代码

[https://huggingface.co/spaces/THUDM/CodeGeeX](https://huggingface.co/spaces/THUDM/CodeGeeX)

如果你真的比较需要这方面的生产力，开个20刀的gpt4 plus或者copilot显然是更好的选择，毕竟就算本地真的跑起来了，差距也比较大

----------------------------
[https://www.qbitai.com/2023/03/43246.html](https://www.qbitai.com/2023/03/43246.html)

量子位的文章，ColossalChat，又一个搭在LLaMA上的AI，有在线demo，不过

甚至中文能力极强，还有完整的RLHF微调过程，太强了

github地址:[https://github.com/hpcaitech/Col ... n/applications/Chat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)

项目说明页:[https://medium.com/@yangyou_berk ... peline-5edf08fb538b](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)

----------------------------

Vicuna-13B，微调 LLaMA13B版本，使用ShareGPT 收集的数据集进行训练

项目主页:[https://vicuna.lmsys.org/](https://vicuna.lmsys.org/)

训练集70k，来源是ShareGPT.com，效果非常不错

注:谷歌的Bard最近也被怀疑使用chatgpt的数据进行训练

更新:Vicuna开源了模型权重，这次不止训练集了

项目地址:[https://github.com/lm-sys/FastChat/#vicuna-weights](https://github.com/lm-sys/FastChat/#vicuna-weights)

除了控制台界面还有对应的gradio ui

从项目主页的说明来看，未来还会放出7b版本

----------------------------

chatGLM版的visual-chatgpt，实现原理也比较类似，readme有演示GIF

[https://github.com/visual-openllm/visual-openllm](https://github.com/visual-openllm/visual-openllm)

----------------------------
[https://github.com/manyoso/haltt4llm](https://github.com/manyoso/haltt4llm)

一个近期的LLM对比基准测试

在使用800k的gpt3.5turbo合成数据训练后的gpt4all(LLaMA7b微调项目)在数项测试中超越gpt3.5turbo

没想到RLHF对性能的影响已经严重到了这种程度，以至于在直接拥有微调样本的情况下可以泛化出更强的性能超越原模型

原项目中已经放出量化好的模型
[https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)

值得一提的是这里

把所有拒绝回答的微调样本从训练集里删了，AI就不会拒绝回答了

----------------------------
[http://mp.weixin.qq.com/s?__biz= ... 771b34d0934ba4facee](http://mp.weixin.qq.com/s?__biz=MzkxNjMzMjM3NA==&amp;mid=2247483735&amp;idx=1&amp;sn=af09f2d5414d6771b34d0934ba4facee)

ChatGLM出官方微调教程了！不过数据集还是要自己整

----------------------------

OpenChatKit出品的GPT-NeoXT-Chat-Base-20B更新到v0.16了

其实这个模型前不久才出的，数据集用了40M也就是4000万指令微调深度优化了，但当时实测起来效果并不是很好，做NLP任务还可以，对话就有点不行了，但是今天更新之后，他们专门根据对话又进行了微调，对话强了很多，虽然外语还比较差，但也有了一定程度的能力，当然最主要的还是英语能力

数据集是和Laion合作的，另外20b的模型可能看着会比较大，不过自带量化int8和cpu推理模式，也不必太过担心

演示demo:[https://huggingface.co/spaces/togethercomputer/OpenChatKit](https://huggingface.co/spaces/togethercomputer/OpenChatKit)

模型仓库地址:[https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B)

----------------------------

GPTrillion！

世界上第一个开源的万亿多模态大模型！

仓库地址:[https://huggingface.co/banana-dev/GPTrillion](https://huggingface.co/banana-dev/GPTrillion)

GPTrillion 在包含各种文本、图像和音频数据的海量数据集上进行训练。使用 BPE 算法对数据集进行预处理和标记化，并分别处理每个模态。训练过程涉及监督和无监督学习技术的组合，以自我监督的方式训练模型。

----------------------------

最近中文LLaMA微调项目也多起来了，配合llama.cpp实装的mmap高速推理，非常劲

[https://github.com/ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

----------------------------

新项目Baize(白泽)

demo地址:[https://huggingface.co/spaces/project-baize/baize-lora-7B](https://huggingface.co/spaces/project-baize/baize-lora-7B)

github地址:[https://github.com/project-baize/baize](https://github.com/project-baize/baize)

遗憾的是目前还不怎么会中文

----------------------------

﹍﹍﹍

评分

 参与人数 2战斗力 +2

|昵称|战斗力|理由|
|----|---|---|
| ambivalence| + 1|哈哈哈烧纸太草|
| qqks| + 1||

查看全部评分

*****

####  squallx  
##### 103#       发表于 2023-4-6 05:03

马克马克

—— 来自 OnePlus GM1910, Android 12上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4


*****

####  二月雨  
##### 104#       发表于 2023-4-6 07:04

插眼，好多知识盲区，中年人想玩这个得学多久？


*****

####  wave14  
##### 105#       发表于 2023-4-6 08:02

是不是自己可以炼一个lora，喂进去一些私有的内容，就可以合成一个自己独有的离线gpt了？


*****

####  INOSU  
##### 106#       发表于 2023-4-6 08:38

插个眼


*****

####  燕山雪  
##### 107#       发表于 2023-4-6 08:59

gpt4all远没有说的那么好，那个https://github.com/manyoso/haltt4llm测试的是模型会不会瞎jb编，跟对话能力毫无关系。novelai应该是个初创公司，主营是atlas，做gpt4all也只是拿别人现成脚本跑一遍，刷注意力而已。


*****

####  燕山雪  
##### 108#       发表于 2023-4-6 09:01

眼下论效果，能在本机跑的英文对话还是无脑vicuna 13b，已经有人转好llama.cpp的4bit版本，可以直接下载了


*****

####  少女终末旅行  
##### 109#       发表于 2023-4-6 10:29

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60344286&amp;ptid=2126390" target="_blank">red2077 发表于 2023-4-5 19:29</a>
—— 来自 S1Fun</blockquote>
群号多少，还能进吗<img src="https://static.saraba1st.com/image/smiley/face2017/075.png" referrerpolicy="no-referrer">


*****

####  stanzgy  
##### 110#       发表于 2023-4-6 10:43

 本帖最后由 stanzgy 于 2023-4-6 10:48 编辑 

中文模型现在试用了几个，感觉baize还不错。国外的确实vicuna最好用，就是在公司没有gpu的服务器上跑，13b用cpu only跑实在太慢了，准备开个colab，自己玩玩应该也用不了多少钱


*****

####  梦飘零  
##### 111#       发表于 2023-4-6 16:13

马克下


*****

####  red2077  
##### 112#       发表于 2023-4-6 16:41

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60349837&amp;ptid=2126390" target="_blank">少女终末旅行 发表于 2023-4-6 10:29</a>
群号多少，还能进吗</blockquote>
青龙圣者的ai画画群

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  zhmouohz  
##### 113#       发表于 2023-4-6 16:49

用intel的显卡跑本地模型会有坑么？intel的移动端显卡比同价位n卡显存大，想买个跑AI用。


*****

####  塔奇克马  
##### 114#       发表于 2023-4-6 16:52

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60348805&amp;ptid=2126390" target="_blank">燕山雪 发表于 2023-4-6 09:01</a>

眼下论效果，能在本机跑的英文对话还是无脑vicuna 13b，已经有人转好llama.cpp的4bit版本，可以直接下载了 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/215.gif" referrerpolicy="no-referrer">有无脑一键包吗？

*****

####  ziyuan008  
##### 115#       发表于 2023-4-6 17:01

lz是干嘛了


*****

####  燕山雪  
##### 116#       发表于 2023-4-6 19:37

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60354311&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-6 16:52</a>

有无脑一键包吗？</blockquote>

<img alt="" border="0" class="vm" src="https://static.saraba1st.com/image/filetype/binary.gif" referrerpolicy="no-referrer">

main.exe
(579.8 KB, 下载次数: 0)

2023-4-6 19:35 上传

点击文件名下载附件

<img alt="" border="0" class="vm" src="https://static.saraba1st.com/image/filetype/text.gif" referrerpolicy="no-referrer">

chat-with-vicuna.txt
(382 Bytes, 下载次数: 0)

2023-4-6 19:35 上传

点击文件名下载附件

其实很简单的，下载官方llama.cpp然后直接cmake就行。实在不会的话下载附件。然后下载[https://huggingface.co/eachadea/ggml-vicuna-13b-4bit](https://huggingface.co/eachadea/ggml-vicuna-13b-4bit)的权重（我用的是rev1那个），把这三个文件放到同一目录然后运行main.exe -m ggml-vicuna-13b-4bit.bin -n 256 --repeat_penalty 1.1 --color -i -r "### Human:" -f chat-with-vicuna.txt就可以了

*****

####  燕山雪  
##### 117#       发表于 2023-4-6 19:39

注意内存至少16G，老爷机就不用试了


*****

####  处男鉴黄师  
##### 118#       发表于 2023-4-6 22:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60356058&amp;ptid=2126390" target="_blank">燕山雪 发表于 2023-4-6 19:37</a>

其实很简单的，下载官方llama.cpp然后直接cmake就行。实在不会的话下载附件。然后下载https://huggin ...</blockquote>
试了下，跑几句就会顿一下是因为算力问题吗？


*****

####  godzillaqqq  
##### 119#       发表于 2023-4-6 23:01

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60358086&amp;ptid=2126390" target="_blank">处男鉴黄师 发表于 2023-04-06 22:51:51</a>
试了下，跑几句就会顿一下是因为算力问题吗？</blockquote>大矿 m40 24g 对跑这些ai模型拉不拉，听说跑图不行

[  -- 来自 有消息提醒的 Stage1官方 Android客户端](https://www.coolapk.com/apk/140634)

*****

####  herryk159  
##### 120#       发表于 2023-4-6 23:08

马克


*****

####  处男鉴黄师  
##### 121#       发表于 2023-4-6 23:14

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60358186&amp;ptid=2126390" target="_blank">godzillaqqq 发表于 2023-4-6 23:01</a>

大矿 m40 24g 对跑这些ai模型拉不拉，听说跑图不行

  -- 来自 有消息提醒的 Stage1官方 Android客户端 ...</blockquote>
这个是CPU跑的<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">


*****

####  燕山雪  
##### 122#       发表于 2023-4-6 23:45

<blockquote>处男鉴黄师 发表于 2023-4-6 22:51
试了下，跑几句就会顿一下是因为算力问题吗？</blockquote>
是……


*****

####  godzillaqqq  
##### 123#       发表于 2023-4-7 02:18

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60356073&amp;ptid=2126390" target="_blank">燕山雪 发表于 2023-04-06 19:39:01</a>
注意内存至少16G，老爷机就不用试了</blockquote>对cpu要求的下限是几代能跑，现在内存不要钱了，32g是标配了

[  -- 来自 有消息提醒的 Stage1官方 Android客户端](https://www.coolapk.com/apk/140634)


*****

####  伊莉伊莉雅  
##### 124#       发表于 2023-4-7 20:17

<img src="https://static.saraba1st.com/image/smiley/carton2017/003.png" referrerpolicy="no-referrer">新建了个群

809851632

点击链接加入群聊【Stage1st 赛博猫娘研究院】：[https://jq.qq.com/?_wv=1027&amp;k=nUxucp4m](https://jq.qq.com/?_wv=1027&amp;k=nUxucp4m)


*****

####  jcwatm1  
##### 125#       发表于 2023-4-8 10:45

请教一下，本地部署用来辅助写材料，用哪个模型好啊


*****

####  perfaceNext  
##### 126#       发表于 2023-4-8 11:45

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60356073&amp;ptid=2126390" target="_blank">燕山雪 发表于 2023-4-6 19:39</a>
注意内存至少16G，老爷机就不用试了</blockquote>
就没有适合老爷机的模型吗？

*****

####  燕山雪  
##### 127#       发表于 2023-4-8 11:50

<blockquote>godzillaqqq 发表于 2023-4-7 02:18
对cpu要求的下限是几代能跑，现在内存不要钱了，32g是标配了

  -- 来自 有消息提醒的 Stage1官方 Android ...</blockquote>
支持avx2

*****

####  燕山雪  
##### 128#       发表于 2023-4-8 11:51

<blockquote>perfaceNext 发表于 2023-4-8 11:45
就没有适合老爷机的模型吗？</blockquote>
见上，否则建议用在线版


*****

####  jimmy_nyc  
##### 129#       发表于 2023-4-8 12:04

Tavern AI + GPT-3.5-turbo，prompt设置好后跑文字冒险，不能更爽<img src="https://static.saraba1st.com/image/smiley/face2017/062.gif" referrerpolicy="no-referrer">


*****

####  jcwatm1  
##### 130#       发表于 2023-4-8 14:57

[https://github.com/imClumsyPanda/langchain-ChatGLM](https://github.com/imClumsyPanda/langchain-ChatGLM)

这个项目大佬们了解么，根据本地资料生成答案？


*****

####  bixinhaner  
##### 131#       发表于 2023-4-8 16:48

略小白想入门有什么途径吗？有什么技术交流群没有？lz发的这些好多都看不懂<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">


*****

####  Machinery  
##### 132#       发表于 2023-4-8 19:55

koala-13b模型，重点研究了数据集对于小模型局限的弥补，发现开源数据集，以及简单经过转换的数量优先的数据集训练的模型并不能直接提升更多的实际能力

demo地址:https://chat.lmsys.org/?model=koala-13b

项目地址:https://bair.berkeley.edu/blog/2023/04/03/koala/

权重地址(有点大):https://drive.google.com/drive/folders/10f7wrlAFoPIy-TECHsx9DKIvbQYunCfl?usp=sharing

<img src="https://img.saraba1st.com/forum/202304/08/195445bdbekq77kiy4zy7d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230408-195126.jpg</strong> (421.15 KB, 下载次数: 0)

下载附件

2023-4-8 19:54 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  勿徊哉  
##### 133#       发表于 2023-4-8 19:58

楼主终于秽土转生了


*****

####  Machinery  
##### 134#       发表于 2023-4-8 20:05

 本帖最后由 Machinery 于 2023-4-8 20:06 编辑 

StackLLaMA

使用StackExchange数据集(也就是写代码的数据集)训练的LLaMa，可以输出代码，StackExchange因为本身有投票机制(类似知乎)，所以也可以用来培训人类反馈(RLHF)

hugface方面详细讲解了微调过程与相关经验:huggingface.co/blog/stackllama

demo地址:https://huggingface.co/spaces/trl-lib/stack-llama

模型权重地址:https://huggingface.co/trl-lib/llama-7b-se-rl-peft

<img src="https://img.saraba1st.com/forum/202304/08/200539neyz2vq21eyezoro.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230408-200522.jpg</strong> (318.74 KB, 下载次数: 0)

下载附件

2023-4-8 20:05 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 135#       发表于 2023-4-8 20:14

 本帖最后由 Machinery 于 2023-4-8 20:18 编辑 

微软的论文
arxiv.org/abs/2304.03277
使用 GPT-4 进行指令调优

项目页面:https://instruction-tuning-with-gpt-4.github.io/

简单来说就是微软内部根据Alpaca的数据集，仅使用问题生成对应的GPT4答案，再使用这个GPT4答案数据集，微调了LLaMa7b模型，发现这个模型可以打过之前使用chatGPT数据微调的13b LLaMa模型，在参数减少的情况下使用更优质的数据集提升了效果

<img src="https://img.saraba1st.com/forum/202304/08/201002oa4v0ta0u23u2zw0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230408-200930.jpg</strong> (59.35 KB, 下载次数: 0)

下载附件

2023-4-8 20:10 上传

<img src="https://img.saraba1st.com/forum/202304/08/201059apzfhcq3qq9qqsc9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230408-201046.jpg</strong> (247.18 KB, 下载次数: 0)

下载附件

2023-4-8 20:10 上传

这篇论文最重要的工作之一就是他们同时制作了完全的中文数据集而且开源了(当然论文中也有中文效果测试对比)

https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM

<img src="https://img.saraba1st.com/forum/202304/08/201841ybnz44zh7ah8agmx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230408-201616.jpg</strong> (335.4 KB, 下载次数: 0)

下载附件

2023-4-8 20:18 上传

<img src="https://img.saraba1st.com/forum/202304/08/201841fs2v2dmfr52a2uog.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230408-201706.jpg</strong> (85.58 KB, 下载次数: 0)

下载附件

2023-4-8 20:18 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 136#       发表于 2023-4-8 20:30

AI界这几天最最最重磅的消息

由meta公司开源的新作品
Segment Anything(简称SAM)

被誉为CV(计算机视觉)界的GPT3时刻
上线github一天获得8.3k star
AR与元宇宙的基础，赛博猫娘的摇篮，神之眼<img src="https://static.saraba1st.com/image/smiley/face2017/019.png" referrerpolicy="no-referrer">

demo地址:https://segment-anything.com/

项目介绍页:https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/

论文地址https://arxiv.org/abs/2304.02643

github页面(含权重下载地址):https://github.com/facebookresearch/segment-anything

数据集:https://ai.facebook.com/datasets/segment-anything/

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 137#       发表于 2023-4-8 20:42

 本帖最后由 Machinery 于 2023-4-8 20:45 编辑 

Firefly(流萤): 中文对话式大语言模型

github地址:

<img src="https://img.saraba1st.com/forum/202304/08/204518h5ivqmvjwjijla6h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230408-204450__01.jpg</strong> (25.74 KB, 下载次数: 0)

下载附件

2023-4-8 20:45 上传

相比其他开源微调项目，最大的差异在于底层模型使用了Bloom大模型，而且公开了对应的高质量的包含1.1M(110万)中文多任务指令微调数据集

同时使用了Belle项目的0.5M数据集，总计165万训练数据微调

<img src="https://img.saraba1st.com/forum/202304/08/204230ovl6neadi3tdlldi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230408-204220.jpg</strong> (226.14 KB, 下载次数: 0)

下载附件

2023-4-8 20:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 138#       发表于 2023-4-8 21:14

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60374320&amp;ptid=2126390" target="_blank">jcwatm1 发表于 2023-4-8 14:57</a>
https://github.com/imClumsyPanda/langchain-ChatGLM

这个项目大佬们了解么，根据本地资料生成答案？ ...</blockquote>
这个项目简单来说就是将本地资料/文件/文档等文件存为向量(vectorstore embedding)数据库

再根据prompt提出的问题，使用langchain作为中间接口，搜索与prompt有关的向量相似度，来进行相关资料检索进行离线知识问答，目前类似的项目有chatpdf等

 自动搜索和选取文件的相关内容填充到chat的上下文中，让AI根据相关内容回答你的问题，其实也可以用作对话数据库

主要原因就是记忆系统太难搞，LLM的序列容量又太小<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">只能曲线救国

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  wave14  
##### 139#       发表于 2023-4-8 21:34

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60377841&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-8 21:14</a>

这个项目简单来说就是将本地资料/文件/文档等文件存为向量(vectorstore embedding)数据库

再根据prompt ...</blockquote>
chatglm下面有个叫 闻达 的项目，好像就是差不多的东西

所有的知识库资料用txt保存，index到db

我试了下感觉还行，做成专业垂直领域的会话机器人应该不错，当对于一款AI版的企业搜索？


*****

####  Machinery  
##### 140#       发表于 2023-4-8 21:41

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60378075&amp;ptid=2126390" target="_blank">wave14 发表于 2023-4-8 21:34</a>
chatglm下面有个叫 闻达 的项目，好像就是差不多的东西

所有的知识库资料用txt保存，index到db

我试了下感 ...</blockquote>
拓展用处挺大的，不过项目大多还在发展，记忆库知识知识库都是可以的，openai不久前开放过一个叫text-embedding的api产品，就是专门做这个相关业务的，不过本地来说还是离线实现的安全性更高就是了，虽然性能表现可能差一些

https://openai.com/blog/introducing-text-and-code-embeddings

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  ziyuan008  
##### 141#       发表于 2023-4-8 22:23

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60378075&amp;ptid=2126390" target="_blank">wave14 发表于 2023-4-8 21:34</a>

chatglm下面有个叫 闻达 的项目，好像就是差不多的东西

所有的知识库资料用txt保存，index到db

我试了下感 ...</blockquote>
这个我也试了试，的确不错


*****

####  伊莉伊莉雅  
##### 142#       发表于 2023-4-9 08:01

截止楼主被塞为止的简单总结。（非专业，有些名词或者介绍可能不标准）主要是帖子里中文llama相关的项目和通用webui项目（非中文和非llama都没咋看），后续再整详细使用方法。

可以自己训练的中文llama分为两类，一种是lora，一种是增量训练，项目里都有训练方法（还没研究训练相关）。

前提：meta开源了llama的模型，是gpt协议且不能商用。原版模型是.pth格式，一般再训练或使用都会转成huggingface格式，简称hf，后缀是.bin。所以这些项目1.不会提供llama原模型，2.需要将llama原模型转换成hf格式使用。但是可以在网上下到原模型，也有其他人转换好的hf格式。通常可以直接下载转换好的hf格式即可。

lora项目有：
[https://github.com/LC1332/Chinese-alpaca-lora](https://github.com/LC1332/Chinese-alpaca-lora)（效果一般，但是可以直接在text generation里添加lora，比如模型选原版llama的hf，lora选这个就可以识别中文了）
[https://github.com/ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)（效果比上面的好，但是不能在text generation里添加lora的形式使用，必须融合原llama的hf模型，这个跟作者提issue了，不知道能不能处理。另外按作者推荐的用法是用llama.cpp用cpu推理，速度挺快，不占用gpu。如果是用gpu的话就是用webui。另外受限于训练数据，多轮对话能力较弱）

增量训练项目：
[https://github.com/ydli-ai/Chinese-ChatLLaMA](https://github.com/ydli-ai/Chinese-ChatLLaMA)（7b的gpu需要25g现存寄，cpu略慢，可能训练数据的问题，感觉多轮对话比Chinese-LLaMA-Alpaca好一些。因为使用了腾讯的预训练框架，又是另一种数据格式，不能用于webui。）

然后是Vicuna，中文水平也挺不错，官网说能达到gpt-4的90%，单对中文能力表示存疑。
[https://huggingface.co/lmsys/vicuna-7b-delta-v0](https://huggingface.co/lmsys/vicuna-7b-delta-v0)，提供的权重不能直接用，需要和llama的hf混了之后才能用（类似Chinese-LLaMA-Alpaca的lora）

然后是webui的项目
[https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat) 这个是Vicuna配套的webui，但是也能给原版llama，alpaca，glm这些用。但是需要开三个程序，很麻烦。demo：[https://chat.lmsys.org/](https://chat.lmsys.org/)
[https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) 这个功能很多，支持的模型也比较多，不过暂时还不支持glm。目前我还没完全摸透用法，有文本生成模式和聊天模式，支持lora，也可以训练lora（看到有选项卡，但是没试过）。启动也方便，不过在windows上启动需要改些东西，稍微麻烦一点点（但还是比FastChat简单），目标是成为对标A1111的sd-webui的text generation的webui。

上面说了llama无法商用，针对这个出的项目[https://github.com/Lightning-AI/lit-llama](https://github.com/Lightning-AI/lit-llama)可以避开这一点，但是上面的项目目前没有支持这个的。

另外gpt4all的webui刚开始部署没成功，后来看gpt4all不支持中文就没继续弄了。
[https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)
[https://github.com/nomic-ai/gpt4all-ui](https://github.com/nomic-ai/gpt4all-ui)

总的来说，推荐

1.使用llama.cpp、alpaca.cpp，配合Chinese-LLaMA-Alpaca与vicuna的模型，因为是用cpu推理，对gpu无要求（前面有坛友分享过用法）。

*****

####  伊莉伊莉雅  
##### 143#       发表于 2023-4-9 08:01

截止楼主被塞为止的简单总结。（非专业，有些名词或者介绍可能不标准）主要是帖子里中文llama相关的项目和通用webui项目（非中文和非llama都没咋看），后续再整详细使用方法。

可以自己训练的中文llama分为两类，一种是lora，一种是增量训练，项目里都有训练方法（还没研究训练相关）。

前提：meta开源了llama的模型，是gpt协议且不能商用。原版模型是.pth格式，一般再训练或使用都会转成huggingface格式，简称hf，后缀是.bin。所以这些项目1.不会提供llama原模型，2.需要将llama原模型转换成hf格式使用。但是可以在网上下到原模型，也有其他人转换好的hf格式。通常可以直接下载转换好的hf格式即可。

lora项目有：
[https://github.com/LC1332/Chinese-alpaca-lora](https://github.com/LC1332/Chinese-alpaca-lora)（效果一般，但是可以直接在text generation里添加lora，比如模型选原版llama的hf，lora选这个就可以识别中文了）
[https://github.com/ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)（效果比上面的好，但是不能在text generation里添加lora的形式使用，必须融合原llama的hf模型，这个跟作者提issue了，不知道能不能处理。另外按作者推荐的用法是用llama.cpp用cpu推理，速度挺快，不占用gpu。如果是用gpu的话就是用webui。另外受限于训练数据，多轮对话能力较弱）

增量训练项目：
[https://github.com/ydli-ai/Chinese-ChatLLaMA](https://github.com/ydli-ai/Chinese-ChatLLaMA)（7b的gpu需要25g现存寄，cpu略慢，可能训练数据的问题，感觉多轮对话比Chinese-LLaMA-Alpaca好一些。因为使用了腾讯的预训练框架，又是另一种数据格式，不能用于webui。）

然后是Vicuna，中文水平也挺不错，官网说能达到gpt-4的90%，单对中文能力表示存疑。
[https://huggingface.co/lmsys/vicuna-7b-delta-v0](https://huggingface.co/lmsys/vicuna-7b-delta-v0)，提供的权重不能直接用，需要和llama的hf混了之后才能用（类似Chinese-LLaMA-Alpaca的lora）

然后是webui的项目
[https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat) 这个是Vicuna配套的webui，但是也能给原版llama，alpaca，glm这些用。但是需要开三个程序，很麻烦。demo：[https://chat.lmsys.org/](https://chat.lmsys.org/)
[https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) 这个功能很多，支持的模型也比较多，不过暂时还不支持glm。目前我还没完全摸透用法，有文本生成模式和聊天模式，支持lora，也可以训练lora（看到有选项卡，但是没试过）。启动也方便，不过在windows上启动需要改些东西，稍微麻烦一点点（但还是比FastChat简单），目标是成为对标A1111的sd-webui的text generation的webui。

上面说了llama无法商用，针对这个出的项目[https://github.com/Lightning-AI/lit-llama](https://github.com/Lightning-AI/lit-llama)可以避开这一点，但是上面的项目目前没有支持这个的。

另外gpt4all的webui刚开始部署没成功，后来看gpt4all不支持中文就没继续弄了。
[https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)
[https://github.com/nomic-ai/gpt4all-ui](https://github.com/nomic-ai/gpt4all-ui)

总的来说，推荐

1.使用llama.cpp、alpaca.cpp，配合Chinese-LLaMA-Alpaca与vicuna的模型，因为是用cpu推理，对gpu无要求（前面有坛友分享过用法）。


*****

####  痴货  
##### 144#       发表于 2023-4-9 08:29

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60381584&amp;ptid=2126390" target="_blank">伊莉伊莉雅 发表于 2023-4-9 08:01</a>

截止楼主被塞为止的简单总结。（非专业，有些名词或者介绍可能不标准）主要是帖子里中文llama相关的项目和 ...</blockquote>
text-generation-webui提到了一个GPT-4chan model，搜了一下，好家伙： <blockquote>GPT-4chan was trained on over 3 years of posts from 4chan's "politically incorrect" (/pol/) board.</blockquote>

这是粪坑练蛊吧，难怪被抱脸下架了<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">


*****

####  Machinery  
##### 145#       发表于 2023-4-9 11:32

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60381681&amp;ptid=2126390" target="_blank">痴货 发表于 2023-4-9 08:29</a>
text-generation-webui提到了一个GPT-4chan model，搜了一下，好家伙：</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">这玩意甚至投入过实战，而且如果不是空回复和发帖太快甚至没人看出来

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  jcwatm1  
##### 146#       发表于 2023-4-9 11:34

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60377841&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-8 21:14</a>
这个项目简单来说就是将本地资料/文件/文档等文件存为向量(vectorstore embedding)数据库

再根据prompt ...</blockquote>
测试了下，还是受限glm 6b小模型的局限性


*****

####  Machinery  
##### 147#       发表于 2023-4-9 16:21

 本帖最后由 Machinery 于 2023-4-9 16:23 编辑 

Grounded-SAM

项目地址:https://github.com/IDEA-Research/Grounded-Segment-Anything

通过混合使用多种模型，如Grounding DINO/Segment Anything/Stable diffusion/blip，实现了多种功能，如自动label与segment区块，生成自动重绘mask区域，自然语言标注等，目前项目还在快速迭代中，效果强大

<img src="https://img.saraba1st.com/forum/202304/09/162109pi2eoyvo22wwe2ew.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230409-161850.jpg</strong> (596.11 KB, 下载次数: 0)

下载附件

2023-4-9 16:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  伊莉伊莉雅  
##### 148#       发表于 2023-4-9 18:52

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60381681&amp;ptid=2126390" target="_blank">痴货 发表于 2023-4-9 08:29</a>

text-generation-webui提到了一个GPT-4chan model，搜了一下，好家伙：</blockquote>
<img src="https://static.saraba1st.com/image/smiley/carton2017/018.gif" referrerpolicy="no-referrer">这模型我加载会报错，之前看issue里也有出一样问题的人

（webui还有个类型可以直接生成4chan风格


*****

####  晓古城  
##### 149#       发表于 2023-4-9 19:08

打个标


*****

####  痴货  
##### 150#       发表于 2023-4-9 23:53

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60386334&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-9 16:21</a>

Grounded-SAM

项目地址:https://github.com/IDEA-Research/Grounded-Segment-Anything</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/077.png" referrerpolicy="no-referrer">这个好啊，方便了抠图和后期


*****

####  大江户战士  
##### 151#       发表于 2023-4-9 23:54

[https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)

AI训练AI的项目，AI的自我进化

我觉得可以期待一下年底之前本地模型达到GPT-3.5-turbo的性能

<img src="https://img.saraba1st.com/forum/202304/09/235433v53ewcwly5sll0yo.png" referrerpolicy="no-referrer">

<strong>GP_02S~LKBAH_JO`$@}T~3D.png</strong> (109.85 KB, 下载次数: 0)

下载附件

2023-4-9 23:54 上传


*****

####  marlun  
##### 152#       发表于 2023-4-10 00:13

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60393235&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-4-9 23:54</a>

https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM

AI训练AI的项目，AI的自我进化</blockquote>
看起来是用GPT4优化过的数据对于参数小的模型也有很大的提升


*****

####  矢吹奈子  
##### 153#       发表于 2023-4-10 05:50

用llama本地部署跑了下试试，6800h卡到不能自理，X3D系列这种三级缓存比较大的CPU是不是比较有优势


*****

####  伊莉伊莉雅  
##### 154#       发表于 2023-4-10 06:39

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60394549&amp;ptid=2126390" target="_blank">矢吹奈子 发表于 2023-4-10 05:50</a>
用llama本地部署跑了下试试，6800h卡到不能自理，X3D系列这种三级缓存比较大的CPU是不是比较有优势 ...</blockquote>
是llama.cpp麽?我5900x跑llama.cpp跑起来很快，占用也不算高。
但是用text generation跑cpu就不行，因为用的是python接的llama.cpp，内存占用是普通的两倍，cpu占用也到了100%，速度也慢，不清楚是py的原因还是啥


*****

####  矢吹奈子  
##### 155#       发表于 2023-4-10 13:05

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60394606&amp;ptid=2126390" target="_blank">伊莉伊莉雅 发表于 2023-4-10 06:39</a>

是llama.cpp麽?我5900x跑llama.cpp跑起来很快，占用也不算高。

但是用text generation跑cpu就不行，因为 ...</blockquote>
是哇，也不是跑不动，就遇到很奇怪的问题，跑起来会对话到一半卡死，然后直接可以继续输入提示词，输完之后它继续输出上一段话再往后接提示词==；

我重新配置下环境看看


*****

####  Machinery  
##### 156#       发表于 2023-4-10 20:09

MM-REACT：提示 ChatGPT 进行多模态推理和行动

微软出品的东西<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

通过多模型协作与复杂设计的ReAct prompt，可以让chatgpt这类LLM模型达到类GPT4级别的多模态任务理解能力(具体请看附图示例)，github的也给了一堆项目接口，能力非常强悍，感觉完全不逊色于GPT4的图片模态

论文:https://ai.papers.bar/paper/8e0ee1dcc84c11edb95839eec3084ddd

github地址:https://github.com/microsoft/MM-REACT

<img src="https://img.saraba1st.com/forum/202304/10/200632psp65lb31po6pzo5.jpg" referrerpolicy="no-referrer">

<strong>20230410_200233.jpg</strong> (107.57 KB, 下载次数: 0)

下载附件

2023-4-10 20:06 上传

<img src="https://img.saraba1st.com/forum/202304/10/200632gccb9p9ohxcx8hyh.jpg" referrerpolicy="no-referrer">

<strong>20230410_200235.jpg</strong> (117.79 KB, 下载次数: 0)

下载附件

2023-4-10 20:06 上传

<img src="https://img.saraba1st.com/forum/202304/10/200633wz5f05fgfznfu0sz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230410-200504.jpg</strong> (357.59 KB, 下载次数: 0)

下载附件

2023-4-10 20:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 157#       发表于 2023-4-10 20:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60405183&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-10 20:09</a>
MM-REACT：提示 ChatGPT 进行多模态推理和行动

微软出品的东西</blockquote>
发点个人感想，之前完全没想到ReAct能达到这种程度，微软虽然控股Openai，但是似乎完全没有接触到GPT4的核心算法，从最近微软的动态来看，似乎是准备走依靠核心LLM，多模型多协作多任务处理的范式，效果和应用程度上都非常不错，而且也大都开源，比较核心的LLM底层进步反而不多，也可能是在藏(不确定)，这方面可能还得看meta或者其他大厂<img src="https://static.saraba1st.com/image/smiley/face2017/005.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 158#       发表于 2023-4-11 05:19

DoctorGLM
在ChatGLM-6B的基础上使用问诊数据集(https://github.com/Toyhom/Chinese-medical-dialogue-data)进行微调

项目主页:https://xionghonglin.github.io/DoctorGLM/
项目地址:https://github.com/xionghonglin/DoctorGLM

注:这项工作处于非常早期的阶段并且包含许多错误，因此不适合任何商业或临床使用。

<img src="https://img.saraba1st.com/forum/202304/11/051701vtr0qb5b1ieb0qv1.png" referrerpolicy="no-referrer">

<strong>3_ret.png</strong> (243.99 KB, 下载次数: 0)

下载附件

2023-4-11 05:17 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 159#       发表于 2023-4-11 05:24

Segment Anything的打包版本
安装与使用十分便捷，且支持视频内容切割

注:如遇到显存不足的情况，可以尝试减少points_per_side和points_per_batch参数的数值

<img src="https://img.saraba1st.com/forum/202304/11/052200jydoeauiza8l78od.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230411-052149.jpg</strong> (88.85 KB, 下载次数: 0)

下载附件

2023-4-11 05:22 上传

项目地址:https://github.com/kadirnar/segment-anything-video

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 160#       发表于 2023-4-11 05:46

 本帖最后由 Machinery 于 2023-4-11 05:49 编辑 

使用Segment-Anything编辑一切

项目地址:https://github.com/sail-sg/EditAnything

使用多模型协作编辑和生成图像中的任何内容，由Segment Anything、ControlNet、 BLIP2、Stable Diffusion等组成
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">
可以理解为自动生成掩码再自动重绘，PS PLUS(简称PSP)，因为SAM切割的细粒度非常好所以理论上这类项目大有前途

<img src="https://img.saraba1st.com/forum/202304/11/054907xzyvo4b4oqyo3h8q.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230411-054835.jpg</strong> (281.76 KB, 下载次数: 0)

下载附件

2023-4-11 05:49 上传

<img src="https://img.saraba1st.com/forum/202304/11/054912doyytxpkttpi5r66.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230411-054854.jpg</strong> (679.37 KB, 下载次数: 0)

下载附件

2023-4-11 05:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  frosta  
##### 161#       发表于 2023-4-11 06:38

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60408836&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-10 13:46</a>

使用Segment-Anything编辑一切

项目地址:https://github.com/sail-sg/EditAnything</blockquote>
这个思路感觉对了，配合SD实现精细微调，哪里看着不对哪里抽个10连


*****

####  okok123  
##### 162#       发表于 2023-4-11 08:48

有大佬可以总结下，现在能本地部署的llm到底到什么程度了呢？啥时候进入stable diffusion 的阶段？


*****

####  fufusako  
##### 163#       发表于 2023-4-11 10:06

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60377374&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-8 20:30</a>

AI界这几天最最最重磅的消息

由meta公司开源的新作品</blockquote>
啊，我怎么感觉这玩意把我公司正在搞的一个视频算法盒子干废了


*****

####  Machinery  
##### 164#       发表于 2023-4-11 11:37

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60410491&amp;ptid=2126390" target="_blank">fufusako 发表于 2023-4-11 10:06</a>
啊，我怎么感觉这玩意把我公司正在搞的一个视频算法盒子干废了</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/002.png" referrerpolicy="no-referrer">正常，我了解的相关从业人员都有这种感觉

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 165#       发表于 2023-4-11 17:55

galpaca-6.7b与30b，以及对应的GPTQ-4bit量化

在说明这个项目前先介绍一下GALACTICA模型

Galactica是meta在前不久出品的模型，训练集使用了规范化的4800万篇论文、教科书和讲义、数以百万计的化合物和蛋白质、科学网站、百科全书以及来自"自然书"数据集的更多内容进行了训练，详细介绍可以查看如下链接
https://zhuanlan.zhihu.com/p/584315977

一共有以下这么几个尺寸的权重，都是开源的，相关地址可以在hugface找到

<img src="https://img.saraba1st.com/forum/202304/11/174644ldhtdka6nbhhnitd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230411-174601.jpg</strong> (40.33 KB, 下载次数: 0)

下载附件

2023-4-11 17:46 上传

这个项目当初上线之时有开放demo试用，但因为被推特和各种媒体狂喷胡说八道以及信息不准确所以飞快下线了demo(同样的表现，不同的命运)

但这并不代表模型本身有问题，相反，Galactica是很优秀的基础模型，而现在则是有人尝试用Alpaca的数据集微调Galactica6.7b与30b版本，并且放出了对应的30b 4位量化模型

6.7b权重仓库https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b

30b权重仓库:https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b

30b 4位量化仓库:https://huggingface.co/TheBloke/galpaca-30B-GPTQ-4bit-128g

注:查看作者仓库有惊喜

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  塔奇克马  
##### 166#       发表于 2023-4-12 12:34

各位有没有Chinese-Alpaca-13B 转换完的模型或者一键包啊,转换要30G内存,我这最多32G机器肯定是不行了.....


*****

####  Machinery  
##### 167#       发表于 2023-4-12 12:43

TurboPilot

一个基于llama.cpp复现本地copilot使用体验的项目，使用的模型底座是CodeGen(地址:https://github.com/salesforce/CodeGen)

项目地址:https://github.com/ravenscroftj/turbopilot

有2b和6b的4位量化权重提供下载，放在googledrive了:https://drive.google.com/drive/mobile/folders/1wFy1Y0pqoK23ZeMWWCp8evxWOJQVdaGh?usp=sharing

还可以使用FauxPilot配合web服务器，教程在项目内有

<img src="https://img.saraba1st.com/forum/202304/12/124309s88k33nz8eqs2pnd.gif" referrerpolicy="no-referrer">

<strong>screenrecording.gif</strong> (846.42 KB, 下载次数: 0)

下载附件

2023-4-12 12:43 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 168#       发表于 2023-4-12 12:47

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60425307&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-12 12:34</a>
各位有没有Chinese-Alpaca-13B 合并转换完的模型或者一键包啊,转换要30G内存,我这最多32G机器肯定是不行了. ...</blockquote>
hugface上有https://huggingface.co/models?search=chinese%20alpaca

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  塔奇克马  
##### 169#       发表于 2023-4-12 12:56

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60425487&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-12 12:47</a>

hugface上有https://huggingface.co/models?search=chinese%20alpaca

—— 来自 S1Fun</blockquote>
chinese-alpaca-13b-merged

就这个像,我试试


*****

####  塔奇克马  
##### 170#       发表于 2023-4-12 17:05

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60425487&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-12 12:47</a>

hugface上有https://huggingface.co/models?search=chinese%20alpaca

—— 来自 S1Fun</blockquote>
下载下来看了下..是hf bin模型...能转pth的吗?


*****

####  Machinery  
##### 171#       发表于 2023-4-12 17:28

 本帖最后由 Machinery 于 2023-4-12 17:30 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60428549&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-12 17:05</a>
下载下来看了下..是hf bin模型...能转pth的吗?</blockquote>
可以考虑使用colab，可能需要自己写几行代码<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

https://colab.research.google.com/drive/1Eak6azD3MLeb-YsfbP8UZC8wrL1ddIMI?usp=sharing

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  塔奇克马  
##### 172#       发表于 2023-4-12 18:09

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60428893&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-12 17:28</a>

可以考虑使用colab，可能需要自己写几行代码

[https://colab.research.google.com/drive/1Eak6azD](https://colab.research.google.com/drive/1Eak6azD) ...</blockquote>

<img src="https://img.saraba1st.com/forum/202304/12/180811eq3qqmp7zjoe412e.png" referrerpolicy="no-referrer">

<strong>Snipaste_2023-04-12_18-07-53.png</strong> (105.81 KB, 下载次数: 0)

下载附件

2023-4-12 18:08 上传

放弃了,搞不懂,那几个脚本都不像....文章里只是把过程陈述一遍,但是这个模型是其中的一个选择分支训练成不能量化的HF文件用来训练的.


*****

####  Machinery  
##### 173#       发表于 2023-4-12 19:37

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60429386&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-12 18:09</a>
放弃了,搞不懂,那几个脚本都不像....文章里只是把过程陈述一遍,但是这个模型是其中的一个选择分支训练成不 ...</blockquote>
你的诉求是将原始的llama的pth与lora合并为pth？

这里有两个脚本，一个是合并lora转换为pth，一个是转换为hf，都是项目库里就有的

如果你觉得本机不足，用colab转换也正常啊

https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/scripts

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  塔奇克马  
##### 174#       发表于 2023-4-12 19:45

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60430297&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-12 19:37</a>

你的诉求是将原始的llama的pth与lora合并为pth？

这里有两个脚本，一个是合并lora转换为pth，一个是转换 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/038.png" referrerpolicy="no-referrer">不是..不是合并是转换..所以都不行的...

所以网上没有提供能用的模型,不搞了 换另一个V羊驼了


*****

####  大江户战士  
##### 175#       发表于 2023-4-12 20:35

 本帖最后由 大江户战士 于 2023-4-12 20:36 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60428549&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-12 17:05</a>

下载下来看了下..是hf bin模型...能转pth的吗?</blockquote>
文件名里的bin改成pth就行了，都是pytorch的权重，只有后缀名不一样


*****

####  塔奇克马  
##### 176#       发表于 2023-4-12 20:42

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60430999&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-4-12 20:35</a>

文件名里的bin改成pth就行了，都是pytorch的权重，只有后缀名不一样</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/030.png" referrerpolicy="no-referrer">那我试试去

*****

####  marlun  
##### 177#       发表于 2023-4-12 20:42

微软在今天开源了一个可以在模型训练中加入完整 RLHF 流程的系统框架 ——DeepSpeed Chat

DeepSpeed Chat: 一键式RLHF训练，让你的类ChatGPT千亿大模型提速省钱15倍

项目地址：[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)

<img src="https://github.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/ds-chat-overview.png?raw=true" id="aimg_btRY0" lazyloadthumb="1" onclick="zoom(this, this.src, 0, 0, 0)" onmouseover="img_onmouseoverfunc(this)"/)

根据文档的说法：在效率和经济性方面，DeepSpeed-HE 在 Azure 云上只需 9 小时即可训练一个OPT-13B模型，只需 18 小时既可训练 OPT-30B模型，分别花费不到 300 美元和 600 美元。在速度和可扩展性方面，即使是 13B 的模型也可以在 1.25 小时内训练，而庞大的 175B 模型可以在不到一天的时间内使用 64 个 GPU 集群进行训练。在 RLHF 的可访问性和普及化方面，DeepSpeed-HE 可以在单个 GPU 上训练超过 130 亿参数的模型。凭借超过一个数量级的更高吞吐量，与现有的 RLHF 系统（如 Colossal-AI 或 HuggingFace DDP）相比，DeepSpeed-HE 拥有在相同时间预算下训练更大的 actor 模型的能力，或者以十分之一的成本训练类似大小的模型的能力。

*****

####  boomerangkid  
##### 178#       发表于 2023-4-12 20:48

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60366376&amp;ptid=2126390" target="_blank">伊莉伊莉雅 发表于 2023-4-7 20:17</a>

新建了个群

809851632

点击链接加入群聊【Stage1st 赛博猫娘研究院】：https://jq.qq.com/?_wv=102 ...</blockquote>
点了一下居然直接进群了，好久么遇到加群不用验证的了


*****

####  大江户战士  
##### 179#       发表于 2023-4-12 21:02

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431085&amp;ptid=2126390" target="_blank">marlun 发表于 2023-4-12 20:42</a>

微软在今天开源了一个可以在模型训练中加入完整 RLHF 流程的系统框架 ——DeepSpeed Chat

DeepSpeed Chat: ...</blockquote>
单卡（指A100）

*****

####  塔奇克马  
##### 180#       发表于 2023-4-12 21:03

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60430999&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-4-12 20:35</a>

文件名里的bin改成pth就行了，都是pytorch的权重，只有后缀名不一样</blockquote>

<img src="https://img.saraba1st.com/forum/202304/12/210328fz9rz29999i9zph4.png" referrerpolicy="no-referrer">

<strong>Snipaste_2023-04-12_21-02-57.png</strong> (74.52 KB, 下载次数: 0)

下载附件

2023-4-12 21:03 上传

<img src="https://static.saraba1st.com/image/smiley/face2017/044.png" referrerpolicy="no-referrer">改了之后字典错误


*****

####  大江户战士  
##### 181#       发表于 2023-4-12 21:11

 本帖最后由 大江户战士 于 2023-4-12 21:13 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431298&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-12 21:03</a>

哎</blockquote>
我看了下这个[https://huggingface.co/minlik/chinese-alpaca-13b-merged/tree/main](https://huggingface.co/minlik/chinese-alpaca-13b-merged/tree/main)里没有params.json这个文件，你把llama原始的放进去试试

*****

####  塔奇克马  
##### 182#       发表于 2023-4-12 21:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431366&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-4-12 21:11</a>

我看了下这个https://huggingface.co/minlik/chinese-alpaca-13b-merged/tree/main里没有params.json这个 ...</blockquote>

<img src="https://img.saraba1st.com/forum/202304/12/211626qt1p1gji9yv39ibh.png" referrerpolicy="no-referrer">

<strong>Snipaste_2023-04-12_21-15-57.png</strong> (366.66 KB, 下载次数: 0)

下载附件

2023-4-12 21:16 上传

<img src="https://static.saraba1st.com/image/smiley/face2017/091.png" referrerpolicy="no-referrer">


*****

####  大江户战士  
##### 183#       发表于 2023-4-12 21:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431436&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-12 21:16</a></blockquote>
我看了下，他这个合并完的有3个文件，直接转换是可以但是需要手动修改很多，不如参照这个来做

[https://colab.research.google.co ... rollTo=5AV4EW5hNhVV](https://colab.research.google.com/drive/1Eak6azD3MLeb-YsfbP8UZC8wrL1ddIMI?usp=sharing#scrollTo=5AV4EW5hNhVV)

*****

####  塔奇克马  
##### 184#       发表于 2023-4-12 21:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431366&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-4-12 21:11</a>

我看了下这个https://huggingface.co/minlik/chinese-alpaca-13b-merged/tree/main里没有params.json这个 ...</blockquote>

<img src="https://img.saraba1st.com/forum/202304/12/212902ne15jn9nnkn1ejnj.png" referrerpolicy="no-referrer">

<strong>Snipaste_2023-04-12_21-28-42.png</strong> (33.85 KB, 下载次数: 0)

下载附件

2023-4-12 21:29 上传

又出错,

然后我发现

params.json应该可以和config.json互转?

<img src="https://img.saraba1st.com/forum/202304/12/212957o5fnakknyksnrvxv.png" referrerpolicy="no-referrer">

<strong>Snipaste_2023-04-12_21-29-49.png</strong> (122.25 KB, 下载次数: 0)

下载附件

2023-4-12 21:29 上传

其他参数可以明显找到对应,那么
"multiple_of"
是什么?

*****

####  大江户战士  
##### 185#       发表于 2023-4-12 21:32

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431564&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-12 21:30</a>

又出错,

然后我发现</blockquote>
不是，他读这个只是为了判断有多少文件，原始的llama-13b只有2个权重文件，但是这个合并版本做的不好给保存成了3个


*****

####  塔奇克马  
##### 186#       发表于 2023-4-12 21:34

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431588&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-4-12 21:32</a>

不是，他读这个只是为了判断有多少文件，原始的llama-13b只有2个权重文件，但是这个合并版本做的不好给保 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">电脑内存不够只有32G,colab要抽卡,我看还是算了

*****

####  大江户战士  
##### 187#       发表于 2023-4-12 21:38

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431619&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-12 21:34</a>

电脑内存不够只有32G,colab要抽卡,我看还是算了</blockquote>
我大概懂为啥要保存成三个了，你的机器估计也没法用llama.cpp来量化两个文件的13b模型


*****

####  塔奇克马  
##### 188#       发表于 2023-4-13 00:17

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60431646&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-4-12 21:38</a>

我大概懂为啥要保存成三个了，你的机器估计也没法用llama.cpp来量化两个文件的13b模型 ...</blockquote>
我通过修改代码直接赋予3完成了转换,但是又遇到问题 (venv) PS F:\Chinese-LLaMA-Alpaca\llama.cpp&gt; ./quantize ./zh-models/13B/ggml-model-f16.bin ./zh-models/13B/ggml-model-q4_0.bin 3 llama.cpp: loading model from ./zh-models/13B/ggml-model-f16.bin llama_model_quantize: failed to quantize: llama.cpp: tensor '' should not be 0-dimensional main: failed to quantize model from './zh-models/13B/ggml-model-f16.bin'复制代码
tokenizer_checklist.chk 我是没有的,我只能从最原始的包里找

tokenizer.model 我用的下载下来的那个..

Snipaste_2023-04-13_00-14-24.png
(29.9 KB, 下载次数: 0)

下载附件

2023-4-13 00:14 上传

<img src="https://img.saraba1st.com/forum/202304/13/001434hwkxggu4ug8g1gz1.png" referrerpolicy="no-referrer">

Snipaste_2023-04-13_00-14-24.png
(29.9 KB, 下载次数: 0)

下载附件

2023-4-13 00:15 上传

<img src="https://img.saraba1st.com/forum/202304/13/001505hfewcwiirpniapcc.png" referrerpolicy="no-referrer">


*****

####  jojog  
##### 189#       发表于 2023-4-13 01:01

[https://twitter.com/_akhaliq/status/1646168119658831874](https://twitter.com/_akhaliq/status/1646168119658831874)

<img src="https://static.saraba1st.com/image/smiley/face2017/112.png" referrerpolicy="no-referrer">一步出图？！

*****

####  伊莉伊莉雅  
##### 190#       发表于 2023-4-13 01:43

 本帖最后由 伊莉伊莉雅 于 2023-4-13 01:47 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60432432&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-13 00:17</a>
我通过修改代码直接赋予3完成了转换,但是又遇到问题

tokenizer_checklist.chk 我是没有的,我只能从最原 ...</blockquote>
你操作不对。
0.如果没有原版hf模型，就把原版模型(pth文件)转换hf模型(bin文件)。有原版hf模型的话直接下一步。
1.用原版hf模型和lora融合，得出融合后pth文件。
2.1.将融合后的pth文件转换成hf模型(.bin文件)。webui加载的是这个模型。

2.2.1.将融合后的pth模型，转换成ggml格式(.bin文件)。
2.2.2.将ggml文件量化成int4或int8，llama.cpp加载的是这个模型。(webui也可以用llama.cpp)

*****

####  伊莉伊莉雅  
##### 191#       发表于 2023-4-13 01:45

群共享里有webui一键包，也有融合之后的模型


*****

####  塔奇克马  
##### 192#       发表于 2023-4-13 13:11

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60432631&amp;ptid=2126390" target="_blank">伊莉伊莉雅 发表于 2023-4-13 01:45</a>

群共享里有webui一键包，也有融合之后的模型</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/245.png" referrerpolicy="no-referrer">我已经在用了,正在调教群主中.


*****

####  Machinery  
##### 193#       发表于 2023-4-13 13:33

 本帖最后由 Machinery 于 2023-4-13 13:35 编辑 

Databricks出品的Dolly，更新到了v2

v2使用了完全由databricks员工编制的新数据集databricks-dolly-15k，使用的基模是EleutherAI 的 Pythia-12b，授权许可Apache-2.0，可以随意商用，支持bf16参数化推理加速

项目介绍:https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm

项目地址:https://github.com/databrickslabs/dolly

数据集页面:https://github.com/databrickslabs/dolly/tree/master/data

<img src="https://img.saraba1st.com/forum/202304/13/133337kgv0nvr0iuaus9gc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230413-133258.jpg</strong> (241.52 KB, 下载次数: 0)

下载附件

2023-4-13 13:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 194#       发表于 2023-4-13 15:33

 本帖最后由 Machinery 于 2023-4-13 15:34 编辑 

ImageReward

清华大学和北京邮电大学合作的项目，ImageReward 是第一个通用的文本到图像人类偏好 RM，它基于文本提示和 DiffusionDB 的相应模型输出，接受了总共 137k 对专家比较的训练，在理解人类对文本到图像合成的偏好方面，ImageReward 优于现有的文本图像评分方法，例如 CLIP、Aesthetic 和 BLIP等

github地址:https://github.com/THUDM/ImageReward

简单来说就是一个给合成图片评分的美学审美项目，权重本身已经开源，1.79G，代码简单易用

<img src="https://img.saraba1st.com/forum/202304/13/153124dyh527whywi2g8gy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230413-153108.jpg</strong> (79 KB, 下载次数: 0)

下载附件

2023-4-13 15:31 上传

论文:https://arxiv.org/abs/2304.05977

<img src="https://img.saraba1st.com/forum/202304/13/153411kcskgyez17vieiez.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230413-143127__01.jpg</strong> (299.67 KB, 下载次数: 0)

下载附件

2023-4-13 15:34 上传

<img src="https://img.saraba1st.com/forum/202304/13/153303deugkkk8skq8c6k7.png" referrerpolicy="no-referrer">

<strong>ImageReward.png</strong> (764.11 KB, 下载次数: 0)

下载附件

2023-4-13 15:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 195#       发表于 2023-4-14 14:03

SEEM：一次分割所有地方的一切

由多家单位与微软联合出品的SEEM
SEEM: Segment Everything Everywhere All at Once

论文地址:https://arxiv.org/abs/2304.06718
github项目地址:https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once

包含以下四个重要特点组成
1.支持多模态自由组合prompt实例分割
2.支持超级多种的自由prompt引导输入
3.用户可以进行多轮交互prompt
4.预测任何mask的语义标签

<img src="https://img.saraba1st.com/forum/202304/14/140227m6gzgox9rzjt69t9.jpg" referrerpolicy="no-referrer">

<strong>9201e1e3-82f4-410b-913b-1aa1c3ec8c77.jpg</strong> (123.58 KB, 下载次数: 0)

下载附件

2023-4-14 14:02 上传

Demo分流演示1:https://8c52faee5271add1.gradio.app/
Demo分流演示2:https://4a489753d0c824e0.gradio.app/
Demo分流演示3:https://36771ee9c49a4631.gradio.app/

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 196#       发表于 2023-4-14 14:17

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60450093&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-14 14:03</a>
SEEM：一次分割所有地方的一切
SEEM: Segment Everything Everywhere All at Once</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">没想到sam一出其他家也发力了，github有专门阐述与sam的比较与特点，不过因为是新鲜出炉的，todo还需要等待一定时间

SEEM + Whisper Demo←目前进度在这里
 SEEM + Whisper + Stable Diffusion Demo
 Inference and installation code
 Hugging Face Demo

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 197#       发表于 2023-4-15 02:43

 本帖最后由 Machinery 于 2023-4-15 02:54 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60377190&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-8 20:14</a>
微软的论文
arxiv.org/abs/2304.03277
使用 GPT-4 进行指令调优</blockquote>
Alpaca LoRA (GPT4版本)

使用论文“Instruction Tuning with GPT-4”的后编辑数据集微调了llama的7b/13b/30b，三个版本，微调训练代码由此项目(https://github.com/tloen/alpaca-lora)支持

lora存储库地址依次如下:
https://huggingface.co/chansung/gpt4-alpaca-lora-7b

https://huggingface.co/chansung/gpt4-alpaca-lora-13b

https://huggingface.co/chansung/gpt4-alpaca-lora-30b

<img src="https://img.saraba1st.com/forum/202304/15/024419kpqhvwphvwj190pm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230415-024404.jpg</strong> (279.05 KB, 下载次数: 0)

下载附件

2023-4-15 02:44 上传

效果看起来十分不错<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202304/15/025357v6xwgi4iwriac4zh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230415-025306.jpg</strong> (481.24 KB, 下载次数: 0)

下载附件

2023-4-15 02:53 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  宇宙之心  
##### 198#       发表于 2023-4-15 07:59

好贴插个眼。

—— 来自 Xiaomi Mi 10, Android 12上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4


*****

####  auirerx  
##### 199#       发表于 2023-4-15 08:22

草。主贴看不到了


*****

####  大江户战士  
##### 200#       发表于 2023-4-15 08:36

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60459022&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-15 02:43</a>

Alpaca LoRA (GPT4版本)

使用论文“Instruction Tuning with GPT-4”的后编辑数据集微调了llama的7b/13b ...</blockquote>
只炼了lora是硬件不行吗<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">


*****

####  勿徊哉  
##### 201#       发表于 2023-4-15 09:50

text to motion

我一直有个梦想，就是galgame一键VR化<img src="https://static.saraba1st.com/image/smiley/face2017/053.png" referrerpolicy="no-referrer">

[https://priormdm.github.io/priorMDM-page/](https://priormdm.github.io/priorMDM-page/)

Human Motion Diffusion as a Generative Prior


*****

####  Machinery  
##### 202#       发表于 2023-4-15 16:36

发错编辑/

*****

####  Machinery  
##### 203#       发表于 2023-4-15 16:37

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60459650&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-4-15 08:36</a>
只炼了lora是硬件不行吗</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer"> DGX上微调出来的

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 204#       发表于 2023-4-15 21:16

 本帖最后由 Machinery 于 2023-4-15 21:19 编辑 

Image2Paragraph

使用ChatGPT、BLIP2、OFA、GRIT、Segment Anything、ControlNet等组件将图像转换为独特的具有丰富细粒度的自然语言描述段落(俗称打标)

github地址:https://github.com/showlab/Image2Paragraph

效果如下

<img src="https://img.saraba1st.com/forum/202304/15/211535twkrlmi3kr5ozhlx.jpg" referrerpolicy="no-referrer">

<strong>a90a8c4c-37ab-456b-b6aa-c4d091b75365.jpg</strong> (104.1 KB, 下载次数: 0)

下载附件

2023-4-15 21:15 上传

<img src="https://img.saraba1st.com/forum/202304/15/211535yivr8ikckr8pci7o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230415-211411.jpg</strong> (564.56 KB, 下载次数: 0)

下载附件

2023-4-15 21:15 上传

注:作者在待办事项中已经准备用其他LLM代替chatgpt实现完全本地化

<img src="https://img.saraba1st.com/forum/202304/15/211709iq445eei9vq5r5h3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230415-211203__01.jpg</strong> (142.91 KB, 下载次数: 0)

下载附件

2023-4-15 21:17 上传

配备对应的gradio ui

<img src="https://img.saraba1st.com/forum/202304/15/211915xgnno3jn7ql0jlej.png" referrerpolicy="no-referrer">

<strong>gradio_visualization.png</strong> (525.46 KB, 下载次数: 0)

下载附件

2023-4-15 21:19 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 205#       发表于 2023-4-15 22:22

rwkv-4-raven

要解释rwkv-4-raven就要解释ChatRWKV，要结解释ChatRWKV就要解释RWKV<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">挨个来吧

RWKV是国人研发的(作者知乎主页:https://www.zhihu.com/people/bopengbopeng)，具有Transformer级LLM模型性能的 RNN模型
(注意，是RNN模型)，结合了RNN和Transformer的优点，更好的性能表现,快速推理,更节省VRAM,更快的训练速度,(相对)无限的上下文长度,自由的语句嵌入等

作者github主页:https://github.com/BlinkDL

<img src="https://img.saraba1st.com/forum/202304/15/220451fdynjb0yfywiyc88.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230415-220415.jpg</strong> (175.82 KB, 下载次数: 0)

下载附件

2023-4-15 22:04 上传

而ChatRWKV则是在这个模型基础设计上进行对话微调的系列模型，根据不同的参数量，训练集语料比例以及版本划分为不同的模型命名，以及对应开源的模型权重

项目地址:https://huggingface.co/BlinkDL/rwkv-4-raven

RWKV-4-Pile系列模型有3B/7B/14B等版本，最近在Alpaca, CodeAlpaca, Guanaco, GPT4All, ShareGPT等 公开数据集上微调，上下文长度v9版本是8192

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 206#       发表于 2023-4-16 01:43

OpenAssistant Conversations (OASST1对话数据集)

OpenAssistant项目模型所公开的对话数据集，相关资料如下

数据集地址:https://huggingface.co/datasets/OpenAssistant/oasst1

<img src="https://img.saraba1st.com/forum/202304/16/014156cbvbv0mvugdvm6sg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230416-014044.jpg</strong> (350.98 KB, 下载次数: 0)

下载附件

2023-4-16 01:41 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  大江户战士  
##### 207#       发表于 2023-4-16 02:31

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60470953&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-16 01:43</a>

OpenAssistant Conversations (OASST1对话数据集)

OpenAssistant项目模型所公开的对话数据集，相关资料如 ...</blockquote>
161K，哈人


*****

####  191634  
##### 208#       发表于 2023-4-16 09:58

那个号称地球上唯一一个能用rnn实现gpt4效果的rwkv怎么样？


*****

####  Machinery  
##### 209#       发表于 2023-4-16 15:40

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60472534&amp;ptid=2126390" target="_blank">191634 发表于 2023-4-16 09:58</a>
那个号称地球上唯一一个能用rnn实现gpt4效果的rwkv怎么样？</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">效果确实挺好的，discord有bot，可以去看看

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 210#       发表于 2023-4-16 15:50

文本到图像生成模型-Paella

Laion组织的新文本到图像生成模型，模型本身在量化的潜在空间中工作，学习方式类似于 MUSE 和扩散模型，也同样是通过逐步加噪方式来生成图像，不过本项目与扩散模型最大的区别在于，Paella 的训练和采样代码极简，几分钟就可以理解，进一步扩展，快速测试，想法测试等非常快。例如，整个采样代码可以写成12行代码，关于方法、训练和采样的更多细节可以在论文和 GitHub 上找到。

<img src="https://img.saraba1st.com/forum/202304/16/154835c9d4zzo9y6k71ddd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230416-154651.jpg</strong> (240.5 KB, 下载次数: 0)

下载附件

2023-4-16 15:48 上传

<img src="https://img.saraba1st.com/forum/202304/16/154841lqfdzkq9rr2q0rxk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230416-154803.jpg</strong> (370.81 KB, 下载次数: 0)

下载附件

2023-4-16 15:48 上传

<img src="https://img.saraba1st.com/forum/202304/16/154849x27waxz6wq2cd07e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230416-154817.jpg</strong> (145.17 KB, 下载次数: 0)

下载附件

2023-4-16 15:48 上传

论文链接:https://arxiv.org/abs/2211.07292

hugface权重下载:https://huggingface.co/dome272/Paella

github项目页:https://github.com/dome272/Paella

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  勿徊哉  
##### 211#       发表于 2023-4-16 19:49

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60470953&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-16 01:43</a>

OpenAssistant Conversations (OASST1对话数据集)

OpenAssistant项目模型所公开的对话数据集，相关资料如 ...</blockquote>
社会化抚养AI


*****

####  死宅真恶心  
##### 212#       发表于 2023-4-17 08:25

借地问一下有无体制内八股文的模型


*****

####  Machinery  
##### 213#       发表于 2023-4-17 14:43

LLMZoo

LLMZoo是由香港中文大学(CUHKSZ)所关联的项目，项目本身旨在梳理近期LLM项目与数据集，同时也发布了两种新模型，同时计划发布更多类型模型

大型语言模型Phoenix(跨语言LLM/使用BLOOMZ作为基础模型)
大型语言模型Chimera(拉丁语与西里尔语言LLM/使用LLaMA作为基础模型)

模型的主要区别在于同时使用了两类数据，即指令数据和对话数据，它们以前分别仅由Alpaca and Vicuna使用，指令数据有助于驯服语言模型以遵守人类指令，而对话数据则有助于模型中对话技能的发展，相辅相成，共同创建更全面的语言模型，同时项目对于模型进行了一定的基准，请于github中查阅

<img src="https://img.saraba1st.com/forum/202304/17/144252p0k09tpc0v7c0ecs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230417-144158.jpg</strong> (118.04 KB, 下载次数: 0)

下载附件

2023-4-17 14:42 上传

项目地址:https://github.com/FreedomIntelligence/LLMZoo

项目整理的相关数据集:https://github.com/FreedomIntelligence/InstructionZoo

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 214#       发表于 2023-4-17 17:17

 本帖最后由 Machinery 于 2023-4-17 17:18 编辑 

Inpaint-Anything

使用SAM切割后重绘任何图片，这个思路本身不算特别出奇，而且已经见到相当多的工程项目，但是这个项目作者用的图片重绘组件不是扩散模型，而是卷积修复模型，这点在今日是相当罕见的，但效果真的特别好

github项目地址:https://github.com/geekyutao/Inpaint-Anything

重点说说作者使用的模型LaMa

LaMa(使用傅里叶卷积核进行高清掩码稳健性重绘)
项目空间:https://advimman.github.io/lama-project/

LaMa 出人意料地很好地泛化了比它在训练期间看到的 (256x256) 更高的分辨率 (~2k❗️)，并且即使在具有挑战性的场景中也能实现出色的性能，例如周期性结构的完成，以下为例图

<img src="https://img.saraba1st.com/forum/202304/17/171243yxz5zi5uzxt58500.jpg" referrerpolicy="no-referrer">

<strong>db9cd83b-fbba-4792-9182-1e8f63e3c5ed.jpg</strong> (158.04 KB, 下载次数: 0)

下载附件

2023-4-17 17:12 上传

<img src="https://img.saraba1st.com/forum/202304/17/171243gffxoodowor6668d.jpg" referrerpolicy="no-referrer">

<strong>7c5b62a9-babd-4cf6-a61f-9587dcd416af.jpg</strong> (183.63 KB, 下载次数: 0)

下载附件

2023-4-17 17:12 上传

<img src="https://img.saraba1st.com/forum/202304/17/171243cbzdsvccquup5o9b.jpg" referrerpolicy="no-referrer">

<strong>c0486921-714d-45ab-8fbd-495bfc8f9f41.jpg</strong> (146.48 KB, 下载次数: 0)

下载附件

2023-4-17 17:12 上传

虽然在生成方面卷积不如扩散模型的语义丰富，但对于图片修复与周期性结构，或者细节填补等，卷积是非常优秀的，而且速度极快(对比扩散模型)，不过扩散近期也有一致性模型的新方法，不知道以后的AR图像编辑会采取什么样的方案

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  lvseqiji  
##### 215#       发表于 2023-4-17 22:23

[https://minigpt-4.github.io/](https://minigpt-4.github.io/)

新项目，基于Vicuna-13B和BLIP2的多模态模型，其实只是做了些缝合+自己搞了些高质量数据集训练了一下，但是结果效果爆炸，GPT-4的根据手绘图输出网页代码都可以做到。。。。

围观群众看到这么简单的操作就能产生这么好的效果都惊了，以前不敢想的能力其实近在眼前。


*****

####  我开P918  
##### 216#       发表于 2023-4-17 22:32

编辑


*****

####  Machinery  
##### 217#       发表于 2023-4-17 22:47

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60497891&amp;ptid=2126390" target="_blank">lvseqiji 发表于 2023-4-17 22:23</a>
https://minigpt-4.github.io/

新项目，基于Vicuna-13B和BLIP2的多模态模型，其实只是做了些缝合+自己搞了 ...</blockquote>
这个多模态项目不错，最近C4多模态数据集也发布了，加上SAM的高细粒度标注，以及越来越多的民间语料与项目合作，比如RedPajama和openAssistant这类的，未来开源多模态LLM越来越光明了<img src="https://static.saraba1st.com/image/smiley/face2017/066.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 218#       发表于 2023-4-17 23:15

简单介绍一下RedPajama，目标是创建一个领先的开源LLM模型系列

目前已经公开了包含超过 1.2 万亿个token的仿LLaMA预训练预处理数据集

项目公开地址:https://www.together.xyz/blog/redpajama

数据集公开地址:https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T

<img src="https://img.saraba1st.com/forum/202304/17/231429lmi0ibc0hicbbd1t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230417-231345.jpg</strong> (400.67 KB, 下载次数: 0)

下载附件

2023-4-17 23:14 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 219#       发表于 2023-4-17 23:38

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60497891&amp;ptid=2126390" target="_blank">lvseqiji 发表于 2023-4-17 22:23</a>
https://minigpt-4.github.io/

新项目，基于Vicuna-13B和BLIP2的多模态模型，其实只是做了些缝合+自己搞了 ...</blockquote>
贴一下效果，真的好到爆炸，感觉完全不输GPT4，至少我用discord的GPT4图片模态，细粒度也不会比这个好太多，更关键的在于只用了13B的LLM模型，看项目署名似乎是沙特土豪和国人的合作项目

<img src="https://img.saraba1st.com/forum/202304/17/233626vzw5tf8fsdd5rf85.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230417-233535.jpg</strong> (92.41 KB, 下载次数: 0)

下载附件

2023-4-17 23:36 上传

<img src="https://img.saraba1st.com/forum/202304/17/233626q444hc9ylirvlr3b.jpg" referrerpolicy="no-referrer">

<strong>20230417_232915.jpg</strong> (128.56 KB, 下载次数: 0)

下载附件

2023-4-17 23:36 上传

<img src="https://img.saraba1st.com/forum/202304/17/233706q9mawcaipffxooda.png" referrerpolicy="no-referrer">

<strong>web_1.png</strong> (711.43 KB, 下载次数: 0)

下载附件

2023-4-17 23:37 上传

<img src="https://img.saraba1st.com/forum/202304/17/233706yo742fona49t2o4g.png" referrerpolicy="no-referrer">

<strong>people_1.png</strong> (248.87 KB, 下载次数: 0)

下载附件

2023-4-17 23:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  冰箱研会长  
##### 220#       发表于 2023-4-18 09:33

今天终于有空好好追一下这个帖子了

想找个用来paraphrase的模型

打算先测试一下Vicuna和gpt4all


*****

####  Machinery  
##### 221#       发表于 2023-4-18 15:19

DINOv2

meta开源的又一个震撼的新作品，一系列基础的自监督模型，产生适用于图像级视觉任务（图像分类、实例检索、视频理解）以及像素级视觉任务（深度估计、语义分割）的通用特征，性能稳健，而且不需要微调(重点)

项目博客:https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/

实时演示:https://dinov2.metademolab.com/

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 222#       发表于 2023-4-18 16:22

 本帖最后由 Machinery 于 2023-4-18 16:27 编辑 

LLaVA-13B-v0

多模态模型，从理解上来说类似minigpt4，但是训练与架构方法不同，结合使用了CLIP ViT-L/14和 Vicuna 以实现通用视觉和语言理解，同时公布了所使用的GPT4指令微调合成多模态数据集

模型地址:https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0

项目空间:https://llava-vl.github.io/

演示地址:https://llava.hliu.cc/

数据集地址:https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K

不知道为啥，演示老是错误，所以去推上找了找示例

<img src="https://img.saraba1st.com/forum/202304/18/162206jbqn55lbtoo99f4s.jpg" referrerpolicy="no-referrer">

<strong>20230418_162153.jpg</strong> (102.85 KB, 下载次数: 0)

下载附件

2023-4-18 16:22 上传

<img src="https://img.saraba1st.com/forum/202304/18/162206zkebsiki43k2bkek.jpg" referrerpolicy="no-referrer">

<strong>20230418_162147.jpg</strong> (144.1 KB, 下载次数: 0)

下载附件

2023-4-18 16:22 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 223#       发表于 2023-4-18 16:39

<img src="https://img.saraba1st.com/forum/202304/18/163633iygocoii1oyyszyo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230418-163123__01.jpg</strong> (44.32 KB, 下载次数: 0)

下载附件

2023-4-18 16:36 上传

注:这里的法学硕士是谷歌的错误机翻，实际上是LLM(大语言模型)

来个比较生草的，最初是刷到这条博客，愣了一下，进去一看，这不是赛博群友嘛！<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202304/18/163826u6e99z999f9nwyyu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230418-163246__01.jpg</strong> (285.87 KB, 下载次数: 0)

下载附件

2023-4-18 16:38 上传

包含全套赛博群友练成指南:https://www.izzy.co/blogs/robo-boys.html

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 224#       发表于 2023-4-18 17:19

LongForm

今日最离谱项目，直接看数据，简单的prompt合成技巧能拉这么多点就离谱，这数据已经完全达到了chatgpt水平，按这个提升，13b小羊驼加上这种微调方法已经完全可以和chatgpt一战了，再加上开源多模态实现，完美实现gpt4平替版，数据太好到有点难以想象<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">
github地址:https://github.com/akoksal/LongForm

<img src="https://img.saraba1st.com/forum/202304/18/171708z9b8z1910czrne9b.jpg" referrerpolicy="no-referrer">

<strong>20230418_165717.jpg</strong> (90.94 KB, 下载次数: 0)

下载附件

2023-4-18 17:17 上传

<img src="https://img.saraba1st.com/forum/202304/18/171703huwa4a2hpa3fg4ut.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230418-165701__01.jpg</strong> (377.54 KB, 下载次数: 0)

下载附件

2023-4-18 17:17 上传

<img src="https://img.saraba1st.com/forum/202304/18/171954fnu5uw4qquamz10w.jpg" referrerpolicy="no-referrer">

<strong>20230418_165720.jpg</strong> (172.94 KB, 下载次数: 0)

下载附件

2023-4-18 17:19 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 225#       发表于 2023-4-18 20:50

项目地址:https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese

<img src="https://img.saraba1st.com/forum/202304/18/204605eu3557iy05qh230u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230418-204513.jpg</strong> (332.21 KB, 下载次数: 0)

下载附件

2023-4-18 20:46 上传

<img src="https://img.saraba1st.com/forum/202304/18/205041glpzi0iko0i0njrr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230418-205027.jpg</strong> (208.97 KB, 下载次数: 0)

下载附件

2023-4-18 20:50 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 226#       发表于 2023-4-18 21:00

Bark
文本prompt提示音频生成模型
项目地址:https://github.com/suno-ai/bark

能说外语，能用prompt提示，能生成音乐风格，流畅连贯，还有语气，github自带演示，效果也太好了<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202304/18/210015s1q6y12s1y116y16.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230418-205728__01.jpg</strong> (341.04 KB, 下载次数: 0)

下载附件

2023-4-18 21:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  nexus1  
##### 227#       发表于 2023-4-19 12:40

<img src="https://static.saraba1st.com/image/smiley/face2017/001.png" referrerpolicy="no-referrer">一个电脑上有多个环境不会相互冲突吧

顺便,古汉语和语言学目前还没找到应用?我感觉这个llm是最适合做古语言内容的了,可以把学界几个世纪的积累从头犁一遍


*****

####  oldttt  
##### 228#       发表于 2023-4-19 13:46

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60510760&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-18 21:00</a>

Bark

文本prompt提示音频生成模型

项目地址:[https://github.com/suno-ai/bark](https://github.com/suno-ai/bark)</blockquote>

我试了感觉中文的一般 腔调奇怪 像影视剧里洋人学中文

*****

####  Machinery  
##### 229#       发表于 2023-4-19 13:48

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60518959&amp;ptid=2126390" target="_blank">oldttt 发表于 2023-4-19 13:46</a>
我试了感觉中文的一般 腔调奇怪 像影视剧里洋人学中文</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">昨天测试时发现了，应该是训练集训练量不足的问题，靠泛化达到这个性能很不错了，实际上我觉得这更有点像语音版的SD，如果能重新训练微调，比现在这些TTS强太多了

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  oldttt  
##### 230#       发表于 2023-4-19 13:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60518979&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-19 13:48</a>
昨天测试时发现了，应该是训练集训练量不足的问题，靠泛化达到这个性能很不错了，实际上我觉得这 ...</blockquote>
那倒是 我比较在意它那些可以调整语气的tag 只是重音这种靠大小写控制的 中文也不知怎么搞


*****

####  塔奇克马  
##### 231#       发表于 2023-4-19 13:56

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60518184&amp;ptid=2126390" target="_blank">nexus1 发表于 2023-4-19 12:40</a>

一个电脑上有多个环境不会相互冲突吧

顺便,古汉语和语言学目前还没找到应用?我感觉这个llm是最适合 ...</blockquote>
不会，先装pyenv 可以随意切换 版本然后再用python -m虚拟环境

*****

####  Machinery  
##### 232#       发表于 2023-4-19 13:56

Guanaco

llama概念验证模型，拥有多语言，使用system prompt进行准确的外部知识推理，多轮对话，类似chatgpt的三位一体角色设定 (System, Assistant, User)，更易于使用等，性能十分强大

项目地址:https://huggingface.co/JosephusCheung/Guanaco

另注:作者不推荐使用llama.cpp运行Guanaco，也不推荐直接量化模型，模型在fp16的情况下工作性能更好，可以使用专用的量化程序，hugface页有写

<img src="https://img.saraba1st.com/forum/202304/19/134956l3b25kx7x77uzu88.jpg" referrerpolicy="no-referrer">

<strong>20230419_134935.jpg</strong> (207 KB, 下载次数: 0)

下载附件

2023-4-19 13:49 上传

<img src="https://img.saraba1st.com/forum/202304/19/135226enkl8mmgflfz7g7o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230419-134629__01.jpg</strong> (231.61 KB, 下载次数: 0)

下载附件

2023-4-19 13:52 上传

<img src="https://img.saraba1st.com/forum/202304/19/135628inurlhbssorusgsb.jpg" referrerpolicy="no-referrer">

<strong>20230419_135614.jpg</strong> (138.44 KB, 下载次数: 0)

下载附件

2023-4-19 13:56 上传

<img src="https://img.saraba1st.com/forum/202304/19/135639gzsjo76o44optw7n.jpg" referrerpolicy="no-referrer">

<strong>20230419_135632.jpg</strong> (114.04 KB, 下载次数: 0)

下载附件

2023-4-19 13:56 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  lvcha  
##### 233#       发表于 2023-4-19 14:08

mark，话说泥潭有gpt群吗？

*****

####  hyde_caesar  
##### 234#       发表于 2023-4-19 14:11

mark一下, 顺便楼主怎么进去了<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">


*****

####  塔奇克马  
##### 235#       发表于 2023-4-19 14:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60519092&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-19 13:56</a>

Guanaco

llama概念验证模型，拥有多语言，使用system prompt进行准确的外部知识整合推理(类似gpt4的泛化知 ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/216.png" referrerpolicy="no-referrer">那这玩意用啥跑？


*****

####  Machinery  
##### 236#       发表于 2023-4-19 14:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60519325&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-4-19 14:16</a>
那这玩意用啥跑？</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">只是不推荐用llama.cpp，可以用webui，他colab也有代码整合

https://colab.research.google.com/drive/1ocSmoy3ba1EkYu7JWT1oCw9vz8qC2cMk#scrollTo=zLORi5OcPcIJ

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 237#       发表于 2023-4-19 14:57

SAM-Adaptor

微调SAM以适应表现不佳的场景
注:论文标题非常生草

项目地址:https://tianrun-chen.github.io/SAM-Adaptor/

github地址:https://github.com/tianrun-chen/SAM-Adapter-PyTorch

<img src="https://img.saraba1st.com/forum/202304/19/145716ldxizqrrzdd093v0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230419-145638.jpg</strong> (169.93 KB, 下载次数: 0)

下载附件

2023-4-19 14:57 上传

<img src="https://img.saraba1st.com/forum/202304/19/145716yiumtlqoif4iiim5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230419-145659.jpg</strong> (210.21 KB, 下载次数: 0)

下载附件

2023-4-19 14:57 上传

构架如下

<img src="https://img.saraba1st.com/forum/202304/19/145722fdmidm37in5aym27.png" referrerpolicy="no-referrer">

<strong>figmain.png</strong> (227.06 KB, 下载次数: 0)

下载附件

2023-4-19 14:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 238#       发表于 2023-4-20 00:09

 本帖最后由 Machinery 于 2023-4-20 00:10 编辑 

StableLM

Stability Ai公布的独立LLM模型，测试版30亿和70亿参数，预定今后还有150亿和650亿版本的，授权使用CC BY-SA-4.0许可，目前还在看

演示Demo:https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat

项目博客:https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models

github地址:https://github.com/Stability-AI/StableLM

<img src="https://img.saraba1st.com/forum/202304/20/000931zyty3ygqovrjf1yu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230420-000904.jpg</strong> (214.02 KB, 下载次数: 0)

下载附件

2023-4-20 00:09 上传

<img src="https://img.saraba1st.com/forum/202304/20/000931w6jggt1kbgygg5go.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230420-000913.jpg</strong> (319.86 KB, 下载次数: 0)

下载附件

2023-4-20 00:09 上传

<img src="https://img.saraba1st.com/forum/202304/20/000931z2ykkzmw30c4kroz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230420-000919.jpg</strong> (177.94 KB, 下载次数: 0)

下载附件

2023-4-20 00:09 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 239#       发表于 2023-4-20 18:32

alpaca-lora-65B-GGML

https://huggingface.co/TheBloke/alpaca-lora-65B-GGML

使用实验性质的2bit量化和4bit量化对alpaca-lora-65b进行cpu推理测试

<img src="https://img.saraba1st.com/forum/202304/20/183107ps4xshx252vyxlho.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230420-182420__01.jpg</strong> (121.39 KB, 下载次数: 0)

下载附件

2023-4-20 18:31 上传

注:非常非常的实验性质，可以看看readme

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 240#       发表于 2023-4-21 04:50

 本帖最后由 Machinery 于 2023-4-21 04:55 编辑 

Anything-3D

切割2D图像，重建到3D物体/视角/场景/面部等，与多种模型与方法搭配使用，效果不错

项目地址:https://github.com/Anything-of-anything/Anything-3D

<img src="https://img.saraba1st.com/forum/202304/21/044949ttea7egg6e0e3tar.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230421-044852.jpg</strong> (239.9 KB, 下载次数: 0)

下载附件

2023-4-21 04:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 241#       发表于 2023-4-21 04:53

thinkgpt

ThinkGPT 是一个 Python 库，旨在为大型语言模型 (LLM) 实施思想链，促使模型思考、推理和创建生成Agent等方法增强模型能力与效果，代码还挺简单易用的…

项目地址:https://github.com/alaeddine-13/thinkgpt

<img src="https://img.saraba1st.com/forum/202304/21/045256vd222o0a6u02d6ul.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230421-045244.jpg</strong> (207.44 KB, 下载次数: 0)

下载附件

2023-4-21 04:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 242#       发表于 2023-4-21 15:49

mpt-1b-redpajama-200b

decoder-only的transformer模型，使用RedPajama数据集的子集(约200b token)进行学习训练，总共参数约13亿，模型由MosaicML训练，使用了MosaicML平台的440个A100-40GBs，花费训练时间约半天

项目地址:https://huggingface.co/mosaicml/mpt-1b-redpajama-200b

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  ffcloudy12  
##### 243#       发表于 2023-4-21 19:15

有没有什么对中文支持比较好的本地部署AI


*****

####  Machinery  
##### 244#       发表于 2023-4-21 23:42

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60551998&amp;ptid=2126390" target="_blank">ffcloudy12 发表于 2023-4-21 19:15</a>
有没有什么对中文支持比较好的本地部署AI</blockquote>
vicuna，chatglm，guanaco，现在再加个moss，等我更新一下

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 245#       发表于 2023-4-22 00:01

MOSS

复旦大学前几个月大出风头的MOSS中文对话模型(负面意义上的)，分别开源了权重与相关数据集

项目地址:https://github.com/OpenLMLab/MOSS

moss-moon-003-base(基础模型，中英双语预训练模型)
moss-moon-003-sft(对话微调模型，使用约110万多轮对话数据微调)
moss-moon-003-sft-plugin(在之前对话模型的基础上微调拓展了使用插件api的能力)

<img src="https://img.saraba1st.com/forum/202304/22/000108yxxdweemwvxejtg0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230422-000051.jpg</strong> (297.54 KB, 下载次数: 0)

下载附件

2023-4-22 00:01 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 246#       发表于 2023-4-22 02:54

COIG

首个大规模、可商用的中文开源指令数据，以下为数据集条目数量，具体细节可查看数据集介绍

经过人工验证的翻译通用指令（67,798）
人工注释的考试指令（63,532）
人类价值观对齐指令（34,471）
反事实修正多轮聊天（13,653）
Leetcode 指令（11,737）

数据集地址:https://huggingface.co/datasets/BAAI/COIG

微信文章介绍:https://mp.weixin.qq.com/s/1hSU5AROH0ZGuDo9oD0bFw

论文地址:https://arxiv.org/abs/2304.07987

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 247#       发表于 2023-4-22 16:38

UltraChat

大规模，多样化的多轮聊天数据，使用ChatGPT Turbo API生成

项目地址:https://github.com/thunlp/UltraChat

<img src="https://img.saraba1st.com/forum/202304/22/163634qgmrq26l22q5r2m2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230422-163557.jpg</strong> (598.9 KB, 下载次数: 0)

下载附件

2023-4-22 16:36 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 248#       发表于 2023-4-22 16:59

CompressGPT

使用CompressGPT(提示压缩)只需 1 行代码即可将您的GPT-4 token使用量(基于LangChain工具的提示的token或其他)减少最多达70%

博客地址:https://musings.yasyf.com/compressgpt-decrease-token-usage-by-70/

完全没想到的技术发展线路<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">不过这类技术的发展也很有趣

<img src="https://img.saraba1st.com/forum/202304/22/165800x78l78a4vmz8cza8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230422-165421__01.jpg</strong> (461.69 KB, 下载次数: 0)

下载附件

2023-4-22 16:58 上传

<img src="https://img.saraba1st.com/forum/202304/22/165800xce7cbb6evehb66b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230422-164536__01.jpg</strong> (508.31 KB, 下载次数: 0)

下载附件

2023-4-22 16:58 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 249#       发表于 2023-4-23 02:15

 本帖最后由 Machinery 于 2023-4-23 02:18 编辑 

VLog

使用如CLIP, BLIP2, GRIT, Whisper, LangChain等工具组合将视频转换为对应的log文档之后提交给chatgpt使之做到视频交互解说等能力

项目地址:https://github.com/showlab/VLog

<img src="https://img.saraba1st.com/forum/202304/23/021501w1n7qzjlbjjjt12b.jpg" referrerpolicy="no-referrer">

<strong>20230423_021452.jpg</strong> (603.22 KB, 下载次数: 0)

下载附件

2023-4-23 02:15 上传

<img src="https://img.saraba1st.com/forum/202304/23/021505a4z4m9gb5go5b500.jpg" referrerpolicy="no-referrer">

<strong>20230423_021442.jpg</strong> (461.31 KB, 下载次数: 0)

下载附件

2023-4-23 02:15 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 250#       发表于 2023-4-23 18:25

geov

GeoV是由Georges Harik设计的大型语言模型，使用具有相对距离的旋转位置嵌入 (RoPER)，开源模型权重参数量为90亿的英文预训练模型

github地址:https://github.com/geov-ai/geov

权重地址:https://huggingface.co/GeoV/GeoV-9b

<img src="https://img.saraba1st.com/forum/202304/23/182425f5lkkv90b5vvvw1l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230423-182406.jpg</strong> (320.89 KB, 下载次数: 0)

下载附件

2023-4-23 18:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  jojog  
##### 251#       发表于 2023-4-23 18:28

后知后觉刚知道vicuna好像不能商用？


*****

####  lvseqiji  
##### 252#       发表于 2023-4-23 18:32

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60580024&amp;ptid=2126390" target="_blank">jojog 发表于 2023-4-23 18:28</a>

后知后觉刚知道vicuna好像不能商用？</blockquote>
基于llama调出来的模型商用都会有问题


*****

####  Machinery  
##### 253#       发表于 2023-4-23 19:19

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60580108&amp;ptid=2126390" target="_blank">lvseqiji 发表于 2023-4-23 18:32</a>
基于llama调出来的模型商用都会有问题</blockquote>
甚至不止是llama模型，stablelm微调用了Alpaca的数据集就直接不能商用了

<img src="https://img.saraba1st.com/forum/202304/23/191944c1ccnc410dg1hs33.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230423-191907.jpg</strong> (81.4 KB, 下载次数: 0)

下载附件

2023-4-23 19:19 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  冰箱研会长  
##### 254#       发表于 2023-4-24 13:28

今天终于用上了vicuna！

尝试了一些paraphrase应用, 不得不说效果比Ludwig.guru的还要好

自由度也更高...


*****

####  Machinery  
##### 255#       发表于 2023-4-24 16:00

GPT4Tools

通过自我指导教导LLM使用工具，GPT4Tools 是一个可以控制多个视觉基础模型的集中式系统。基于LLaMA，使用71K自建指令数据微调。通过分析语言内容，GPT4Tools能够自动判断、控制和利用不同的视觉基础模型，让用户在对话中与图像进行交互。

GTP4Tools主要包含三个部分：用于指令的LLM、用于适配的LoRA、用于提供功能的Visual Agent。它是一个灵活且可扩展的系统，可以轻松扩展以支持更多的工具和功能。例如，用户可以用自己的模型替换现有的 LLM 或工具，或者向系统中添加新工具。 唯一需要的是使用提供的指令对 LoRA 进行微调，该指令教导 LLM 使用提供的工具。

项目页面:https://gpt4tools.github.io/

模型lora:https://huggingface.co/stevengrove/gpt4tools-vicuna-13b-lora

github地址(含数据集):https://github.com/StevenGrove/GPT4Tools

<img src="https://img.saraba1st.com/forum/202304/24/155706nkv6hxfqhewawwk8.jpg" referrerpolicy="no-referrer">

<strong>20230424_155315.jpg</strong> (154.15 KB, 下载次数: 0)

下载附件

2023-4-24 15:57 上传

<img src="https://img.saraba1st.com/forum/202304/24/155709p36ppppftpatcdq6.png" referrerpolicy="no-referrer">

<strong>demo_14.png</strong> (244.42 KB, 下载次数: 0)

下载附件

2023-4-24 15:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 256#       发表于 2023-4-24 17:06

godot-dodo

通过finetune指定代码语言于llm模型，提升模型的专用代码能力，在小众语言上可以与gpt4竞争，同时显著提高生成代码的一次性通过率，范例为GDScript代码生成，但也可以用于其他语言，越特化越强大，包含从数据集收集到微调以及评测的详细教学

项目地址https://github.com/minosvasilias/godot-dodo

评测对比:https://github.com/minosvasilias/godot-dodo/blob/main/models

<img src="https://img.saraba1st.com/forum/202304/24/170519v7xt8qqxr8ewvzvr.png" referrerpolicy="no-referrer">

<strong>results.png</strong> (144.99 KB, 下载次数: 0)

下载附件

2023-4-24 17:05 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 257#       发表于 2023-4-24 19:13

H2O LLM Studio

用于个人或专业用户微调LLM的集成框架和无代码GUI操作界面(简称炼丹)

github地址:https://github.com/h2oai/h2o-llmstudio

<img src="https://img.saraba1st.com/forum/202304/24/191420dga030b34huhbaye.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230424-190947__01.jpg</strong> (228.49 KB, 下载次数: 0)

下载附件

2023-4-24 19:14 上传

<img src="https://img.saraba1st.com/forum/202304/24/191302g01zg4bdg61g4kug.png" referrerpolicy="no-referrer">

<strong>233859311-32aa1f8c-4d68-47ac-8cd9-9313171ff9f9.png</strong> (480.87 KB, 下载次数: 0)

下载附件

2023-4-24 19:13 上传

<img src="https://img.saraba1st.com/forum/202304/24/191302a4sf9h7pt4co9wph.png" referrerpolicy="no-referrer">

<strong>233859315-e6928aa7-28d2-420b-8366-bc7323c368ca.png</strong> (482.71 KB, 下载次数: 0)

下载附件

2023-4-24 19:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 258#       发表于 2023-4-24 19:48

 本帖最后由 Machinery 于 2023-4-24 19:52 编辑 

h2oGPT

一句话目标

<img src="https://img.saraba1st.com/forum/202304/24/194028udojosa8idlgddzv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230424-194015.jpg</strong> (42.47 KB, 下载次数: 0)

下载附件

2023-4-24 19:40 上传

使用的模型基座分别为:
20b参数版本/EleutherAI/gpt-neox-20b
12b参数版本/EleutherAI/pythia-12b
6.9b参数版本/EleutherAI/pythia-6.9b

分别在多种数据集上的基础上继续优化数据集，与各种指令调优数据集同时使用，效果不错，不仅模型，后整理数据集也同时发布了

github项目地址:https://github.com/h2oai/h2ogpt

模型权重仓库:https://huggingface.co/h2oai

Demo演示:https://gpt.h2o.ai/
注:有一定的中文能力，项目有自带webui与便捷的使用方法

<img src="https://img.saraba1st.com/forum/202304/24/194903eb5eth8ql431ebt9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230424-193806.jpg</strong> (142.57 KB, 下载次数: 0)

下载附件

2023-4-24 19:49 上传

很全的开源llm项目相关整理
https://github.com/h2oai/h2ogpt/blob/main/LINKS.md
—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  perfaceNext  
##### 259#       发表于 2023-4-24 20:26

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60595296&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-24 17:06</a>
godot-dodo

通过finetune指定代码语言于llm模型，提升模型的专用代码能力，在小众语言上可以与gpt4竞争， ...</blockquote>
居然godot都有，我惊呆了，回去一定要下载了看看


*****

####  Machinery  
##### 260#       发表于 2023-4-25 11:48

Alpaca-CoT

指令微调Cot(推理链)数据集以及统一推理
github地址:https://github.com/PhoebusSi/Alpaca-CoT

如果不太清楚推理链是什么，可以直接查看数据集的内容即可理解…

<img src="https://img.saraba1st.com/forum/202304/25/114817ebi6vfvk99z69k2f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230425-114632.jpg</strong> (385.64 KB, 下载次数: 0)

下载附件

2023-4-25 11:48 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 261#       发表于 2023-4-25 17:04

Track Anything

追踪一切视频对象并利用多重组件进行长期可记忆视频对象分割/追踪/修改等功能

项目地址:https://github.com/gaomingqi/Track-Anything

组件XMem:https://github.com/hkchengrex/XMem

效果惊人，特别是组件里的XMem，演示太强了，使用Atkinson-Shiffrin记忆模型的长期视频对象分割，其中用到了长短期记忆机制<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">，现在SAM出现之后切割细粒度更高了

<img src="https://img.saraba1st.com/forum/202304/25/170142fu6c46rm36a6ffju.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230425-170044.jpg</strong> (964.69 KB, 下载次数: 0)

下载附件

2023-4-25 17:01 上传

<img src="https://img.saraba1st.com/forum/202304/25/170350bokbp75yct37l31l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230425-170300__01.jpg</strong> (207.04 KB, 下载次数: 0)

下载附件

2023-4-25 17:03 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 262#       发表于 2023-4-25 22:37

WizardLM

通过自进化机制随机深化或者广化等方法指引大语言模型创造复杂任务指令跟随数据集并进行微调，在llama上以同等数目的微调数据集的情况下质量超越vicuna，并在部分条件情况下战胜chatgpt输出

论文地址:https://arxiv.org/abs/2304.12244
github地址:https://github.com/nlpxucan/WizardLM

注:截止发帖时间，github尚未更新，等待研究人员更新

<img src="https://img.saraba1st.com/forum/202304/25/223341fzhgl8vr6llpyl0r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230425-223324.jpg</strong> (201.58 KB, 下载次数: 0)

下载附件

2023-4-25 22:33 上传

<img src="https://img.saraba1st.com/forum/202304/25/223536bnqa2ykkwy8nae8r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230425-223415.jpg</strong> (88.6 KB, 下载次数: 0)

下载附件

2023-4-25 22:35 上传

<img src="https://img.saraba1st.com/forum/202304/25/223540metzrbv8ypst7rrx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230425-223522.jpg</strong> (159.54 KB, 下载次数: 0)

下载附件

2023-4-25 22:35 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 263#       发表于 2023-4-26 16:13

LaMini-LM

从chatgpt3.5turbo中蒸馏生成了258万对指令调优数据集并对多种模型包含不同参数版本进行了微调以及评测，数据集遵循CC BY-NC 4.0非商业授权开源

github地址:https://github.com/mbzuai-nlp/laMini-LM

数据集地址:https://huggingface.co/datasets/MBZUAI/LaMini-instruction

<img src="https://img.saraba1st.com/forum/202304/26/160934otvnrob4zn31ob5m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230426-160924.jpg</strong> (159.96 KB, 下载次数: 0)

下载附件

2023-4-26 16:09 上传

<img src="https://img.saraba1st.com/forum/202304/26/161317lyyxluq5duy5w5ox.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230426-161153.jpg</strong> (220.33 KB, 下载次数: 0)

下载附件

2023-4-26 16:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 264#       发表于 2023-4-27 05:52

DeepFloyd IF

stability ai所推出的新架构图像模型，原理上参考了imagen的级联放大与t5文本编码器，使模型能够理解并书并生成“文字”，鸽了好几个月，总算要公布了

推特账号:https://twitter.com/deepfloydai?s=09

github项目地址:https://github.com/deep-floyd/IF

<img src="https://img.saraba1st.com/forum/202304/27/055211gxzx4a1a1mc428q8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230427-054840.jpg</strong> (720.86 KB, 下载次数: 0)

下载附件

2023-4-27 05:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  ffcloudy12  
##### 265#       发表于 2023-4-27 19:36

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60629089&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-27 05:52</a>

DeepFloyd IF

stability ai所推出的新架构图像模型，原理上参考了imagen的级联放大与t5文本编码器，使模型 ...</blockquote>
这个可以支持生成中文吗


*****

####  Machinery  
##### 266#       发表于 2023-4-27 19:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60638914&amp;ptid=2126390" target="_blank">ffcloudy12 发表于 2023-4-27 19:36</a>
这个可以支持生成中文吗</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">t5编码器生成中文的实验有其他论文，实验过也确实是可以的，但这个行不行就不好说了

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  真田丸  
##### 267#       发表于 2023-4-27 20:09

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60372917&amp;ptid=2126390" target="_blank">jimmy_nyc 发表于 2023-4-8 12:04</a>

Tavern AI + GPT-3.5-turbo，prompt设置好后跑文字冒险，不能更爽</blockquote>
啥prompt都能设吗？


*****

####  Machinery  
##### 268#       发表于 2023-4-27 20:20

SCM4LLMs

使用自控制记忆系统释放LLM模型的长对话与长摘要能力，SCM 系统可以与任何 LLM 集成，使他们无需任何修改或微调即可处理超长文本

SCM 系统使未针对多轮对话优化的 LLM 能够实现与 ChatGPT 相当的多轮对话能力，并且在涉及超长文档摘要或长期对话的场景中优于 ChatGPT

论文地址:https://arxiv.org/abs/2304.13343

github项目地址:https://github.com/wbbeyourself/SCM4LLMs

注:此项目目前仍在迭代中，Alpaca, Vicuna,ChatGLM等模型会很快进行支持

<img src="https://img.saraba1st.com/forum/202304/27/201707trxtxtf5nkrtm8mz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230427-201315.jpg</strong> (291.61 KB, 下载次数: 0)

下载附件

2023-4-27 20:17 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 269#       发表于 2023-4-27 21:05

 本帖最后由 Machinery 于 2023-4-27 21:07 编辑 

SegmentAnyRGBD

使用SAM从图片中提取MASK并进行视觉几何信息深度图渲染，并转换为可视化3D空间分割，可提高AI对于物体的整体辨识能力

项目地址:https://github.com/Jun-CEN/SegmentAnyRGBD

<img src="https://img.saraba1st.com/forum/202304/27/210305bq38uk5bqbk8j3qz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230427-210255.jpg</strong> (361.13 KB, 下载次数: 0)

下载附件

2023-4-27 21:03 上传

<img src="https://img.saraba1st.com/forum/202304/27/210649sm9w0wilzizmwjjo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230427-210637.jpg</strong> (282.81 KB, 下载次数: 0)

下载附件

2023-4-27 21:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 270#       发表于 2023-4-28 02:49

项目预告 LLaMA-Adapter V2 多模态指令跟随模型

LLaMA-Adapter的v2版本，增加了多模态输入，等待技术报告和模型权重

<img src="https://img.saraba1st.com/forum/202304/28/024920bph4phhap3353fp3.png" referrerpolicy="no-referrer">

<strong>multimodal.png</strong> (842.4 KB, 下载次数: 0)

下载附件

2023-4-28 02:49 上传

<img src="https://img.saraba1st.com/forum/202304/28/024724gbvaasvbj8k8bsk7.png" referrerpolicy="no-referrer">

<strong>multi_model_example_4.png</strong> (283.57 KB, 下载次数: 0)

下载附件

2023-4-28 02:47 上传

<img src="https://img.saraba1st.com/forum/202304/28/024724as5nq40v93xvsvn4.png" referrerpolicy="no-referrer">

<strong>multi_model_example_2.png</strong> (256.41 KB, 下载次数: 0)

下载附件

2023-4-28 02:47 上传

<img src="https://img.saraba1st.com/forum/202304/28/024724yzddke1kbe4bz7ek.png" referrerpolicy="no-referrer">

<strong>multi_model_example_1.png</strong> (292.52 KB, 下载次数: 0)

下载附件

2023-4-28 02:47 上传

<img src="https://img.saraba1st.com/forum/202304/28/024724b91n4seua179k5a9.png" referrerpolicy="no-referrer">

<strong>multi_model_example_3.png</strong> (266.49 KB, 下载次数: 0)

下载附件

2023-4-28 02:47 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 271#       发表于 2023-4-28 17:00

TANGO

使用冻结FLAN T5文本编码器进行指令引导扩散的文本到音频生成，音效生成方面效果十分不错，模型和推理训练代码均已开源

github地址:https://github.com/declare-lab/tango

模型权重下载:https://huggingface.co/declare-lab/tango

项目地址:https://tango-web.github.io/

<img src="https://img.saraba1st.com/forum/202304/28/165758r0359q000a29252y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230428-165630.jpg</strong> (401.21 KB, 下载次数: 0)

下载附件

2023-4-28 16:57 上传

调用十分简单

<img src="https://img.saraba1st.com/forum/202304/28/170038tefdy7ehhydzy7hj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230428-170027.jpg</strong> (73.59 KB, 下载次数: 0)

下载附件

2023-4-28 17:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 272#       发表于 2023-4-28 18:09

mPLUG-Owl

一种面向多模态语言模型的模块化的训练范式。
能学习与语言空间相适应的视觉知识，并支持在多模态场景下进行多轮对话。
涌现多图关系理解，场景文本理解和基于视觉的文档理解等能力。(注:根据说明，训练数据并不包含多图的样本，但实际使用中出现了多图关联理解能力)

<img src="https://img.saraba1st.com/forum/202304/28/180932b4vzl16idiqhiiy9.png" referrerpolicy="no-referrer">

<strong>case_1.png</strong> (382.14 KB, 下载次数: 0)

下载附件

2023-4-28 18:09 上传

<img src="https://img.saraba1st.com/forum/202304/28/180938inzgqtpggqto33pv.png" referrerpolicy="no-referrer">

<strong>case_2.png</strong> (384.86 KB, 下载次数: 0)

下载附件

2023-4-28 18:09 上传

提出了针对视觉相关指令的测评集OwlEval，用以评估多模态语言模型的对带有视觉信息上下文的理解能力。

已经开放权重下载(包含中文说明):https://github.com/X-PLUG/mPLUG-Owl/blob/main/README_zh.md

演示demo:https://modelscope.cn/studios/damo/mPLUG-Owl/summary

<img src="https://img.saraba1st.com/forum/202304/28/180626bl1ewc3w33ndxedx.png" referrerpolicy="no-referrer">

<strong>model.png</strong> (333.98 KB, 下载次数: 0)

下载附件

2023-4-28 18:06 上传

<img src="https://img.saraba1st.com/forum/202304/28/180728u91ll1f34lkl7a97.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230428-180715.jpg</strong> (188.18 KB, 下载次数: 0)

下载附件

2023-4-28 18:07 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 273#       发表于 2023-4-28 18:44

 本帖最后由 Machinery 于 2023-4-28 18:47 编辑 

ChatVideo

以轨迹追踪为中心的多模态多功能视频理解系统

我愿称之为截止目前最强的视频时空理解，项目地址有实际演示视频

论文地址:https://arxiv.org/abs/2304.14407

<img src="https://img.saraba1st.com/forum/202304/28/184706ibfvd7iz7ofi0fx0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230428-184353__01.jpg</strong> (223.54 KB, 下载次数: 0)

下载附件

2023-4-28 18:47 上传

项目地址:https://www.wangjunke.info/ChatVideo/

<img src="https://img.saraba1st.com/forum/202304/28/184156evpeuz88ocp3ttpt.png" referrerpolicy="no-referrer">

<strong>architecture.png</strong> (776.04 KB, 下载次数: 0)

下载附件

2023-4-28 18:41 上传

<img src="https://img.saraba1st.com/forum/202304/28/184225duguzj04jgbq6bxa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230428-184211.jpg</strong> (604.27 KB, 下载次数: 0)

下载附件

2023-4-28 18:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 274#       发表于 2023-4-29 04:23

Fastchat-T5

以问答的形式处理 ShareGPT 数据。每个 ChatGPT 响应都作为答案处理，而用户与 ChatGPT 之间的先前对话作为问题处理

可商用，在参数量少于Dolly-V2四倍的情况下优于输出表现前者，模型基座Flan-t5-xl（3B 参数/自回归模型），数据集为ShareGPT

模型仓库:https://huggingface.co/lmsys/fastchat-t5-3b-v1.0

可使用FastChat快速(一行代码)加载模型:https://github.com/lm-sys/FastChat#FastChat-T5

<img src="https://img.saraba1st.com/forum/202304/29/042418y5ltgymomtym47o5.jpg" referrerpolicy="no-referrer">

<strong>20230429_042406.jpg</strong> (227.03 KB, 下载次数: 0)

下载附件

2023-4-29 04:24 上传

<img src="https://img.saraba1st.com/forum/202304/29/042418aqrc5nrnhwe9z81f.jpg" referrerpolicy="no-referrer">

<strong>20230429_042408.jpg</strong> (113.89 KB, 下载次数: 0)

下载附件

2023-4-29 04:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 275#       发表于 2023-4-29 04:42

 本帖最后由 Machinery 于 2023-4-29 04:45 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60508132&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-18 17:19</a>
LongForm

今日最离谱项目，直接看数据，简单的prompt合成技巧能拉这么多点就离谱，这数据已经完全达到了ch ...</blockquote>
LongForm-LLaMA-7B-diff

<img src="https://img.saraba1st.com/forum/202304/29/044519cz48zol89i9t3p8p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230429-044448.jpg</strong> (138.6 KB, 下载次数: 0)

下载附件

2023-4-29 04:45 上传

本次更新的是使用LongForm方法微调的LLaMA7b，性能评测如下

模型权重差异合并文件地址(教程readme中有写):https://huggingface.co/akoksal/LongForm-LLaMA-7B-diff

<img src="https://img.saraba1st.com/forum/202304/29/044344vodzkg283woomgdy.jpg" referrerpolicy="no-referrer">

<strong>ExponentAsset-05767f5bc9411a35fb0adb06e0ab2e74.jpg</strong> (90.94 KB, 下载次数: 0)

下载附件

2023-4-29 04:43 上传

<img src="https://img.saraba1st.com/forum/202304/29/044108je46yz3u8j6ue8yz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230429-043859.jpg</strong> (110.65 KB, 下载次数: 0)

下载附件

2023-4-29 04:41 上传

<img src="https://img.saraba1st.com/forum/202304/29/044112kob1vurboruuuqpa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230429-043948.jpg</strong> (164.04 KB, 下载次数: 0)

下载附件

2023-4-29 04:41 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 276#       发表于 2023-4-29 05:03

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60629089&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-27 05:52</a>
DeepFloyd IF

stability ai所推出的新架构图像模型，原理上参考了imagen的级联放大与t5文本编码器，使模型 ...</blockquote>
已开源

具体介绍请看引用

官方项目介绍地址:https://stability.ai/blog/deepfloyd-if-text-to-image-model

权重地址(需要同意协议): https://huggingface.co/DeepFloyd

<img src="https://img.saraba1st.com/forum/202304/29/050314k60a675jav5mzm2o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230429-050302.jpg</strong> (180.54 KB, 下载次数: 0)

下载附件

2023-4-29 05:03 上传

Gradio demo演示: https://huggingface.co/spaces/DeepFloyd/IF

官方讨论区: https://linktr.ee/deepfloyd

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 277#       发表于 2023-4-29 05:43

Graphit

用于各种图像编辑任务的统一框架，构架与各种功能类似controlnet，不过是在统一构架的基础上，这个大家应该很熟悉了

github项目地址:https://github.com/navervision/Graphit

<img src="https://img.saraba1st.com/forum/202304/29/054050agxpa8k9wpk6ttj4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230429-054009.jpg</strong> (321.52 KB, 下载次数: 0)

下载附件

2023-4-29 05:40 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 278#       发表于 2023-4-29 17:46

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60643047&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-28 02:49</a>
项目预告 LLaMA-Adapter V2 多模态指令跟随模型

LLaMA-Adapter的v2版本，增加了多模态输入，等待技术报告 ...</blockquote>
LLaMA-Adapter-V2

相比LLaMA-Adapter(以及minigpt4与LLAVA等)有以下变化

1.解锁了更多可学习的参数，但总体上依然是参数高效的微调，且使用偏置微调策略提升了模型的指令跟随能力
2.使用早期融合策略解决了图像-文本对训练对模型的指令跟随能力干扰的影响
3.在没有使用视觉指令跟随数据集(minigpt4与LLaVA都使用了)的情况下达到了对于VQA提问更丰富的细粒度与准确性，同时多轮对话能力与指令跟随能力丝毫不弱
4.使用了模块化的设计，可以融合其他专家模型进行视觉辅助，且效果不错
5.全程只用了coco caption数据集就打赢了blip2
6. 在GPT4的输出质量对比中战胜chatgpt，并且大幅跑赢Vicuna

<img src="https://img.saraba1st.com/forum/202304/29/174122gwwzu2efufsw00lw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230429-173958.jpg</strong> (102.58 KB, 下载次数: 0)

下载附件

2023-4-29 17:41 上传

<img src="https://img.saraba1st.com/forum/202304/29/174122ofn67eu66euxwfsz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230429-174011.jpg</strong> (111.8 KB, 下载次数: 0)

下载附件

2023-4-29 17:41 上传

项目地址:https://github.com/ZrrSkywalker/LLaMA-Adapter/tree/main/llama_adapter_v2_chat65b

演示视频:https://www.youtube.com/watch?v=GAJyWkkSd8M

论文地址:https://github.com/ZrrSkywalker/LLaMA-Adapter/blob/main/LLaMA-Adapter-V2-arXiv.pdf

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  pf67  
##### 279#       发表于 2023-4-29 20:14

技术迭代得太快了

—— 来自 OPPO PCLM10, Android 12上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4


*****

####  Machinery  
##### 280#       发表于 2023-4-29 23:15

Multi-Party Chat

要介绍Multi-Party Chat得先介绍Light，Light研究项目是meta搞的基于文本的AI对话研究平台，部分也有开源文字冒险游戏进程的相关研究，既然涉及了这两方，就需要达到能够进行多方模拟会话，设计群组中的对话代理与模型架构

Light的github项目地址:https://github.com/facebookresearch/LIGHT

而Multi-Party Chat正是使用Light作为平台研究的多方会话与情景聊天探索，可以参考的类似项目有Ai Dungeon

论文重点研究了对话流程中，对应的不同角色代理何时说话，或保持沉默，其次则是对于自我身份的明确性以及对于自我身份的认知与状态理解(即使只是两方会谈对于gpt4或者chatgpt也是非常困难的挑战，更别提多方会谈了)

<img src="https://img.saraba1st.com/forum/202304/29/230155soi0jqgglqgjohjj.jpg" referrerpolicy="no-referrer">

<strong>20230429_230147.jpg</strong> (149.27 KB, 下载次数: 0)

下载附件

2023-4-29 23:01 上传

论文地址:https://arxiv.org/abs/2304.13835

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  fat  
##### 281#       发表于 2023-4-30 00:05

<img src="https://static.saraba1st.com/image/smiley/face2017/001.png" referrerpolicy="no-referrer">太快了。目不暇接


*****

####  okok123  
##### 282#       发表于 2023-4-30 09:44

多是多，但没看到哪个全能产品或细分领域产品能接近3.5。


*****

####  Machinery  
##### 283#       发表于 2023-4-30 15:28

 本帖最后由 Machinery 于 2023-4-30 15:30 编辑 

chatVRM

在浏览器中与3D角色进行对话，模型的操作由three-vrm(https://github.com/pixiv/three-vrm)支持，会话由ChatGPT API支持，声音合成由Koeiro API支持，理论上经过一定的修改可以做到全局本地化

项目介绍:https://inside.pixiv.blog/2023/04/28/160000

github项目地址:https://github.com/pixiv/ChatVRM

<img src="https://img.saraba1st.com/forum/202304/30/152518e0fhrtv0uo3tw3ht.jpg" referrerpolicy="no-referrer">

<strong>20230430_151602.jpg</strong> (231.55 KB, 下载次数: 0)

下载附件

2023-4-30 15:25 上传

<img src="https://img.saraba1st.com/forum/202304/30/152836s5zpa5u88usptvu8.png" referrerpolicy="no-referrer">

<strong>20230428122034.png</strong> (383.21 KB, 下载次数: 0)

下载附件

2023-4-30 15:28 上传

<img src="https://img.saraba1st.com/forum/202304/30/153000drjtj070kxzwkaz7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230430-152944.jpg</strong> (150.21 KB, 下载次数: 0)

下载附件

2023-4-30 15:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 284#       发表于 2023-5-1 00:09

 本帖最后由 Machinery 于 2023-5-1 00:11 编辑 

Otter

Otter，基于OpenFlamingo的指令调优模型，已针对上下文进行了多模态指令跟随(含上下文学习)微调，介绍请看下方，预计未来还会公开Otter-9B for Videos与Otter-15B两种模型

项目地址:https://github.com/Luodian/otter

演示demo:https://otter.cliangyu.com/

<img src="https://img.saraba1st.com/forum/202305/01/000608aqa7aia0zw8qhgbh.jpg" referrerpolicy="no-referrer">

<strong>257aa235-f0a0-4656-9ae4-0d201873ead6.jpg</strong> (126.04 KB, 下载次数: 0)

下载附件

2023-5-1 00:06 上传

<img src="https://img.saraba1st.com/forum/202305/01/000614r8hxfouivufhvvib.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230501-000555.jpg</strong> (412.29 KB, 下载次数: 0)

下载附件

2023-5-1 00:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  didapick  
##### 285#       发表于 2023-5-1 00:49

技术迭代得太快了+1


*****

####  schneehertz  
##### 286#       发表于 2023-5-1 01:35

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60678592&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-5-1 00:09</a>

Otter

Otter，基于OpenFlamingo的指令调优模型，已针对进行了多模态指令跟随(含上下文范例学习)微调，介绍 ...</blockquote>

<img src="https://img.saraba1st.com/forum/202305/01/013519vbalbbcacb2bagca.png" referrerpolicy="no-referrer">

<strong>image.png</strong> (169.48 KB, 下载次数: 0)

下载附件

2023-5-1 01:35 上传

错的很离谱


*****

####  Machinery  
##### 287#       发表于 2023-5-1 02:44

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60679609&amp;ptid=2126390" target="_blank">schneehertz 发表于 2023-5-1 01:35</a>
错的很离谱</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">拟合是这样的，这类一般是靠数据集泛化，不太可能达到人类视觉皮层的鲁棒性，如果真需要可以使用前边的LLama adapter v2配合多类专家模型辅助提取视觉特征，因为LLama adapter v2单独使用的话也有数据集外分布失败案例

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 288#       发表于 2023-5-1 03:11

 本帖最后由 Machinery 于 2023-5-1 03:24 编辑 

InstructCTG

使用自然语言指令生成可约束的受控文本，不涉及直接解码过程修改，使用一些弱监督方法进行增强合成数据集生成，配合元上下文学习(相关介绍https://zhuanlan.zhihu.com/p/450731895)进行微调

<img src="https://img.saraba1st.com/forum/202305/01/025303bd0zduulu0clhd90.jpg" referrerpolicy="no-referrer">

<strong>20230501_025137.jpg</strong> (183.73 KB, 下载次数: 0)

下载附件

2023-5-1 02:53 上传

效果如图

<img src="https://img.saraba1st.com/forum/202305/01/031208imw5rwtmvppjpsup.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230501-031135__01.jpg</strong> (315.95 KB, 下载次数: 0)

下载附件

2023-5-1 03:12 上传

论文地址:https://arxiv.org/abs/2304.14293

github地址:https://github.com/MichaelZhouwang/InstructCTG

注:Will release the data generation pipeline, training script, and pre-trained models 【soon】

注2:值得一提的是论文作者特别提到chatgpt对于风格约束与长度约束生成非常不擅长<img src="https://static.saraba1st.com/image/smiley/face2017/066.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 289#       发表于 2023-5-1 03:29

 本帖最后由 Machinery 于 2023-5-1 03:30 编辑 

PandaLM

可重现的自动化语言模型评估，目的是离线自动化模型性能与基准评测，PandaLM-7B在评测准确性方面达到了ChatGPT的94%

github项目地址:https://github.com/WeOpenML/PandaLM

<img src="https://img.saraba1st.com/forum/202305/01/032737tvfszvfwkmsagkmz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230501-032718.jpg</strong> (418.57 KB, 下载次数: 0)

下载附件

2023-5-1 03:27 上传

<img src="https://img.saraba1st.com/forum/202305/01/032910fkhe6rwjzhch7fee.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230501-032822.jpg</strong> (94.53 KB, 下载次数: 0)

下载附件

2023-5-1 03:29 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  某浩  
##### 290#       发表于 2023-5-1 15:14

楼主怎么被砍头了。ai太快了，目不暇接，反而不知道怎么往哪方面应用了


*****

####  Machinery  
##### 291#       发表于 2023-5-1 18:20

PMC-LLaMA

在医学论文上进一步微调LLaMA7b，微调总共使用了480万篇生物医学学术论文

<img src="https://img.saraba1st.com/forum/202305/01/181516mwyh1my9m9d9z7jd.jpg" referrerpolicy="no-referrer">

<strong>1-0.jpg</strong> (31.92 KB, 下载次数: 0)

下载附件

2023-5-1 18:15 上传

论文地址:arxiv.org/abs/2304.14454

模型权重:https://huggingface.co/chaoyi-wu/PMC_LLAMA_7B

训练详细过程与代码:https://github.com/chaoyi-wu/PMC-LLaMA

最有趣的还是这张关于不同微调方式的loss曲线

<img src="https://img.saraba1st.com/forum/202305/01/181756bwon4uyyevoe42jw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230501-181725.jpg</strong> (97.77 KB, 下载次数: 0)

下载附件

2023-5-1 18:17 上传

以及性能影响

<img src="https://img.saraba1st.com/forum/202305/01/181959gw228a2vet5cfp84.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230501-181929.jpg</strong> (88.23 KB, 下载次数: 0)

下载附件

2023-5-1 18:19 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 292#       发表于 2023-5-1 19:32

 本帖最后由 Machinery 于 2023-5-1 19:34 编辑 

Megatron-LLM

英伟达开源的系列大型语言模型，按不同的模型基础架构不同与参数分为多个类型，大概是为了推广NeMo Megatron而推出的

不同版本与权重介绍，包含数据集介绍
nvidia的hugface主页:https://huggingface.co/nvidia

<img src="https://img.saraba1st.com/forum/202305/01/193211waxsjadkr88rlzzf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230501-193155.jpg</strong> (235.7 KB, 下载次数: 0)

下载附件

2023-5-1 19:32 上传

NeMo Megatron的相关介绍:https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/intro.html

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 293#       发表于 2023-5-2 00:49

AUTOMATIC1111/stable-diffusion-webui更新了1.1.0zip版

zip官方打包版:https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases

注:不需要任何依赖和环境，just 双击 run.bat

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 294#       发表于 2023-5-2 03:52

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60519092&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-19 13:56</a>
Guanaco

llama概念验证模型，拥有多语言，使用system prompt进行准确的外部知识整合推理(类似gpt4的泛化知 ...</blockquote>
GuanacoVQA

GuanacoVQA是Guanaco的升级版，对齐了多模态VQA(视觉问答任务)任务，使用的线性连接层对齐视觉特征，灵感来源于MiniGPT-4，同时继承了Guanaco的多语言能力

GuanacoVQA权重地址:https://huggingface.co/JosephusCheung/GuanacoVQA

bilibili视频介绍:https://www.bilibili.com/video/BV1rg4y1L73w/

具体介绍请查看b站评论区置顶

<img src="https://img.saraba1st.com/forum/202305/02/035240flwhb95l4xl5g51r.jpg" referrerpolicy="no-referrer">

<strong>20230502_035232.jpg</strong> (103.85 KB, 下载次数: 0)

下载附件

2023-5-2 03:52 上传

<img src="https://img.saraba1st.com/forum/202305/02/035240gyitj4zbavfbubtt.jpg" referrerpolicy="no-referrer">

<strong>20230502_035230.jpg</strong> (116.14 KB, 下载次数: 0)

下载附件

2023-5-2 03:52 上传

<img src="https://img.saraba1st.com/forum/202305/02/035240xwb6cnm6ezbs6m6m.jpg" referrerpolicy="no-referrer">

<strong>20230502_035229.jpg</strong> (111.39 KB, 下载次数: 0)

下载附件

2023-5-2 03:52 上传

<img src="https://img.saraba1st.com/forum/202305/02/035240tb8mmbfba1qmffqq.jpg" referrerpolicy="no-referrer">

<strong>20230502_035227.jpg</strong> (53.05 KB, 下载次数: 0)

下载附件

2023-5-2 03:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 295#       发表于 2023-5-2 04:56

Retrieval-based-Voice-Conversion-WebUI

基于VITS的简单易用的语音转换（变声器）框架，简称RVC，github项目内有中文说明，同时也有实时合成相关项目，B站也有大量攻略视频，这里就不废话了

github项目地址:https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI

视频演示地址:https://www.bilibili.com/video/BV1pm4y1z7Gm/

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 296#       发表于 2023-5-2 05:26

 本帖最后由 Machinery 于 2023-5-2 05:27 编辑 

GPT2(023)

GPT2构架的最小型号语言模型，参数量为124m(million)，使用2.23B的token训练，接近chinchilla完美缩放比例(1.3B from common crawl sites from 2023, 540M from ArXiv, and 390M from GitHub)，因为是预训练模型所以并不怎么支持指令跟随，请尝试使用续写方式生成

权重下载地址:https://huggingface.co/crumb/gpt2023

总结:不错的玩具<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202305/02/052552pxx5xo66h10wzaoa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230502-052537.jpg</strong> (14.49 KB, 下载次数: 0)

下载附件

2023-5-2 05:25 上传

<img src="https://img.saraba1st.com/forum/202305/02/052512neileg3uxy3yei4i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230502-052402.jpg</strong> (227.15 KB, 下载次数: 0)

下载附件

2023-5-2 05:25 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  icer  
##### 297#       发表于 2023-5-2 08:04

本地部署的还是没有达到chatGPT水平啊，要是有专业特化（比如代码）可以达到同水平的就爽了


*****

####  Machinery  
##### 298#       发表于 2023-5-2 19:34

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60691679&amp;ptid=2126390" target="_blank">icer 发表于 2023-5-2 08:04</a>
本地部署的还是没有达到chatGPT水平啊，要是有专业特化（比如代码）可以达到同水平的就爽了 ...</blockquote>
可以试试phind.com，而且还支持中文<img src="https://static.saraba1st.com/image/smiley/face2017/075.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 299#       发表于 2023-5-2 21:49

fastdup

快速，大规模管理、清理和整理视觉数据，用于图像和视频数据集分析的无监督开源预处理工具，简单来说就是数据集管理

github项目地址:[https://github.com/visual-layer/fastdup](https://github.com/visual-layer/fastdup)

<img src="https://img.saraba1st.com/forum/202305/02/214831s01ov9nxrs1tsnxt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230502-214805.jpg</strong> (209.01 KB, 下载次数: 0)

下载附件

2023-5-2 21:48 上传

<img src="https://img.saraba1st.com/forum/202305/02/214831uonu6pn6ux3ouojz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230502-214751.jpg</strong> (313.27 KB, 下载次数: 0)

下载附件

2023-5-2 21:48 上传


*****

####  塔奇克马  
##### 300#       发表于 2023-5-2 22:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60699052&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-5-2 19:34</a>
可以试试phind.com，而且还支持中文

—— 来自 S1Fun</blockquote>
咋回事？<blockquote>Thank you for using Phind. Our priority is to provide the best possible experience for all our users. At the moment, we feel that we aren't fully equipped to support Chinese users in the way they deserve. We face challenges in understanding specific use-cases and comprehending feedback due to the language barrier and the unique context of software engineering in China. We're committed to offering a great experience for everyone, so for now, we're focusing on the users we understand best while working on improvements. We apologize for any inconvenience this might cause and appreciate your understanding. Rest assured, we're striving to grow and evolve so we can offer an even better product for you and our users in China in the future.</blockquote>
—— 来自 HUAWEI HLK-AL00, Android 9上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4


*****

####  Machinery  
##### 301#       发表于 2023-5-3 00:48

 本帖最后由 Machinery 于 2023-5-3 00:50 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60701295&amp;ptid=2126390" target="_blank">塔奇克马 发表于 2023-5-2 22:30</a>
咋回事？

—— 来自 HUAWEI HLK-AL00, Android 9上的 S1Next-鹅版 v2.5.4</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">我之前用的时候是完全支持中文的，而且能力特别强

ps:测试了一下，还是支持的啊，而且不止中文编程，其他任何回答也是可以的，不过现在好像笨了一点，以前只要提问是中文就会用中文回答，现在偶尔会用英文

<img src="https://img.saraba1st.com/forum/202305/03/004719py43mm3usituy8tq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230503-004652.jpg</strong> (161.62 KB, 下载次数: 0)

下载附件

2023-5-3 00:47 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  冬眠的龙凰  
##### 302#       发表于 2023-5-3 01:33

看不过来，还是等社区搞出个各方面都好用还能本地部署的大宝贝再说吧


*****

####  Machinery  
##### 303#       发表于 2023-5-3 03:07

jsonformer

让模型输出结构化JSON文本很困难
解决方案：只让模型生成文本中的关键数据token

<img src="https://img.saraba1st.com/forum/202305/03/030652rw1o3s20asowroai.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230503-030532.jpg</strong> (224.06 KB, 下载次数: 0)

下载附件

2023-5-3 03:06 上传

github项目地址:https://github.com/1rgs/jsonformer

<img src="https://img.saraba1st.com/forum/202305/03/030606cvvfl4xvnv5m5vxt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230503-030454.jpg</strong> (115.19 KB, 下载次数: 0)

下载附件

2023-5-3 03:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  fat  
##### 304#       发表于 2023-5-3 17:32

 本帖最后由 fat 于 2023-5-3 17:37 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60678592&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-5-1 00:09</a>

Otter

Otter，基于OpenFlamingo的指令调优模型，已针对进行了多模态指令跟随(含上下文范例学习)微调，介绍 ...</blockquote>
Otter草。<img src="https://static.saraba1st.com/image/smiley/face2017/035.png" referrerpolicy="no-referrer">

第一个图能认出是美短，很不错。

但是第二张理解错了，觉得是在“走过”房间

S_ZMWWR[1}KQZ`J9K)RWELQ.png
(46.9 KB, 下载次数: 0)

下载附件

2023-5-3 17:37 上传

<img src="https://img.saraba1st.com/forum/202305/03/173707kx6omq6oq6wqvyha.png" referrerpolicy="no-referrer">


*****

####  Machinery  
##### 305#       发表于 2023-5-3 17:46

replit-code-v1-3b

AI编程独角兽公司Replit开源的编程LLM，replit-code-v1-3b拥有27亿参数，在5250亿 token训练集上训练，性能优越，授权协议CC BY-SA-4.0允许商用

具体介绍可以看这里:https://www.qbitai.com/2023/04/49667.html

支持语言:Markdown, Java, JavaScript, Python, TypeScript, PHP, SQL, JSX, reStructuredText, Rust, C, CSS, Go, C++, HTML, Vue, Ruby, Jupyter Notebook, R, Shell

hugface权重仓库:https://huggingface.co/replit/replit-code-v1-3b

演示demo:https://huggingface.co/spaces/replit/replit-code-v1-3b-demo

值得一提的是，本次开源replit-code-v1-3b并非finetune版本，finetune版本依照评估性能应该更强

<img src="https://img.saraba1st.com/forum/202305/03/174554yj8qk4kj3t8cm8k0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230503-174125.jpg</strong> (210.83 KB, 下载次数: 0)

下载附件

2023-5-3 17:45 上传

<img src="https://img.saraba1st.com/forum/202305/03/174559bl2doeeoceklwxcx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230503-174136.jpg</strong> (261.59 KB, 下载次数: 0)

下载附件

2023-5-3 17:45 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 306#       发表于 2023-5-3 18:17

Perfusion

全称Key-Locked Rank One Editing for Text-to-Image Personalization，一种新的文本到图像个性化训练生成方法，Perfusion只需100KB的模型大小，就可以创造性地描绘个性化对象，允许显著改变它们的外观，同时保持它们的个性化特征，使用被称之为“键锁”的新颖机制，Perfusion可以将单独学习的概念组合成单个生成的图像

项目主页:https://research.nvidia.com/labs/par/Perfusion/

工作原理

<img src="https://img.saraba1st.com/forum/202305/03/181156if0lrfzlkzfrsoo9.png" referrerpolicy="no-referrer">

<strong>Architecture.png</strong> (528.16 KB, 下载次数: 0)

下载附件

2023-5-3 18:11 上传

可以轻松创建吸引人的图像

<img src="https://img.saraba1st.com/forum/202305/03/181341m4ehrbdm88lazmaa.jpg" referrerpolicy="no-referrer">

<strong>597eefd2-b1fe-48b8-b78d-a5fe795fd0c4.jpg</strong> (130.57 KB, 下载次数: 0)

下载附件

2023-5-3 18:13 上传

<img src="https://img.saraba1st.com/forum/202305/03/181348e18z4an822h2m60w.jpg" referrerpolicy="no-referrer">

<strong>12c023d2-9c8f-406e-bac1-80966bcfefd2.jpg</strong> (138.78 KB, 下载次数: 0)

下载附件

2023-5-3 18:13 上传

能够在推理时控制视觉保真度和文本对齐之间的权衡，高偏差值会降低概念的影响，而低偏差值会使其更具影响力

<img src="https://img.saraba1st.com/forum/202305/03/181609jq2oc92cboh22w23.jpg" referrerpolicy="no-referrer">

<strong>b16e7dd2-5d00-42e5-a306-7486d7dc5f93.jpg</strong> (104.84 KB, 下载次数: 0)

下载附件

2023-5-3 18:16 上传

1-shot 个性化，当使用单个图像进行训练时，可以生成具有高视觉保真度和文本对齐的图像。

<img src="https://img.saraba1st.com/forum/202305/03/181700hkkkhihc0h8sz7sv.jpg" referrerpolicy="no-referrer">

<strong>0923f1f3-14ca-4493-8eeb-54edb1060bf2.jpg</strong> (140.5 KB, 下载次数: 0)

下载附件

2023-5-3 18:17 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 307#       发表于 2023-5-4 00:02

 本帖最后由 Machinery 于 2023-5-4 00:06 编辑 

PickScore

用于文本到图像生成的用户偏好的开放数据集与相关开源模型，研究组专门构架了一个web图片生成评价应用网站(网址:https://pickapic.io/)收集人类对于生成图片的评价以构造美学数据集，并使用Pick-a-Pic数据集对CLIP-H模型进行微调后，效果更好，甚至超越了人类专家水平(下方参考图)

论文地址:https://arxiv.org/abs/2305.01569

github项目地址:https://github.com/yuvalkirstain/PickScore

权重下载与使用:https://huggingface.co/yuvalkirstain/PickScore_v1

<img src="https://img.saraba1st.com/forum/202305/04/000406r63uk6f4hk2h4728.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230503-234308__01.jpg</strong> (55.25 KB, 下载次数: 0)

下载附件

2023-5-4 00:04 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 308#       发表于 2023-5-4 01:02

 本帖最后由 Machinery 于 2023-5-4 01:04 编辑 

OpenLLaMA预览版

对于LLaMA模型的完全开源开放复现，详细介绍请看下方，目前已在2000亿Token的RedPajama训练集上训练，项目组预计OpenLLaMA的性能在完成对 1 万亿令牌的训练后，将进一步提高

github项目地址:https://github.com/openlm-research/open_llama

<img src="https://img.saraba1st.com/forum/202305/04/010134rttu2gzg2gjq2jgq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230504-005904.jpg</strong> (375.63 KB, 下载次数: 0)

下载附件

2023-5-4 01:01 上传

目前的性能对比:

<img src="https://img.saraba1st.com/forum/202305/04/010410llzlcccekqmkellw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230504-010215__01.jpg</strong> (156.44 KB, 下载次数: 0)

下载附件

2023-5-4 01:04 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  大江户战士  
##### 309#       发表于 2023-5-4 01:35

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60703011&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-5-3 00:48</a>

我之前用的时候是完全支持中文的，而且能力特别强

ps:测试了一下，还是支持的啊，而且不止中文编 ...</blockquote>
显然是量化了，GPTQ之类的量化很容易造成多语言能力下降


*****

####  Machinery  
##### 310#       发表于 2023-5-4 02:43

 本帖最后由 Machinery 于 2023-5-4 02:46 编辑 

MEMIT

GPT模型内部事实定位与编辑，研究组的前置工作ROME中提出了于GPT系列模型内部定位黑箱知识(事实)的工作，并初步提出了编辑定位的方法，且拥有泛化知识(事实)的能力，在后续MEMIT中，将编辑数量拓展到了数千个对应的知识样本中

前置工作ROME介绍:https://rome.baulab.info/
后续工作MEMIT介绍:https://memit.baulab.info/

使用MEMIT编辑过的GPT-J模型DEMO演示:
https://memit.baulab.us/#/

MEMIT代码库:https://github.com/kmeng01/memit

<img src="https://img.saraba1st.com/forum/202305/04/024129f32knkwcxxyndw1p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230504-024027.jpg</strong> (490 KB, 下载次数: 0)

下载附件

2023-5-4 02:41 上传

<img src="https://img.saraba1st.com/forum/202305/04/024132t32m1n45c24h3nn2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230504-024053.jpg</strong> (350.81 KB, 下载次数: 0)

下载附件

2023-5-4 02:41 上传

<img src="https://img.saraba1st.com/forum/202305/04/024540iksudkqwzsqqs0zs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230504-024526.jpg</strong> (87.69 KB, 下载次数: 0)

下载附件

2023-5-4 02:45 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 311#       发表于 2023-5-4 02:48

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60713379&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-5-4 01:35</a>
显然是量化了，GPTQ之类的量化很容易造成多语言能力下降</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">LLM的运营成本还是太贵了，也算预期之内吧

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 312#       发表于 2023-5-4 04:50

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60377190&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-8 20:14</a>
微软的论文
arxiv.org/abs/2304.03277
使用 GPT-4 进行指令调优</blockquote>
GPT4-LLM-Cleaned

过滤了所有OpenAI免责声明和拒绝等内容的数据集，使用了以下脚本(https://huggingface.co/datasets/ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered/blob/main/wizardlm_clean.py)

数据集地址:https://huggingface.co/datasets/teknium/GPT4-LLM-Cleaned

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 313#       发表于 2023-5-4 17:26

Codegen2

知名编程语言大模型Codegen2代，总共4个不同参数版本

论文地址:https://arxiv.org/abs/2305.02309

github项目地址:https://github.com/salesforce/CodeGen2

权重仓库:
1B:https://huggingface.co/Salesforce/codegen2-1B
3.7B:https://huggingface.co/Salesforce/codegen2-3_7B
7B:https://huggingface.co/Salesforce/codegen2-7B
16B:https://huggingface.co/Salesforce/codegen2-16B

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  ffcloudy12  
##### 314#       发表于 2023-5-4 17:42

技术迭代的很快。但是目前为止可以媲美gpt-3.5的还是没有

*****

####  Machinery  
##### 315#       发表于 2023-5-4 17:48

 本帖最后由 Machinery 于 2023-5-4 17:50 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60721829&amp;ptid=2126390" target="_blank">ffcloudy12 发表于 2023-5-4 17:42</a>
技术迭代的很快。但是目前为止可以媲美gpt-3.5的还是没有</blockquote>
因为学界的目标根本不是复现gpt3.5，当然不会专门朝这个方向做，何况就算真的有了类似的开源模型，哪怕只要20b参数，也不是家用能运行的，现在开源的可定制性，包括数据集，训练框架，多语言，多模态，工具特化和本地需求，比chat可好太多了…包括各种不同的基模，训练集提升方法，算法和运行性能的权衡，chat怎么说也只是个闭源产品，无敌只存在于想象之中

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 316#       发表于 2023-5-4 19:28

 本帖最后由 Machinery 于 2023-5-4 19:31 编辑 

Unlimiformer

可以使任何预训练编码器-解码器结构Transformer模型具有无限输入长度的长程Transformer方法，在多个长文档和多文档摘要基准测试中展示了Unlimiformers的功效，表明它甚至可以从BookSum数据集中总结摘要350k Token长的输入，并且在测试时没有任何输入截断。

论文地址:https://arxiv.org/abs/2305.01625

github项目地址:https://github.com/abertsch72/unlimiformer

<img src="https://img.saraba1st.com/forum/202305/04/192801quiu11r4xk8kzwmo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230504-045813__01.jpg</strong> (55.9 KB, 下载次数: 0)

下载附件

2023-5-4 19:28 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 317#       发表于 2023-5-5 00:02

CLIP-ViT-L-14-DataComp.XL-s13B-b90K

使用OpenCLIP配合DataComp的1B子集微调CLIP ViT-L/14，关于DataComp项目请参考本论文(https://arxiv.org/abs/2304.14108)，由stability.ai赞助算力训练

ImageNet零样本(zero shot)准确率79.2%，同时也优于在Laion2B子集上训练的ViT-g/14

OpenCLIP模型集:https://github.com/mlfoundations/open_clip/blob/main/docs/datacomp_models.md

CLIP-ViT-L-14-DataComp.XL-s13B-b90K权重地址:https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K

<img src="https://img.saraba1st.com/forum/202305/05/000156v7h160m1yg60g46k.jpg" referrerpolicy="no-referrer">

<strong>20230505_000152.jpg</strong> (103.69 KB, 下载次数: 0)

下载附件

2023-5-5 00:01 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 318#       发表于 2023-5-5 03:40

 本帖最后由 Machinery 于 2023-5-5 03:41 编辑 

StarCoder

StarCoderBase模型由bigcode训练，拥有15.5B参数，拥有8192Token上下文长度，在80+种以上的许可训练编程代码种类训练集上训练(The Stack (v1.2)数据集:https://huggingface.co/datasets/bigcode/the-stack)

使用Multi Query注意力(论文:https://arxiv.org/abs/1911.02150)，同时搭配Fill-in-the-Middle objective策略(论文:https://arxiv.org/abs/2207.14255)训练

在预先使用prompt策略(相关:https://huggingface.co/datasets/bigcode/ta-prompt)的情况下可以达到40% pass@1 HumanEval测试集通过率，详见下图

hugface权重仓库地址:https://huggingface.co/bigcode/starcoderbase

<img src="https://img.saraba1st.com/forum/202305/05/034051xnbwqdkbevgrzrai.jpg" referrerpolicy="no-referrer">

<strong>20230505_033233.jpg</strong> (153.64 KB, 下载次数: 0)

下载附件

2023-5-5 03:40 上传

<img src="https://img.saraba1st.com/forum/202305/05/034055qdzch89cccitpde2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230505-033224.jpg</strong> (384.2 KB, 下载次数: 0)

下载附件

2023-5-5 03:40 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  琉璃苑軒風  
##### 319#       发表于 2023-5-5 09:41

<img src="https://static.saraba1st.com/image/smiley/face2017/072.png" referrerpolicy="no-referrer">目前对文字转语音有没有什么推荐的本地AI方案？


*****

####  Benighted  
##### 320#       发表于 2023-5-5 13:32

[https://mp.weixin.qq.com/s/SdQbWvy2awLTMlH6hFr9mA](https://mp.weixin.qq.com/s/SdQbWvy2awLTMlH6hFr9mA)

谷歌内部文件遭泄露｜我们没有壁垒，OpenAI也没有

原创 深思圈 深思圈 2023-05-05 07:14 发表于浙江

图片

就在今天美国时间早上，海外AI圈开始流传一篇SemiAnalysis博客的文章，这篇文章详细介绍了最近在Discord群组由匿名人士泄露的一份Google内部的文件，该文件声称，开源AI将击败谷歌和OpenAI，同时谷歌声称：“我们没有护城河，OpenAI也没有”。一石激起千层浪，这篇文章在海外引发了热议，有人认同，也有人反对。笔者在此借助GPT-4快速翻译了这篇文章，希望能够给大家也带来最新的思考和启发。


*****

####  Machinery  
##### 321#       发表于 2023-5-5 17:04

SELF-ALIGN-Dromedary

Dromedary使用类似宪法人工智能思路，在尽可能少的人工监督下进行原则驱动的自对齐数据集拓展

少于300行的人工标注(包括小于200 个种子提示，16 个通用原则，和5个用于上下文学习的示例)

论文地址:https://arxiv.org/abs/2305.03047

项目空间:https://mitibmdemos.draco.res.ibm.com/dromedary

github代码库地址:https://github.com/IBM/Dromedary

<img src="https://img.saraba1st.com/forum/202305/05/170009k9323jr8lr66bsss.png" referrerpolicy="no-referrer">

<strong>self_align_pipeline.png</strong> (455.64 KB, 下载次数: 0)

下载附件

2023-5-5 17:00 上传

<img src="https://img.saraba1st.com/forum/202305/05/170429o76ehed67h9hihnh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230505-170251.jpg</strong> (237.67 KB, 下载次数: 0)

下载附件

2023-5-5 17:04 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 322#       发表于 2023-5-5 17:32

OpenAlpaca

完全开源可商用与学术研究的基于OpenLLaMA项目的指令跟随模型数据集构建与微调，以及权重下载等

github项目地址:https://github.com/yxuansu/OpenAlpaca

openllama预览版微调权重下载:https://huggingface.co/openllmplayground/openalpaca_7b_preview_2bt

<img src="https://img.saraba1st.com/forum/202305/05/173053xrdr08y288dp8dpy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230505-172827.jpg</strong> (361.24 KB, 下载次数: 0)

下载附件

2023-5-5 17:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 323#       发表于 2023-5-5 18:17

TCOW

通过容器和遮挡物认知进行永久动态对象追踪，在常识中即使对象不再可见，它们也应该继续存在，因此可以根据这一方法进行动态对象追踪。

TCOW经过训练不仅可以预测表示目标实例的MASK，而且还可以明确被标记对象周围存在的遮挡物或容器，同时研究组还发布了一个新的数据集集合，其中包括逼真的合成训练集和真实世界的评估基准。

论文地址:https://arxiv.org/abs/2305.03052

项目空间:https://tcow.cs.columbia.edu/

<img src="https://img.saraba1st.com/forum/202305/05/181719bdcbtzplk1cjkecz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230505-181639.jpg</strong> (186.65 KB, 下载次数: 0)

下载附件

2023-5-5 18:17 上传

<img src="https://img.saraba1st.com/forum/202305/05/181301ma4ggzm6jmh1mhem.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230505-180644.jpg</strong> (146.64 KB, 下载次数: 0)

下载附件

2023-5-5 18:13 上传

<img src="https://img.saraba1st.com/forum/202305/05/181301ngggm6am9gaghmme.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230505-180658.jpg</strong> (131.28 KB, 下载次数: 0)

下载附件

2023-5-5 18:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 324#       发表于 2023-5-5 19:04

 本帖最后由 Machinery 于 2023-5-5 19:10 编辑 

待编辑/


*****

####  Machinery  
##### 325#       发表于 2023-5-5 19:51

Seggpt

通过上下文推理在图像或视频中执行任意分割任务，例如对象实例、东西、部分、轮廓和文本。SegGPT 在广泛的任务上进行了评估，包括少镜头语义分割、视频对象分割、语义分割和全景分割，效果强大

最近发现代码开源了，所以补充上来，本身能力不用怀疑，之前因为一直没开源权重与代码忘了发上来，具体效果在上一楼的对比表单里有

Demo演示:https://huggingface.co/spaces/BAAI/SegGPT

使用说明:https://github.com/baaivision/Painter/blob/main/SegGPT/SegGPT_inference/README.md

权重文件地址:https://huggingface.co/BAAI/SegGPT/blob/main/seggpt_vit_large.pth

<img src="https://img.saraba1st.com/forum/202305/05/195051u33wr2fg38wzat4s.png" referrerpolicy="no-referrer">

<strong>seggpt_teaser.png</strong> (629.6 KB, 下载次数: 0)

下载附件

2023-5-5 19:50 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 326#       发表于 2023-5-5 20:31

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60613006&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-25 22:37</a>
WizardLM

通过自进化机制随机深化或者广化等方法指引大语言模型创造复杂任务指令跟随数据集并进行微调，在 ...</blockquote>
WizardVicunaLM

总结:Wizard的数据集+ChatGPT的对话扩展+Vicuna的调优方式，微调完成后使用GPT4对于回答质量进行评估

github项目地址:https://github.com/melodysdreamj/WizardVicunaLM

注:虽然作者使用了100%的英文数据集调整模型，但奇怪的是，其他国家（例如韩语、中文和日语）的语言能力虽然本应该有所下降，但实际却得到了增强

<img src="https://img.saraba1st.com/forum/202305/05/203031ir7rcpg7m3jvkewg.jpg" referrerpolicy="no-referrer">

<strong>20230505_202423.jpg</strong> (19.96 KB, 下载次数: 0)

下载附件

2023-5-5 20:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 327#       发表于 2023-5-6 01:55

 本帖最后由 Machinery 于 2023-5-6 02:02 编辑 

MPT-7B

MPT-7B是由MosaicML带来的最新基础模型系列，预训练数据集总量1万亿Token，MPT-7B完全开源且授权允许商业使用(许可证：Apache-2.0)

与LLaMA-7B的质量相匹配，MPT-7B 在 9.5 天内在 MosaicML 平台上进行了培训，零人工干预，成本约为 20 万美元。

同时开源微调代码等，可以训练、微调和部署您自己的私有 MPT 模型，从开源的权重检查点之一开始微调，或者从头开始训练。

除了基础MPT-7B模型之外，还发布了三个微调模型：MPT-7B-Instruct、MPT-7B-Chat 和 MPT-7B-StoryWriter-65k+，借助ALiBi技术(相关论文:https://arxiv.org/abs/2108.12409)其中最后一个使用 65k 的上下文长度Token，比GPT4-32K还多一倍

项目地址:https://www.mosaicml.com/blog/mpt-7b

<img src="https://img.saraba1st.com/forum/202305/06/020153fpc3b3hp7zc3uy3u.jpg" referrerpolicy="no-referrer">

<strong>20230506_015709.jpg</strong> (174.9 KB, 下载次数: 0)

下载附件

2023-5-6 02:01 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 328#       发表于 2023-5-6 06:25

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60498577&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-17 23:15</a>
简单介绍一下RedPajama，目标是创建一个领先的开源LLM模型系列

目前已经公开了包含超过 1.2 万亿个token的 ...</blockquote>
RedPajama-INCITE

由构建开放预训练数据集的TOGETHER所训练的系列模型，基础模型分为3B参数与7B参数版本(7B目前依然是未完成完全训练的预览版)，并且同时开源对应参数版本的指令跟随微调模型与chat模型，所有模型均根据Apache 2.0许可发布，允许研究和商业使用

项目公开主页:https://www.together.xyz/blog/redpajama-models-v1

<img src="https://img.saraba1st.com/forum/202305/06/062210pikymyaszddyo6a6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-061950.jpg</strong> (204.91 KB, 下载次数: 0)

下载附件

2023-5-6 06:22 上传

<img src="https://img.saraba1st.com/forum/202305/06/062214fixvvajfxhaqtxfe.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-062033.jpg</strong> (220.91 KB, 下载次数: 0)

下载附件

2023-5-6 06:22 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  yhaos  
##### 329#       发表于 2023-5-6 17:04

我觉得现在做视频，提取数据特性和写代码这三个费人工还需要看整体效果的方向特别需要提高效率，有没有这几个方向的能本地部署的模型？

*****

####  Machinery  
##### 330#       发表于 2023-5-6 19:13

AG3D

从2D图像集生成3D模型化身，使用对抗生成模型生成具有丰富衣物细节，真实和完整的3D外观和适合的几何形状衣物

论文地址:https://arxiv.org/abs/2305.02312

项目主页:https://zj-dong.github.io/AG3D/

github代码库(项目组准备中):https://github.com/zj-dong/AG3D

<img src="https://img.saraba1st.com/forum/202305/06/191248bqxezdi4uhab0qcu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-190926.jpg</strong> (209.51 KB, 下载次数: 0)

下载附件

2023-5-6 19:12 上传

<img src="https://img.saraba1st.com/forum/202305/06/191251d7xlsv8wli0wswl6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-190948.jpg</strong> (150.17 KB, 下载次数: 0)

下载附件

2023-5-6 19:12 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 331#       发表于 2023-5-6 19:33

 本帖最后由 Machinery 于 2023-5-6 19:37 编辑 

AltDiffusion-m18

由智源研究院开源的多语言AltDiffusion-m18，支持18种语言的文图生成，包括中文、英文、日语、泰语、韩语、印地语、乌克兰语、阿拉伯语、土耳其语、越南语、波兰语、荷兰语、葡萄牙语、意大利语、西班牙语、德语、法语、俄语。

AltDiffusion-m18  在英文的 FID、IS、CLIP score 客观评测上达到了 Stable Diffusion 95~99% 效果，在中文、日文上达到了最优水平，同时填补了其余 15 种语言文图生成模型的空白

项目介绍:https://mp.weixin.qq.com/s/zBVxDf3ELZOV73OWfrKRKw

Huggingface:https://huggingface.co/BAAI/AltDiffusion-m18

GitHub:https://github.com/FlagAI-Open/FlagAI/blob/master/examples/AltDiffusion-m18

<img src="https://img.saraba1st.com/forum/202305/06/193328y8w2daef28pa8dd0.png" referrerpolicy="no-referrer">

<strong>640.png</strong> (551.88 KB, 下载次数: 0)

下载附件

2023-5-6 19:33 上传

<img src="https://img.saraba1st.com/forum/202305/06/193521etv1iwunnthiww1d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-193114__01.jpg</strong> (291.82 KB, 下载次数: 0)

下载附件

2023-5-6 19:35 上传

<img src="https://img.saraba1st.com/forum/202305/06/193731glxfg67gnmnfqtqx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-193631.jpg</strong> (265.01 KB, 下载次数: 0)

下载附件

2023-5-6 19:37 上传

<img src="https://img.saraba1st.com/forum/202305/06/193731y2odippomonieopl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-193625.jpg</strong> (466.39 KB, 下载次数: 0)

下载附件

2023-5-6 19:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 332#       发表于 2023-5-6 19:41

 本帖最后由 Machinery 于 2023-5-6 19:44 编辑 

Shap.E

OpenAI发布的文字生成3D模型Shap.E

论文地址：https://arxiv.org/pdf/2305.02463.pdf 

模型地址：https://github.com/openai/shap-e 

相关中文介绍:https://hub.baai.ac.cn/view/26673

<img src="https://img.saraba1st.com/forum/202305/06/194159qtgotqsetmttkt7a.png" referrerpolicy="no-referrer">

<strong>4cb2aa4d1bee231ff9c94a04c6e962ce.png</strong> (747.02 KB, 下载次数: 0)

下载附件

2023-5-6 19:41 上传

<img src="https://img.saraba1st.com/forum/202305/06/194350je9cbefb2fboj293.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-194308.jpg</strong> (460.1 KB, 下载次数: 0)

下载附件

2023-5-6 19:43 上传

<img src="https://img.saraba1st.com/forum/202305/06/194356ogis7ow9e7vhg2e6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230506-194332.jpg</strong> (317.28 KB, 下载次数: 0)

下载附件

2023-5-6 19:43 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 333#       发表于 2023-5-7 04:31

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60507358&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-4-18 16:22</a>
LLaVA-13B-v0

多模态模型，从理解上来说类似minigpt4，但是训练与架构方法不同，结合使用了CLIP ViT-L/14 ...</blockquote>
LLaVA-Lightning-MPT-7B-preview

模型基座为MPT 7B，来自LAION/CC/SBU的558K过滤图文对数据集，80K由GPT生成的多模态指令跟随数据集

权重地址:https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview

也可以使用LLaVA项目自动下载运行:https://github.com/haotian-liu/LLaVA#LLaVA-MPT-7b

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 334#       发表于 2023-5-7 05:05

PandaLLM

Panda系列中文语言模型，模型基座为Llama7B与13B，针对推理能力在中文benchmark上进行了基准测试，同时开源训练相关数据集等

论文地址:https://arxiv.org/abs/2305.03025

github项目地址:https://github.com/dandelionsllm/pandallm

<img src="https://img.saraba1st.com/forum/202305/07/050625smq4ylgm2bguul30.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230507-050118__01.jpg</strong> (202.98 KB, 下载次数: 0)

下载附件

2023-5-7 05:06 上传

<img src="https://img.saraba1st.com/forum/202305/07/050516sa0ppgofcfebbpdy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230507-050454.jpg</strong> (101.99 KB, 下载次数: 0)

下载附件

2023-5-7 05:05 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 335#       发表于 2023-5-7 05:15

NeMo-Guardrails

英伟达开源的大语言模型护栏技术，控制模型输出，过滤输入的内容，保护模型免受prompt注入等功能的开源工具包

github地址:https://github.com/NVIDIA/NeMo-Guardrails

使用示例:https://github.com/NVIDIA/NeMo-Guardrails/blob/main/examples/README.md

<img src="https://img.saraba1st.com/forum/202305/07/051617g4wqr4togkrrztii.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230507-051603.jpg</strong> (225.92 KB, 下载次数: 0)

下载附件

2023-5-7 05:16 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 336#       发表于 2023-5-7 05:27

RelateAnything

使用SAM分割图片MASK，并生成MASK间对应的多种关联关系

github项目地址:https://github.com/Luodian/RelateAnything

演示Demo:
https://huggingface.co/spaces/mmlab-ntu/relate-anything-model

<img src="https://img.saraba1st.com/forum/202305/07/052504ypzn4otyppn4m4tp.png" referrerpolicy="no-referrer">

<strong>68747470733a2f2f692e706f7374696d672e63632f434b6838745342342f64656d6f2e706e67.png</strong> (311.85 KB, 下载次数: 0)

下载附件

2023-5-7 05:25 上传

<img src="https://img.saraba1st.com/forum/202305/07/052508iqcizcqjici2kq2v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230507-052424.jpg</strong> (482.32 KB, 下载次数: 0)

下载附件

2023-5-7 05:25 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 337#       发表于 2023-5-8 13:21

dolly-japanese-gpt-1b

1.3B参数量的日语GPT-2模型，使用聊天对话数据集「databricks-dolly-15k-ja」、 「oasst1-89k-ja」、 「OjousamaTalkScriptDataset」、 「train_data/zundamon.json」微调，上下文长度1024Token

https://huggingface.co/inu-ai/dolly-japanese-gpt-1b

测试性能

<img src="https://img.saraba1st.com/forum/202305/08/132059u3glsg2hg2ssltqh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230508-131953.jpg</strong> (108.23 KB, 下载次数: 0)

下载附件

2023-5-8 13:20 上传

实际输出效果

<img src="https://img.saraba1st.com/forum/202305/08/132103nmb02evzzavbva33.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230508-131932.jpg</strong> (82.3 KB, 下载次数: 0)

下载附件

2023-5-8 13:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 338#       发表于 2023-5-8 13:48

Vera

通用常识性陈述合理性估计模型

<img src="https://img.saraba1st.com/forum/202305/08/134700wslvvs4o46s1va47.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230508-134637.jpg</strong> (333.64 KB, 下载次数: 0)

下载附件

2023-5-8 13:47 上传

论文地址:https://arxiv.org/abs/2305.03695

权重地址(等待研究组整理):https://huggingface.co/liujch1998/vera

演示Demo:https://huggingface.co/spaces/liujch1998/vera

<img src="https://img.saraba1st.com/forum/202305/08/134756qwrv6w6e4v5453j3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230508-134746.jpg</strong> (73.14 KB, 下载次数: 0)

下载附件

2023-5-8 13:47 上传

<img src="https://img.saraba1st.com/forum/202305/08/134803oi9hsh3n8o7y56n5.jpg" referrerpolicy="no-referrer">

<strong>20230508_134111.jpg</strong> (14.03 KB, 下载次数: 0)

下载附件

2023-5-8 13:48 上传

<img src="https://img.saraba1st.com/forum/202305/08/134803cwmtliwwgwifif9z.jpg" referrerpolicy="no-referrer">

<strong>20230508_134113.jpg</strong> (18.79 KB, 下载次数: 0)

下载附件

2023-5-8 13:48 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 339#       发表于 2023-5-8 21:57

CVPR2023-Papers-with-Code

集合了CVPR 2023的论文与开源项目

github地址:https://github.com/amusi/CVPR2023-Papers-with-Code

<img src="https://img.saraba1st.com/forum/202305/08/215733ogg4l0t01xxt3gfd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230508-215716.jpg</strong> (444.39 KB, 下载次数: 0)

下载附件

2023-5-8 21:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 340#       发表于 2023-5-9 03:26

 本帖最后由 Machinery 于 2023-5-9 03:27 编辑 

threestudio

开源3D模型生成框架，论文复现项目，整合了DeepFloyd IF(可选)模型提升性能，可以从文本prompt，单张或者少量图像生成3D建模

github地址:https://github.com/threestudio-project/threestudio

注:虽然项目组为了显存优化了不少地方，但性能要求依然较高…

<img src="https://img.saraba1st.com/forum/202305/09/032602bpi5lq5ldzzx559q.jpg" referrerpolicy="no-referrer">

<strong>6f810345-5ba2-4a41-8ac3-756f838f2024.jpg</strong> (47.16 KB, 下载次数: 0)

下载附件

2023-5-9 03:26 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 341#       发表于 2023-5-11 22:01

DB-GPT

私有化本地问答与数据库相关查询反馈开源部署项目

项目地址https://github.com/csunny/DB-GPT

<img src="https://img.saraba1st.com/forum/202305/11/215940l2d62ook42k3hkoa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230511-215849.jpg</strong> (329.4 KB, 下载次数: 0)

下载附件

2023-5-11 21:59 上传

<img src="https://img.saraba1st.com/forum/202305/11/220131ol4zxwtzy7oxlewx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230511-220118.jpg</strong> (935.18 KB, 下载次数: 0)

下载附件

2023-5-11 22:01 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 342#       发表于 2023-5-11 22:12

 本帖最后由 Machinery 于 2023-5-11 22:15 编辑 

ImageBind

meta开源的多模态聚类向量AI，支持跨模态检索、使用算术组合模态、跨模态检测和生成，而且效果也非常不错，可应用范围广泛<img src="https://static.saraba1st.com/image/smiley/face2017/033.png" referrerpolicy="no-referrer">

项目说明地址:https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/

github地址:https://github.com/facebookresearch/ImageBind

权重下载地址:https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth

<img src="https://img.saraba1st.com/forum/202305/11/221246iuc7d3mwd7cd0wg7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230511-220945.jpg</strong> (125.98 KB, 下载次数: 0)

下载附件

2023-5-11 22:12 上传

<img src="https://img.saraba1st.com/forum/202305/11/221410yjb5ihtnq1krtrdi.gif" referrerpolicy="no-referrer">

<strong>236859695-ffa13364-3e39-4d99-a8da-fbfab17f9a6b.gif</strong> (689.46 KB, 下载次数: 0)

下载附件

2023-5-11 22:14 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 343#       发表于 2023-5-11 22:24

StarChat Alpha

从StarCoder通过oasst1与databricks-dolly-15k对话数据集微调而来的对话编程模型，擅长编程指令与多语言多代码对话与生成

项目说明:https://huggingface.co/blog/starchat-alpha

权重下载地址:https://huggingface.co/HuggingFaceH4/starchat-alpha

Demo演示:https://huggingface.co/spaces/HuggingFaceH4/starchat-playground

<img src="https://img.saraba1st.com/forum/202305/11/222402odtparvpyqcval7l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230511-222343.jpg</strong> (438.46 KB, 下载次数: 0)

下载附件

2023-5-11 22:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 344#       发表于 2023-5-11 22:31

PaLM

Google PaLM 模型的开源实现复现
(注:谷歌的PaLm2依然不开源)

在c4数据集上以8k上下文长度训练，目前有150m、410m、1b三种参数版本，2b版本与指令调优模型依然在训练中

github地址:https://github.com/conceptofmind/PaLM

410m参数输出样本示例:

<img src="https://img.saraba1st.com/forum/202305/11/223033k242848wzcj9jp0o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230511-223021.jpg</strong> (357.85 KB, 下载次数: 0)

下载附件

2023-5-11 22:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 345#       发表于 2023-5-11 22:42

lit llm系列

使用最简原则进行开源llm构建，基于nanoGPT的LLaMA/StableLM/Pythia/RedPajama-INCITE等语言模型的实现，同时支持flash attention、LLaMA-Adapter微调、预训练、量化等技术，包含简单有效的推理与应用优化，使用Apache-2.0 许可证

项目地址:
https://github.com/Lightning-AI/lit-llama
https://github.com/Lightning-AI/lit-parrot

<img src="https://img.saraba1st.com/forum/202305/11/224240ciqm6ry65x65oyto.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230511-224228.jpg</strong> (84.63 KB, 下载次数: 0)

下载附件

2023-5-11 22:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 346#       发表于 2023-5-12 17:53

BIRD-SQL

项目地址:https://bird-bench.github.io/

github地址:https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird

<img src="https://img.saraba1st.com/forum/202305/12/175245jaay3mpqbb77my8x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230512-175228.jpg</strong> (334.57 KB, 下载次数: 0)

下载附件

2023-5-12 17:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 347#       发表于 2023-5-12 21:36

 本帖最后由 Machinery 于 2023-5-12 21:41 编辑 

InstructBLIP

新多模态模型，InstructBLIP实现了SoTA零样本性能，InstructBLIP在零样本评估方面远远优于BLIP-2 和最大的Flamingo，当用作下游任务的基础模型时，还具有 SOTA微调性能(如ScienceQA IMG 的准确率为 90.7%)

论文地址:https://arxiv.org/abs/2305.06500

github项目地址(提供直接使用方法):https://github.com/salesforce/LAVIS/tree/main/projects/instructblip

<img src="https://img.saraba1st.com/forum/202305/12/213527qwkdnq2zcq3c3qkw.jpg" referrerpolicy="no-referrer">

<strong>20230512_212757.jpg</strong> (185.67 KB, 下载次数: 0)

下载附件

2023-5-12 21:35 上传

<img src="https://img.saraba1st.com/forum/202305/12/213727fjjzuiucxdx6jgdc.png" referrerpolicy="no-referrer">

<strong>showcase.png</strong> (879.25 KB, 下载次数: 0)

下载附件

2023-5-12 21:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 348#       发表于 2023-5-13 01:12

 本帖最后由 Machinery 于 2023-5-13 01:15 编辑 

LLMTune

在单张消费级GPU显卡上通过4-Bit量化实现运行与微调30B/65B模型

项目地址:https://github.com/kuleshov-group/llmtune

<img src="https://img.saraba1st.com/forum/202305/13/011252cia82j9a98xm00xm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230513-011240.jpg</strong> (150.28 KB, 下载次数: 0)

下载附件

2023-5-13 01:12 上传

<img src="https://img.saraba1st.com/forum/202305/13/011126krkqw6cyhouq6kmu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230513-011106.jpg</strong> (97.05 KB, 下载次数: 0)

下载附件

2023-5-13 01:11 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 349#       发表于 2023-5-14 02:10

OpenBuddy

功能强大的开源跨语言对话模型，能回答问题并在各种语言之间进行翻译任务

github中文说明主页:https://github.com/OpenBuddy/OpenBuddy/blob/main/README.zh.md

<img src="https://img.saraba1st.com/forum/202305/14/021022fco3yx469x4ksz6y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230514-020936.jpg</strong> (609.57 KB, 下载次数: 0)

下载附件

2023-5-14 02:10 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 350#       发表于 2023-5-14 13:35

privateGPT

langchain配合向量数据库相似度检索实现文档问答与本地化，适合私有部署

github项目地址:https://github.com/imartinez/privateGPT

<img src="https://img.saraba1st.com/forum/202305/14/133344hufga6v0v2ka6046.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230514-133333.jpg</strong> (170.36 KB, 下载次数: 0)

下载附件

2023-5-14 13:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 351#       发表于 2023-5-14 13:51

DetGPT

github项目地址:https://github.com/OptimalScale/DetGPT

<img src="https://img.saraba1st.com/forum/202305/14/135021gg2xmulldlxutu2h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230514-134955.jpg</strong> (251.96 KB, 下载次数: 0)

下载附件

2023-5-14 13:50 上传

<img src="https://img.saraba1st.com/forum/202305/14/135053tslksyssmx6h2h6y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230514-134853.jpg</strong> (110.9 KB, 下载次数: 0)

下载附件

2023-5-14 13:50 上传

<img src="https://img.saraba1st.com/forum/202305/14/135100xxdxxy2pcmdpymd8.jpeg" referrerpolicy="no-referrer">" src="https://static.saraba1st.com/image/common/none.gif" referrerpolicy="no-referrer">

<strong>ex1.jpeg</strong> (64.51 KB, 下载次数: 0)

下载附件

2023-5-14 13:51 上传

<img src="https://img.saraba1st.com/forum/202305/14/135101fxe6x4i7r71y1i5i.png" referrerpolicy="no-referrer">

<strong>ex4.png</strong> (438.05 KB, 下载次数: 0)

下载附件

2023-5-14 13:51 上传

<img src="https://img.saraba1st.com/forum/202305/14/135101d66zezc2f52zog76.png" referrerpolicy="no-referrer">

<strong>ex6.png</strong> (595.33 KB, 下载次数: 0)

下载附件

2023-5-14 13:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 352#       发表于 2023-5-15 14:41

ArtGPT-4

更懂艺术气息的MiniGPT-4，使用Laion-aesthetic子集数据集微调，在艺术相关方面能力更好，例如制作更美观的网页或对艺术作品的理解

论文地址:https://arxiv.org/abs/2305.07490

hugface仓库:https://huggingface.co/Tyrannosaurus/ArtGPT-4

<img src="https://img.saraba1st.com/forum/202305/15/143931nk1mmj3jg1gjh2xr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230515-143837.jpg</strong> (162.42 KB, 下载次数: 0)

下载附件

2023-5-15 14:39 上传

<img src="https://img.saraba1st.com/forum/202305/15/143932gzpzbbbz2xpp5k2b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230515-143919.jpg</strong> (260.51 KB, 下载次数: 0)

下载附件

2023-5-15 14:39 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 353#       发表于 2023-5-16 15:29

CodeT5+

CodeT5+系列开源模型，一共有220M/770M/2B/6B/16B五个版本，适用于各种代码理解和生成任务，Instruct16B版本在HumanEval上取得了新的 SoTA 代码生成性能

论文地址:https://arxiv.org/abs/2305.07922

github项目地址:https://github.com/salesforce/CodeT5/tree/main/CodeT5%2B

<img src="https://img.saraba1st.com/forum/202305/16/152753hek6z96a6n60bkym.jpg" referrerpolicy="no-referrer">

<strong>20230516_151322.jpg</strong> (96.98 KB, 下载次数: 0)

下载附件

2023-5-16 15:27 上传

结构非常灵活，可以在不同模式（即encoder-only、decoder-only和encoder-decoder）下运行，以支持广泛的代码理解和生成任务，CodeT5+的个别模块可灵活拆卸和组合，以适应不同的下游应用，包括zero shot、微调或指令跟随等

<img src="https://img.saraba1st.com/forum/202305/16/152832luvyfjfidduv1jcc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230516-152253__01.jpg</strong> (186.55 KB, 下载次数: 0)

下载附件

2023-5-16 15:28 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 354#       发表于 2023-5-17 01:50

MasaCtrl

使用互自注意力进行无需微调的合成图像高度可控编辑

论文:https://arxiv.org/abs/2304.08465

项目主页:https://ljzycmd.github.io/projects/MasaCtrl/

<img src="https://img.saraba1st.com/forum/202305/17/014949epmbmx72k2gl7r71.png" referrerpolicy="no-referrer">

<strong>duck_flying_standing.png</strong> (608.23 KB, 下载次数: 0)

下载附件

2023-5-17 01:49 上传

<img src="https://img.saraba1st.com/forum/202305/17/014949dyvnndmffywwx2nw.png" referrerpolicy="no-referrer">

<strong>old_man_sideview.png</strong> (791.77 KB, 下载次数: 0)

下载附件

2023-5-17 01:49 上传

<img src="https://img.saraba1st.com/forum/202305/17/014949c9p8n2vy5nby9yt9.png" referrerpolicy="no-referrer">

<strong>cat_sitting_walking.png</strong> (647.47 KB, 下载次数: 0)

下载附件

2023-5-17 01:49 上传

<img src="https://img.saraba1st.com/forum/202305/17/014949zv49voc84xoexh9l.png" referrerpolicy="no-referrer">

<strong>goose_standing_sitting.png</strong> (604.72 KB, 下载次数: 0)

下载附件

2023-5-17 01:49 上传

<img src="https://img.saraba1st.com/forum/202305/17/014949bmp974p4zhmlslm9.png" referrerpolicy="no-referrer">

<strong>man_from_behind.png</strong> (699.88 KB, 下载次数: 0)

下载附件

2023-5-17 01:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 355#       发表于 2023-5-17 17:08

 本帖最后由 Machinery 于 2023-5-17 17:13 编辑 

local CPU-powered API

因为一堆本地应用工具只能用openai key，GPT4All Chat Client集成了本地自建API网络服务代替openai key的需求，还可以换用其他多种模型

相关说明:https://docs.gpt4all.io/gpt4all_chat.html

<img src="https://img.saraba1st.com/forum/202305/17/170739vw1w47nolesjl4w7.jpg" referrerpolicy="no-referrer">

<strong>20230517_170451.jpg</strong> (85.58 KB, 下载次数: 0)

下载附件

2023-5-17 17:07 上传

关于GPT4All Chat Client，本地模型运行的chatbot
具体请看这里(https://gpt4all.io/index.html)

<img src="https://img.saraba1st.com/forum/202305/17/171304ux6jzzz03wjxmen6.gif" referrerpolicy="no-referrer">

<strong>landing.gif</strong> (887.11 KB, 下载次数: 0)

下载附件

2023-5-17 17:13 上传

支持的模型非常多样

<img src="https://img.saraba1st.com/forum/202305/17/171256obpabb32bfmwv3fn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230517-171216.jpg</strong> (187.69 KB, 下载次数: 0)

下载附件

2023-5-17 17:12 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 356#       发表于 2023-5-17 17:27

 本帖最后由 Machinery 于 2023-5-17 17:51 编辑 

Open-Calm-LLM

OpenCALM 是一系列不同参数的仅解码器的语言模型，在日语数据集上进行了预训练，由 Cyber​​Agent, Inc. 开发。

模型项目仓库集合:https://huggingface.co/cyberagent

<img src="https://img.saraba1st.com/forum/202305/17/172614g707e7y7d10ldjdi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230517-172538.jpg</strong> (89.91 KB, 下载次数: 0)

下载附件

2023-5-17 17:26 上传

<img src="https://img.saraba1st.com/forum/202305/17/175122uk5kwj5u8le56jkj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230517-175052.jpg</strong> (211.08 KB, 下载次数: 0)

下载附件

2023-5-17 17:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  maxwenn  
##### 357#       发表于 2023-5-17 20:44

感谢赞美，收藏了

[  -- 来自 有消息提醒的 Stage1官方 Android客户端](https://www.coolapk.com/apk/140634)


*****

####  Machinery  
##### 358#       发表于 2023-5-18 00:47

 本帖最后由 Machinery 于 2023-5-18 00:48 编辑 

大型语言模型的条件预训练

除了SFT与RLHF之外的第三种微调与训练方法，条件预训练，简单来说就是用文字Tag给文字内容打标签，再进行训练，方便引导模型生成相关内容
(注:在谷歌的palm2项目中，也有使用类似质量标签的方法)

项目说明:https://laion.ai/notes/cpretrain/

Colab Demo:https://colab.research.google.com/drive/1fbXOqeEkqygnWKSPKddQtaMiZEc0KYFY?usp=sharing

redpajama基础7B模型，在约200万个2048上下文条件预训练示例上进行了微调:https://huggingface.co/Rallio67/7B-redpajama-conditional-alpha

redpajama基础3B模型，在约200万个2048上下文条件预训练示例上进行了微调:https://huggingface.co/Rallio67/3B-redpajama-conditional-alpha

gpt-neox-20B基础模型，在约60万个2048上下文条件预训练示例上进行了微调:https://huggingface.co/Rallio67/neox-20b-conditional-alpha

flan-ul2-20b LoRA 微调模型，可以为文本内容生成相关标签:https://huggingface.co/Rallio67/condlabeler-alpha

<img src="https://img.saraba1st.com/forum/202305/18/004156zl99aaayhauyphh4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230518-002213__01.jpg</strong> (392.48 KB, 下载次数: 0)

下载附件

2023-5-18 00:41 上传

<img src="https://img.saraba1st.com/forum/202305/18/004200tipa77vlublufteu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230518-002328__01.jpg</strong> (684.76 KB, 下载次数: 0)

下载附件

2023-5-18 00:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 359#       发表于 2023-5-18 01:24

 本帖最后由 Machinery 于 2023-5-18 01:26 编辑 

StableStudio

Stability AI's官方开源的本地版DreamStudio，功能上类似webui，进行了一定程度的本地化修改，可以在本地进行图片生成等，默认情况下运行在localhost:3000端口

项目说明页:https://stability.ai/blog/stablestudio-open-source-community-driven-future-dreamstudio-release

github项目地址:https://github.com/Stability-AI/StableStudio

<img src="https://img.saraba1st.com/forum/202305/18/012401jqqtl0bmh00v3roh.jpg" referrerpolicy="no-referrer">

<strong>45179a68-6707-49ff-9ecd-08300545910c.jpg</strong> (199.79 KB, 下载次数: 0)

下载附件

2023-5-18 01:24 上传

<img src="https://img.saraba1st.com/forum/202305/18/012401rmmmmlaykljt8ret.png" referrerpolicy="no-referrer">

<strong>EditScreenshot.png</strong> (387.02 KB, 下载次数: 0)

下载附件

2023-5-18 01:24 上传

未来还会加入本地LLM模型聊天等机能

<img src="https://img.saraba1st.com/forum/202305/18/012613skjj75jfulbjbjkb.png" referrerpolicy="no-referrer">

<strong>Untitled+(8).png</strong> (161.09 KB, 下载次数: 0)

下载附件

2023-5-18 01:26 上传

<img src="https://img.saraba1st.com/forum/202305/18/012613uvgrqj11ajemg11a.png" referrerpolicy="no-referrer">

<strong>Untitled+(9).png</strong> (109.82 KB, 下载次数: 0)

下载附件

2023-5-18 01:26 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 360#       发表于 2023-5-18 19:58

 本帖最后由 Machinery 于 2023-5-18 20:02 编辑 

VisualGLM-6B

项目地址:https://github.com/THUDM/VisualGLM-6B

<img src="https://img.saraba1st.com/forum/202305/18/195640uvwbhx6aw7p7wlgg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230518-195613.jpg</strong> (494.33 KB, 下载次数: 0)

下载附件

2023-5-18 19:56 上传

<img src="https://img.saraba1st.com/forum/202305/18/195757qtor5votit99bpj9.png" referrerpolicy="no-referrer">

<strong>chat_example1.png</strong> (750.92 KB, 下载次数: 0)

下载附件

2023-5-18 19:57 上传

<img src="https://img.saraba1st.com/forum/202305/18/195757trxxx3jioe99692i.png" referrerpolicy="no-referrer">

<strong>chat_example3.png</strong> (659.63 KB, 下载次数: 0)

下载附件

2023-5-18 19:57 上传

<img src="https://img.saraba1st.com/forum/202305/18/195757ekx61xibci4lb4rk.png" referrerpolicy="no-referrer">

<strong>chat_example2.png</strong> (503.02 KB, 下载次数: 0)

下载附件

2023-5-18 19:57 上传

<img src="https://img.saraba1st.com/forum/202305/18/200240ycvuucxcxuzug3c1.png" referrerpolicy="no-referrer">

<strong>thu.png</strong> (456.96 KB, 下载次数: 0)

下载附件

2023-5-18 20:02 上传

<img src="https://img.saraba1st.com/forum/202305/18/200240h23rpnsnnhfj3o7u.png" referrerpolicy="no-referrer">

<strong>web_demo.png</strong> (335.96 KB, 下载次数: 0)

下载附件

2023-5-18 20:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  astkaasa  
##### 361#       发表于 2023-5-18 20:05

光把帖子放出来了。。。啥时候把楼主也放出来啊


*****

####  Machinery  
##### 362#       发表于 2023-5-19 01:36

dreamGPT

利用大型语言模型的幻觉输出产生新颖想法与思考，dreamGPT的目标是探索尽可能多的可能性，而不是大多数其他GPT所专注的使用解决方案解决特定问题

github项目地址:https://github.com/DivergentAI/dreamGPT

<img src="https://img.saraba1st.com/forum/202305/19/013539jr09vneboxgfsx7e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230519-013426.jpg</strong> (100.83 KB, 下载次数: 0)

下载附件

2023-5-19 01:35 上传

<img src="https://img.saraba1st.com/forum/202305/19/013543x6wsjksz2cew6g6q.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230519-013436.jpg</strong> (223.2 KB, 下载次数: 0)

下载附件

2023-5-19 01:35 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 363#       发表于 2023-5-19 01:38

LaWGPT

基于中文法律知识的大语言模型微调

项目地址:https://github.com/pengxiao-song/LaWGPT

<img src="https://img.saraba1st.com/forum/202305/19/013751o10ujs7qyl990jus.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230519-013730.jpg</strong> (306.36 KB, 下载次数: 0)

下载附件

2023-5-19 01:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 364#       发表于 2023-5-19 01:53

 本帖最后由 Machinery 于 2023-5-19 01:58 编辑 

japanese-gpt-neox-3.6b

由rinna会社(https://rinna.co.jp/)训练的日语大语言模型japanese-gpt-neox-3.6b以及对应的对话微调模型(japanese-gpt-neox-3.6b-instruction-sft)

使用来自Japanese CC-100,Japanese C4,Japanese Wikipedia相关的3125亿日语Token数据集进行了预训练，最终验证困惑度值为8.68

微调模型使用了Anthropic HH RLHF data、FLAN Instruction Tuning data、Stanford Human Preferences Dataset数据集的日语版本进行了对话微调
(注:这个版本的微调数据集并未同步开源)

japanese-gpt-neox-3.6b权重仓库:https://huggingface.co/rinna/japanese-gpt-neox-3.6b

微调模型(japanese-gpt-neox-3.6b-instruction-sft)权重仓库:https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft

(注2:使用微调模型时请务必注意格式的差异，如下)

<img src="https://img.saraba1st.com/forum/202305/19/015318pbyswt0xsy7y54sa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230519-015255.jpg</strong> (159.55 KB, 下载次数: 0)

下载附件

2023-5-19 01:53 上传

<img src="https://img.saraba1st.com/forum/202305/19/015817q7pfpskdp7bkxtvg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230519-015759.jpg</strong> (112.17 KB, 下载次数: 0)

下载附件

2023-5-19 01:58 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 365#       发表于 2023-5-19 02:05

guidance

微软开源的langchain竞品

<img src="https://img.saraba1st.com/forum/202305/19/020138xqjh3qo162arx6ao.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230519-020004.jpg</strong> (137.68 KB, 下载次数: 0)

下载附件

2023-5-19 02:01 上传

引导使您能够更有效和高效地控制现代语言模型,而不是传统的提示或联想。引导程序允许您在单个连续流中交替生成,提示和逻辑控制,以匹配语言模型实际处理文本的方式。

github项目地址:https://github.com/microsoft/guidance

<img src="https://img.saraba1st.com/forum/202305/19/020506pbqsbb9dx2l9dlx7.gif" referrerpolicy="no-referrer">

<strong>proverb_animation.gif</strong> (146.76 KB, 下载次数: 0)

下载附件

2023-5-19 02:05 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  s1234y  
##### 366#       发表于 2023-5-19 02:17

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60897451&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-5-19 02:05</a>
 guidance  微软开源的langchain竞品</blockquote>
微软出了多少类似的东西了…


*****

####  Machinery  
##### 367#       发表于 2023-5-19 02:34

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60897478&amp;ptid=2126390" target="_blank">s1234y 发表于 2023-5-19 02:17</a>
微软出了多少类似的东西了…</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">涉及llm底层改进的东西是几乎一点没发，都是应用方面

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 368#       发表于 2023-5-19 12:20

 本帖最后由 Machinery 于 2023-5-19 12:22 编辑 

fastcomposer

具有局部注意力的免微调多主题图像生成，简单来说就是无需微调即可实现高效、个性化、多主题的文本到图像生成，可以只用一张图片，也可以用两张不同的图片组合

项目主页:https://fastcomposer.mit.edu/

github项目地址:https://github.com/mit-han-lab/fastcomposer/tree/main

演示Demo(写完对应的人物描述在prompt加上&lt;A*&gt;对齐图像):https://17283ded5673112d93.gradio.live/

<img src="https://img.saraba1st.com/forum/202305/19/121817oizo6dkteej6vidi.jpg" referrerpolicy="no-referrer">

<strong>6c6609dc-f436-493c-bfdd-0cca0504ffa5.jpg</strong> (246.15 KB, 下载次数: 0)

下载附件

2023-5-19 12:18 上传

<img src="https://img.saraba1st.com/forum/202305/19/121936k9zx6ukqzrzx2lmq.jpg" referrerpolicy="no-referrer">

<strong>f2ed2309-1bb0-4f63-9e7e-70520f59fbca.jpg</strong> (461.93 KB, 下载次数: 0)

下载附件

2023-5-19 12:19 上传

目前进度:

<img src="https://img.saraba1st.com/forum/202305/19/122207jvjtjaue55sbsvyy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230519-122145.jpg</strong> (56.06 KB, 下载次数: 0)

下载附件

2023-5-19 12:22 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 369#       发表于 2023-5-19 16:24

 本帖最后由 Machinery 于 2023-5-19 16:31 编辑 

Drag Your GAN

使用基于点的交互式操作生成到达目标的流形图像，效果请看演示

效果演示:https://video.twimg.com/ext_tw_video/1659424687921807362/pu/vid/400x316/Ov3o8W6yyLLTKa4_.mp4?tag=12

相关论文:https://arxiv.org/abs/2305.10973

项目主页:https://vcai.mpi-inf.mpg.de/projects/DragGAN/

github项目地址:https://github.com/XingangPan/DragGAN

<img src="https://img.saraba1st.com/forum/202305/19/162349yxv4477t8x1h1441.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230519-161601__01.jpg</strong> (346.06 KB, 下载次数: 0)

下载附件

2023-5-19 16:23 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 370#       发表于 2023-5-20 03:07

BLOOMChat

使用英文对话微调数据集微调BLOOM基础模型(176B参数版本)，数据集来自OIG dataset from OpenChatKit、Dolly 2.0与Oasst1，虽然完全使用英文数据集微调，但也兼具一定的外语能力(包括中文)

演示Demo:https://api.together.xyz/bloom-chat

模型权重下载地址:https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1

<img src="https://img.saraba1st.com/forum/202305/20/030713olg4c4hnvxh3g6gc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230520-030651.jpg</strong> (302.65 KB, 下载次数: 0)

下载附件

2023-5-20 03:07 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 371#       发表于 2023-5-20 03:21

VLPart

来自meta的开源项目，通过更高细粒度的开放式物体衍生分割进行更精准的认知与识别

对象检测从有限的几个类别扩展到开放词汇。展望未来,一个完整的智能视觉系统需要理解更精细的对象描述,对象部件。在这项工作中,提出了一个能够同时预测开放词汇对象和其部件分割的检测器。这种能力来自两个设计:

在部件级别,对象级别和图像级别的数据的联合上训练检测器。  
通过其与基础对象的密集语义对应关系来解析新对象为其部件

论文地址:https://arxiv.org/abs/2305.11173

github项目地址:https://github.com/facebookresearch/VLPart

<img src="https://img.saraba1st.com/forum/202305/20/032009o6hzuwpsyb26spps.jpg" referrerpolicy="no-referrer">

<strong>f027777a-361a-40df-bd95-46bd79c868ff.jpg</strong> (144.5 KB, 下载次数: 0)

下载附件

2023-5-20 03:20 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 372#       发表于 2023-5-20 04:01

muzic-GETMusic

微软的开源项目muzic下属子项目GETMusic，使用统一表示和扩散框架生成任意音乐音符曲目，旋律动人

muzic项目地址:https://github.com/microsoft/muzic/tree/main

GETMusic子项目主页地址:https://ai-muzic.github.io/getmusic/

GETMusic子项目github主页地址:https://github.com/microsoft/muzic/tree/main

<img src="https://img.saraba1st.com/forum/202305/20/035956luf8nu5spsz1k8sk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230520-035930.jpg</strong> (355.37 KB, 下载次数: 0)

下载附件

2023-5-20 03:59 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  不热爱讨论  
##### 373#       发表于 2023-5-20 11:35

话说比起自己组主机，是不是直接用云主机更合适啊？  有推荐的网站么？


*****

####  Machinery  
##### 374#       发表于 2023-5-20 19:27

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60914285&amp;ptid=2126390" target="_blank">不热爱讨论 发表于 2023-5-20 11:35</a>
话说比起自己组主机，是不是直接用云主机更合适啊？  有推荐的网站么？</blockquote>
从直觉上来说其实是相反的，长租的话比直接买显卡亏很多，除非有特别优惠或者套餐减免<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 375#       发表于 2023-5-20 19:33

bert-japanese

東北大NLP团体开源的全套从训练到模型NLP教程与相关的日语BERT权重，完全复现了谷歌论文的BERT结构，包含语料处理，预训练，分词，基准测试结果等

<img src="https://img.saraba1st.com/forum/202305/20/193036m222hjbphqc35ph2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230520-193009.jpg</strong> (50.78 KB, 下载次数: 0)

下载附件

2023-5-20 19:30 上传

github项目地址:https://github.com/cl-tohoku/bert-japanese

<img src="https://img.saraba1st.com/forum/202305/20/193321s3n3a3lnn8gutcjt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230520-193307.jpg</strong> (159.71 KB, 下载次数: 0)

下载附件

2023-5-20 19:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 376#       发表于 2023-5-20 22:21

StyleGAN

类似DragGan，使用用户操作作为引导生成图像变体，达到高度可控的图像编辑方式

项目主页:http://www.cgg.cs.tsukuba.ac.jp/~endo/projects/UserControllableLT/

相关论文:https://arxiv.org/abs/2208.12408

github代码库:https://github.com/endo-yuki-t/UserControllableLT

<img src="https://img.saraba1st.com/forum/202305/20/221941kg54jzkippttky4f.gif" referrerpolicy="no-referrer">

<strong>anime.gif</strong> (528.69 KB, 下载次数: 0)

下载附件

2023-5-20 22:19 上传

<img src="https://img.saraba1st.com/forum/202305/20/221941avwy2skmks0rr1os.gif" referrerpolicy="no-referrer">

<strong>thumb.gif</strong> (401.33 KB, 下载次数: 0)

下载附件

2023-5-20 22:19 上传

<img src="https://img.saraba1st.com/forum/202305/20/221941fgtzrxr9f9jckzgx.gif" referrerpolicy="no-referrer">

<strong>church.gif</strong> (371.14 KB, 下载次数: 0)

下载附件

2023-5-20 22:19 上传

<img src="https://img.saraba1st.com/forum/202305/20/221941qookycc40kctkooc.jpg" referrerpolicy="no-referrer">

<strong>teaser.jpg</strong> (336.44 KB, 下载次数: 0)

下载附件

2023-5-20 22:19 上传

<img src="https://img.saraba1st.com/forum/202305/20/221833dnngq4d4g4kj4qkk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230520-221654.jpg</strong> (151.52 KB, 下载次数: 0)

下载附件

2023-5-20 22:18 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 377#       发表于 2023-5-21 18:25

naturalspeech2-pytorch

微软论文项目NaturalSpeech 2的复现实现， 使用潜在扩散模型进行最新零样本语音与歌曲合成，虽然微软开源了论文中的实现架构，但并未开源，于是需要这个复现项目………

NaturalSpeech 2项目地址:https://speechresearch.github.io/naturalspeech2/

使用Pytorch的开源复现:https://github.com/lucidrains/naturalspeech2-pytorch

相关架构以及与之前的其他相关工作的对比:

<img src="https://img.saraba1st.com/forum/202305/21/182437rtme8c8dteec9tz8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-182226.jpg</strong> (191.38 KB, 下载次数: 0)

下载附件

2023-5-21 18:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 378#       发表于 2023-5-21 18:32

chat-with-nerf

使用交互式对话使AI定位nerf 3D场景物体

nerf通称神经辐射场技术，被认为是下一代3D与空间技术的主流发展，相关技术与发展速度迅猛，有兴趣可以自行搜索了解，本项目的立意在于使用LLM进行推理思考定位3D nerf场景内的物体

项目地址(目前依然需要GPT4  API):https://github.com/sled-group/chat-with-nerf

<img src="https://img.saraba1st.com/forum/202305/21/182813dleopnc8bzigmfpz.png" referrerpolicy="no-referrer">

<strong>demo.png</strong> (325.94 KB, 下载次数: 0)

下载附件

2023-5-21 18:28 上传

项目已经计划使用LLAVA代替GPT4+BLIP的工作模式进行纯本地化使用

<img src="https://img.saraba1st.com/forum/202305/21/183210xplpd9p7767tprv7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-183128.jpg</strong> (49.25 KB, 下载次数: 0)

下载附件

2023-5-21 18:32 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 379#       发表于 2023-5-21 18:53

LERF

可查询CLIP语言嵌入的神经辐射场

<img src="https://img.saraba1st.com/forum/202305/21/185441acszlc755zcbr2mz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-185431.jpg</strong> (117.27 KB, 下载次数: 0)

下载附件

2023-5-21 18:54 上传

项目地址https://www.lerf.io/

lerf的官方实现github仓库:https://github.com/kerrj/lerf

<img src="https://img.saraba1st.com/forum/202305/21/185330qcpgvl4vpwzscfsr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-185215.jpg</strong> (420.15 KB, 下载次数: 0)

下载附件

2023-5-21 18:53 上传

<img src="https://img.saraba1st.com/forum/202305/21/185335b442ttuo51539u5h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-185136.jpg</strong> (960.2 KB, 下载次数: 0)

下载附件

2023-5-21 18:53 上传

<img src="https://img.saraba1st.com/forum/202305/21/185339dc9crb9v7mqhq7cx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-185041.jpg</strong> (444.93 KB, 下载次数: 0)

下载附件

2023-5-21 18:53 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 380#       发表于 2023-5-21 19:07

Ziya-LLaMA-13B-v1

项目地址:https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1

介绍如下

<img src="https://img.saraba1st.com/forum/202305/21/190646elrebvbbrjj9bal4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-190524.jpg</strong> (73.56 KB, 下载次数: 0)

下载附件

2023-5-21 19:06 上传

<img src="https://img.saraba1st.com/forum/202305/21/190650uxineuygfnfmctiz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-190539.jpg</strong> (250.67 KB, 下载次数: 0)

下载附件

2023-5-21 19:06 上传

<img src="https://img.saraba1st.com/forum/202305/21/190654dglq7ggqpq6u8eyy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-190559.jpg</strong> (191.81 KB, 下载次数: 0)

下载附件

2023-5-21 19:06 上传

<img src="https://img.saraba1st.com/forum/202305/21/190658bqbuq5c85m35qa84.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230521-190622.jpg</strong> (203.79 KB, 下载次数: 0)

下载附件

2023-5-21 19:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  伊莉伊莉雅  
##### 381#       发表于 2023-5-22 08:09

<img src="https://static.saraba1st.com/image/smiley/face2017/143.png" referrerpolicy="no-referrer">感觉各种本地llm还是太弱了，指令执行能力相比gpt3.5和claude都差太多了


*****

####  s1234y  
##### 382#       发表于 2023-5-22 12:35

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60939259&amp;ptid=2126390" target="_blank">伊莉伊莉雅 发表于 2023-5-22 08:09</a>
 感觉各种本地llm还是太弱了，指令执行能力相比gpt3.5和claude都差太多了</blockquote>
试试vicuna-13b，这个效果还是挺不错的


*****

####  larthasliu  
##### 383#       发表于 2023-5-22 13:37

语言模型自己能部署的版本我觉得都没啥实用价值，就只能自己微调玩玩


*****

####  Machinery  
##### 384#       发表于 2023-5-22 16:49

Tree of Thoughts

通过考虑多种不同的推理路径和自我评估选择来

 决定下一步的行动，以及展望未来或回溯，做出全局选择所必需的策略与动作，形式上类似广度优先算法或深度优先算法

github项目地址(coming soon):https://github.com/ysymyth/tree-of-thought-llm

<img src="https://img.saraba1st.com/forum/202305/22/164649wz9o9bzdbgspdp7r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-164459.jpg</strong> (80.59 KB, 下载次数: 0)

下载附件

2023-5-22 16:46 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 385#       发表于 2023-5-22 16:54

 本帖最后由 Machinery 于 2023-5-22 16:56 编辑 

CoDi(Composable Diffusion)

微软的开源项目，多模态输入与多模态输出，组合多个不同模态输入以生成一个输出或多个输出，已经预定发布可使用的检查点权重

论文:https://arxiv.org/abs/2305.11846

项目主页:https://codi-gen.github.io/

github项目地址:https://github.com/microsoft/i-Code/tree/main/i-Code-V3

<img src="https://img.saraba1st.com/forum/202305/22/165236pe5nwi4ohln2net3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-165209.jpg</strong> (125.1 KB, 下载次数: 0)

下载附件

2023-5-22 16:52 上传

<img src="https://img.saraba1st.com/forum/202305/22/165239n2jmjhh38v81znhj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-165225.jpg</strong> (42.1 KB, 下载次数: 0)

下载附件

2023-5-22 16:52 上传

<img src="https://img.saraba1st.com/forum/202305/22/165443hd654nn6v561h5g5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-165421.jpg</strong> (523.44 KB, 下载次数: 0)

下载附件

2023-5-22 16:54 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 386#       发表于 2023-5-22 17:10

Segment-Any-Anomaly

检测任何异常，在没有任何微调的情况下，组合使用SAM与Grouding DINO解决zero shot工业异常检测需求的解决方案

github项目地址:https://github.com/caoyunkang/Segment-Any-Anomaly

<img src="https://img.saraba1st.com/forum/202305/22/171026z72li49lxeblwe7g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-170948.jpg</strong> (311.59 KB, 下载次数: 0)

下载附件

2023-5-22 17:10 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 387#       发表于 2023-5-22 17:27

CLAPSpeech

字节跳动出品，从比对语言-音频预训练中通过文本上下文学习声音韵律合成

1）通过文本编码器和韵律编码器的设计，鼓励模型在联合多模态空间中将文本上下文与其相应的韵律模式联系起来
2) 引入了多尺度预训练管道来捕获多个级别的韵律模式
3) 展示了如何将 CLAPSpeech 整合到现有的 TTS 模型中以获得更好的韵律

项目主页:https://clapspeech.github.io/
(注:即将开源相关代码，目前只有音频sample)

<img src="https://img.saraba1st.com/forum/202305/22/172709gkxnrdk7n77kn9wf.png" referrerpolicy="no-referrer">

<strong>main.png</strong> (471.81 KB, 下载次数: 0)

下载附件

2023-5-22 17:27 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 388#       发表于 2023-5-22 17:34

 本帖最后由 Machinery 于 2023-5-22 17:41 编辑 

Pengi

微软的开源项目，解决音频任务的音频语言模型，通过将所有音频任务构建为文本生成任务来达成迁移学习，以录音和文本作为输入，并生成自由格式的文本作为输出

输入音频由音频编码器表示为一系列连续的嵌入， 文本编码器对相应的文本输入做同样的事情， 两个序列组合为前缀以提示预训练的冻结LLM

Pengi的统一架构支持开放式任务和封闭式任务，无需任何额外的微调或特定于任务的扩展，对22个下游任务进行评估时，Pengi在其中几个任务中产生了最先进的Sota性能

github项目地址(代码将在检查后发布):https://github.com/microsoft/Pengi

<img src="https://img.saraba1st.com/forum/202305/22/173301zbp6a5zoez265oot.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-173227.jpg</strong> (91.31 KB, 下载次数: 0)

下载附件

2023-5-22 17:33 上传

(注:图片仅供说明，实际输入模型的为音频)

<img src="https://img.saraba1st.com/forum/202305/22/173432isbb38tbp3pl9lra.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-173414.jpg</strong> (63.95 KB, 下载次数: 0)

下载附件

2023-5-22 17:34 上传

用户同样可以通过额外附加其他不同文本输入提示Pengi模型

<img src="https://img.saraba1st.com/forum/202305/22/174105utdy0ii55mmtymuy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-174029.jpg</strong> (123.42 KB, 下载次数: 0)

下载附件

2023-5-22 17:41 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 389#       发表于 2023-5-22 18:01

TapTap

本作品为清华大学、复旦大学、Sea AI Lab三方合作，生成式大型表格预训练，使用大型表格数据进行预训练或微调后，可以提升模型对于高质量合成表格数据生成能力

根据团队说述，灵感来源于此项目(https://github.com/kathrinse/be_great)，基本设计理念为首先在大型公开表格数据上预训练，再之后在特定的表格上微调表格生成效果

论文地址:https://arxiv.org/abs/2305.09696

TapTap预训练数据集:https://huggingface.co/datasets/ztphs980/taptap_datasets

github项目地址:https://github.com/ZhangTP1996/TapTap

预训练模型下载:https://huggingface.co/models?search=ztphs980/taptap

<img src="https://img.saraba1st.com/forum/202305/22/180204sful3gduj3vqsz85.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-180059.jpg</strong> (280.33 KB, 下载次数: 0)

下载附件

2023-5-22 18:02 上传

<img src="https://img.saraba1st.com/forum/202305/22/175550eq1fjqjb4mb5y14j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-175524.jpg</strong> (344.76 KB, 下载次数: 0)

下载附件

2023-5-22 17:55 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 390#       发表于 2023-5-22 18:11

WebGUM

微调的指令跟随多模态Web自动导航操作模型，遵循指令的多模态代理WebGUM

依照用户指令，模型观察网页截图和 HTML页面并输出网络导航操作，例如自动点击和输入字符，WebGUM 通过在大型演示语料库上联合微调指令跟随语言模型和视觉转换器进行训练，这个配方提高了代理的基础视觉感知、HTML理解和多步推理的能力，远远优于之前的作品

在 MiniWoB 基准测试中，比之前最好的离线学习方法提高了 31.9% 以上，接近在线微调学习的SoTA水平，在WebShop基准测试中，30亿参数的WebGUM模型实现了优于现有SoTA PaLM-540B的性能

同时研究组还使用了训练有素的WebGUM模型收集了 347K 高质量演示数据，比之前的工作大 38 倍

论文地址:https://arxiv.org/abs/2305.11854

项目主页:https://sites.google.com/view/mm-webnav/

github代码仓库(等待整理):https://github.com/google-research/google-research/tree/master/mm_webnav

<img src="https://img.saraba1st.com/forum/202305/22/181116iacarb222bm69maj.png" referrerpolicy="no-referrer">

<strong>image.png</strong> (183.39 KB, 下载次数: 0)

下载附件

2023-5-22 18:11 上传

<img src="https://img.saraba1st.com/forum/202305/22/181121evvei50y9veaxeaa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-181010.jpg</strong> (70.25 KB, 下载次数: 0)

下载附件

2023-5-22 18:11 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 391#       发表于 2023-5-22 18:23

Text2NeRF

香港城市大学与腾讯人工智能实验室出品，Text2NeR 是一种文本驱动的3D场景生成框架，结合了神经辐射场 (NeRF) 和预训练的文本到图像扩散模型技术，可以从自然语言描述中生成多种视图一致的室内和室外3D场景，简单来说就是文本到3D NERF场景合成

项目主页:https://eckertzhang.github.io/Text2NeRF.github.io/

演示地址:https://youtu.be/mMdCMaqgdtk

代码(coming soon)

<img src="https://img.saraba1st.com/forum/202305/22/182024v6smazfsimu7oavw.jpg" referrerpolicy="no-referrer">

<strong>6f6b9e66-9563-4971-8e5f-a34b33c581c0.jpg</strong> (112.18 KB, 下载次数: 0)

下载附件

2023-5-22 18:20 上传

<img src="https://img.saraba1st.com/forum/202305/22/182134aeii9l0c7jl097ev.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-182105.jpg</strong> (143.11 KB, 下载次数: 0)

下载附件

2023-5-22 18:21 上传

<img src="https://img.saraba1st.com/forum/202305/22/182139aa021a1x1b50pbxb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-182055.jpg</strong> (369.15 KB, 下载次数: 0)

下载附件

2023-5-22 18:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 392#       发表于 2023-5-22 18:45

 本帖最后由 Machinery 于 2023-5-22 18:52 编辑 

LIMA

秉持少就是多的微调模型目标，LIMA 是一个 65B 参数的LLaMa语言模型，仅在1000个精心策划的提示和响应样本上使用标准监督损失进行了微调，没有任何强化学习或人类偏好建模。

LIMA展示了非常强大的性能，仅从训练数据中的少数示例中学习遵循特定的响应格式，包括从计划旅行路线到推测复杂的架空历史。而且，该模型倾向于很好地泛化到未出现在训练数据中不存在的任务。

在一项人类对照研究中，在43%的情况下，LIMA的反应与GPT-4相当或严格首选；与Bard相比，这一统计数据高达58%，与经过人工反馈训练的DaVinci003相比，这一数据高达65%。

总而言之，这些结果强烈表明，大型语言模型中的几乎所有知识都是在预训练期间学习的，并且只需要有限的指令调优数据来教授模型以产生高质量的输出。

论文地址:https://arxiv.org/abs/2305.11206

WTL测试

<img src="https://img.saraba1st.com/forum/202305/22/184532azs0866o064vzo8b.jpg" referrerpolicy="no-referrer">

<strong>20230522_182817.jpg</strong> (69.82 KB, 下载次数: 0)

下载附件

2023-5-22 18:45 上传

微调训练数据构成

<img src="https://img.saraba1st.com/forum/202305/22/184540tkjifyy4hh44h5gt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-182924.jpg</strong> (109.69 KB, 下载次数: 0)

下载附件

2023-5-22 18:45 上传

一个仅在1000个单轮互动中微调过的模型能进行多轮对话吗？我们在10个对话测试中测试了LIMA模型，将每条响应标记为失败、通过或优秀。

LIMA的响应对于一个零样本聊天机器人来说惊人地连贯，它参考了对话前几步中的信息。然而，很明显，该模型的运作超出了其应用范围，在10个对话的6个测试中，LIMA在3次互动内未能遵循提示。

为提高其对话能力,研究组收集了30个多轮对话链。其中10个对话由作者构造，其余20个基于Stack Exchange的评论链， 同时对样本进行了编辑以适应助手的风格。

使用组合的1030个例子从预训练的LLaMa模型中微调出LIMA的新版本，并根据用于零样本模型的相同提示进行10次对话。论文有这次对话的摘录。

图7显示了响应质量的分布。添加对话可以显著提高生成质量，优秀响应的比例从45.2%提高到76.1%。此，失败率从每42轮15次失败(零样本)下降到每46轮1次失败(微调)。

进一步比较了整个对话的质量，发现微调模型在10个对话的7个里明显更好，在3个里与零样本模型持平。这种从仅30个例子获得的飞跃能力，以及零样本模型能够对话这一事实，证实了这些能力是在预训练过程中学习到的，可以通过有限的监督调用。

<img src="https://img.saraba1st.com/forum/202305/22/184548kailuwgada2mfomf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230522-184212.jpg</strong> (51.37 KB, 下载次数: 0)

下载附件

2023-5-22 18:45 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  伊莉伊莉雅  
##### 393#       发表于 2023-5-22 22:11

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60943288&amp;ptid=2126390" target="_blank">s1234y 发表于 2023-5-22 12:35</a>
试试vicuna-13b，这个效果还是挺不错的</blockquote>
试过8bit量化，也不行，生成速度还慢(
让llm按照指定格式输出，chatglm和羊驼系试了一堆都没有符合条件的。
比如让她输出一些附加状态(可以简单理解为hp,mp等)
目前看老婆的首选还是claude(效果相比gpt4不会差太多，速度快，可白嫖)


*****

####  Machinery  
##### 394#       发表于 2023-5-23 05:28

 本帖最后由 Machinery 于 2023-5-23 05:31 编辑 

open_llm_leaderboard

hugface的固定基准LLM评测项目，说明如下图

项目地址:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

<img src="https://img.saraba1st.com/forum/202305/23/053114rhhhvazjzha5nnaz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-053033.jpg</strong> (346.16 KB, 下载次数: 0)

下载附件

2023-5-23 05:31 上传

<img src="https://img.saraba1st.com/forum/202305/23/052836pl10z0o9s0evss5l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-052804.jpg</strong> (320.25 KB, 下载次数: 0)

下载附件

2023-5-23 05:28 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 395#       发表于 2023-5-23 06:10

XTREME-UP

谷歌出品，随着语言技术的快速发展，让尽可能多的语言从这些快速发展的技术中受益非常重要，XTREME-UP，一个评估多语言模型的基准框架

github存储库包含用于在XTREME-UP上运行实验和评估模型的代码，同时也还包含相关的公开结果成绩跟踪器，其中列有已在 XTREME-UP 上评估的所有模型的结果和预测。

github地址:https://github.com/google-research/xtreme-up

评测相关的公开结果成绩跟踪器页面(待整理):https://github.com/google-research/xtreme-up/blob/main/evaluation/FULL_RESULTS.md

<img src="https://img.saraba1st.com/forum/202305/23/060835o1747gqggjzywgbb.jpg" referrerpolicy="no-referrer">

<strong>20230523_060821.jpg</strong> (396.47 KB, 下载次数: 0)

下载附件

2023-5-23 06:08 上传

<img src="https://img.saraba1st.com/forum/202305/23/060835y9uv9cu9gp1g7909.jpg" referrerpolicy="no-referrer">

<strong>20230523_060823.jpg</strong> (284.02 KB, 下载次数: 0)

下载附件

2023-5-23 06:08 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 396#       发表于 2023-5-23 06:20

ToolkenGPT

通过工具嵌入使用大量工具增强冻结的大型语言模型(如chatgpt等)

扩充大规模语言模型(LLM)使用外部工具已成为解决复杂问题的一种有希望方法，然而传统方法通过工具演示数据微调LLM既昂贵又局限于预定义的工具集。

最近的上下文学习范例缓解了这些问题，但有限的上下文长度只允许少许演示，导致模型对工具的使用理解不佳。

此外，当有许多工具可供选择时，上下文学习可能完全无法正常工作，在本文中提出了一种替代方法。

论文地址:https://arxiv.org/abs/2305.11554

<img src="https://img.saraba1st.com/forum/202305/23/061950f6664wg6y4g46uuy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-061714.jpg</strong> (170.49 KB, 下载次数: 0)

下载附件

2023-5-23 06:19 上传

<img src="https://img.saraba1st.com/forum/202305/23/061955nxaau7a1l1s1alwa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-061738.jpg</strong> (196.28 KB, 下载次数: 0)

下载附件

2023-5-23 06:19 上传

<img src="https://img.saraba1st.com/forum/202305/23/062000jbt0cktvmpvkppbv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-061759.jpg</strong> (197.12 KB, 下载次数: 0)

下载附件

2023-5-23 06:20 上传

<img src="https://img.saraba1st.com/forum/202305/23/062004foh2gsfbohz2hyso.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-061823.jpg</strong> (304.2 KB, 下载次数: 0)

下载附件

2023-5-23 06:20 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 397#       发表于 2023-5-23 06:33

 本帖最后由 Machinery 于 2023-5-23 06:35 编辑 

MMS

大规模多语言语音识别，可以用1100多种语言进行语音到文本生成(speech2text)和文本语音生成(TTS)，可以识别4000多种口语，在CC-BY-NC 4.0授权许可下可用的代码和模型，对比Whisper只有的一半单词错误率

Massively Multilingual Speech (MMS) 项目通过构建支持1100多种语言（是以前的10倍以上）的单一多语言语音识别模型，将语音技术从大约100种语言扩展到 1000 多种语言，语言识别模型能够识别4000多种语言（比以前多 40 倍），支持1400多种语言的预训练模型，以及支持1100多种语言的文本到语音模型。

有300M参数与1B参数两种预训练版本，同时还有三款对于特别语言微调过的版本，发布了能在GPU上实现的高效强制对齐的算法工具，该算法能够处理非常长的音频文件

模型与权重下载:https://github.com/facebookresearch/fairseq/tree/main/examples/mms

论文地址:https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/

相关的支持语言列表:https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html

项目主页:https://ai.facebook.com/blog/multilingual-model-speech-recognition/

<img src="https://img.saraba1st.com/forum/202305/23/063302zizia415pqd4qq4d.jpg" referrerpolicy="no-referrer">

<strong>20230523_063149.jpg</strong> (145.05 KB, 下载次数: 0)

下载附件

2023-5-23 06:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 398#       发表于 2023-5-23 06:42

 本帖最后由 Machinery 于 2023-5-23 06:44 编辑 

scikit-llm

将ChatGPT与其他强大的LLM(这个目前倒是还在鸽)无缝集成到scikit-learn中增强文本分析任务

github项目地址:https://github.com/iryna-kondr/scikit-llm

<img src="https://img.saraba1st.com/forum/202305/23/064223sgnyxmxeycgjyn3f.jpg" referrerpolicy="no-referrer">

<strong>20230523_063929.jpg</strong> (150.49 KB, 下载次数: 0)

下载附件

2023-5-23 06:42 上传

<img src="https://img.saraba1st.com/forum/202305/23/064205hnnnenlet1jvkxlg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-064146.jpg</strong> (92.63 KB, 下载次数: 0)

下载附件

2023-5-23 06:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 399#       发表于 2023-5-23 07:01

MLC LLM

通过使用各种深度定制技术使开源LLM可以方便地部署在浏览器、移动设备、消费级GPU和其他平台上，帮助使开源LLM易于访问与使用

利用设备本身的GPU，带来了LLM在 AMD、NVIDIA 和 Intel GPU、Apple Silicon、iPhone和Android手机上的普遍部署

github项目地址:https://github.com/mlc-ai/mlc-llm

项目说明页:https://mlc.ai/blog/2023/05/22/bringing-open-large-language-models-to-consumer-devices

<img src="https://img.saraba1st.com/forum/202305/23/065947wbkd92jj9zba53xs.jpg" referrerpolicy="no-referrer">

<strong>7fb474d9-aff5-4fb2-8f60-407ef54d78fc.jpg</strong> (44.78 KB, 下载次数: 0)

下载附件

2023-5-23 06:59 上传

<img src="https://img.saraba1st.com/forum/202305/23/065952dxgauabc11u1dgxx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-065457__01.jpg</strong> (63.99 KB, 下载次数: 0)

下载附件

2023-5-23 06:59 上传

<img src="https://img.saraba1st.com/forum/202305/23/065956jbcezft4f5e45494.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-065521.jpg</strong> (204.63 KB, 下载次数: 0)

下载附件

2023-5-23 06:59 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 400#       发表于 2023-5-23 07:06

 本帖最后由 Machinery 于 2023-5-23 07:11 编辑 

SpeechGPT

具有内在跨模态对话能力的大型语言模型，能够感知和生成多模态内容，利用离散语音表示，研究组首先构建了SpeechInstruct，一个大规模的跨模态语音指令数据集

之后采用了三阶段训练策略，包括模态适应预训练、跨模态指令微调和模态链指令微调，实验结果表明，SpeechGPT 具有令人印象深刻的能力来遵循多模态人类指令，并突出了使用一个模型处理多种模态的潜力

注:以下部分项目相关待整理

github项目地址:https://github.com/0nutation/SpeechGPT

开源相关数据集:https://github.com/0nutation/SpeechGPT#dataset

开源模型权重:https://github.com/0nutation/SpeechGPT#models

微调speechgpt模型https://github.com/0nutation/SpeechGPT#fine-tune-speechgpt

<img src="https://img.saraba1st.com/forum/202305/23/070702efyycvwas1zrbarn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-070522.jpg</strong> (209.88 KB, 下载次数: 0)

下载附件

2023-5-23 07:07 上传

<img src="https://img.saraba1st.com/forum/202305/23/070709euz3jowgnzix3aga.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-070623.jpg</strong> (515.85 KB, 下载次数: 0)

下载附件

2023-5-23 07:07 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 401#       发表于 2023-5-23 07:37

VisionLLM

大型语言模型同时也可以是以视觉为中心的任务相关的开放式解码者，关键思想，将图像输入视为通用LLM 解码器的某种外语

尽管有众多强大的视觉基础模型（VFM）可用，但它们仍然局限于预定义形式的任务，难以匹配 LLM 的开放式任务能力

在这项工作中，为以视觉为中心的任务提出了一个基于 LLM 的框架，称为 VisionLLM。该框架通过将图像视为外语并将以视觉为中心的任务与可以使用语言指令灵活定义和管理的语言任务对齐，为视觉和语言任务提供了统一的视角

基于 LLM 的解码器可以根据这些指令为开放式任务做出适当的预测，大量实验表明，所提出的 VisionLLM 可以通过语言指令实现不同级别的任务定制，从细粒度的对象级到粗粒度的任务级定制，都取得了良好的效果

值得注意的是，使用基于通用LLM的框架，VisonLLM模型可以在COCO上实现超过60%的 mAP水平，与特定微调检测的模型几乎相当

github项目地址:https://github.com/OpenGVLab/VisionLLM

演示Demo:https://github.com/OpenGVLab/InternGPT

<img src="https://img.saraba1st.com/forum/202305/23/073618rfk269kq62d4bg37.jpg" referrerpolicy="no-referrer">

<strong>20230523_073033.jpg</strong> (219.26 KB, 下载次数: 0)

下载附件

2023-5-23 07:36 上传

<img src="https://img.saraba1st.com/forum/202305/23/073622ustlii77ls4ki5l7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-073441.jpg</strong> (503.8 KB, 下载次数: 0)

下载附件

2023-5-23 07:36 上传

<img src="https://static.saraba1st.com/image/smiley/face2017/066.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202305/23/073716h1028ubu1lr2spvu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-073700.jpg</strong> (32.3 KB, 下载次数: 0)

下载附件

2023-5-23 07:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 402#       发表于 2023-5-23 07:47

 本帖最后由 Machinery 于 2023-5-23 07:49 编辑 

Reprompting

微软出品，自动化最佳CoT prompt命令采样生成器，Reprompting算法使用了一种称为Gibbs采样的迭代采样方法，它将问题框定为从CoT相关配方样本的联合分布中采样

由于分布难以直接表征， 所以使用近似方法采样，这种采样方法通过尝试不同的指令并确定哪个最有效来帮助确定最佳CoT指令输入

github项目地址(待整理):https://github.com/weijia-xu/Reprompting

<img src="https://img.saraba1st.com/forum/202305/23/074401bmvrn13gpl0pvlps.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-074154.jpg</strong> (468.76 KB, 下载次数: 0)

下载附件

2023-5-23 07:44 上传

<img src="https://img.saraba1st.com/forum/202305/23/074743mfsqdobfzsl7jrfs.jpg" referrerpolicy="no-referrer">

<strong>ea896664-ff5e-4bd3-8ed4-442dcd3c34c9.jpg</strong> (197.72 KB, 下载次数: 0)

下载附件

2023-5-23 07:47 上传

<img src="https://img.saraba1st.com/forum/202305/23/074746s44h4h84onh54kch.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-074705.jpg</strong> (587.11 KB, 下载次数: 0)

下载附件

2023-5-23 07:47 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 403#       发表于 2023-5-23 16:04

DDPO

使用强化学习实现自动对齐难以学习的扩散模型生成目标

项目地址:https://rl-diffusion.github.io/

<img src="https://img.saraba1st.com/forum/202305/23/160257jls1adsbgjludful.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-155142.jpg</strong> (272.23 KB, 下载次数: 0)

下载附件

2023-5-23 16:02 上传

<img src="https://img.saraba1st.com/forum/202305/23/160303nuw8sudwwrc8cr3w.jpg" referrerpolicy="no-referrer">

<strong>20230523_160016.jpg</strong> (205.33 KB, 下载次数: 0)

下载附件

2023-5-23 16:03 上传

<img src="https://img.saraba1st.com/forum/202305/23/160320r3nwnwnr1uvf3gus.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-155136.jpg</strong> (281.99 KB, 下载次数: 0)

下载附件

2023-5-23 16:03 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 404#       发表于 2023-5-23 16:49

 本帖最后由 Machinery 于 2023-5-23 16:51 编辑 

RecurrentGPT

使用LLM的提示工程复现长期RNN网络的工作原理实现内容迭代(虽然很离谱但是是真的<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

github项目地址:https://github.com/aiwaves-cn/RecurrentGPT

<img src="https://img.saraba1st.com/forum/202305/23/165111fgxsxs1c8sxo8ov4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-165023.jpg</strong> (352.98 KB, 下载次数: 0)

下载附件

2023-5-23 16:51 上传

<img src="https://img.saraba1st.com/forum/202305/23/165115g2n5544s5abb6fjb.png" referrerpolicy="no-referrer">

<strong>recurGPT-prompt.png</strong> (477.45 KB, 下载次数: 0)

下载附件

2023-5-23 16:51 上传

<img src="https://img.saraba1st.com/forum/202305/23/165121vhpwhfp7bv3yybpp.png" referrerpolicy="no-referrer">

<strong>recurGPT-case.png</strong> (480.45 KB, 下载次数: 0)

下载附件

2023-5-23 16:51 上传

<img src="https://img.saraba1st.com/forum/202305/23/165124cyjfs5rrs8gcddgr.png" referrerpolicy="no-referrer">

<strong>web_demo.png</strong> (568.24 KB, 下载次数: 0)

下载附件

2023-5-23 16:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 405#       发表于 2023-5-23 21:28

seahorse

谷歌出品，Seahorse是一个用于多语言、多方面摘要评估的数据集，包含96K的摘要，并在6个质量维度上进行了人工评分：可理解性、重复性、语法、归因、主要思想和简洁性

涵盖6种语言、9个系统和4个数据集，由于其规模和范围，SEAHORSE 既可以作为评估学习指标的基准，也可以作为训练此类指标的大规模数据集资源

相关论文:https://arxiv.org/abs/2305.13194

github项目地址:https://github.com/google-research-datasets/seahorse

数据集压缩包地址:https://storage.googleapis.com/seahorse-public/seahorse_data.zip

<img src="https://img.saraba1st.com/forum/202305/23/212828p02y161lv0r6kkpk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-212703.jpg</strong> (185.95 KB, 下载次数: 0)

下载附件

2023-5-23 21:28 上传

<img src="https://img.saraba1st.com/forum/202305/23/212834vbevqz7zn7ssb5zv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230523-212741.jpg</strong> (429.6 KB, 下载次数: 0)

下载附件

2023-5-23 21:28 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  右代宫嘉音  
##### 406#       发表于 2023-5-24 13:11

太难翻了。有个RMKV有人转过没？我已经下了傻瓜包在玩了。


*****

####  大江户战士  
##### 407#       发表于 2023-5-24 13:12

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60971479&amp;ptid=2126390" target="_blank">右代宫嘉音 发表于 2023-5-24 13:11</a>

太难翻了。有个RMKV有人转过没？我已经下了傻瓜包在玩了。</blockquote>
RWKV中文比较差，而且因为结构原因对prompt非常敏感


*****

####  某浩  
##### 408#       发表于 2023-5-24 13:43

这个帖子是怎么一回事，权限低一点都进不来


*****

####  第五秋  
##### 409#       发表于 2023-5-24 15:42

OPEN AI最近发了个特别恶心的声明，摆明了说要求其他公司和开源都不能研究高水平模型，笑吸了

希望早日有竞品然后被打死


*****

####  右代宫嘉音  
##### 410#       发表于 2023-5-24 16:39

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60971500&amp;ptid=2126390" target="_blank">大江户战士 发表于 2023-5-24 13:12</a>

RWKV中文比较差，而且因为结构原因对prompt非常敏感</blockquote>
确实蠢，叫它“翻译 侧身”，是想让它把侧身翻译成英文，结果它就直接翻译上一段


*****

####  Machinery  
##### 411#       发表于 2023-5-24 21:42

ChainForge

开源可视化节点化LLM编程测试框架，用于LLM的数据流提示工程环境，用于分析和评估LLM的输出响应等

项目说明:https://ianarawjo.medium.com/introducing-chainforge-a-visual-programming-environment-for-prompt-engineering-bc6910be01cf

github项目主页:https://github.com/ianarawjo/ChainForge

<img src="https://img.saraba1st.com/forum/202305/24/214155ofec88occco6j327.png" referrerpolicy="no-referrer">

<strong>239989325-83757804-4288-4fc2-b28d-fd0826bae6a1.png</strong> (450.7 KB, 下载次数: 0)

下载附件

2023-5-24 21:41 上传

<img src="https://img.saraba1st.com/forum/202305/24/214231dqbloa0pq4kcoio3.png" referrerpolicy="no-referrer">

<strong>239989404-43c87ab7-aabd-41ba-8d9b-e7e9ebe25c75.png</strong> (320.85 KB, 下载次数: 0)

下载附件

2023-5-24 21:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 412#       发表于 2023-5-24 21:52

AudioToken

利用潜在扩散模型生成以录音音频输入为条件的图像，使用预训练的音频编码模型，将音频编码为新的标记，可以将其视为音频和文本表示之间的适配层

项目主页:https://pages.cs.huji.ac.il/adiyoss-lab/AudioToken/

github仓库地址:https://github.com/guyyariv/AudioToken

<img src="https://img.saraba1st.com/forum/202305/24/215027n13qc8bh4wci3kz4.png" referrerpolicy="no-referrer">

<strong>pipeline.png</strong> (151.94 KB, 下载次数: 0)

下载附件

2023-5-24 21:50 上传

<img src="https://img.saraba1st.com/forum/202305/24/215023t00crt6dqqrx3dtr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-214934.jpg</strong> (105.38 KB, 下载次数: 0)

下载附件

2023-5-24 21:50 上传

<img src="https://img.saraba1st.com/forum/202305/24/215239eg2g774wk5z257zs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-215124.jpg</strong> (234.37 KB, 下载次数: 0)

下载附件

2023-5-24 21:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 413#       发表于 2023-5-24 22:13

 本帖最后由 Machinery 于 2023-5-24 22:16 编辑 

AlpacaFarm

低成本快速复现RLHF等人类反馈微调过程的整合框架与相关代码库

根据指令进行建模通常分三个步骤进行:
1. 通过示范进行监督微调
2. 从人工反馈中学习
3. 通过交互进行人工评估

AlpacaFarm 的目标是提供三个关键组件来解决第二步和第三步:
1. 利用API模型(如GPT-4、ChatGPT)低成本模拟成对反馈  
2. 自动评估方法开发  
3. 提供学习算法的参考实现,用于比较和修改

项目说明主页:https://crfm.stanford.edu/2023/05/22/alpaca-farm.html

github项目主页:https://github.com/tatsu-lab/alpaca_farm

<img src="https://img.saraba1st.com/forum/202305/24/220621srj95arwr5w5btbn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-220559.jpg</strong> (371.94 KB, 下载次数: 0)

下载附件

2023-5-24 22:06 上传

<img src="https://img.saraba1st.com/forum/202305/24/220624shuu11jxudl28q13.jpg" referrerpolicy="no-referrer">

<strong>fig1.jpg</strong> (219.25 KB, 下载次数: 0)

下载附件

2023-5-24 22:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 414#       发表于 2023-5-24 22:40

Perception Test

多模态视频模型的评估基准，提出了一种新颖的多模态视频基准——感知测试——来评估预训练多模态模型（例如 Flamingo、BEiT-3 或 GPT-4）的感知和推理技能。与侧重于计算任务（例如分类、检测或跟踪）的现有基准相比，感知测试侧重于跨视频、音频的技能（记忆、抽象、物理、语义）和推理类型（描述性、解释性、预测性、反事实）和文本模态，提供全面高效的评估工具。

基准测试以零样本/少量样本或有限微调机制探测预训练模型的迁移能力。为此，感知测试引入了11.6k数量的真实世界视频，平均视频长度为 23 秒，旨在展示感知上有趣的情况，由全球约 100 名参与者拍摄。

这些视频用六种类型的标签（多项选择和基础视频问答、对象和点轨迹、时间动作和声音片段）进行了严密的密集标注，从而可以进行语言和非语言的评估。

基准的微调和验证拆分数据集是公开可用的（CC-BY 许可），此外还有一个带有保留测试拆分的挑战服务器。

与最先进的视频QA模型相比，人类基线结果显示，双方性能存在显著差距（91.4% 对 43.6%），这表明在多模态视频理解方面存在很大的改进空间。

论文地址:https://arxiv.org/abs/2305.13786

github项目仓库:https://github.com/deepmind/perception_test

<img src="https://img.saraba1st.com/forum/202305/24/224010nkk3il7autfr4ipk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-223842__01.jpg</strong> (356.42 KB, 下载次数: 0)

下载附件

2023-5-24 22:40 上传

<img src="https://img.saraba1st.com/forum/202305/24/224015i4debhaalfovlh4l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-223918__01.jpg</strong> (245.49 KB, 下载次数: 0)

下载附件

2023-5-24 22:40 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 415#       发表于 2023-5-24 23:20

MQUAKE

通过多跳问题评估(经过)知识编辑的语言模型关联问答能力

研究组通过首先创建一个基于维基数据的知识图谱来构建MQuAKE，包含问题实体和它们之间的关系，然后根据知识图谱中的事实链以及编辑来创建多跳关联问题

<img src="https://img.saraba1st.com/forum/202305/24/231919rn9udpa5adkd0ndt.jpg" referrerpolicy="no-referrer">

<strong>20230524_231102.jpg</strong> (88.42 KB, 下载次数: 0)

下载附件

2023-5-24 23:19 上传

之后评估现有的知识编辑技术(如ROME与MEMIT，本贴前边有相关介绍)，现有的知识编辑方法可以为模型注入事实并准确地回忆它们（高编辑准确度），但在多跳问题上却灾难性地失败（低多跳准确度）

<img src="https://img.saraba1st.com/forum/202305/24/231939spzsd42iiysi4bvi.jpg" referrerpolicy="no-referrer">

<strong>20230524_230942.jpg</strong> (124.09 KB, 下载次数: 0)

下载附件

2023-5-24 23:19 上传

<img src="https://img.saraba1st.com/forum/202305/24/231944fopqa28he13am1aa.jpg" referrerpolicy="no-referrer">

<strong>20230524_231317.jpg</strong> (36.02 KB, 下载次数: 0)

下载附件

2023-5-24 23:19 上传

研究组提出了一个简单而有效的基线模型MeLLo，不需要训练，并存储编辑记忆在内存中，使任何关联检索都可以访问，MeLLo通过自行检查提示LLM进行实时事实编辑

<img src="https://img.saraba1st.com/forum/202305/24/231954vszwan3wzks9ss9a.jpg" referrerpolicy="no-referrer">

<strong>20230524_231812.jpg</strong> (37.68 KB, 下载次数: 0)

下载附件

2023-5-24 23:19 上传

MeLLo在不同设置下表现出色，优于 SoTA 方法效果，同时它适用于不同规模的语言模型

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 416#       发表于 2023-5-24 23:35

 本帖最后由 Machinery 于 2023-5-24 23:37 编辑 

Ultrachat-UltraLLaMA

通过扩展高质量的教学对话来增强聊天语言模型

论文地址:https://arxiv.org/abs/2305.14233

github项目地址:https://github.com/thunlp/UltraChat

ultrachat数据集地址:https://huggingface.co/datasets/stingning/ultrachat

研究组开源了一个经过系统设计的、多样化的、信息丰富的、大规模的教学对话数据集UltraChat

UltraChat包含150万个高质量的多轮对话，涵盖广泛的主题和说明，同时对UltraChat进行了统计分析揭示了其在各种关键指标上的优势，包括规模、平均长度、多样性、连贯性等

在UltraChat的基础上，研究组还微调了一个LLaMA模型以创建一个强大的对话模型，名为UltraLLaMA，经过相关评估表明，UltraLLaMA 始终优于其他开源模型，包括 Vicuna，这是以前公认的最先进的开源模型

<img src="https://img.saraba1st.com/forum/202305/24/233658pf8lt862ex5tbvxz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-233559.jpg</strong> (120.27 KB, 下载次数: 0)

下载附件

2023-5-24 23:36 上传

<img src="https://img.saraba1st.com/forum/202305/24/233658c6tf89wjewaq5e71.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-233625.jpg</strong> (69.79 KB, 下载次数: 0)

下载附件

2023-5-24 23:36 上传

<img src="https://img.saraba1st.com/forum/202305/24/233658jhbblphk44kfkkb3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-233638.jpg</strong> (61.68 KB, 下载次数: 0)

下载附件

2023-5-24 23:36 上传

<img src="https://img.saraba1st.com/forum/202305/24/233507jvvzv6t5evu4x76e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-233356.jpg</strong> (200 KB, 下载次数: 0)

下载附件

2023-5-24 23:35 上传

注:

<img src="https://img.saraba1st.com/forum/202305/24/233543ut77asmz76c76c6x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230524-233412.jpg</strong> (47.3 KB, 下载次数: 0)

下载附件

2023-5-24 23:35 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 417#       发表于 2023-5-25 00:14

 本帖最后由 Machinery 于 2023-5-25 00:19 编辑 

QLoRA

一种高效的4-bit微调方法，可以大幅减少显存使用量，足以在单个48GB GPU上微调65B参数量级的模型，同时还能保留完整的16-bit微调性能

QLoRA通过冻结的4-bit量化预训练语言模型将梯度反向传播到低阶适配器(LoRA)进行微调训练，QLoRA引入了许多创新来节省显存而不牺牲表现性能

研究组最好的模型系列，命名为Guanaco，在 Vicuna的基准测试中优于所有以前公开发布的模型，达到ChatGPT性能水平的99.3%，同时只需要在单个 GPU 上进行24小时的微调

使用QLoRA对1000多个模型进行微调，提供跨8个指令数据集、多种模型类型（LLaMA、T5）和无法通过常规微调运行的模型规模（例如 33B 和65B参数模型），结果表明，即使使用比以前的 SoTA 更小的模型，QLoRA对小型高质量数据集的微调也会产生最先进的结果

论文地址:https://arxiv.org/abs/2305.14314

github项目代码库:github.com/artidoro/qlora

模型ELO评分测试，由GPT4评估

<img src="https://img.saraba1st.com/forum/202305/25/001307pb8cccv6cq7z8kve.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-001230.jpg</strong> (201.23 KB, 下载次数: 0)

下载附件

2023-5-25 00:13 上传

改善的显存占用

<img src="https://img.saraba1st.com/forum/202305/25/001444snjubx0ngq3x7ju7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-001359.jpg</strong> (98.76 KB, 下载次数: 0)

下载附件

2023-5-25 00:14 上传

相关评估

<img src="https://img.saraba1st.com/forum/202305/25/001810cws2iiq3e12h1irg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-001623.jpg</strong> (118.81 KB, 下载次数: 0)

下载附件

2023-5-25 00:18 上传

<img src="https://img.saraba1st.com/forum/202305/25/001810kz47c2gkgkw2psok.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-001706.jpg</strong> (236.49 KB, 下载次数: 0)

下载附件

2023-5-25 00:18 上传

<img src="https://img.saraba1st.com/forum/202305/25/001810y0z282fd8w2ljdsk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-001751.jpg</strong> (153.11 KB, 下载次数: 0)

下载附件

2023-5-25 00:18 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 418#       发表于 2023-5-25 00:31

AdaptiveConsistency

使用LLM进行高效的自我一致性推理

(注:关于模型的自我一致性推理，可以简单理解为多次对同一个输入进行输出采样结果，之后按照结果的一致性选择出现频率最高的结果，这个方法可以有效提升模型的推理能力)

<img src="https://img.saraba1st.com/forum/202305/25/002728cwk76rjw64rkghk1.png" referrerpolicy="no-referrer">

<strong>ac_teaser_new.png</strong> (195.27 KB, 下载次数: 0)

下载附件

2023-5-25 00:27 上传

关于AdaptiveConsistency(自适应一致性)，这是一种经济高效、与模型无关的技术，它使用轻量级动态停止标准调整每个问题的自我一致性样本采样数量

(这个想法说明起来很简单：抽取40个样本并使用多数作为答案，但是如果前10个输出相同呢？我们还要等所有40个采样结果输出完吗？)

研究组通过对13个数据集和两个LLM的实验表明，自适应一致性将样本的算力预算减少了多达6倍，平均准确率下降不到0.1%

项目主页:http://sample-step-by-step.info/

github的代码实现库:https://github.com/Pranjal2041/AdaptiveConsistency

相关评估

<img src="https://img.saraba1st.com/forum/202305/25/003142b62tyzn4lb1y4mni.jpg" referrerpolicy="no-referrer">

<strong>20230525_003122.jpg</strong> (62.65 KB, 下载次数: 0)

下载附件

2023-5-25 00:31 上传

<img src="https://img.saraba1st.com/forum/202305/25/003142bk5bcc0c022zx2a0.jpg" referrerpolicy="no-referrer">

<strong>20230525_003124.jpg</strong> (82.98 KB, 下载次数: 0)

下载附件

2023-5-25 00:31 上传

<img src="https://img.saraba1st.com/forum/202305/25/003142aa0dx1pzvuavjoc4.jpg" referrerpolicy="no-referrer">

<strong>20230525_003127.jpg</strong> (99.3 KB, 下载次数: 0)

下载附件

2023-5-25 00:31 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 419#       发表于 2023-5-25 01:19

 本帖最后由 Machinery 于 2023-5-25 01:21 编辑 

IfQA

在反事实预设下的大型开放域问答数据集

尽管反事实推理是智能的一个基本方面，但由于缺乏大规模的反事实开放域问答 (QA) 基准，因此难以评估和改进这种能力的模型。

为了解决这个空白，研究组引入了第一个这样的数据集，名为IfQA，其中每个问题都基于通过“if”子句的反事实预设。例如，如果洛杉矶位于美国东海岸，那么洛杉矶和巴黎之间的时差是多少？

此类问题要求模型不仅仅从 Web 检索直接的事实知识，它们必须识别正确的信息以检索和推理甚至可能与参数中内置的事实相悖的想象场景情况。

IfQA数据集包含超过3800个由众包工作者在相关百科段落上注释的问题。实证分析表明，IfQA 数据集对于现有的开放域 QA 方法具有很高的挑战性

论文地址:https://arxiv.org/abs/2305.14010

数据集地址:待更新

<img src="https://img.saraba1st.com/forum/202305/25/011852j7xllol7xchz7eli.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-011326.jpg</strong> (899.53 KB, 下载次数: 0)

下载附件

2023-5-25 01:18 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 420#       发表于 2023-5-25 01:52

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=60979547&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-5-25 00:14</a>
QLoRA

一种高效的4-bit微调方法，可以大幅减少显存使用量，足以在单个48GB GPU上微调65B参数量级的模型， ...</blockquote>
简单介绍下本篇论文的内容：

论文主要贡献：NormalFloat数据类型(4-bit的浮点数据类型)、分页优化器、双量化(技巧)

12个小时内在1个消费类GPU上微调出可以达到97% ChatGPT性能的模型，微调出的模型在所有程度测试和评估中都匹配了16-bit模型的性能

<img src="https://img.saraba1st.com/forum/202305/25/014123m0bmqrqbxw1erzbr.jpg" referrerpolicy="no-referrer">

<strong>20230525_014117.jpg</strong> (46.17 KB, 下载次数: 0)

下载附件

2023-5-25 01:41 上传

“但是与全微调相比，使用 LoRA 进行微调不会更差吗？”诚然，常规的LoRA微调表现不佳，通过应用的超参数调整，如果将 LoRA 附加到所有线性层，结果证明它工作得非常好，没有任何问题

<img src="https://img.saraba1st.com/forum/202305/25/014312jejjjhd9d0jzd0j4.jpg" referrerpolicy="no-referrer">

<strong>20230525_014218.jpg</strong> (18.07 KB, 下载次数: 0)

下载附件

2023-5-25 01:43 上传

借助QLoRA技术，微调非常有效，每天可以在华盛顿大学的小型 GPU 集群上微调100多个LLaMA模型

微调了所有常用的指令跟随数据集，有些数据集不好，有些很好，FLAN v2数据集最好在指令遵循方面取得好成绩

<img src="https://img.saraba1st.com/forum/202305/25/015239utt00dtzotmpjioi.jpg" referrerpolicy="no-referrer">

<strong>20230525_015235.jpg</strong> (120.97 KB, 下载次数: 0)

下载附件

2023-5-25 01:52 上传

指令调优数据集有利于指令遵循，但不利于聊天机器人的性能表现，数据集质量远远重要于数据集数量，9000+数量的数据集击败一百万级别的微调数据集，Open Assistant数据集是高质量的，FLAN v2数据集适合指令微调，不适合聊天对话

 Guanaco模型在由人类和GPT-4共同评判的 Vicuna 基准竞赛中击败了 ChatGPT，但是Vicuna基准测试太小，因此，研究组在Open Assistant数据上创建了一个新的基准，它看起来更可靠

收集了Guanaco模型的失败案例：它不擅长数学，但有利于给出错误信息和心智理论的建议

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 421#       发表于 2023-5-25 16:54

 本帖最后由 Machinery 于 2023-5-25 16:55 编辑 

PEARL

来自微软，使用提示框架对大型语言模型进行操作提醒，提升模型在长文档上规划和执行操作的能力

PEARL，一个改进长文档推理的提示框架，它包括三个阶段：动作挖掘、计划制定和计划执行。

更具体地说，给定一个关于长文档的问题，PEARL将问题分解为一系列动作（例如，SUMMARIZE(总结)、FIND_EVENT(寻找目标)、FIND_RELATION(寻找关联)等操作），然后在文档上执行它们以获得答案。

PEARL的每个阶段操作都是通过LLM（在研究组的工作中是 GPT-4）的零样本或少样本提示来实现的，只需要最少量的人工输入

在QuALITY数据集的一个具有挑战性的子集上评估 了PEARL的效果，其中包含需要对长篇叙述文本进行复杂推理的问题。PEARL在此数据集上的表现优于零样本和思维链提示，并且消融实验表明PEARL的每个阶段对其性能都至关重要

github项目地址:https://github.com/SimengSun/pearl

<img src="https://img.saraba1st.com/forum/202305/25/165432yvd7mmwajyjovw2m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-165211.jpg</strong> (193.4 KB, 下载次数: 0)

下载附件

2023-5-25 16:54 上传

<img src="https://img.saraba1st.com/forum/202305/25/165438uxni4rrrr90rrwkx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-165231.jpg</strong> (105.08 KB, 下载次数: 0)

下载附件

2023-5-25 16:54 上传

<img src="https://img.saraba1st.com/forum/202305/25/165442edztvxxcd22wn024.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-165304.jpg</strong> (231.6 KB, 下载次数: 0)

下载附件

2023-5-25 16:54 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 422#       发表于 2023-5-25 17:01

Gorilla

连接大量 API 的大型语言模型，经过微调的基于 LLaMA 的模型，在编写API调用方面超越了GPT-4 的性能

Gorilla在三个大型机器学习中心数据集上进行训练：Torch Hub、TensorFlow Hub 和 HuggingFace

零样本准确率上优于GPT-4、Chat-GPT和Claude。Gorilla非常可靠，并且显着减少了幻觉错误的API输出

当与文档检索器结合使用时，Gorilla展示了适应测试时文档更改的强大能力，支持灵活的用户更新或版本更改，它还**减轻了直接提示LLM时经常遇到的幻觉问题

项目主页:https://gorilla.cs.berkeley.edu/

构架流程

<img src="https://img.saraba1st.com/forum/202305/25/165818ilf3glfsdg7so3ln.png" referrerpolicy="no-referrer">

<strong>gorilla_method.png</strong> (1009.25 KB, 下载次数: 0)

下载附件

2023-5-25 16:58 上传

<img src="https://img.saraba1st.com/forum/202305/25/170102t8iokj60ng1c1b0s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-170046.jpg</strong> (177.87 KB, 下载次数: 0)

下载附件

2023-5-25 17:01 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 423#       发表于 2023-5-25 17:20

MPC

提示LLM作为聊天机器人模块，进行长时间的开放域对话

高质量的对话代理一般需要进行微调，MPC方法利用预训练的大型语言模型（LLM）作为用于长期一致性的独立对话模块，通过灵活使用诸如小样本提示、思维链 (CoT)、和外部存储器等进行长期开放域对话

经过人工评价结果表明MPC与经过微调的开放域对话聊天模型(Blenderbot)性能接近，使其成为一个有效的非微调对话解决方案

官方实现github项目仓库:https://github.com/krafton-ai/MPC

<img src="https://img.saraba1st.com/forum/202305/25/172016omfnm4sy3b6gfn39.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-171816.jpg</strong> (270.75 KB, 下载次数: 0)

下载附件

2023-5-25 17:20 上传

<img src="https://img.saraba1st.com/forum/202305/25/172025seg82k89oibep82p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-171937.jpg</strong> (93.69 KB, 下载次数: 0)

下载附件

2023-5-25 17:20 上传

<img src="https://img.saraba1st.com/forum/202305/25/172030ozu8y87nktcttk0z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-171951.jpg</strong> (239.42 KB, 下载次数: 0)

下载附件

2023-5-25 17:20 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 424#       发表于 2023-5-25 17:31

PandaGPT

单模型进行全模态指令追随，PandaGPT是一种既能看又能听的通用指令遵循模型，试点实验表明，PandaGPT可以执行复杂的任务，例如生成详细的图像描述、根据视频编写故事以及回答有关音频的问题

更有趣的是，PandaGPT可以同时接受多模态输入并自然地组合它们的语义，例如，PandaGPT可以连接物体在照片中的样子和它们在音频中的声音

PandaGPT结合了ImageBind的多模态编码器和 Vicuna 的大型语言模型，值得注意的是，虽然 PandaGPT 在六种模态(text, image/video, audio, depth, thermal, IMU)上展示了令人印象深刻的跨模态能力，由于 ImageBind 提供的共享嵌入空间，它仅使用对齐的图像文本对进行训练

项目空间:https://panda-gpt.github.io/

github项目仓库:https://github.com/yxuansu/PandaGPT

演示Demo:https://huggingface.co/spaces/GMFTBY/PandaGPT

指令数据集:https://huggingface.co/datasets/openllmplayground/pandagpt_visual_instruction_dataset

模型下载权重:https://huggingface.co/openllmplayground/pandagpt_13b_max_len_400

<img src="https://img.saraba1st.com/forum/202305/25/172927y88bqx8ca3a3r8aa.jpg" referrerpolicy="no-referrer">

<strong>7fa2e390-2d1e-47f2-a196-064d428fd34b.jpg</strong> (51.3 KB, 下载次数: 0)

下载附件

2023-5-25 17:29 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 425#       发表于 2023-5-25 18:01

VPEval

可视化编程文本到图像生成和评估，简单来说就是像写代码一样写出你要生成的图片，优点是编程，缺点也是编程

VPGen，一个可解释的分步T2I(文本到图像)生成框架，将T2I生成分解为三个步骤：对象/计数生成、布局生成和图像生成，使用语言模型来处理前两个步骤（对象/计数生成和布局生成），方法是在文本布局对上对其进行微调，逐步T2I生成框架提供比端到端模型更强的空间控制，此外，利用预训练语言模型的世界知识，克服了以前只能处理预定义对象类的布局引导T2I作品的局限性

其次，是VPEval，这是一个基于可视化编程的可解释和可解释的T2I生成评估框架，与以往采用单一评分模型的 T2I 评估在某些技能上准确但在其他技能上不可靠不同，VPEval 生成的评估程序调用一组视觉模块，这些模块是不同技能的专家，并且还提供评估结果的视觉+文本解释，与广泛使用的基于单一模型的评估相比，VPEval 为特定技能和开放式提示提供了更与人类相关的评估

项目页面:https://vp-t2i.github.io/

github项目仓库:https://github.com/aszala/VPEval

<img src="https://img.saraba1st.com/forum/202305/25/180151koe5umu5intomzt5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-174103.jpg</strong> (191.8 KB, 下载次数: 0)

下载附件

2023-5-25 18:01 上传

<img src="https://img.saraba1st.com/forum/202305/25/180158q2z3gphj7b0c99fx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-174130.jpg</strong> (130.26 KB, 下载次数: 0)

下载附件

2023-5-25 18:01 上传

<img src="https://img.saraba1st.com/forum/202305/25/180204pn5didd1kzqbic55.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-174140.jpg</strong> (170.72 KB, 下载次数: 0)

下载附件

2023-5-25 18:02 上传

<img src="https://img.saraba1st.com/forum/202305/25/180209ehlc58zt8ucul8kl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-174152.jpg</strong> (158.38 KB, 下载次数: 0)

下载附件

2023-5-25 18:02 上传

<img src="https://img.saraba1st.com/forum/202305/25/180215c1oa6y1qtapyzty6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-175418.jpg</strong> (149.8 KB, 下载次数: 0)

下载附件

2023-5-25 18:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 426#       发表于 2023-5-25 19:22

BLIP-Diffusion

预训练主题(subject/personality/concept)表示的可控文本到图像生成与编辑，将内置的多模态文本和主题控制引入扩散模型

面向主题的图像生成模型可以根据文本提示为输入主题创建新的表现形式，现有的模型需要进行冗长的微调，并难以保持主题的真实性

BLIP-Diffusion，一个新的面向主题的图像生成模型，支持多模态控制，可以定制主题图像和文本提示的输入，与其他面向主题的生成模型不同，BLIP-Diffusion引入了一个新的多模态编码器，该编码器通过BLIP-2进行预训练，以产生与文本对齐的视觉表示

首先根据BLIP-2预训练多模态编码器，以产生与文本对齐的视觉表征，然后，研究组设计了一个名为提示上下文生成的主题表征学习任务，该任务使扩散模型能够利用这种视觉表征并生成新的主题表现形式

与以前的方法(如DreamBooth)相比，BLIP-Diffusion模型实现了零样本主题驱动的生成，并实现了自定义主题的高效微调，加速度高达20倍

BLIP-Diffusion可以灵活地与现有技术如ControlNet和prompt-to-prompt相结合，以实现全新的面向主题的生成和编辑应用

项目空间:https://dxli94.github.io/BLIP-Diffusion-website/

<img src="https://img.saraba1st.com/forum/202305/25/192145sofacfroicsclefl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230525-184928.jpg</strong> (218.45 KB, 下载次数: 0)

下载附件

2023-5-25 19:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 427#       发表于 2023-5-26 03:16

 本帖最后由 Machinery 于 2023-5-26 03:18 编辑 

LayoutGPT

谷歌出品，使用LLM充当高效可控的布局生成器辅助文本到图像生成

在图像生成中获得高度可控性通常需要复杂、细粒度的输入，如布局引导细节等，然而，与简单的文本输入相比，这样的输入给用户带来了很大的负担

研究组考虑了使用大型语言模型(LLM)通过从文本条件生成布局来充当视觉规划器，从而与图像生成模型协作生成合理图像

LayoutGPT，一种以样式表语言编写上下文视觉演示的方法，以提高LLM的视觉规划技能，LayoutGPT可以在多个领域生成合理的布局，从2D图像到3D室内场景皆可

LayoutGPT在将语言概念（如数字和空间关系）转换为布局安排以实现忠实的文本到图像生成方面也表现出卓越的性能，当与下游图像生成模型相结合时，LayoutGPT 的性能优于直接的文本到图像模型/系统 20-40%，并且在设计视觉布局的数字和空间正确性方面实现了与人类用户相当的性能

最后，LayoutGPT在3D室内场景合成中实现了与监督方法相当的性能，展示了其在多个视觉领域的有效性和潜力

github项目地址:https://github.com/weixi-feng/LayoutGPT

<img src="https://img.saraba1st.com/forum/202305/26/031225s5qu7ify1fyvipky.jpg" referrerpolicy="no-referrer">

<strong>20230526_031218.jpg</strong> (248.18 KB, 下载次数: 0)

下载附件

2023-5-26 03:12 上传

<img src="https://img.saraba1st.com/forum/202305/26/031643pnixj0zpppju8xl9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-031530__01.jpg</strong> (356 KB, 下载次数: 0)

下载附件

2023-5-26 03:16 上传

<img src="https://img.saraba1st.com/forum/202305/26/031643i8ybd7xy6x8ghezx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-031517__01.jpg</strong> (410.2 KB, 下载次数: 0)

下载附件

2023-5-26 03:16 上传

<img src="https://img.saraba1st.com/forum/202305/26/031643i3j4sujzjuhqj2t2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-031502__01.jpg</strong> (412.25 KB, 下载次数: 0)

下载附件

2023-5-26 03:16 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 428#       发表于 2023-5-26 03:38

Goat

使用微调的LLaMA在算术任务上优于GPT-4

Goat是一种经过微调的LLaMA模型，在一系列算数任务上明显优于GPT-4，Goat在综合生成的合成数据集上进行微调，在BIG-bench基准的算数子任务上实现了最先进的性能，特别是，零样本(zero shot)Goat-7B的精度就可以与少样本(few shot)PaLM-540B模型的精度相当甚至超过

Goat仅通过有监督的微调就可以在大数加减法上达到近乎完美的精度，这在以前的预训练语言模型（如 Bloom、OPT、GPT-NeoX 等）中几乎是不可能的

研究组将Goat的卓越性能归功于LLaMA 对数字的一致标记化，为了解决更具挑战性的任务，如大数乘法和除法，研究组提出了一种根据可学习性对任务进行分类的方法，随后利用基本算术原理将不可学习的任务（例如多位数乘法和除法）分解为一系列可学习的任务

研究组彻底检查了模型的性能，对提出的分解步骤的方法的有效性进行了全面评估，此外，还可以在24GB VRAM GPU上使用LoRA轻松训练Goat-7B，从而促进其他研究人员的可重复性

论文地址:https://arxiv.org/abs/2305.14201

github项目地址:https://github.com/liutiedong/goat

<img src="https://img.saraba1st.com/forum/202305/26/033646r5p5ou1uybyyxquf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-033252.jpg</strong> (147.66 KB, 下载次数: 0)

下载附件

2023-5-26 03:36 上传

可学习和不可学习的算术任务的总结和示例

<img src="https://img.saraba1st.com/forum/202305/26/033653y8nj1n7iiu3gxz77.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-033357.jpg</strong> (154.29 KB, 下载次数: 0)

下载附件

2023-5-26 03:36 上传

微调LLaMA的提示和目标示例

<img src="https://img.saraba1st.com/forum/202305/26/033707zwx90c9s0yj1ayll.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-033417.jpg</strong> (139.91 KB, 下载次数: 0)

下载附件

2023-5-26 03:37 上传

GPT-4和Goat-7B在BIG-bench基准的Arithmetic子任务和额外选择的算术任务上的结果，使用指标精确字符串匹配/数字匹配，以百分比显示结果，测试 中的GPT-4和山羊使用完全相同的问题和提示，使用的是5月10日GPT-4的API版本

<img src="https://img.saraba1st.com/forum/202305/26/033722qkmm2l1gg2ihfubh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-033441.jpg</strong> (336.24 KB, 下载次数: 0)

下载附件

2023-5-26 03:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 429#       发表于 2023-5-26 03:56

 本帖最后由 Machinery 于 2023-5-26 04:02 编辑 

SAIL-7B

搜索增强语言模型，SAIL-7B是一种针对内部和外部搜索引擎生成的复杂搜索结果进行微调的模型，SAIL-7b优于搜索增强的ChatGPT和Vicuna

提问:搜索引擎总能改进语言模型吗？
回答:不会，通过研究发现，通过应用于LLM的搜索引擎结果，在某些种类任务上的改进是微乎其微的，虽然搜索引擎检索范围广泛的最新信息，但检索结果可能会引起争议或分散注意力，这样的基础信息不一定对语言模型有帮助。

提问:如何用搜索引擎改进语言模型？
回答:研究组微调了一个基于真实搜索引擎输出的大型语言模型 (LLaMA-7B版本)，经过微调的模型可以自动提取信息丰富的搜索结果并标记分散注意力的项目，通过搜索增强微调，SAIL-7b可以通过搜索引擎得到显着提升，其性能优于最先进的聊天机器人，包括ChatGPT和Vicuna-13B，同时参数更少。

提问:如何评估搜索增强的大型语言模型？
回答:使用 GPT-4 对指令进行自动评分。SAIL-7b 比搜索增强的ChatGPT和Vicuna模型获得更高的分数，同时在开放式 QA 基准测试中测试基于搜索的回答性能，同时使用事实和公平性检查，目标之一是与大型语言模型与错误信息、仇恨和刻板印象作斗争，在UniLC基准测试上测试了几个 LLM。

项目地址:https://openlsr.org/sail-7b

演示Demo:https://huggingface.co/spaces/luohy/SAIL-7B

github项目地址(待整理):https://github.com/luohongyin/SAIL

<img src="https://img.saraba1st.com/forum/202305/26/035604uymmkyh5ykq1zmdk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-035539.jpg</strong> (201.96 KB, 下载次数: 0)

下载附件

2023-5-26 03:56 上传

<img src="https://img.saraba1st.com/forum/202305/26/040115grnswrmm0vqnnzlm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-035941__01.jpg</strong> (629.46 KB, 下载次数: 0)

下载附件

2023-5-26 04:01 上传

<img src="https://img.saraba1st.com/forum/202305/26/040122an2z3astsloansax.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-035955__01.jpg</strong> (262.96 KB, 下载次数: 0)

下载附件

2023-5-26 04:01 上传

<img src="https://img.saraba1st.com/forum/202305/26/040127vv3hcqhzcthhvctu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-040030__01.jpg</strong> (342.45 KB, 下载次数: 0)

下载附件

2023-5-26 04:01 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 430#       发表于 2023-5-26 04:15

C-Eval

C-Eval是全面的中文基础模型评估套件，涵盖了52个不同学科的13948个多项选择题，分为四个难度级别，以对LLM的中文性能进行细致而严密的能力判断，同时希望C-Eval能够帮助开发人员跟踪模型开发的进展，以及分析开发中模型的优点和弱点。

github项目地址:https://github.com/SJTU-LIT/ceval/blob/main/README_zh.md

<img src="https://img.saraba1st.com/forum/202305/26/041508c46zucyucceekzz3.png" referrerpolicy="no-referrer">

<strong>overview.png</strong> (156.31 KB, 下载次数: 0)

下载附件

2023-5-26 04:15 上传

<img src="https://img.saraba1st.com/forum/202305/26/041514pp7qkvoecj1ecsiz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230526-041453.jpg</strong> (238.16 KB, 下载次数: 0)

下载附件

2023-5-26 04:15 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 431#       发表于 2023-5-27 02:42

Falcon-40B
Falcon-40B-Instruct
Falcon-7B
Falcon-7B-Instruct
RefinedWeb数据集
Falcon-RW-7B
Falcon-RW-1B

Falcon-40B是一个40B参数的因果解码器模型，在由TII构建，并在RefinedWeb的1000BToken标记上训练，并使用精选语料库进行了增强。

根据TII Falcon LLM授权许可提供商业使用允许
(授权文件:https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE.txt)

Falcon-40B优于LLaMA、StableLM、RedPajama、MPT等系列模型，请参阅OpenLLM 排行榜

<img src="https://img.saraba1st.com/forum/202305/27/023159pmr5f85saff7apza.jpg" referrerpolicy="no-referrer">

<strong>20230527_021745.jpg</strong> (169.69 KB, 下载次数: 0)

下载附件

2023-5-27 02:31 上传

Falcon-40B是一个原始的、预训练的模型，原理上应该针对大多数下游任务进一步微调，如果您正在寻找更适合在聊天格式中遵循通用指令的版本，您或许可以尝试查看Falcon-40B-Instruct
(https://huggingface.co/tiiuae/falcon-40b-instruct)

Falcon-40B支持以下语言 (NLP)：英语、德语、西班牙语、法语

Falcon-40B主要接受英语、德语、西班牙语、法语的训练，在意大利语、葡萄牙语、波兰语、荷兰语、罗马尼亚语、捷克语和瑞典语方面的能力也有限。

此外，由于它是在代表网络的大规模语料库上进行训练的，因此它会带有网上常见的刻板印象和偏见。

Technology Innovation Institute项目主页:https://huggingface.co/tiiuae

<img src="https://img.saraba1st.com/forum/202305/27/024236fxpzh6zsuvrvatot.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230527-024222.jpg</strong> (132.1 KB, 下载次数: 0)

下载附件

2023-5-27 02:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 432#       发表于 2023-5-27 04:01

Voyager

一种无梯度学习方法，Minecraft中第一个完全由LLM驱动的具身终身学习代理者，它可以在没有人为干预的情况下不断探索世界，获得多样化的技能，并进行新的发现

Voyager获得的独特物品的数量增加了3.3倍，行进距离增加了2.3倍，解锁关键技术树的里程碑的速度比之前的SOTA快了15.3倍

<img src="https://img.saraba1st.com/forum/202305/27/040040uive0aemamtwb4wt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230527-035240.jpg</strong> (138.72 KB, 下载次数: 0)

下载附件

2023-5-27 04:00 上传

之前经典方法普遍采用强化学习 (RL) 和对原始动作进行操作的模仿学习，这对于系统探索、可解释性和泛化来说可能具有挑战性，而基于大型语言模型(LLM)的代理者的最新进展利用封装在预训练的 LLM 中的世界知识来生成一致的行动计划或可执行策略

Minecraft没有强加预定义的最终目标或固定的故事情节，而是提供了一个具有无限可能性的独特游乐场，一个有效的终身学习代理者应该具有与人类玩家相似的能力：

1.根据其当前的技能水平和世界状态提出合适的任务，例如，如果它发现自己在沙漠而不是森林中，学会在铁之前收获沙子和仙人掌
 2.根据环境反馈提炼技能并将掌握的技能记忆以便将来在类似情况下重用(例如，学会像打蜘蛛一样打僵尸)
3.不断探索世界，以自我驱动的方式寻找新的任务

Voyager也由以上三种定义的关键组件组成

<img src="https://img.saraba1st.com/forum/202305/27/040055hj4v02x5mzum2jvd.jpg" referrerpolicy="no-referrer">

<strong>25c8519a-2fd9-412e-8eda-e9371a33bab3.jpg</strong> (91.94 KB, 下载次数: 0)

下载附件

2023-5-27 04:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 433#       发表于 2023-5-27 17:51

ProlificDreamer

具有变分分数蒸馏的高保真和多样化文本到3D生成

分数蒸馏采样(SDS/Score distillation sampling)通过提取预训练的大规模文本到图像扩散模型，在文本到 3D 生成方面显示出巨大潜力，但存在过饱和、过度平滑和低多样性问题

在本工作中，研究组建议将3D参数建模为随机变量而不是SDS中的常数，并提出变分分数蒸馏(VSD/variational score distillation)，一个原则性的基于粒子的变分框架，用于解释和解决文本到3D生成中的上述问题

研究表明SDS是VSD的特例，会导致较小和较大的CFG权重样本质量更差，相比之下，VSD与各种CFG权重一起作为来自扩散模型的ancestral采样效果很好，同时使用共同的CFG权重（即 7.5）时提高了多样性和样本质量

进一步介绍了文本到3D设计空间的各种改进，例如蒸馏时间安排和密度初始化，这些与蒸馏算法紧密相关，但尚未得到很好的探索

整体方法，称为ProlificDreamer，可以生成高渲染分辨率（即512x512）和具有丰富结构和复杂效果（例如烟雾和水滴）的高保真NeRF，此外，如果从NeRF初始化，VSD微调的网格纹理可以非常细致且逼真

项目地址:https://ml.cs.tsinghua.edu.cn/prolificdreamer/

<img src="https://img.saraba1st.com/forum/202305/27/175106fn92hbpb1dr12p12.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230527-175008.jpg</strong> (450.95 KB, 下载次数: 0)

下载附件

2023-5-27 17:51 上传

<img src="https://img.saraba1st.com/forum/202305/27/175112xeplayot8aawm2au.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230527-175035.jpg</strong> (402.47 KB, 下载次数: 0)

下载附件

2023-5-27 17:51 上传

<img src="https://img.saraba1st.com/forum/202305/27/175119tlzkq1n2ok22pn4l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230527-175049.jpg</strong> (174.96 KB, 下载次数: 0)

下载附件

2023-5-27 17:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 434#       发表于 2023-5-27 19:05

listen-denoise-action

使用扩散模型进行音频驱动的人体动作合成

相关论文:https://arxiv.org/abs/2211.09707

项目主页:https://www.speech.kth.se/research/listen-denoise-action/

相关文章:https://levtech.jp/media/article/column/detail_246/

从语音自动生成 3D avatar运动，为配合各种音乐而生成的舞蹈动作质量非常高，超越了以往的动作生成模型，本文提出了音频驱动运动合成的去噪扩散概率模型，并在舞蹈和姿势的运动捕捉数据集上进行了训练

一般来说，姿势和舞蹈的动作生成与音频的上下文密切相关，因此有必要考虑音频的上下文以实现适当的动作，它在从语音生成动作的任务中也非常重要

然而，生成音频驱动的动作充满了困难，姿势是个体的、不确定的，并且通常很难由音频决定，舞蹈也是如此，它通常与拍号和小节等音乐结构同步，但除此之外，即使是单个作品或表演类型也可以采用多种形式

在机器学习中，很难处理这种模棱两可的处理，只有非常强大的模型才能准确捕捉到它们，缺乏令人信服和可控的运动合成模型迫使我们依赖昂贵的动作捕捉和体力劳动形式的影视动作人工合成

通过广泛的用户研究和客观评估，证明了本文的模型能够生成具有顶级运动质量的全身姿势和舞蹈，具有独特的风格

除了合成高质量舞蹈动作的结果外，发现所提出的方法还可以应用于按照给定路径生成程式化步行动作的任务。所提出的模型使用 100STYLE 数据集（该数据集包含超过400万帧的运动捕捉数据，用于100种不同的运动方式）进行训练，可适应请求的转弯和速度变化，能够生成自然的多种风格的任意轨迹行走动作

<img src="https://img.saraba1st.com/forum/202305/27/190113ko1mnaw9nnrn2rul.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230527-190058.jpg</strong> (107.21 KB, 下载次数: 0)

下载附件

2023-5-27 19:01 上传

同时将发布用于训练我们的舞蹈模型的 Motorica 舞蹈数据集，收集了包含超过 6 小时的音乐和 8 种风格的高质量舞蹈动作捕捉记录

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 435#       发表于 2023-5-28 19:38

Prompt-Free-Diffusion

无提示扩散：从文本到图像扩散模型中提取“文本”

简单来说就是不需要输入文本prompt，只需要输入图片组成上下文prompt生成图片，而且组件性能优良

相关论文:https://arxiv.org/abs/2305.16223

github项目地址:https://github.com/SHI-Labs/Prompt-Free-Diffusion

<img src="https://img.saraba1st.com/forum/202305/28/193728s8fn889useksa9mn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230528-192907.jpg</strong> (434.79 KB, 下载次数: 0)

下载附件

2023-5-28 19:37 上传

<img src="https://img.saraba1st.com/forum/202305/28/193748zt0jv901mtu66y9n.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230528-193702.jpg</strong> (300.32 KB, 下载次数: 0)

下载附件

2023-5-28 19:37 上传

<img src="https://img.saraba1st.com/forum/202305/28/193740x95f323am8q3i5zy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230528-193522.jpg</strong> (626.97 KB, 下载次数: 0)

下载附件

2023-5-28 19:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 436#       发表于 2023-5-29 00:14

启真

利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调

github项目地址:https://github.com/CMKRG/QiZhenGPT

<img src="https://img.saraba1st.com/forum/202305/29/001421j5m7rp67did5by76.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230529-001308.jpg</strong> (948.31 KB, 下载次数: 0)

下载附件

2023-5-29 00:14 上传

<img src="https://img.saraba1st.com/forum/202305/29/001425v0mm252fxfc0fjef.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230529-001401.jpg</strong> (160.66 KB, 下载次数: 0)

下载附件

2023-5-29 00:14 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 437#       发表于 2023-5-29 05:29

 本帖最后由 Machinery 于 2023-5-29 05:32 编辑 

NeTI

用于文本到图像个性化概念训练生成的神经时空表示方法(A Neural Space-Time Representation for Text-to-Image Personalization)

<img src="https://img.saraba1st.com/forum/202305/29/052853t5dzemf8mc8s6m82.jpg" referrerpolicy="no-referrer">

<strong>6e2fef83-654e-457c-a1bf-ecd8d5b7141c.jpg</strong> (201.13 KB, 下载次数: 0)

下载附件

2023-5-29 05:28 上传

项目主页:https://neuraltextualinversion.github.io/NeTI/

NeTI实现代码库:https://github.com/NeuralTextualInversion/NeTI

研究组探索了一个新的文本条件化空间(text-conditioning space)，专注于优化已经存在的去噪时间步长(时间)和去噪U-Net层(空间)两种条件组合的优化效果，展示了引人注目的特性

个性化概念在这个时间与空间组合中，被压缩成数百个向量，每个向量又对应时间和空间的双重组合，使得这个空间难以直接优化

因此，通过本文提出的方法，使用一个小的神经映射器来优化这个空间中的隐式表示的概念，该映射器接收当前的时间和空间参数(前文的去噪时间步长与去噪U-Net层)，输出匹配的对应标记嵌入，这样，个性化概念由学习的映射器的参数来表达，通过可学习的映射器的参数，产生紧凑同时富有表现力的输出结果，与其他个性化方法类似，这个神经映射器的输出位于文本编码器的输入空间中

同时观察到可以通过引入文本旁路显著提高个性化概念的收敛性和视觉保真度，所以这个神经映射器将额外输出一个残差，该残差被添加到文本编码器的输出中

<img src="https://img.saraba1st.com/forum/202305/29/052910cn6wybbogkbnbwko.png" referrerpolicy="no-referrer">

<strong>Overview.png</strong> (501.04 KB, 下载次数: 0)

下载附件

2023-5-29 05:29 上传

最后，展示了如何对我们的隐式表示的结果施加基于重要性的排序，使用已经训练的单一模型让用户控制需要学习的个性化概念的重建和可编辑性

通过针对一系列概念和提示的结果，证明了我们的方法的有效性，展示了无需微调生成模型本身的任何参数，即可生成高质量和可控组合的能力的方法

<img src="https://img.saraba1st.com/forum/202305/29/053033l9p06v8fbb2a8baa.jpg" referrerpolicy="no-referrer">

<strong>editability.jpg</strong> (938.02 KB, 下载次数: 0)

下载附件

2023-5-29 05:30 上传

使用 Nested Dropout 控制可编辑性，用户可以在推理时控制生成图像的视觉保真度和文本保真度之间的平衡，当应用更强的dropout时，会得到更粗糙/语义化的概念表示，更适合编辑和新组合

<img src="https://img.saraba1st.com/forum/202305/29/053142e5beiz88x9bzl6kz.jpg" referrerpolicy="no-referrer">

<strong>style_mixing.jpg</strong> (742.97 KB, 下载次数: 0)

下载附件

2023-5-29 05:31 上传

还可以进行风格混合两个学习概念的几何和外观，通过在不同的时间步开始执行混合，可以控制需要从几何概念传递到输出图像的信息量


*****

####  Machinery  
##### 438#       发表于 2023-5-29 05:52

ToolBench-ToolLLaMA

本项目旨在构建开源、大规模、高质量的指令调优SFT数据，以促进构建具有通用工具使用能力的强大LLM，提供数据集、相应的训练和评估脚本，以及在ToolBench上经过微调的功能强大的模型ToolLLaMA

github项目地址:https://github.com/OpenBMB/ToolBench

<img src="https://img.saraba1st.com/forum/202305/29/055200n0zenf7c9te3unit.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230529-055027.jpg</strong> (360.2 KB, 下载次数: 0)

下载附件

2023-5-29 05:52 上传

注:

<img src="https://img.saraba1st.com/forum/202305/29/055238t56hs877phf401s2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230529-055206.jpg</strong> (65.71 KB, 下载次数: 0)

下载附件

2023-5-29 05:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  oldttt  
##### 439#       发表于 2023-5-29 06:01

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61032403&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-5-29 05:29</a>
NeTI

用于文本到图像个性化概念训练生成的神经时空表示方法(A Neural Space-Time Representation for Text ...</blockquote>
这个有意思


*****

####  Machinery  
##### 440#       发表于 2023-5-30 02:56

 本帖最后由 Machinery 于 2023-5-30 02:57 编辑 

LLM-ToolMaker

通过使用外部工具可以提高大型语言模型 (LLM) 的问题解决能力，然而，这取决于现有工具的可用性

在这项工作中提出了被称为LLMs As Tool Makers (LATM)的闭环框架，迈出了消除这种依赖性的第一步，LLMs在其中创建自己的可重用工具来解决问题

论文地址:https://arxiv.org/abs/2305.17126

github代码实现仓库:https://github.com/ctlllll/LLM-ToolMaker

<img src="https://img.saraba1st.com/forum/202305/30/025309ituzff55z5pfqfmp.jpg" referrerpolicy="no-referrer">

<strong>20230530_025258.jpg</strong> (61.43 KB, 下载次数: 0)

下载附件

2023-5-30 02:53 上传

方法包括两个阶段：
1.工具制作：LLM充当工具制造者，为给定任务制作工具，其中工具作为Python实用函数实现
2.工具使用：LLM作为工具使用者，应用LATM构建的工具解决问题

相关工具的使用者甚至可以是与工具的生成者相同或不同的 LLM模型，Tool-making使LLM可以不断生成可应用于不同请求的工具，以便将来的请求可以在有利于解决任务时调用相应的API

此外，LLM之间在工具制造和工具使用阶段的分工，引入了在不降低生成工具和问题解决方案质量的情况下，实现更高成本效益的方法

例如，认识到制造工具比使用工具需要更复杂的能力，我们可以将强大但资源密集型的模型用作工具制造者，将轻量级但具有成本效益的模型用作工具用户

<img src="https://img.saraba1st.com/forum/202305/30/025728xkivjk4kilq3v6me.jpg" referrerpolicy="no-referrer">

<strong>20230530_025721.jpg</strong> (307.43 KB, 下载次数: 0)

下载附件

2023-5-30 02:57 上传

研究组验证了LATM在各种复杂推理任务（包括 Big-Bench 任务）中的有效性，其中包括以GPT-4作为工具制造者，GPT-3.5作为工具使用者

当使用GPT-4作为Tool Maker和GPT-3.5 Turbo作为 Tool User时，本文方法可以通过GPT-3.5 Turbo的低成本和快速实现与GPT-4相当的性能，并且通过在BigBench基准和实际应用程序的几个复杂推理任务上验证了本方法

<img src="https://img.saraba1st.com/forum/202305/30/025518u708b9v7xa57x905.jpg" referrerpolicy="no-referrer">

<strong>20230530_025333.jpg</strong> (202.81 KB, 下载次数: 0)

下载附件

2023-5-30 02:55 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 441#       发表于 2023-5-30 03:24

SumCoT

具有内容元素感知摘要能力的大语言模型，以及专家对齐的评估的思维链方法(SumCoT)，

github项目代码库(待整理):https://github.com/Alsace08/SumCoT

作为自动生成简明摘要，摘要中需要包含源文档的关键思想，新闻子领域最主流的数据集，CNN/DailyMail和BBC XSum已被广泛用于性能基准测试

然而，这些数据集的参考摘要被证明是嘈杂的，在事实幻觉和信息冗余等方面，为了应对这一挑战，首先按照Lasswell (1948)提出的“Lasswell 通信模型”对新的专家编写的元素感知测试集进行注释，使参考摘要能够客观、全面地关注更细粒度的新闻元素

利用构造的新的测试集，观察到LLM令人惊讶的零样本总结能力，解决了先前工作中LLM的零样本摘要的人类偏好与自动评估指标之间结果不一致的问题

此外，提出了一种名为Summary Chain-of-Thought (SumCoT)技术来引导LLM逐步生成摘要，这有助于将源文档的更多细粒度细节整合到与人类写作思维相关的最终摘要中

<img src="https://img.saraba1st.com/forum/202305/30/031943dnre35nnnna9dz9a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-031553.jpg</strong> (158.13 KB, 下载次数: 0)

下载附件

2023-5-30 03:19 上传

<img src="https://img.saraba1st.com/forum/202305/30/031948l224p9ocibaxijp4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-031623.jpg</strong> (216.94 KB, 下载次数: 0)

下载附件

2023-5-30 03:19 上传

<img src="https://img.saraba1st.com/forum/202305/30/031952q5x37dx5zalklapt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-031851.jpg</strong> (313.05 KB, 下载次数: 0)

下载附件

2023-5-30 03:19 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 442#       发表于 2023-5-30 04:23

Decker

用异构知识与双重检查进行常识性事实验证

github项目地址:https://github.com/Anni-Zou/Decker

常识事实验证作为常识问答(QA)的一个具有挑战性的分支，旨在通过事实来验证给定的常识性主张是否正确

回答常识性问题需要结合不同层次的知识，然而，现有研究主要依赖于从结构化知识库中获取非结构化证据或潜在推理路径，而未能同时利用异构知识的优势

鉴于此，研究组提出了Decker，这是一种常识性事实验证模型，能够通过揭示结构化知识和非结构化知识之间的潜在关系来桥接不同结构的知识

<img src="https://img.saraba1st.com/forum/202305/30/034959gp2wgvlelle6wdzz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-034946.jpg</strong> (135.91 KB, 下载次数: 0)

下载附件

2023-5-30 03:49 上传

来自CSQA2.0的示例QA，通过进行双重检查的异构知识，通过推理抓住有价值的信息来得出答案

尽管最近基于知识图谱(KG/Knowledge Graph)的方法在常识QA上取得了显着的表现，它们更适合和适应多项选择题，因为它们强调发现问题和候选答案之间的关联模式

比如回答螃蟹生活在什么样的环境中的问题？ 以及候选回答咸水、加拉帕戈斯群岛和鱼市场，基于知识图谱的方法设法捕获图谱中的螃蟹-海水-咸水路径，导向了正确的预测，尽管如此，他们还是遇到了一个处理常识性事实时的瓶颈

例如：
当问世界范围内七月是否总是夏天？
知识图谱的方法具有倾向于检测七月和夏天之间的强烈偏置关联，这有可能会导致模型进行错误的预测

总的来说，以前的研究有两个主要的局限性， 一方面，结构化的知识充满了实体之间的结构信息，但存在稀疏性和覆盖面有限的问题，另一方面，非结构化知识提供了丰富而广泛的上下文感知信息，但会遇到嘈杂的问题，这两种知识可以自然地相互补充

DECKER，一种常识性事实验证器
1.首先，检索异构知识包括知识图谱子图和先验后的几个事先构建的相关事实

 2.构建了一个关于问题组成事实的图谱对问题和事实进行编码，然后使用关系图卷积网络(R-GCN)来对异构知识进行推理和过滤

 3.最后，它采用多头注意力池机制来获得丰富的知识表示的最终细化，并将其与下游任务的问题组合使用

<img src="https://img.saraba1st.com/forum/202305/30/041526qasvz98f5rf46csv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-041514.jpg</strong> (91.19 KB, 下载次数: 0)

下载附件

2023-5-30 04:15 上传

<img src="https://img.saraba1st.com/forum/202305/30/041621xq02wgflfzlv2pv2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-041548.jpg</strong> (65.72 KB, 下载次数: 0)

下载附件

2023-5-30 04:16 上传

CREAK和CSQA2.0数据集上的实验结果，评估指标是准确率，在只使用449M参数的情况下战胜了3B模型的效果

<img src="https://img.saraba1st.com/forum/202305/30/041904byy2p8vn4ni5m7zv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-041832__01.jpg</strong> (304.41 KB, 下载次数: 0)

下载附件

2023-5-30 04:19 上传

一个示例，显示了Decker模型如何工作以获得正确答案，而作为基线的模型失败了，其中紫色文本表示事实，绿色文本表示概念

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 443#       发表于 2023-5-30 04:35

MAPS-mt

用大型语言模型探索类人模式翻译策略

github项目代码库:https://github.com/zwhe99/MAPS-mt

大型语言模型 (LLM) 在一般场景中展示了令人印象深刻的能力，表现出接近、在某些方面甚至超过人类智能的水平

在众多技能中，LLM的翻译能力备受关注，与仅专注于源-目标映射的传统机器翻译相比，基于LLM的翻译可以通过模仿人工翻译过程，该过程需要许多准备步骤来确保高质量的翻译

具体来说MAPS框架让LLM们能够首先分析给定的源文本，并提取翻译相关知识的三个方面：关键词，主题和相关演示来指导翻译过程

为了过滤掉嘈杂和无用的知识，采用了一种基于质量估计的选择机制，通过实验表明，MAPS在来自最新的WMT22测试集的八个翻译方向上对text-davinci-003和Alpaca模型带来了显著且一致的改进

通过进一步分析表明，提取的知识对于解决翻译中高达59%的幻觉错误至关重要

<img src="https://img.saraba1st.com/forum/202305/30/043029uj2tgv2j5w2eaed5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-042850.jpg</strong> (136.81 KB, 下载次数: 0)

下载附件

2023-5-30 04:30 上传

<img src="https://img.saraba1st.com/forum/202305/30/043034zxady7t52dnaticj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-042911.jpg</strong> (206.88 KB, 下载次数: 0)

下载附件

2023-5-30 04:30 上传

MAPS的框架分为三个阶段：
1.Knowledge Mining：LLM分析源语句并生成对翻译有用的三个方面的知识：关键词、主题和相关匹配域

 2.知识整合：以不同类型的知识分别为导向，LLM生成多个翻译候选和基线候选翻译
3.知识选择：知识最高的候选翻译，选择QE分数最高的作为最终翻译

相关评测结果:

<img src="https://img.saraba1st.com/forum/202305/30/043448gh9denz7gfn0innm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-043409.jpg</strong> (343.72 KB, 下载次数: 0)

下载附件

2023-5-30 04:34 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 444#       发表于 2023-5-30 05:11

Stable-Alignment

在模拟人类社会中训练符合与社会对齐观念的语言模型

github项目仓库:https://github.com/agi-templar/Stable-Alignment

仓库包括以下相关资料:
1.运行模拟社会沙盒的相关代码
2.169K的相关数据集
3.stable alignment的训练代码
4.社会友好的语言模型权重

人工智能系统中的社会一致性旨在确保这些模型根据既定的社会价值观行事，然而，与通过社交互动获得价值判断共识的人类不同，当前的语言模型(LM)被训练为严格地孤立地复制他们的训练语料库，导致在不熟悉的场景中泛化不佳，并且容易受到对抗性攻击(模式对抗)

这项工作提出了一种新颖的训练范式，允许LM从模拟的仿真社交互动中学习，与现有方法相比，本方法更具可扩展性和效率，在对齐基准和人工评估中展示了卓越的性能

<img src="https://img.saraba1st.com/forum/202305/30/044935hnnl31akgljaz0ab.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-044841.jpg</strong> (126.5 KB, 下载次数: 0)

下载附件

2023-5-30 04:49 上传

与采用额外的代理模型(如RLHF)不同，Stable Alignment直接在语言模型和模拟的社会互动之间建立对齐，通过运行规则引导的模拟社会来收集细粒度的互动数据，其中包括集体评级、详细反馈和“逐步”修订的响应

与现有方法相比，Stable Alignment有效地解决了与基于奖励的RL优化相关的不稳定性问题和困难的进行正确的社会价值观标注的问题，同时也减轻了大规模监督微调社会对齐语料昂贵的人工标注要求

<img src="https://img.saraba1st.com/forum/202305/30/050120ifr2t2omlap6tha4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-050052__01.jpg</strong> (422.01 KB, 下载次数: 0)

下载附件

2023-5-30 05:01 上传

使用Back-Scatter对沙盒中的社交互动进行建模。，通过考虑来自AI同伴们集体反馈的结果，AI社会代理者能够更好地调整他们的价值观

还演示了如何构建三种对齐方式：数据模仿、自我批评和重新调整来自模拟的交互，为对齐训练构建了169k个数据样本，与这种方法一致，同时引入了Stable Alignment，一种轻盈的对齐算法，可以学习来自模拟的社交互动

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 445#       发表于 2023-5-30 05:30

 本帖最后由 Machinery 于 2023-5-30 05:31 编辑 

Macaw-LLM

具有图像、视频、音频和文本集成的多模态语言模型

<img src="https://img.saraba1st.com/forum/202305/30/051756jugzsubbykb9k2fi.png" referrerpolicy="no-referrer">

<strong>alignment.png</strong> (687.98 KB, 下载次数: 0)

下载附件

2023-5-30 05:17 上传

github项目仓库:https://github.com/lyuchenyang/Macaw-LLM

最近几年，语言建模领域取得了显著进展，然而，整合多模态信息，如图像、视频、音频和文本，仍然是一个具有挑战性的任务，Macaw-LLM是首个将处理视觉、听觉和文本信息的最新模型结合在一起的模型，具体来说则是CLIP、Whisper和LLaMA

使用 CLIP 和 Whisper编码多模态特征，将编码后的特征输入注意力函数，其中多模态特征作为查询query，LLaMA的embedding矩阵作为key和value，将输出注入LLaMA的输入序列（在指令标记之前），允许使用最少的附加参数简化对齐过程。

Macaw-LLM 具有以下独特功能:
1.简单快速的对齐:Macaw-LLM通过简单快速的对齐LLM嵌入实现无缝整合多模式数据，这个高效的过程确保快速适应各种数据类型
2.单阶段指令微调:通过单阶段指令微调简化适应过程，促进更高效的学习体验

Macaw-LLM 由三个主要部分组成：
CLIP：负责对图像和视频帧进行编码
Whisper：负责对音频数据进行编码
LLM (LLaMA/Vicuna/Bloom)：编码指令并生成响应的语言模型

注:

<img src="https://img.saraba1st.com/forum/202305/30/052938wzchgvvc4cqyzety.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-052926.jpg</strong> (160.01 KB, 下载次数: 0)

下载附件

2023-5-30 05:29 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 446#       发表于 2023-5-30 05:50

DPOK

用于微调文本到图像扩散模型的强化学习

论文地址:https://arxiv.org/abs/2305.16381

从人类反馈中学习已被证明可以改进文本到图像模型，这些技术项目首先学习一个奖励函数，捕捉人类在任务中关心的内容，然后根据学习到的奖励函数改进模型

尽管已经研究了相对简单的方法（例如，基于奖励分数的拒绝抽样微调），但使用奖励函数微调文本到图像模型仍然具有挑战性。在本文工作中，研究组建议使用在线强化学习 (RL) 来微调文本到图像模型

同时专注于扩散模型，将微调任务定义为RL问题，并使用策略梯度更新预训练的文本到图像扩散模型以最大化反馈训练的奖励

DPOK方法将策略优化与KL正则化相结合，对RL微调和监督微调的KL正则化进行了分析。在实验中表明DPOK在图像文本对齐和图像质量方面通常优于监督微调

两种微调范式的对比:

<img src="https://img.saraba1st.com/forum/202305/30/054430fjbdbsdd7kra1ssj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-054309.jpg</strong> (159.01 KB, 下载次数: 0)

下载附件

2023-5-30 05:44 上传

原始稳定扩散模型生成的图像比较，监督微调 (SFT) 模型和RL微调模型，同一列中的图像使用相同的随机种子

seen text prompts: “A green colored rabbit” (color), “A cat and a dog” (composition), “Four wolves in the park” (count), and “A dog on the moon” (location).

Images from unseen text prompts: “A green colored cat” (color), “A cat and a cup” (composition),

“Four birds in the park” (count), and “A lion on the moon” (location).

<img src="https://img.saraba1st.com/forum/202305/30/054708luy24r7kh410402u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-054613.jpg</strong> (375.35 KB, 下载次数: 0)

下载附件

2023-5-30 05:47 上传

ImageReward模型分数与美学模型分数实际效果对比

<img src="https://img.saraba1st.com/forum/202305/30/054818nki5bcvm8mz6zyik.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-054724.jpg</strong> (225.53 KB, 下载次数: 0)

下载附件

2023-5-30 05:48 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 447#       发表于 2023-5-30 06:17

CaMA

中英双语全量微调LLaMA模型

github项目地址:https://github.com/zjunlp/CaMA

<img src="https://img.saraba1st.com/forum/202305/30/061741jhchp42vkakvhmvh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-061712.jpg</strong> (639.96 KB, 下载次数: 0)

下载附件

2023-5-30 06:17 上传

<img src="https://img.saraba1st.com/forum/202305/30/061744q0w7aqj33wa7adj7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-061729.jpg</strong> (274.74 KB, 下载次数: 0)

下载附件

2023-5-30 06:17 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 448#       发表于 2023-5-30 06:21

中文基座模型CPM-Bee

介绍文章:https://hub.baai.ac.cn/view/27225

Github地址:https://github.com/OpenBMB/CPM-Bee

 Hugging Face地址:https://huggingface.co/openbmb/cpm-bee-10b

<img src="https://img.saraba1st.com/forum/202305/30/062043paqlk7s5r9q4rk7i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-062027.jpg</strong> (972.33 KB, 下载次数: 0)

下载附件

2023-5-30 06:20 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 449#       发表于 2023-5-30 06:51

Impossible Distillation

不可置信的蒸馏，从低质量模型到高质量数据集和用于总结和释义的模型

论文地址:https://arxiv.org/abs/2305.16635

普遍认为最强的语言模型 (LM) 依赖于大规模、指令数据和人类反馈的组合来执行专门的任务

<img src="https://img.saraba1st.com/forum/202305/30/064930qfmbk5wby4sfsrdr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-064803.jpg</strong> (264.03 KB, 下载次数: 0)

下载附件

2023-5-30 06:49 上传

在本文中，研究组提出语言模型可以学习总结和释义句子，而无需这3个因素，并构造了一种名为Impossible Distillation的方法，这是一个直接从现成的LM中提取任务特定数据集的框架，即使 LM 本身不可能可靠地解决任务

通过在生成的数据集上训练学生模型并通过自蒸馏放大其能力，可以从低质量的教师模型中生成高质量的模型和数据集，而无需规模化或监督

<img src="https://img.saraba1st.com/forum/202305/30/064945h73575gc6gdl7pgp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-064822.jpg</strong> (145.13 KB, 下载次数: 0)

下载附件

2023-5-30 06:49 上传

使用Impossible Distillation，正如自动和人工评估所证实的那样，能够提炼出一个数量级更小的模型（只有 770M 参数），在质量和可控性方面优于175B参数GPT-3

<img src="https://img.saraba1st.com/forum/202305/30/064959ollbuuklsu231730.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-064836.jpg</strong> (194.32 KB, 下载次数: 0)

下载附件

2023-5-30 06:49 上传

此外，作为方法的有用副产品，研究组获得了 DIMSUM+，一个包含 3.4M 句子摘要和释义的高质量数据集，通过分析表明，这个数据集作为一个纯粹的LM生成的语料库，比所有人工创作的数据集（包括具有4M样本的Gigaword）更多样化，可以更有效地泛化到未见的任务领域

<img src="https://img.saraba1st.com/forum/202305/30/064954pe1vd1g1khdek9pt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-064854.jpg</strong> (104.57 KB, 下载次数: 0)

下载附件

2023-5-30 06:49 上传

在Impossible Distillation中,数据集生成涉及仅使用现成的LM(例如GPT-2)，在没有指令调优模型或任何形式的初始数据的帮助下，为给定任务搜索高质量的输入-输出对(例如句子-摘要对等)

使这个过程可行的关键思路是
(1)通过约束解码有效缩小LM搜索空间以获取输入-输出对
(2)通过生成后过滤器确保高质量的提取，这些过滤器源自目标任务的明确定义。通过在此生成的数据集上训练学生模型,然后通过自蒸馏进一步增强其能力，创造了一个紧凑又强大的终端模型，在自动和人工评估中都超过规模较大的LM的性能

Impossible Distillation完全独立于大型昂贵的模型或任务特定的监督，允许从任何初始LM(或LM组合)中提炼学生模型，在实践中从3个不同的LM(规模均约16亿参数)中提炼一个紧凑的任务模型(7.7亿参数)，涵盖新闻/Reddit/生物医学领域

尽管其规模较小,但提炼后的模型产生的摘要和释义惊人地更可控且质量更高，性能比200倍大的GPT-3更好

此外,作为这种提炼的自然副产品,我们获得了DIMSUM +，尽管DIMSUM +纯粹是由LM生成的,但它实际上展示了比人工数据集更丰富的词汇多样性和更广泛的摘要类型，它甚至在未见域的测试集上显示出更好的适应性:在我们的数据集上训练的摘要器超过相同模型在较大的人工数据集(如Gigaword)训练的模型

<img src="https://img.saraba1st.com/forum/202305/30/065009xa57u3993kjqybu9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230530-064913.jpg</strong> (292.96 KB, 下载次数: 0)

下载附件

2023-5-30 06:50 上传

更广泛地说，本文的工作表明，即使模型本身无法可靠地解决任务，小型现成的LM也可以模拟丰富的任务特定知识源，通过识别和放大此知识以生成高质量的数据集，Impossible Distillation展示了通过高效、有效和可重用的流程训练任务模型的一种有希望的方式

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 450#       发表于 2023-5-31 00:53

RAPHAEL

通过大型混合路径去噪生成扩散模型进行文本到图片生成

项目地址:https://raphael-painter.github.io/

提出了一种文本条件图像扩散模型，名为RAPHAEL，可用于生成高度艺术化的图像，高度理解精确描绘文本提示，包括多个名词、形容词和动词

通过堆叠数十个专家混合(MoE)层实现，即空间MoE层和时间MoE层，网络输入到输出间有数十亿条扩散路径(路线)，每条路径都可以直观的看做在扩散时间步长内在指定图像区域描绘特定文本概念的“画家”

空间MoE层负责在特定图像区域描绘不同的概念，而时间MoE层着重在不同的扩散时间步长上描绘这些概念，以及一个边缘监督学习模块，以进一步提高生成图像的质量和美学诉求

通过全面实验显示，RAPHAEL的性能已超过近期各类先进模型，如Stable Diffusion、ERNIE-ViLG 2.0、DeepFloyd和DALL-E 2，无论是在图像质量还是美学诉求方面

RAPHAEL在跨多种风格切换图像方面表现优异，如日本漫画、现实主义、赛博朋克和墨线插图等，其次，作为一个30亿参数的单模型，通过在1000块A100 GPU上训练了两个月，在COCO数据集上达到了6.61的最佳零样本FID得分

此外RAPHAEL在人工评估的ViLG-300基准测试中显著超过其对手(如DeepFloyd)

<img src="https://img.saraba1st.com/forum/202305/31/003316ooo2u3qa78x77a7u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-003217.jpg</strong> (632.23 KB, 下载次数: 0)

下载附件

2023-5-31 00:33 上传

↑RAPHAEL(第一行竖列)与近期具有代表性的图片生成项目的比较，Stable Diffusion XL、DeepFloyd、DALL-E 2和ERNIE-ViLG 2.0，使用的都是相同的文本提示输入

其中红色突出文本都是创作者希望在生成的图像中保留下来的重要元素，这些图像样本都并非cheerypick，可以看到以前的模型大多都无法准确保留所需的概念，只有RAPHAEL生成的图像准确反映了相关概念文本

<img src="https://img.saraba1st.com/forum/202305/31/004232eddjniq1u4q2zcd3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-004156.jpg</strong> (533.28 KB, 下载次数: 0)

下载附件

2023-5-31 00:42 上传

↑模型生成效果:

<img src="https://img.saraba1st.com/forum/202305/31/004239dcmk6m6mqidgan9m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-004212.jpg</strong> (775.13 KB, 下载次数: 0)

下载附件

2023-5-31 00:42 上传

↑模型构架:

<img src="https://img.saraba1st.com/forum/202305/31/004851v7detz1i7ixzfq4t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-004325__01.jpg</strong> (303.08 KB, 下载次数: 0)

下载附件

2023-5-31 00:48 上传

↑左图:通过可视化从网络输入到输出的扩散路径(路线)，使用16个空间MoE层,每个层包含6个MoE专家，这些路径与100个形容词密切相关，如“景色宜人”、“宁静”和“壮丽”,这些形容词代表最常出现的词汇来描述作品，这些建议来自GPT-3.5，鉴于GPT-3.5在数万亿个令牌上进行了训练，可以相信这些形容词反映了一个多样化的真实世界分布，同时通过实验发现表明，不同的路径独特地代表各种形容词

右图:描绘了COCO数据集内十个类别(即名词)的扩散路径，观察结果显示，不同的类别以异构的方式激活不同的路径，重叠的路径所显示的颜色相混合

<img src="https://img.saraba1st.com/forum/202305/31/004948aelrmggm0o02ql4w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-004402__01.jpg</strong> (261.35 KB, 下载次数: 0)

下载附件

2023-5-31 00:49 上传

RAPHAEL 与最近具有代表性的文本到图像生成模型的MS-COCO 256 × 256零样本FID-30k成绩

RAPHAEL与DALL-E 2、Stable Diffusion XL (SD XL)、ERNIE-ViLG 2.0和DeepFloyd对比，ViLG-300 基准，95%置信区间，可以看到RAPHAEL生成的图片质量更高、更符合提示的图像

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 451#       发表于 2023-5-31 01:22

TaleCrafter

具有多个角色的可交互的可视化角色故事生成

github代码库:https://github.com/VideoCrafter/TaleCrafter

准确的将故事可视化需要几个必要的元素，例如跨帧的身份一致性、纯文本和视觉内容之间的对齐以及图像中对象的合理布局，大多数以前的作品通过在一组具有相同风格和相同字符的视频上拟合文本到图像 (T2I) 模型来努力满足这些要求，例如FlintstonesSV数据集

然而，学习到的T2I模型通常很难适应新的角色、场景和风格，并且往往缺乏修改合成图像布局的灵活性，本文提出了一种通用交互式故事可视化系统，能够处理多个小说角色并支持布局和局部结构的编辑

它是利用大型语言模型和T2I模型的先验知识开发的，在海量语料库上训练，该系统包括四个相互关联的组件：故事到提示生成 (S2P)、文本到布局生成 (T2L)、可控文本到图像生成 (C-T2I) 和图像到视频动画 (I2V)

<img src="https://img.saraba1st.com/forum/202305/31/012156wq3kimi2mg9mngi6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-011837__01.jpg</strong> (148.28 KB, 下载次数: 0)

下载附件

2023-5-31 01:21 上传

<img src="https://img.saraba1st.com/forum/202305/31/012208k1cysomkwyj166gc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-011850__01.jpg</strong> (127.39 KB, 下载次数: 0)

下载附件

2023-5-31 01:22 上传

首先，S2P模块将简洁的故事信息转化为后续阶段所需的详细提示，接下来，T2L根据提示生成多样合理的布局，让用户可以根据自己的喜好调整和细化布局，核心组件 C-T2I 支持创建由布局、草图和演员特定标识符引导的图像，以保持可视化的一致性和细节，最后，I2V 通过对生成的图像进行动画处理来丰富可视化过程

<img src="https://img.saraba1st.com/forum/202305/31/012134lks7nq0ksvbkmv5s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-011827__01.jpg</strong> (902.69 KB, 下载次数: 0)

下载附件

2023-5-31 01:21 上传

<img src="https://img.saraba1st.com/forum/202305/31/012141b7e69avhgeu9695p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-011906__01.jpg</strong> (690.93 KB, 下载次数: 0)

下载附件

2023-5-31 01:21 上传

<img src="https://img.saraba1st.com/forum/202305/31/012146fc7i67la97l1ak4n.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-011922__01.jpg</strong> (339.58 KB, 下载次数: 0)

下载附件

2023-5-31 01:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 452#       发表于 2023-5-31 01:57

FairEval

大型语言模型不是公平的评估者

github项目地址:https://github.com/i-Eval/FairEval

在采用大型语言模型(如GPT-4)作为评估员对生成的候选回应结果质量进行评分的评估范式中存在系统性偏差

候选回应结果的质量排名可以通过简单地改变上下文中的出现顺序来轻易操纵，这种操作允许我们扭曲评估结果，使一种模型似乎远胜于另一种模型，例如，vicuna可以在80个测试查询中的66个查询中击败ChatGPT

为了解决这个问题，本文研究组提出了两种简单而有效的校准策略:
1.多证据校准，要求评估器模型在给出评级之前生成多条详细的证据
2.均衡位置校准，综合各个顺序的结果来确定最终得分

实验表明本方法成功地减轻了评估偏差，与人类判断的结果更加一致，为了促进对更稳健的大型语言模型比较的未来研究，本文中的技术将集成到一个易于使用的工具包中(上文的github仓库)

<img src="https://img.saraba1st.com/forum/202305/31/015440o2z028xfyodov01g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-015414__01.jpg</strong> (289.47 KB, 下载次数: 0)

下载附件

2023-5-31 01:54 上传

评估质量的细粒度分析，本文提出的Multiple Evidence Calibration(MEC)和Balanced Position Calibration(BPC)策略改进了ChatGPT和GPT-4在几乎所有类别中的评估表现，特别是在复杂的任务类别上，例如ChatGPT 的常识、编码和数学

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 453#       发表于 2023-5-31 02:43

DPO

直接偏好优化：你的语言模型是隐秘的奖励模型

论文地址:https://arxiv.org/abs/2305.18290

虽然大规模无监督语言模型(LM 学习广泛的世界知识和一些推理技能，但由于其训练的完全无监督性质，很难精确控制其行为

获得这种可控性的现有方法，通过收集模型生成的相对质量的人类标签，并微调无监督LM以符合这些偏好，通常通过人类反馈 (RLHF) 的强化学习过程，然而RLHF是一个复杂且通常不稳定的过程，需要首先拟合反映人类偏好的奖励模型，然后使用强化学习对大型无监督 LM 进行微调，以在不偏离原始模型太远的情况下最大化该估计奖励

在本文中，我们利用奖励函数和最优策略之间的映射来证明这个受约束的奖励最大化问题可以通过单阶段的策略训练来精确优化，解决了人类偏好数据的分类问题

由此产生的算法，称之为直接偏好优化(DPO)，稳定、高性能且计算轻量级，无需拟合奖励模型、微调期间从LM采样或执行重要的超参数调整

实验表明，DPO可以微调LM以符合人类偏好，甚至优于现有方法，使用DPO调整的性能超过RLHF，在控制生成文本的情感和提高摘要与单轮对话的回复质量的能力方面，同时DPO的参数调整方法实现和训练起来也简单得多

<img src="https://img.saraba1st.com/forum/202305/31/022227s4t2c3rpr2bgddo3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-021958.jpg</strong> (62.56 KB, 下载次数: 0)

下载附件

2023-5-31 02:22 上传

DPO 针对人类偏好进行优化，同时避免使用强化学习，DPO通过简单的分类直接优化最能满足偏好的策略目标，没有明确的奖励函数或RL

DPO算法隐式地优化现有RLHF算法的相同目标(最大化奖励与KL散度约束)，但实现简单，训练方法直接，直观地说，DPO更新增加了首选响应相对于不首选响应的对数概率，但它包含一个动态的，每个样本的重要性权重，该权重可防止模型在某些朴素概率比例目标函数中退化

像现有算法一样，DPO依赖于理论偏好模型，该模型测量给定奖励函数与经验偏好数据的对齐程度，然而，现有方法使用偏好模型来定义偏好损失训练奖励模型，然后训练优化学习奖励模型的策略，DPO则使用变量变换来将偏好损失定义为策略的直接函数

给定人类对模型响应的偏好数据集，DPO可以使用简单的二元交叉熵目标来优化策略，而无需显式学习奖励函数或在训练期间从策略中采样

实验显示，DPO至少与现有方法一样有效，包括基于PPO的RLHF，用于在情感，摘要和对话等任务中学习偏好，包含多达6B参数的语言模型

<img src="https://img.saraba1st.com/forum/202305/31/024051yhne1dwsnnn3sf19.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-024029.jpg</strong> (150.17 KB, 下载次数: 0)

下载附件

2023-5-31 02:40 上传

<img src="https://img.saraba1st.com/forum/202305/31/024051mljjdithdj0laiw7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-023938.jpg</strong> (90.28 KB, 下载次数: 0)

下载附件

2023-5-31 02:40 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 454#       发表于 2023-5-31 03:01

 本帖最后由 Machinery 于 2023-5-31 03:03 编辑 

Ghost in the Minecraft:《我的世界》中的幽灵

通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供通用能力的代理

项目地址:https://github.com/OpenGVLab/GITM

Minecraft近年来吸引了大量研究兴趣，作为开发能够在开放世界环境中运行的智能代理的丰富环境平台

然而，当前的研究领域主要集中在特定目标上，例如流行的“ObtainDiamond”任务，尚未显示出对更广泛任务的有效泛化

此外，“ObtainDiamond”任务目前领先的成功率约为 20%，突出了现有方法中使用的基于强化学习 (RL) 的局限性

<img src="https://img.saraba1st.com/forum/202305/31/025937yp21r12ulfpp2dpb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-025857.jpg</strong> (298.29 KB, 下载次数: 0)

下载附件

2023-5-31 02:59 上传

为了应对这些挑战，本文在Minecraft中引入了Ghost(GITM)，这是一个将大型语言模型 (LLM) 与基于文本的知识和记忆相结合的新框架，旨在在 Minecraft 中创建通用代理 (GCA)

这些具备LLM逻辑和常识能力的智能体可以通过基于文本的交互巧妙地驾驭复杂的、奖励稀少的环境，通过开发一组结构化操作并利用LLM生成操作计划供代理执行

由此产生的基于LLM的代理明显超越了以前的方法，在“ObtainDiamond”任务上实现了+47.5%的显著提高，与传统的基于RL学习相比展示了卓越的鲁棒性(稳健性)

值得注意的是，本文的智能代理者是第一个完成了Minecraft Overworld技术树中所有项目的人，展示了其广泛的能力

<img src="https://img.saraba1st.com/forum/202305/31/025917i3kav172cpak71cr.png" referrerpolicy="no-referrer">

<strong>fig1.png</strong> (721.42 KB, 下载次数: 0)

下载附件

2023-5-31 02:59 上传

<img src="https://img.saraba1st.com/forum/202305/31/030038evf4k0j48e4y0gy4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-030021.jpg</strong> (613.45 KB, 下载次数: 0)

下载附件

2023-5-31 03:00 上传

<img src="https://img.saraba1st.com/forum/202305/31/030127qdzvdm1fvh8iyh1h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-030116.jpg</strong> (307.7 KB, 下载次数: 0)

下载附件

2023-5-31 03:01 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 455#       发表于 2023-5-31 03:17

 本帖最后由 Machinery 于 2023-5-31 03:20 编辑 

MindEye

使用对比学习和扩散先验的fMRI-to-Image(功能性磁共振成像引导的图像生成)

MindEye可以从大脑活动中检索和重建观察到的图像，模型包含两个并行的子模块，专门用于检索(使用对比学习/clip)和重建(使用扩散先验/图像生成)

MindEye可以将扫描的fMRI大脑活动结果映射到任何高维多模态的潜在空间，如CLIP等，从而使用接受来自该潜在空间的embedding，使用生成模型实现图像重建

论文地址:https://arxiv.org/abs/2305.18274

项目主页:https://medarc-ai.github.io/mindeye

MindEye甚至可以在高度相似的候选图像中检索到准确的原始图像，这表明大脑embedding中保留了细粒度的图像特定信息

这使我们能够准确地从LAION-5B等大型数据集数据库中检索图像，通过消融实验证明，MindEye相对于以前方法的性能改进源于用于检索和重建的专门子模块、改进的训练技术以及具有更多数量级参数的训练模型

此外，MindEye可以通过使用img2img(图像到图像生成)以及来自单独自动编码器的输出更好地保留重建中的低级图像特征，进行更精准的重建

<img src="https://img.saraba1st.com/forum/202305/31/032013tpmqn0rnct0ozzmq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-031938.jpg</strong> (130.4 KB, 下载次数: 0)

下载附件

2023-5-31 03:20 上传

<img src="https://img.saraba1st.com/forum/202305/31/032013omca422m2ff2leyc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-031947.jpg</strong> (145.83 KB, 下载次数: 0)

下载附件

2023-5-31 03:20 上传

<img src="https://img.saraba1st.com/forum/202305/31/032013k7hemk781ksxk1bc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-031952.jpg</strong> (236.42 KB, 下载次数: 0)

下载附件

2023-5-31 03:20 上传

<img src="https://img.saraba1st.com/forum/202305/31/032013ahuilseeirshrzbr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-031958.jpg</strong> (224.95 KB, 下载次数: 0)

下载附件

2023-5-31 03:20 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 456#       发表于 2023-5-31 04:04

 本帖最后由 Machinery 于 2023-5-31 04:07 编辑 

Mix-of-Show

扩散模型的多概念定制以及分散式低秩适应(lora)

项目地址:https://showlab.github.io/Mix-of-Show/

github代码仓库:https://github.com/TencentARC/Mix-of-Show

开源的大规模文本到图像扩散模型，例如Stable Diffusion，已经引起了社区的极大关注，这些模型可以使用低秩适应(LoRA/low-rank adaptations)轻松针对新概念进行定制

然而，利用LoRA进行支持多个定制概念的融合多概念生成问题对此提出了挑战，本文将这种场景称为去中心化的多概念定制生成，它涉及单概念调整和中心节点的概念融合

在本文中，我们提出了一个名为Mix-of-Show的新框架，它解决了分散式多概念定制的挑战，包括现有单独概念LoRA调整和模型融合过程中的特点丢失导致的概念冲突

Mix-of-Show采用embedding分解的LoRA(ED-LoRA) 进行单概念调整和中心节点的梯度融合，以保留单个概念的域内本质，并支持理论上无限的概念融合

此外，我们引入了区域可控采样，它扩展了空间可控采样(例如，ControlNet 和 T2I-Adaptor)以解决多概念采样中的属性绑定和对象丢失问题

大量实验表明，Mix-of-Show 能够以高保真度组合多个自定义概念，包括角色、对象和场景，同时解决多概念采样中的属性绑定和缺失对象问题

<img src="https://img.saraba1st.com/forum/202305/31/035050e9p05201k3sn4pg7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-035018__01.jpg</strong> (431.84 KB, 下载次数: 0)

下载附件

2023-5-31 03:50 上传

通过Mix-of-Show进行去中心化的概念定制

<img src="https://img.saraba1st.com/forum/202305/31/035230ezu7dcl17xaceng5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-035202.jpg</strong> (564.72 KB, 下载次数: 0)

下载附件

2023-5-31 03:52 上传

如何生成类似哈利波特和灭霸在同一图像中这种任务，这两个(甚至更多概念)来自不同的作品，Mix-of-Show支持多种定制的复杂组合，具有单独训练的概念LoRA组合(例如，角色、对象、场景)

<img src="https://img.saraba1st.com/forum/202305/31/035606xqn9yqf9b0yl92db.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-035540__01.jpg</strong> (766.9 KB, 下载次数: 0)

下载附件

2023-5-31 03:56 上传

embedding调整和联合embedding权重调整之间的单概念和多概念定制，P∗ = “Photo of a V , near the beach"，Φ0 和 ∆Φ 表示预训练模型和 LoRA 权重

<img src="https://img.saraba1st.com/forum/202305/31/040634jmkf36t4k8f798yd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-035840__01.jpg</strong> (337.73 KB, 下载次数: 0)

下载附件

2023-5-31 04:06 上传

Mix-of-Show的Pipeline，在单概念调整中，ED-LoRA 采用分层嵌入和多词表示，在中心节点中，采用梯度融合来融合多个概念LoRA，同时支持组合那些定制的概念

<img src="https://img.saraba1st.com/forum/202305/31/040206m101vb0mbb120gwb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-040148.jpg</strong> (217.66 KB, 下载次数: 0)

下载附件

2023-5-31 04:02 上传

区域可控制的多概念生成采样

<img src="https://img.saraba1st.com/forum/202305/31/040405h1d68111nnd69ynr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-040335__01.jpg</strong> (621.41 KB, 下载次数: 0)

下载附件

2023-5-31 04:04 上传

高质量的单概念与多概念生成

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 457#       发表于 2023-5-31 04:42

 本帖最后由 Machinery 于 2023-5-31 04:43 编辑 

Photoswap

对图像进行个性化主题交换

项目主页:https://photoswap.github.io/

github代码实现仓库:https://github.com/eric-ai-lab/photoswap

设想用您自己顽皮的小狗无缝地替换照片中懒洋洋地躺在阳光明媚的窗台上的虎斑猫，同时保留图像的原始魅力和构图

本文提出了Photoswap，一种新颖的方法，可通过现有图像中的个性化主题交换来实现这种身临其境的图像编辑体验

Photoswap首先从参考图像中学**体的视觉概念，然后以免训练的方式使用预训练的扩散模型将其交换到目标图像中

通过实验确定一个概念化的视觉主题可以适当的进行一些自注意力和交叉注意力操作无缝地转移到任何图像，同时保持交换对象的姿势和图像的整体连贯性

综合实验了Photoswap在个性化主题交换方面的功效和可控性，此外，Photoswap 在主题交换、背景保存和整体质量方面明显优于人类评级的基线方法，揭示了其从娱乐到专业编辑的巨大应用潜力

<img src="https://img.saraba1st.com/forum/202305/31/042026jcizp4cz5n4vbcz0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-042017.jpg</strong> (218.07 KB, 下载次数: 0)

下载附件

2023-5-31 04:20 上传

Photoswap 可以毫不费力地替换源图像中的主题，可以是合成的（前两行）或真实的（最后一行），具有在参考中指定的个性化主题图像，同时保留原始主体姿势和源图像的组成

<img src="https://img.saraba1st.com/forum/202305/31/042304nq76scnyt1k7sqzv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-042123.jpg</strong> (175.33 KB, 下载次数: 0)

下载附件

2023-5-31 04:23 上传

给定一个新概念的几张图片，扩散模型首先学习概念并将其转换为令牌，上半部分是源图像的生成过程，下半部分是目标图像的生成过程，初始噪声特征 ztT是从源 zsT 复制而来的，源图像生成过程中的注意力输出和注意力图将被转移到目标图像生成过程中，最后对特征 zt0 进行解码，输出目标图像

U-Net由包含重复的自我注意和交叉注意块的层组成，本研究的重点是操纵自注意力和交叉注意力，以实现个性化主题交换的任务

<img src="https://img.saraba1st.com/forum/202305/31/042936jhcxdsg8cl6sndcc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-042907.jpg</strong> (344.98 KB, 下载次数: 0)

下载附件

2023-5-31 04:29 上传

Photoswap结果跨越各种对象和图像域，展示了其广泛的适用性，从日常物品到卡通，主题交换任务的多样性，展示了这个框架在不同情况下的多功能性和稳健性

<img src="https://img.saraba1st.com/forum/202305/31/043105njqtq7glw7z8e7gs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-043043.jpg</strong> (170.06 KB, 下载次数: 0)

下载附件

2023-5-31 04:31 上传

多主题和遮挡主题场景的照片交换结果，结果表明，Photoswap可以一次解开和替换多个主题，此外，Photoswap还可以识别目标对象，同时避免影响非主题的像素

<img src="https://img.saraba1st.com/forum/202305/31/043231e9lvnzjcn6ny3p9w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-043222.jpg</strong> (265.37 KB, 下载次数: 0)

下载附件

2023-5-31 04:32 上传

Photoswap和P2P+dreamboth方法之间的人工评估，可以观察到，P2P+dreamboth也完成了挑战任务，然而，它面临着准确地保留背景和参考对象的挑战，而对于 Photoswap，可以稳健的处理各种情况

Photoswap 和 P2P 之间的人工评估，Photoswap和 P2P利用相同的概念学习方法DreamBoth

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 458#       发表于 2023-5-31 05:02

DNA-GPT

用于GPT生成文本的无训练的差异N-Gram分析检测

github项目仓库:https://github.com/Xianjun-Yang/DNA-GPT

大型语言模型(LLM)显着增强了机器生成文本的流畅性和多样性，然而，这一进展也对检测给定文本的来源提出了重大挑战，目前对检测方法的研究落后于 LLM 的快速发展

传统的基于训练的方法在灵活性方面存在局限性，尤其是在适应新领域时，而且它们往往缺乏解释力。为了解决这一差距，我们提出了一种新的无训练检测策略，称为差异N-Gram分析(DNA-GPT)

给定一个文本，首先在中间截断它，然后仅使用前面的部分作为LLM的输入以重新生成新的剩余部分，通过黑盒中的N-gram分析或白盒中的概率差异分析原始和新剩余部分之间的差异

可以清楚地说明机器生成的文本和人工编写的文本之间的显着区别，通过对OpenAI最先进的LLM进行了广泛的实验，包括 text-davinci-003、GPT-3.5-turbo和 GPT-4，以及GPT-NeoX-20B和LLaMa-13B等开源模型

结果表明，该方法的零样本性能在四个英语和一个德语数据集上区分人类和GPT生成的文本方面表现出最先进的性能，优于OpenAI自己的分类器，而后者在数百万文本上进行了训练

此外，本方法提供了合理的解释和证据来支持研究组的主张，这是可解释检测的一个独特特征，本方法在修改后的文本攻击下也很稳健，并且可以额外解决模型来源问题

<img src="https://img.saraba1st.com/forum/202305/31/045905musp8pp4d44ppp8s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-045755.jpg</strong> (216.33 KB, 下载次数: 0)

下载附件

2023-5-31 04:59 上传

<img src="https://img.saraba1st.com/forum/202305/31/050230xt02wx29x3dxxxgd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230531-050201.jpg</strong> (305.03 KB, 下载次数: 0)

下载附件

2023-5-31 05:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 459#       发表于 2023-6-1 01:27

SelFee

迭代自我修改的大型语言模型，由自我反馈生成提供支持

项目主页:https://kaistai.github.io/SelFee/

Demo演示:https://kaistai.github.io/SelFee/

github项目代码库:https://github.com/kaistAI/SelFee

hugface训练代码:https://huggingface.co/datasets/kaist-ai/selfee-train

SelFee，一种指令遵循语言模型，可以根据反馈生成对其响应的自我反馈和自我修改，通过使用包含ChatGPT生成的178K个自我反馈和自我修订数据个训练实例样本对LLaMA模型（7B、13B）进行微调

<img src="https://img.saraba1st.com/forum/202306/01/010750uijxuqcqqzyadqd1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-005709.jpg</strong> (429.84 KB, 下载次数: 0)

下载附件

2023-6-1 01:07 上传

<img src="https://img.saraba1st.com/forum/202306/01/010750sx1auxxxx7wqqvhh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-005724.jpg</strong> (410.79 KB, 下载次数: 0)

下载附件

2023-6-1 01:07 上传

在Vicuna评估基准上，两个版本的SelFee模型（7B、13B）都优于 LLaMA、Alpaca、Vicuna、Guanaco，与ChatGPT相当，使它们成为当今最强大的开源模型之一

SelFee在创意写作或长篇文本生成方面特别有效，因为它需要一个迭代的写作过程来为人类生成高质量的文本，然而，与其他开源模型类似，与ChatGPT或Bard等封闭API模型相比，SelFee模型在数学、推理、事实性和编码任务上也失败了

这表明需要更好的基础 LM 才能获得更好的真实性（相关论文:https://arxiv.org/abs/2305.15717），此外，研究组承认评估设置的全面性和一致性存在局限性，计划使用更全面、一致和可靠的评估设置来评估SelFee模型，同时对本文中的说明持保留态度

<img src="https://img.saraba1st.com/forum/202306/01/010734tzzfdov2dj54d2ia.jpg" referrerpolicy="no-referrer">

<strong>cdb7fe78-0473-47f4-9887-19e6acb8c5b4.jpg</strong> (37.22 KB, 下载次数: 0)

下载附件

2023-6-1 01:07 上传

语言模型可以通过自然语言反馈（Reflexion、Self-Correct、RL4F）得到增强，在不依赖外部模型或工具的情况下自行生成反馈的语言模型已被证明是有效的（PEER、Self-Refine、Self-Critique）

与以前的方法不同，SelFee不需要文档检索过程、用于上下文学习的少量演示、大小超过100B的大型LLM或特定于任务的模型等要求

<img src="https://img.saraba1st.com/forum/202306/01/011259eduydip2icipdqzp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-011246.jpg</strong> (47.2 KB, 下载次数: 0)

下载附件

2023-6-1 01:12 上传

SelFee经过微调可以不断修改自己的答案，直到它在单个推理中提供高质量的响应，当出现指令 Q 时，模型不仅会生成初始答案 A0，还会生成自我反馈序列 F0，通过分析生成的反馈 F0 的内容，模型确定是否需要修改，如果认为有必要进行修改，模型会根据反馈 F0 生成修改后的答案 A1，此外，它还会生成 F1，代表对 A1 的反馈，等等

整个过程是在一次推理中完成的，通过完成这项简单的任务，SelFee比现有的基于LLaMA的模型有明显改进

相关贡献：
 1. 从各种来源（ShareGPT、Alpaca、Math、Code 和Flan Collection）收集了指令数据，形成了一个包含178K个训练实例的数据集
 2. 我们通过LLM教师ChatGPT的提炼增加了反馈和修订实例，这种蒸馏过程有助于以更实惠的成本解决反馈和修订数据的稀缺问题
 3. 通过强制模型进行修改，我们观察到最终答案的改进，增加所需修订的最小数量对应于性能的相应增加，这意味着缩放推理计算以生成更长的序列可能比缩放模型大小本身更有效

<img src="https://img.saraba1st.com/forum/202306/01/011933w1j0jsyy98aagrg3.png" referrerpolicy="no-referrer">

<strong>selfee_revision.png</strong> (188.84 KB, 下载次数: 0)

下载附件

2023-6-1 01:19 上传

强制修订的影响
在推理过程中，观察到答案的质量随着修改次数的增加而提高，当通过ChatGPT扩充训练数据时，更多的修改并不总能保证得到更高质量的答案，尤其明显的是，在ChatGPT认为不必要时强制进行修订实际上会降低输出质量

相比之下，SelFee被训练以自回归方式使用反馈自主修改输出，调查了强制要求SelFee模型来修改每个问题是否会提高输出质量，如上图所示，执行最少 3 次修订可获得最佳性能，生成至少3个修订的7B SelFee模型优于不需要生成修订的13B SelFee模型(101.4 与 98.2)

<img src="https://img.saraba1st.com/forum/202306/01/012330tvz4ih4fph4z6vgg.png" referrerpolicy="no-referrer">

<strong>selfee_abtest.png</strong> (407.65 KB, 下载次数: 0)

下载附件

2023-6-1 01:23 上传

采用Vicuna评估基准，其中涉及使用80个包含不同主题的问题，使用GPT-4作为评估器进行试点评估，报告了与教师模型ChatGPT相比的相对分数，考虑到已知GPT-4会表现出位置偏差，采用双向评估，这意味着每个评估实例都会被评估两次，具体取决于其位置

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 460#       发表于 2023-6-1 01:52

视觉探索和灵感的概念分解

论文地址:https://arxiv.org/abs/2305.18203

项目主页:https://inspirationtree.github.io/inspirationtree/

为给定的独特概念提供了树状结构的视觉探索空间，树的节点("v_i")是新学习的文本向量嵌入，注入到预训练的文本到图像模型的潜在空间中

节点编码感兴趣主题的不同方面，通过检查树内和树间的组合，不同方面可以激发创作新的设计和概念，如下所示

<img src="https://img.saraba1st.com/forum/202306/01/014151x55q6u6zqttky7u3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-013936.jpg</strong> (269.59 KB, 下载次数: 0)

下载附件

2023-6-1 01:41 上传

<img src="https://img.saraba1st.com/forum/202306/01/014151k7odozyn2ju73frw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-013948.jpg</strong> (284.71 KB, 下载次数: 0)

下载附件

2023-6-1 01:41 上传

一个创造性的想法通常是从现有的捕捉各种概念的视觉示例中转换、组合和修改想法而产生的，但是，不能简单地将概念作为一个整体来复制，而是通过检查概念的某些方面来获得灵感

因此，通常有必要将一个概念分成不同的方面，以提供新的视角，在本文中，提出了一种将表示为一组图像的视觉概念分解为以分层树结构编码的不同视觉方面的方法

我们利用大型视觉语言模型及其丰富的潜在空间进行概念分解和生成。 树中的每个节点代表一个子概念，使用学习的向量嵌入注入预训练的文本到图像模型的潜在空间

使用一组正则化来指导优化节点中编码的嵌入向量以遵循树的层次结构，允许探索和发现源自原始概念的新概念，树提供了在每个节点进行无限视觉采样的可能性，允许用户探索感兴趣对象的隐藏子概念

每个节点中学习的方面可以在树内和树之间组合以创建新的视觉创意，并且可以用于自然语言句子中以将这些方面应用于新设计

<img src="https://img.saraba1st.com/forum/202306/01/014225g6vds811vsss8ss1.jpg" referrerpolicy="no-referrer">

<strong>pipeline.jpg</strong> (756.67 KB, 下载次数: 0)

下载附件

2023-6-1 01:42 上传

给定一小组描述所需视觉概念的图像，目标是构建一个丰富的视觉探索空间来表达输入概念的不同方面，将探索空间建模为二叉树，其节点是与添加到预定义词典中的新发现单词相对应的学习向量嵌入，代表原始概念的不同方面，探索树从上到下逐渐构建为二叉树，我们一次迭代地添加两个新节点

为了创建两个子节点，根据从父节点中描述的概念生成的输入图像集优化新的嵌入向量，在构建过程中，我们定义了两个要求来鼓励学习的嵌入遵循树结构

1.Binary Reconstruction
每对子节点应该一起封装其父节点描述的概念

 2.Coherency
每个单独的节点应该描述一个与其兄弟节点不同的连贯概念，接下来，就是为满足这些要求而设计损失函数和程序

<img src="https://img.saraba1st.com/forum/202306/01/014938cvqevy4beydbgbvk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-014847.jpg</strong> (206.4 KB, 下载次数: 0)

下载附件

2023-6-1 01:49 上传

<img src="https://img.saraba1st.com/forum/202306/01/014938csscn29onooqbo7v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-014911.jpg</strong> (218.12 KB, 下载次数: 0)

下载附件

2023-6-1 01:49 上传

<img src="https://img.saraba1st.com/forum/202306/01/014938wp1z1o1g2p12c23c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-014921.jpg</strong> (188.9 KB, 下载次数: 0)

下载附件

2023-6-1 01:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 461#       发表于 2023-6-1 02:11

GlyphControl

字形条件控制的视觉文本图像生成

github项目代码库(待整理):https://github.com/AIGText/GlyphControl-release

在本文中，提出了一种新颖且高效的方法，称为 GlyphControl，来解决图片中的文字生成任务，与依赖字符识别文本的现有方法不同，像ByT5这样的编码器，需要重新训练文本到图像模型，通过利用额外的字形条件信息来提高性能，现成的稳定扩散模型在生成准确的视觉文本方面图片

通过字形指令，用户可以自定义内容、位置和生成文本的大小， 同时为了方便在视觉文本生成方面的进一步研究，构建了一个训练基准数据集，称为LAION-Glyph

测量评估了方法的有效性，生成的视觉文本的OCR的指标和CLIP分数，评估表明GlyphControl在OCR准确性和CLIP分数方面优于最近的DeepFloyd IF模型

<img src="https://img.saraba1st.com/forum/202306/01/020945hy4fnqq091ycnzo1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-020739.jpg</strong> (699.12 KB, 下载次数: 0)

下载附件

2023-6-1 02:09 上传

<img src="https://img.saraba1st.com/forum/202306/01/020945zw62pqpjrqwqrqlj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-020903.jpg</strong> (122.36 KB, 下载次数: 0)

下载附件

2023-6-1 02:09 上传

<img src="https://img.saraba1st.com/forum/202306/01/020945o85a5d8dd68aasdd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-020910.jpg</strong> (61.36 KB, 下载次数: 0)

下载附件

2023-6-1 02:09 上传

<img src="https://img.saraba1st.com/forum/202306/01/021131zw1un13g3gww31kg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-021059.jpg</strong> (129.26 KB, 下载次数: 0)

下载附件

2023-6-1 02:11 上传

<img src="https://img.saraba1st.com/forum/202306/01/021131yx6lkx6lylx0099k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-021119.jpg</strong> (739.75 KB, 下载次数: 0)

下载附件

2023-6-1 02:11 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 462#       发表于 2023-6-1 02:53

GILL

使用多模态语言模型生成图像

提出了一种方法，通过嵌入空间之间的映射，将冻结的纯文本大型语言模型(LLM)与预训练的图像编码器和解码器模型融合在一起

项目主页:https://jykoh.com/gill

github项目仓库:https://github.com/kohjingyu/gill

<img src="https://img.saraba1st.com/forum/202306/01/023859r0pcgkoo7d6qo5ud.jpg" referrerpolicy="no-referrer">

<strong>20230601_023836.jpg</strong> (665.08 KB, 下载次数: 0)

下载附件

2023-6-1 02:38 上传

展示了一套广泛的多模态功能：图像检索、新图像生成和多模态对话

本方法是第一种能够对任意交错的图像和文本输入进行调节以生成连贯图像(和文本)输出的方法，为了在图像生成方面实现强大的性能，提出了一种高效的映射网络，将 LLM 建立在现成的文本到图像生成模型上

该映射网络将文本的隐藏表示转换为视觉模型的嵌入空间，使我们能够利用LLM的强文本表示能力进行视觉输出

本方法在使用更长、更复杂的语言的任务上明显优于基线生成模型，除了新颖的图像生成，GILL还能够从预先指定的数据集中检索图像，并在推理时决定是检索还是生成

这是通过学习决策模块完成的，该模块以LLM的隐藏表示为条件，与之前的多模态语言模型相比，GILL展示了更广泛的功能，它可以处理图像和文本输入，并生成检索图像、生成图像和生成文本，同时又在衡量上下文依赖性的多个文本到图像任务中，性能优于基于非LLM的生成模型

<img src="https://img.saraba1st.com/forum/202306/01/024316bgqwtwhzqpqzhttw.png" referrerpolicy="no-referrer">

<strong>architecture.png</strong> (83.07 KB, 下载次数: 0)

下载附件

2023-6-1 02:43 上传

研究结果表明，尽管两个模型使用完全不同的文本，但可以有效地将冻结的纯文本 LLM 的输出嵌入空间映射到冻结的文本到图像生成模型（在这项工作中为稳定扩散模型）的嵌入空间编码器，与需要交错图像文本训练数据集的其他方法相比

通过微调 图像字幕对 上的少量参数来实现这一点，GILL方法计算效率高，不需要在训练时运行图像生成模型

<img src="https://img.saraba1st.com/forum/202306/01/024530pk87y1yl87qr07sg.png" referrerpolicy="no-referrer">

<strong>inference.png</strong> (97.92 KB, 下载次数: 0)

下载附件

2023-6-1 02:45 上传

在推理过程中，该模型接受任意交错的图像和文本输入，并生成与图像嵌入交错的文本，在决定是检索还是生成一组特定的标记后，它会返回适当的图像输出（检索或生成）

<img src="https://img.saraba1st.com/forum/202306/01/024718vp9y29x3pdzy06px.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-024657.jpg</strong> (109.72 KB, 下载次数: 0)

下载附件

2023-6-1 02:47 上传

与基于非LLM的文本到图像生成模型相比，GILL可以为交错的图像和文本的复杂输入为条件来生成更相关的图像

<img src="https://img.saraba1st.com/forum/202306/01/024846m4ze484on8ooov36.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-024830.jpg</strong> (177.02 KB, 下载次数: 0)

下载附件

2023-6-1 02:48 上传

引入的GILLMapper模块使模型能够有效地映射到SD图像生成模块上，对于来自PartiPrompt的许多示例，GILL性能优于或匹配SD

<img src="https://img.saraba1st.com/forum/202306/01/025057sy4og1f7zu7d6yyg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-025029.jpg</strong> (172.15 KB, 下载次数: 0)

下载附件

2023-6-1 02:50 上传

GILL可以复合多模态信息以产生相关的图像和文本输出，完全胜过仅限于图像检索的基线模型

GILL的局限
1.GILL并不总是在提示时或（显然）对对话有用时生成图像
2.GILL 的局限性在于其有限的视觉处理，目前的GILL仅使用4个视觉向量来表示每个输入图像（由于计算限制），这可能无法捕获下游任务所需的所有相关视觉信息
3.模型继承了LLM的一些意外行为，例如产生幻觉的可能性，它可能会生成错误的或与输入数据不相关的内容，还有时会生成重复的文本，并且并不总是生成连贯的对话文本
4.GILL的优势之一是它是模块化的，可以受益于未来发布的更强大的视觉和语言模型，它也可能受益于更强大的文本到图像生成主干，或者通过微调生成主干而不仅仅是GILLMapper 模块

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 463#       发表于 2023-6-1 03:34

PerSAM

github项目地址:https://github.com/NielsRogge/Transformers-Tutorials/tree/master/PerSAM

方法来自论文:https://arxiv.org/abs/2305.03048

注:在Colab中打开仓库文件使用

PerSAM 是一种很酷的个性化Meta AI开源的SAM模型的方法

仅使用给定概念（如“狗”）的一个示例（图像和遮罩对），该方法就可以确保SAM模型能够稳定分割其他图像中的相似概念

<img src="https://img.saraba1st.com/forum/202306/01/033014idlyo0looojexajl.jpeg" referrerpolicy="no-referrer">" src="https://static.saraba1st.com/image/common/none.gif" referrerpolicy="no-referrer">

<strong>68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e6766.jpeg</strong> (754.25 KB, 下载次数: 0)

下载附件

2023-6-1 03:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 464#       发表于 2023-6-1 03:53

VisorGPT

通过生成预训练学习视觉先验

项目主页:https://sierkinhane.github.io/visor-gpt/

github代码库:https://github.com/Sierkinhane/VisorGPT

hugface Demo演示:https://huggingface.co/spaces/szukevin/VISOR-GPT

视觉数据中的各种东西具有特定的特征，这些特征可以通过深度神经网络学习，并隐含地表示为模型中的视觉先验，例如对象位置和形状，这种先验可能会影响许多视觉任务

例如，在条件图像合成中，空间条件不符合先验会导致视觉上不准确的合成结果，这项工作旨在明确学习视觉先验并实现定制采样，受语言建模进步的启发，可以通过称为VisorGPT的生成预训练来学习视觉先验

通过将对象（例如边界框、人体姿势和实例分割掩码）的视觉位置离散化为序列，VisorGPT 可以通过似然最大化对视觉先验进行建模，另外，进行了提示工程研究以统一各种视觉位置，并能够对来自先验知识的顺序输出进行定制采样

实验结果表明，VisorGPT 可以有效地对视觉先验进行建模，可用于许多视觉任务，例如为ControlNet等条件图像生成模型定制准确的人体姿势

<img src="https://img.saraba1st.com/forum/202306/01/034705u5k7aojoakkkq22w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-034538.jpg</strong> (198.53 KB, 下载次数: 0)

下载附件

2023-6-1 03:47 上传

根据用户的提示，VisorGPT可以相应地从学习到的先验中采样一个序列，该序列可以进行空间解码以实现图像生成，解码后的条件符合先验，合成的“杯子”、“餐桌”和“甜甜圈”看起来真实且与预期的语义一致

这一发现证明我们可以从许多方面连续定制空间条件，例如数据类型、物体大小、实例数量和类，使用VisorGPT，随着条件图像合成的进步，生成数量无限的合成图像及其相应的细粒度注释成为可能，同时也可以为训练更加稳健和泛化的视觉智能模型提供充足的资源

<img src="https://img.saraba1st.com/forum/202306/01/035001ennsmrvwmnimwe5v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-034929.jpg</strong> (174.98 KB, 下载次数: 0)

下载附件

2023-6-1 03:50 上传

来自对象框的合成图像比较，遵循先验（左）或不遵循（右）的生成结果

<img src="https://img.saraba1st.com/forum/202306/01/035034agzlgjlxj68jxmgd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-034855.jpg</strong> (482.54 KB, 下载次数: 0)

下载附件

2023-6-1 03:50 上传

<img src="https://img.saraba1st.com/forum/202306/01/035133ic0tid6et9xa79m7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-035106__01.jpg</strong> (676.9 KB, 下载次数: 0)

下载附件

2023-6-1 03:51 上传

<img src="https://img.saraba1st.com/forum/202306/01/035310mkiav826im7kgx6l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-035243.jpg</strong> (730.22 KB, 下载次数: 0)

下载附件

2023-6-1 03:53 上传

<img src="https://img.saraba1st.com/forum/202306/01/035310r6gbgp72fpbyufbp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-035256.jpg</strong> (578.65 KB, 下载次数: 0)

下载附件

2023-6-1 03:53 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 465#       发表于 2023-6-1 05:15

SwiftSage

一种具有快思维/慢思维的生成代理者，用于复杂的交互任务

项目地址:https://yuchenlin.xyz/swiftsage/

github代码实现仓库:https://github.com/yuchenlin/swiftsage/

这是一种受人类认知双过程理论(注:诺贝尔经济学奖得主丹尼尔·卡尼曼著作《思考，快与慢》)启发的新型代理框架，旨在为复杂的交互式推理任务制定出色的行动计划

SwiftSage集成了行为克隆和提示大型语言模型(LLM)的优势，以增强任务完成性能。该框架包括两个主要模块：Swift模块，代表快速和直观的思维，以及Sage模块，模拟深思熟虑的思维过程

Swift模块是一个小型编码器-解码器LM，在oracle agent的动作轨迹上进行了微调，而Sage模块使用GPT-4等LLM进行子目标规划和基础

本文开发了一种启发式方法来和谐地整合这两个模块，从而实现更高效、更稳健的问题解决过程，它比以前的智能体（例如DRRN、SayCan、ReAct和 Reflexion）能更好地解决复杂的交互任务

<img src="https://img.saraba1st.com/forum/202306/01/050210ka27x1o4gt3z3x1o.jpg" referrerpolicy="no-referrer">

<strong>20230601_050203.jpg</strong> (713.81 KB, 下载次数: 0)

下载附件

2023-6-1 05:02 上传

将SwiftSage与先前的代理进行比较：ReAct有子目标规划，Reflexion增加了自我反思，然而，这些方法既昂贵又脆弱，在环境中执行和制定行动/计划也很困难又容易出错

<img src="https://img.saraba1st.com/forum/202306/01/050524xzlznweoendeo4ud.jpg" referrerpolicy="no-referrer">

<strong>20230601_050459.jpg</strong> (338.17 KB, 下载次数: 0)

下载附件

2023-6-1 05:05 上传

<img src="https://img.saraba1st.com/forum/202306/01/050524xsmxluv0ymh0x0sl.jpg" referrerpolicy="no-referrer">

<strong>20230601_050501.jpg</strong> (382.53 KB, 下载次数: 0)

下载附件

2023-6-1 05:05 上传

Swift是一个用于快速思考的小型LM(770m参数)，通过模仿学习超级熟练目标环境，Sage提示LLM在两个阶段进行缓慢思考：计划和落地，并获得与环境交互的行动缓冲区

<img src="https://img.saraba1st.com/forum/202306/01/051119ss9xdfbeshiofsdi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-051105.jpg</strong> (359.91 KB, 下载次数: 0)

下载附件

2023-6-1 05:11 上传

SwiftSage的特点： 利用模仿学习训练小LM快速思考，只在需要的时候提示LLM（比如5步后没有奖励），在提示LLMs的时候分开规划和落地子目标，每个LLM调用获得多种行动

<img src="https://img.saraba1st.com/forum/202306/01/051350gcwv2uhhc45f45u4.jpg" referrerpolicy="no-referrer">

<strong>20230601_051303.jpg</strong> (414.88 KB, 下载次数: 0)

下载附件

2023-6-1 05:13 上传

<img src="https://img.saraba1st.com/forum/202306/01/051350q57vf0w5x3xjxv5v.jpg" referrerpolicy="no-referrer">

<strong>20230601_051305.jpg</strong> (718.48 KB, 下载次数: 0)

下载附件

2023-6-1 05:13 上传

<img src="https://img.saraba1st.com/forum/202306/01/051350mig5gg5p5g2p5llr.jpg" referrerpolicy="no-referrer">

<strong>20230601_051334.jpg</strong> (342.08 KB, 下载次数: 0)

下载附件

2023-6-1 05:13 上传

<img src="https://img.saraba1st.com/forum/202306/01/051457n18f4dttz65g1dg8.jpg" referrerpolicy="no-referrer">

<strong>20230601_051336.jpg</strong> (471.07 KB, 下载次数: 0)

下载附件

2023-6-1 05:14 上传

使用ScienceWorld进行评估，它是一个基于文本的引擎，有30种任务类型、10个位置、200多个对象和25个可选动作这些任务可能非常复杂且时间跨度很大，而且还需要异常处理，SwiftSage比其他产品好2倍且花费的成本低得多

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 466#       发表于 2023-6-1 06:04

 本帖最后由 Machinery 于 2023-6-1 06:07 编辑 

AlignScore

使用统一对齐函数评估事实一致性

论文地址:https://arxiv.org/abs/2305.16739

github项目仓库:https://github.com/yuh-zha/AlignScore

许多文本生成应用程序要求生成的文本与输入信息事实一致，事实一致性的自动评估具有挑战性，以前的工作已经开发出各种指标，这些指标通常取决于特定功能，例如在有限数据上训练的自然语言推理 (NLI) 或问答 (QA)，因此，这些指标很难评估在不同任务的不同输入/输出（例如，句子、文档）中发生的各种事实不一致（例如，矛盾、幻觉）

在本文中，提出了AlignScore，这是一种新的整体指标，适用于上述各种事实不一致的情况，AlignScore 基于两个任意文本片段之间的信息对齐的通用功能

通过整合大量不同的数据源开发了对齐功能的统一训练框架，从7个NLP任务中收集了470万个训练样本，即自然语言推理、释义、事实验证、语义文本相似性、问答、信息检索和摘要

对包括22个评估数据集在内的大规模基准进行了广泛的实验，其中19个数据集从未在对齐训练中出现过，AlignScore 比以前的各种指标都取得了实质性的改进，此外，AlignScore（355M 参数）匹配甚至优于基于ChatGPT和GPT-4的指标，后者的参数大了几个数量级

<img src="https://img.saraba1st.com/forum/202306/01/055413gj19fkmftoh12jf1.jpg" referrerpolicy="no-referrer">

<strong>20230601_055312.jpg</strong> (184.92 KB, 下载次数: 0)

下载附件

2023-6-1 05:54 上传

这个庞大的数据集使研究组能够训练一个对齐函数，该函数可以决定要求中的所有信息：
1) 是否出现在上下文中，2) 是否与上下文相矛盾

<img src="https://img.saraba1st.com/forum/202306/01/055523j3joudprogxbgh2m.jpg" referrerpolicy="no-referrer">

<strong>20230601_055517.jpg</strong> (3.68 KB, 下载次数: 0)

下载附件

2023-6-1 05:55 上传

对齐函数将文本对(a, b)映射到表征对齐级别的标签y

<img src="https://img.saraba1st.com/forum/202306/01/055730gndpub2m8ojezmuu.jpg" referrerpolicy="no-referrer">

<strong>20230601_055716.jpg</strong> (222.65 KB, 下载次数: 0)

下载附件

2023-6-1 05:57 上传

为了对长文本进行推理并获得与人类更好相关的分数，还提出了一种语句块分割方法，直观上，它显示了与上下文事实一致的声明的比例

<img src="https://img.saraba1st.com/forum/202306/01/055933zd0vh01bt30030dh.jpg" referrerpolicy="no-referrer">

<strong>20230601_055910.jpg</strong> (144.6 KB, 下载次数: 0)

下载附件

2023-6-1 05:59 上传

<img src="https://img.saraba1st.com/forum/202306/01/055933hmx0vifwk0x6f0n6.jpg" referrerpolicy="no-referrer">

<strong>20230601_055912.jpg</strong> (152.29 KB, 下载次数: 0)

下载附件

2023-6-1 05:59 上传

<img src="https://img.saraba1st.com/forum/202306/01/055933w7kii66hpi26ki60.jpg" referrerpolicy="no-referrer">

<strong>20230601_055923.jpg</strong> (184.61 KB, 下载次数: 0)

下载附件

2023-6-1 05:59 上传

实验表明，AlignScore在SummaC和TRUE基准以及其他流行的事实一致性数据集上优于竞争基线（QAFactEval、UniEval、CTC、BARTScore、SummaC、BERTScore等方法）

<img src="https://img.saraba1st.com/forum/202306/01/060050e7u9hyxqaxcyy8xc.jpg" referrerpolicy="no-referrer">

<strong>20230601_060036.jpg</strong> (98.78 KB, 下载次数: 0)

下载附件

2023-6-1 06:00 上传

甚至优于无数倍大的GPT-4基准

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 467#       发表于 2023-6-1 17:08

prm800k

通过过程监督改进数学推理

项目博客:https://openai.com/research/improving-mathematical-reasoning-with-process-supervision

监督数据集:https://github.com/openai/prm800k

量子位的说明文章:https://zhuanlan.zhihu.com/p/633877440

<img src="https://img.saraba1st.com/forum/202306/01/170812vfw4vvz1whvq8iq1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230601-170627.jpg</strong> (304.11 KB, 下载次数: 0)

下载附件

2023-6-1 17:08 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 468#       发表于 2023-6-2 02:13

Neuralangelo

来自NVIDIA，3D场景/物体重建

项目博客:https://blogs.nvidia.com/blog/2023/06/01/neuralangelo-ai-research-3d-reconstruction/

<img src="https://img.saraba1st.com/forum/202306/02/021336ci2yyynygn16i15l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-015736.jpg</strong> (59.43 KB, 下载次数: 0)

下载附件

2023-6-2 02:13 上传

Neuralangelo是NVIDIA Research使用神经网络构建的新模型，可以进行3D重建，具体来说则是可以将2D剪辑视频转换为详细的3D结构，生成包括建筑物、雕塑和其他现实世界物体的逼真虚拟复制品

就像米开朗基罗从大理石块上雕刻出令人惊叹、栩栩如生的视觉效果一样，Neuralangelo可以生成具有复杂细节和纹理的3D结构，然后，相关专业创作人士可以将这些3D对象导入设计应用程序，进一步编辑它们以用于艺术、视频游戏开发、机器人和工业数字孪生等应用

Neuralangelo将复杂材料（包括屋顶瓦、玻璃板和光滑大理石等）的纹理从2D视频转换为3D资产的能力**超过了以前的其他方法，高保真度使其进行3D重建更容易，开发人员和专业创意人士甚至可以使用智能手机拍摄的镜头为他们的项目快速创建可用的虚拟对象

“Neuralangelo 提供的 3D 重建功能将为创作者带来巨大好处，帮助他们在数字世界中重建真实世界，”该论文的高级研究主管兼论文合着者说。“该工具最终将使开发人员能够将详细的对象——无论是小型雕像还是大型建筑物——导入视频游戏或工业数字孪生的虚拟环境中。”

在演示中，NVIDIA 研究人员展示了该模型如何重现像米开朗基罗的大卫那样具有标志性的物体，以及像平板卡车一样司空见惯的物体。Neuralangelo 还可以重建建筑物的内部和外部——这点将通过NVIDIA湾区园区公园的详细3D模型进行演示

演示地址:https://youtu.be/PQMNCXR-WF8

先前用于重建3D场景的AI模型一直难以准确捕捉重复的纹理图案、均匀的颜色和强烈的颜色变化，Neuralangelo采用即时神经图形初始化，NVIDIA Instant NeRF背后的技术来帮助捕捉这些更精细的细节

使用从不同角度拍摄的物体或场景的2D视频，该模型选择几个捕捉不同视点的帧，就像艺术家从多个侧面考虑一个主题以获得深度、大小和形状的感觉

一旦确定了每一帧的相机位置，Neuralangelo 的 AI 就会创建场景的粗略 3D 表示，就像雕刻家开始凿出对象的形状一样，然后，该模型会优化渲染以锐化细节，就像雕刻家煞费苦心地凿石头以模仿织物或人物形象的质地一样，最终结果是生成可用于虚拟现实应用、数字孪生或机器人开发的3D对象或大型场景

在 6 月 18 日至 22 日的 CVPR 上查找NVIDIA Research，Neuralangelo是NVIDIA Research将在 6 月 18 日至 22 日于温哥华举行的计算机视觉和模式识别会议 (CVPR) 上展示的近 30 个项目之一，这些论文项目涵盖的主题将包括从姿势估计、3D重建到视频生成

<img src="https://img.saraba1st.com/forum/202306/02/021323yuzn7xsuxdyyo13k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-021205.jpg</strong> (69.84 KB, 下载次数: 0)

下载附件

2023-6-2 02:13 上传

其中一个项目，DiffCollage，是一种创建大规模内容的扩散方法——包括长横向、360度全景和循环运动拓展图像，当输入具有标准宽高比的图像训练数据集时，DiffCollage将这些较小的图像视为较大视觉效果的一部分——就像拼贴画的一部分，这使得扩散模型能够生成具有凝聚力的大型内容，而无需在相同比例的图像上进行训练

<img src="https://img.saraba1st.com/forum/202306/02/021317ittto8ck4szoyqjl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-021251.jpg</strong> (71.65 KB, 下载次数: 0)

下载附件

2023-6-2 02:13 上传

该技术还可以将文本提示转换为视频序列，使用捕捉人体动作的预训练扩散模型进行了演示

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 469#       发表于 2023-6-2 02:33

HiFA

具有高级扩散引导的高保真文本到生成3D

项目地址:https://hifa-team.github.io/HiFA-site/

github项目仓库:https://github.com/HiFA-team/HiFA

演示视频:https://youtu.be/E4sD4ng4VH0

自动文本到3D合成通过优化3D模型取得了显着进步。 现有方法通常依赖于预训练的文本到图像生成模型，例如稳定扩散模型，为神经辐射场(NeRF)的2D渲染提供分数并用于优化NeRF

然而，由于对3D几何的理解有限，这些方法经常遇到跨多个视图的伪影和不一致，为了解决这些限制，可以通过使用扩散先验重新优化损失函数提升效果

此外，还介绍了一种新的训练方法，可以释放先验扩散模型的潜力，为了改进3D几何表示，对NeRF渲染图像应用辅助深度监督并规范NeRF的密度场

最终广泛的实验证明了我们的方法优于以前的工作，从而产生了先进的照片真实感和改进的多视图一致性

通过将生成图像与目标图像之间的图像重建损失替换分数密度蒸馏(SDS)损失来改变SDS损失，这个新的损失是通过“添加噪声和稳定扩散稳定去噪”获得的

在某些情况下，这个损失会产生与SDS完全相同的梯度，然而，可以通过在RGB空间中计算重建误差，增加额外的去噪步骤，甚至交换调度程序来增强我们的损失，我们还发现，能够通过随着当前训练迭代的平方根与时间步长成反比地减少添加噪音的强度来达到更好的损失结果

此外，还提出了几个辅助损失来提高生成的3D模型的质量，包括使用预训练的单目深度预测模型，进行度量不变的深度重建损失，以及z方差损失，z方差损失特别有价值，因为它消除了以前基于辐射场方法(例如DreamFusion/Score Jacobian Chaining)中看到的伪影(例如云状几何)的存在，并使我们能够仅使用NeRF模型(即单阶段3D建模)获得令人印象深刻的结果，从而跳过网格微调阶段(双阶段3D建模)

<img src="https://img.saraba1st.com/forum/202306/02/023130xd49mmcim8se9zm1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-023020.jpg</strong> (232.56 KB, 下载次数: 0)

下载附件

2023-6-2 02:31 上传

<img src="https://img.saraba1st.com/forum/202306/02/023130e9uuobobndcdow99.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-023033.jpg</strong> (162.06 KB, 下载次数: 0)

下载附件

2023-6-2 02:31 上传

注:文本到3D合成结果，左边是NERF渲染图像，右侧显示相应的网格，没有使用DMTet表面纹理微调，没有使用shading

这种自由使我们能够生成逼真的结果，同时避免与基于网格的方法相关的某些限制，包括关于存在表面模型的假设，有限的网格分辨率和固定的着色器类等等

<img src="https://img.saraba1st.com/forum/202306/02/023054vrdb0f9pb5b5tpwq.png" referrerpolicy="no-referrer">

<strong>method_scaled.png</strong> (598.48 KB, 下载次数: 0)

下载附件

2023-6-2 02:30 上传

额外的生成结果测试:

<img src="https://img.saraba1st.com/forum/202306/02/023333fh64424cib44qh4s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-023319.jpg</strong> (233.91 KB, 下载次数: 0)

下载附件

2023-6-2 02:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 470#       发表于 2023-6-2 02:51

Coeditor

开源的python自动“编辑”模型 ，利用上下文更改进行多轮代码自动编辑，Coeditor以CodeT5为基础，在您编码时预测您的下一个**编辑操作**

项目地址:https://github.com/MrVPlusOne/Coeditor

开发人员通常会花费大量时间来维护和重构现有代码，然而，大多数关于代码生成模型的先前工作只关注创建新代码，而忽略了编辑现有代码的独特要求

在这项工作中，研究组探索了多轮代码自动编辑设置，旨在根据同一代码库中的最新更改预测对代码区域的编辑，模型Coeditor是经过微调的CodeT5模型，具有专为代码编辑任务设计的增强功能

通过使用行差异格式编码代码变更，并采用静态分析来形成大型定制模型上下文，以确保预测编辑操作所需的适当信息

从1650个开源Python项目的提交历史中收集了一个代码编辑数据集用于训练和评估，在简化的单轮单编辑任务中,Coeditor的精确匹配准确性明显高于最佳代码完成方法，尽管使用的模型参数规模较小,但准确性几乎翻了一番，这表明可结合编辑历史进行代码完成具有显著好处

在多轮多次编辑测试中，我们观察到通过迭代地提示模型额外的用户编辑获得的显着收益，研究组开源了代码，数据和模型权重，以鼓励未来的研究，并发布了一个由Coeditor模型驱动的VSCode扩展程序用于交互式使用

<img src="https://img.saraba1st.com/forum/202306/02/024844nn82komtp3lmw6ll.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-024514.jpg</strong> (89.76 KB, 下载次数: 0)

下载附件

2023-6-2 02:48 上传

<img src="https://img.saraba1st.com/forum/202306/02/024902acyhqgq9psu19h6h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-024809.jpg</strong> (250.54 KB, 下载次数: 0)

下载附件

2023-6-2 02:49 上传

(a) 用户首先编辑pack_batch函数,以从每个输入行中读取额外的字典关键字“cost”。
(b) 然后,用户删除group_to_batches函数顶部的3行。 
(c) 现在,用户在同一函数的底部调用Coeditor。Coeditor正确地建议向字典变量row添加“cost”键,但它未能解决现在未定义的红色下划线变量。
(d) 但是,如果用户接受建议的更改并在第209行手动引入两个新变量,则Coeditor可以相应地建议正确的更改。

<img src="https://img.saraba1st.com/forum/202306/02/025137u3kul402u3h6d33j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-025022.jpg</strong> (272.29 KB, 下载次数: 0)

下载附件

2023-6-2 02:51 上传

<img src="https://img.saraba1st.com/forum/202306/02/025137hpk1zur34h4rzzxh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-025034.jpg</strong> (70.58 KB, 下载次数: 0)

下载附件

2023-6-2 02:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 471#       发表于 2023-6-2 03:32

AutoCompressor

调整语言模型以压缩的方式拓展上下文长度

github项目地址:https://github.com/princeton-nlp/AutoCompressors

基于Transformer的语言模型(LM)是强大和广泛适用的工具，但它们的实用性受到有限的上下文窗口和处理长文本文档的昂贵计算成本的限制

本文提出了将预训练的LM调整为AutoCompressor，这些模型能够将长上下文压缩成紧凑的摘要向量，然后这些向量可以作为软提示被模型访问并使用

摘要向量使用无监督目标进行训练，其中长文件被分段处理，并使用所有预先进行分段的摘要向量进行语言建模，之后对OPT模型进行精调，使其可以处理长达30720个Token的序列，结果表明AutoCompressor可以利用长上下文来改进文本困惑度

研究组通过压缩任务演示来评估AutoCompressor中的上下文学习发现，摘要向量可以很好地替代纯文本演示，在降低推理成本的同时提高准确性

最后，总结了将摘要向量应用于检索增强语言建模来预计算大型语料库的摘要向量的好处，总的来说，AutoCompressor是扩展LM上下文窗口并加速长上下文推理的简单且经济的解决方案

<img src="https://img.saraba1st.com/forum/202306/02/032220nt38mmswa8twj0ci.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-032142.jpg</strong> (84.72 KB, 下载次数: 0)

下载附件

2023-6-2 03:22 上传

AutoCompressors处理长文档并通过递归地生成摘要向量，这些向量将作为软提示传递给所有的后续段

<img src="https://img.saraba1st.com/forum/202306/02/032929mxhqkpzfutxmlieq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-032918.jpg</strong> (67.6 KB, 下载次数: 0)

下载附件

2023-6-2 03:29 上传

在测试数据集的2048个token上计算perplexity(困惑度)，同时改变前文的长度，对RMT和AutoCompressor模型，使用摘要向量作为条件，扩展的全注意力模型使用4096个token的扩展上下文窗口进行微调，因此不能使用超过2048个token作为条件，“最佳prompt”显示了每个模型达到最佳perplexity所需的有效额外序列长度(粗体数字)

<img src="https://img.saraba1st.com/forum/202306/02/033221ohn9vvys6nc516wy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-033203.jpg</strong> (86.7 KB, 下载次数: 0)

下载附件

2023-6-2 03:32 上传

使用AutoCompressor进行高效的检索增强语言建模，大型语料库可以预处理成压缩的摘要向量，这些向量可以便宜地存储，检索时，压缩的摘要可以融合以在单个前向传递中高效访问多个文档

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 472#       发表于 2023-6-2 04:05

 本帖最后由 Machinery 于 2023-6-2 04:13 编辑 

FuseCap

利用大型语言模型将视觉数据融合到丰富的图像说明数据集中

项目主页:https://rotsteinnoam.github.io/FuseCap/

github项目代码库:https://github.com/RotsteinNoam/FuseCap

hugface演示:https://huggingface.co/spaces/noamrot/FuseCap

<img src="https://img.saraba1st.com/forum/202306/02/035828rczceqjs6lnjjz4k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-035731.jpg</strong> (213.59 KB, 下载次数: 0)

下载附件

2023-6-2 03:58 上传

图像字幕是计算机视觉中的一项核心任务，随着视觉语言预训练技术的出现，它已经取得了长足的进步，在本文中，我们强调了字幕模型经常被忽视的一个局限性，该局限性通常无法捕获语义上重要的元素

这个缺点可以追溯到文本图像数据集，虽然他们的标题通常提供图像内容的一般描述文本，但经常省略显着的细节

为了减轻这种限制，本文提出了FuseCap，一种用额外的视觉信息来丰富字幕的新方法，这些视觉信息是从视觉专家模型那里获得的，例如对象检测器、属性识别器和光学字符识别器 (OCR)

通过使用大型语言模型 (LLM) 将此类视觉专家的输出与原始字幕融合在一起，产生丰富的标题，呈现全面的图像描述

定量和定性分析验证了所提出的字幕丰富方法的有效性，然后被用来整理一个文本标注模型的训练集，基于BLIP，该模型在生成准确和详细的字幕方面超越了当前最先进的方法，同时使用的参数和训练数据少得多

作为额外的贡献，同时提供了一个由1200万个图像丰富的字幕对组成的数据集，并表明所提出的方法在很大程度上改进了图像文本检索

<img src="https://img.saraba1st.com/forum/202306/02/035848i4exurrrn1eokkre.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-035835.jpg</strong> (161.92 KB, 下载次数: 0)

下载附件

2023-6-2 03:58 上传

两阶段框架图
第一阶段，使用FUSECAP策略丰富现有的图像说明，由视觉专家模从图像中提取有意义的信息，由LLM Fuser将其与原始字幕融合成丰富的字幕

 第二阶段，其中图像数据集与第一阶段的丰富说明相结合，用于预训练和微调综合的图像模型

<img src="https://img.saraba1st.com/forum/202306/02/040400au6joc9loxf9yloc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-040249.jpg</strong> (443.6 KB, 下载次数: 0)

下载附件

2023-6-2 04:04 上传

<img src="https://img.saraba1st.com/forum/202306/02/040400h4qjqm6649hj9m46.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-040300.jpg</strong> (46.66 KB, 下载次数: 0)

下载附件

2023-6-2 04:04 上传

<img src="https://img.saraba1st.com/forum/202306/02/040400ywgvnqk5qguipvnd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-040318.jpg</strong> (60.96 KB, 下载次数: 0)

下载附件

2023-6-2 04:04 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 473#       发表于 2023-6-2 08:41

PanoGen

视觉与语言导航的文本条件全景图生成

项目主页:https://pano-gen.github.io/

github项目地址:https://github.com/jialuli-luka/PanoGen

视觉与语言导航任务要求智能体根据语言指令在3D环境中导航，任务的一个主要挑战是现实感训练环境的有限数量，这使得导航很难推广到新的和未见过的环境

为了解决这个问题，研究组提出了PanoGen,这是一个可以基于文本输入生成无限潜在数量的多样化全景环境的生成方法

具体来说，通过为Matterport3D环境中的房间图像添加字幕来收集房间描述，并利用最先进的文本到图像扩散模型生成新的全景环境

使用生成图像上的递归填充来创建一致的360度全景视图，新的全景环境通过基于文本描述进行条件设置与原始环境共享类似的语义信息，这可以确保全景图中对象的共现遵循人类的直觉，并通过图像填充在房间外观和布局上产生足够的多样性

最后探索了两种方式来利用PanoGen在VLN(视觉到文本导航)预训练和微调中，为PanoGen环境中的路径生成指令，这些指令是基于经过预训练的视觉和语言模型为VLN预训练而构建的，在微调期间使用生成的全景环境增强智能体的视觉观察，以避免过度依赖已见环境

实践证明，使用PanoGen环境学习可以达到Room-to-Room，Room-for-Room和CVDN数据集的Sota成绩，此外还发现使用PanoGen speaker数据集对CVDN预训练尤其有效，CVDN的指令不够详细，需要常识知识才能达到目标

智能体可以从训练更多生成的全景环境中受益，这表明PanoGen环境的规模放大可以增强智能体对未见环境的推广

<img src="https://img.saraba1st.com/forum/202306/02/083406srn1x9drrxr4w1xr.png" referrerpolicy="no-referrer">

<strong>overview.png</strong> (778.95 KB, 下载次数: 0)

下载附件

2023-6-2 08:34 上传

首先为Matterport3D数据集中的所有房间全景图生成说明文本，之后将每个全景图离散为36个视图，示范中显示15个视图，以便更好地查看每个离散图像，然后通过对从文本标题生成的单个图像进行递归填充来生成全景环境

<img src="https://img.saraba1st.com/forum/202306/02/083631x9hh5548unhzznnh.png" referrerpolicy="no-referrer">

<strong>method.png</strong> (804.94 KB, 下载次数: 0)

下载附件

2023-6-2 08:36 上传

提出了一种“递归”图像输出方法，该方法基于文本说明重建图像中的缺失区域，首先在全景图中选择一张生成的图像作为起点，逐渐左右、上下旋转相机角度，然后根据文字描述画出潜在的观察结果

<img src="https://img.saraba1st.com/forum/202306/02/083931jfl0k5lc7yoflc0f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-083809.jpg</strong> (132.05 KB, 下载次数: 0)

下载附件

2023-6-2 08:39 上传

在左侧生成的全景图中，它包含走廊视图，并显示连接到走廊的多个房间（例如，卧室和浴室），这种布局也遵循了人类的常识，卧室和卫生间可以用走廊相连

评估成绩

<img src="https://img.saraba1st.com/forum/202306/02/084111jyiuh11c8gouipiu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230602-084017.jpg</strong> (491.49 KB, 下载次数: 0)

下载附件

2023-6-2 08:41 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 474#       发表于 2023-6-3 02:22

Thought Cloning

思维克隆:通过模仿人类的思维学习在行动中思考

github项目地址:https://github.com/ShengranHu/Thought-Cloning

语言往往被视为人类思维的关键方面，为我们提供了发展、探索、计划、重新计划和适应新情况的卓越能力，然而，强化学习(RL)代理远未达到人类在任何这些能力中的表现水平

假设其中一个原因是它们缺乏用语言思考的好处，我们可以通过训练AI代理思考像人类那样来提高它们，因此，提出一个新的模仿学习框架——思维克隆，其理念不是仅仅克隆人类演示者的行为，而且还要克隆人类在执行这些行为时所拥有的思想

虽然我们期望思维克隆在大规模的人类在行事时思考的数据集(例如，带有字幕的在线视频)上真正发挥作用，但在这里仅在一个领域内进行实验，思考和行动数据是人工生成的

结果表明，思维克隆的学习速度远快于行为克隆，它的性能优势在分布式测试任务之外增长得更远，凸显了其更好处理新情况的能力

思维克隆也为AI安全性和可解释性提供了重要好处，并使调试和改进AI变得更容易，因为我们可以观察代理的思想，所以可以
(1)发现为什么代理会出错，从而更容易修复问题
(2)通过修正其思想来控制代理
(3)防止它计划要做的不安全事情

总的来说，通过训练代理如何思考和行事，思维克隆创造了更安全、更强大的代理

<img src="https://img.saraba1st.com/forum/202306/03/021712iy6yo6y03bbkz10r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-021544.jpg</strong> (78.81 KB, 下载次数: 0)

下载附件

2023-6-3 02:17 上传

思想克隆 (TC) 的总体框架，TC 代理有两个组件：上层组件和下层组件，在每个时间步，TC 智能体都会收到一个观察结果、任务和思想历史作为输入

上层组件产生思想，下层组件根据这些思想产生行动，将生成的想法和行动与演示数据集中的基本事实进行比较，以计算损失

<img src="https://img.saraba1st.com/forum/202306/03/021825zi460e9zixn46298.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-021808.jpg</strong> (115.42 KB, 下载次数: 0)

下载附件

2023-6-3 02:18 上传

左：BabyAI环境示例，环境包含各种有色物品（球、钥匙、盒子、门），智能体可以捡起、放下和移动物体或打开和关闭门，而锁着的门只能用颜色匹配的钥匙解锁，代理可以观察到它前面的 7 × 7 格单元格，这些单元格可以被墙壁和关闭的门挡住

右图：来自受过训练的思想克隆代理规划和重新规划的示例。 该任务需要到达紫色方格，但一个紫色球挡住了去路，代理的思想和行动在遇到障碍时表现出重新规划，移除它，并恢复之前的目标

<img src="https://img.saraba1st.com/forum/202306/03/022114vhok4aqrquzogqo4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-022030.jpg</strong> (155.85 KB, 下载次数: 0)

下载附件

2023-6-3 02:21 上传

微调后的代理者在分布外环境中的成功率

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 475#       发表于 2023-6-3 02:32

3DCoMPaT++

3DCoMPaT++是一个丰富注释的大规模数据集，包含10000个独特的模型，每款模型有1000种生成的样式

项目主页:https://3dcompat-dataset.org/doc/intro.html#

模型在部件实例级别上进行注释，遵循每个形状类别的42个手工定义的一致命名法，有两个不同的语义级别:粗粒度和细粒度

每个对象采用的部件与材料组合从四个相同的等距视图以及四个随机视图角度进行渲染，对于每个视图，还提供深度图，部件分割蒙版，材料分割蒙版和渲染相机参数等

<img src="https://img.saraba1st.com/forum/202306/03/023052h3drevvr1qiokry1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-023028.jpg</strong> (144.98 KB, 下载次数: 0)

下载附件

2023-6-3 02:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 476#       发表于 2023-6-3 02:56

StyleDrop

进行任何风格的文本到图像生成

论文地址:https://arxiv.org/abs/2306.00983

项目主页:https://styledrop.github.io/

StyleDrop能够生成忠实遵循特定风格参考的图像，由Muse(相关模型:https://muse-model.github.io/)提供支持，StyleDrop用途极为广泛，可以捕捉用户提供的风格的细微差别和细节，例如配色方案、阴影、设计模式以及局部和全局效果

StyleDrop 的工作原理是通过微调极少的可训练参数（不到总模型参数的 1%）来有效地学习新风格，并通过人工或自动反馈的迭代训练来提高质量，更好的是，StyleDrop即使用户仅提供指定所需样式的单个图像，也能够提供令人印象深刻的结果

广泛的研究表明，对于调整文本到图像模型的风格任务，Muse上的Styledrop优于其他方法，包括Imagen和stable diffusion上的dreamboth与Textual Inversion

<img src="https://img.saraba1st.com/forum/202306/03/025603buvfqezkgq1x350v.jpg" referrerpolicy="no-referrer">

<strong>6b530beb-7950-494a-a4f5-fa976686aef9.jpg</strong> (294.64 KB, 下载次数: 0)

下载附件

2023-6-3 02:56 上传

<img src="https://img.saraba1st.com/forum/202306/03/025603xb5dcmmv5i59yyri.jpg" referrerpolicy="no-referrer">

<strong>1e2dd743-e3d3-4ff4-ab1d-76e53ea117a5.jpg</strong> (323.53 KB, 下载次数: 0)

下载附件

2023-6-3 02:56 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 477#       发表于 2023-6-3 04:53

decision-oriented dialogues

人机协作的决策导向综合对话系统

项目主页:https://collaborative-dialogue.github.io/

github项目地址:https://github.com/jlin816/dialop

decision-oriented dialogues(决策导向的对话沟通任务)，在这些任务中，AI助手必须通过自然语言与一个或多个人类协作，帮助他们做出复杂的决定

规范化了三个测试区域，在这些区域中，用户面临日常决定:
1.选择为会议论文分配审阅者
2.计划城市中的多步骤行程
3.为一群朋友协商共同的旅行计划

在每一个挑战中，AI助手和用户都有不同的情况范围，他们必须结合起来得出最佳决定:AI助手可以访问和处理大量信息，而用户本身有系统外的偏好和相关约束

对于每项任务，通过构建一个对话环境，在这个环境中，代理根据他们达成的最终决定的质量获得奖励

使用这些环境，通过收集了人与人之间的对话，人类扮演助手的角色，同时比较了当前的AI助手在这些设置中的交流能力，使用了大型语言模型进行自我对战的基准进行对比

在以决策为导向的对话中，有一个基于用户最终决定的客观效用衡量标准，能够在这些挑战中自动评估当前AI助手的质量

<img src="https://img.saraba1st.com/forum/202306/03/043834p4a8njgatmm7sjmz.jpg" referrerpolicy="no-referrer">

<strong>20230603_043325.jpg</strong> (219.78 KB, 下载次数: 0)

下载附件

2023-6-3 04:38 上传

<img src="https://img.saraba1st.com/forum/202306/03/043905yxpn0nzpzzmnx06n.png" referrerpolicy="no-referrer">

<strong>selfplay-len.png</strong> (279.93 KB, 下载次数: 0)

下载附件

2023-6-3 04:39 上传

发现GPT-3（在自我对弈中）的对话比人类更长，同时在所有的任务上得分都更低

在评估中包括了一种称为提示自我对弈 (PSP/prompted self-play)的新技术的评估结果，在这种情况中用50%或75%相应的人与人对话提示模型，并让模型从这些对话前缀后继续进行自我对弈

还考虑了一种情况，在这种情况下，模型可以访问人与人对话中除最终提议结果之外的所有消息，并且必须自己生成最终提议，结果如下

<img src="https://img.saraba1st.com/forum/202306/03/044300l9g69cxfmdzgxgea.png" referrerpolicy="no-referrer">

<strong>psp.png</strong> (84.6 KB, 下载次数: 0)

下载附件

2023-6-3 04:43 上传

1.在Optimization中，两个代理担任会议区域**的角色，每个代理只有关于审稿人与论文亲和力的部分信息时，将审稿人分配给会议论文
(游戏链接:http://reviewer-matching.herokuapp.com/)

2.在Planning中，具有城市知识的助手必须协助人类根据他们的喜好制定行程
(游戏链接:https://itinerary-planning.herokuapp.com/)

3.在Mediation中，多个用户必须与助手协作才能解决旅行调度任务
(游戏链接:https://collaborative-dialogue.herokuapp.com/)

所有三个任务的模型性能都不同，然而，模型无法胜过经过优化的随机基线)，在Planning中，模型的人类前缀越长表现越好，这表明它们能够根据人类信息进行调节，在Mediation中，模型的表现优于随机基线，但仍比人类差，在不同的PSP条件下没有显示出太大差异，这些结果表明，模型尚未缩小与人类在有效沟通以协作制定良好解决方案方面的表现差距

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 478#       发表于 2023-6-3 05:35

Cocktail

混合多模态条件控制文本到图像生成模型

项目主页:https://mhh0318.github.io/cocktail/

github项目仓库:https://github.com/mhh0318/Cocktail

hugface模型权重下载:https://huggingface.co/MichaelHu/cocktail

文本条件扩散模型能够生成内容多样的高保真图像，然而，使用文本表示经常对设想的客观图像表现出模棱两可的描述，需要结合额外的控制信号来增强文本引导扩散模型的功效

在这项工作中，提出了Cocktail，一种将各种模态信息混合到一个embedding嵌入中的工作流程，与广义ControlNet(gControlNet)、可控归一化 (ControlNorm) 和空间引导采样方法相结合，以实现多模态和空间精细化控制文本条件扩散模型

具体来说，引入了一个超网络gControlNet，专门用于将来自不同模态的控制信号对齐和注入预训练的扩散模型，gControlNet能够接受灵活的模态信号，包括同时接收到的模态信号的任意组合，或多种模态信号的补充融合，然后根据ControlNorm将控制信号融合并注入骨干模型

此外，先进的空间引导采样方法可以熟练地将控制信号整合到指定区域，从而避免在生成的图像中出现不需要的对象，展示了本方法在控制各种模式方面的结果，证明了对多个外部信号的高质量合成和保真度

<img src="https://img.saraba1st.com/forum/202306/03/052627h5jufkk0w0wx9nu0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-052617.jpg</strong> (85.76 KB, 下载次数: 0)

下载附件

2023-6-3 05:26 上传

各种控制方式的比较方法，Cocktail只需要一个通用模型，不像以前需要多个模型用于多种模式

1.引入了广义控制网络 (gControlNet)，这是一种能够自适应集成多模态信息的分支网络，有效解决了模态之间不平衡引起的问题

2.提出了可控规范化 (ControlNorm) 来优化分支网络中信息的利用，从而产生更有效的结果

3.引入了一种基于注意力图中操作的空间引导采样方法，以生成适合区域上下文的相关信息，防止包含指定区域之外的不需要的对象

<img src="https://img.saraba1st.com/forum/202306/03/053012oxaawjwoxohgwsab.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-052919.jpg</strong> (211.19 KB, 下载次数: 0)

下载附件

2023-6-3 05:30 上传

具有相同提示的模型示例，给定文本提示和各种模态信号，Cocktail能够使用单个模型合成满足所有输入条件或这些条件的任意子集的图像，在示例提示中是：一个女孩抱着一只猫

<img src="https://img.saraba1st.com/forum/202306/03/053407wfs9pxf3tjfnryfx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-053140.jpg</strong> (163.72 KB, 下载次数: 0)

下载附件

2023-6-3 05:34 上传

<img src="https://img.saraba1st.com/forum/202306/03/053407fy5kkzmdy44t9545.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-053157.jpg</strong> (251.94 KB, 下载次数: 0)

下载附件

2023-6-3 05:34 上传

<img src="https://img.saraba1st.com/forum/202306/03/053407b16fh6spz6y10cc1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-053208.jpg</strong> (423.12 KB, 下载次数: 0)

下载附件

2023-6-3 05:34 上传

<img src="https://img.saraba1st.com/forum/202306/03/053407hh23ze77znzngkdm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-053222.jpg</strong> (277.47 KB, 下载次数: 0)

下载附件

2023-6-3 05:34 上传

<img src="https://img.saraba1st.com/forum/202306/03/053407t8rjw9e87z2x8elf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-053237.jpg</strong> (324.17 KB, 下载次数: 0)

下载附件

2023-6-3 05:34 上传

<img src="https://img.saraba1st.com/forum/202306/03/053407do33m33qq3im7m59.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-053258.jpg</strong> (229.02 KB, 下载次数: 0)

下载附件

2023-6-3 05:34 上传

<img src="https://img.saraba1st.com/forum/202306/03/053408z5ywy5cs3kspyaa8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-053323.jpg</strong> (536.19 KB, 下载次数: 0)

下载附件

2023-6-3 05:34 上传

<img src="https://img.saraba1st.com/forum/202306/03/053408zbzcfybvrere511b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-053342.jpg</strong> (513.8 KB, 下载次数: 0)

下载附件

2023-6-3 05:34 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 479#       发表于 2023-6-3 19:45

Nous-Hermes-13b

说明参照下图

hugface模型权重:https://huggingface.co/NousResearch/Nous-Hermes-13b

<img src="https://img.saraba1st.com/forum/202306/03/194517flr5r8nq7qjzcprj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-194334.jpg</strong> (323.96 KB, 下载次数: 0)

下载附件

2023-6-3 19:45 上传

<img src="https://img.saraba1st.com/forum/202306/03/194521vjhs6hspojpo2u26.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-194413.jpg</strong> (169.94 KB, 下载次数: 0)

下载附件

2023-6-3 19:45 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 480#       发表于 2023-6-3 20:15

 本帖最后由 Machinery 于 2023-6-3 20:17 编辑 

BundleSDF

神经网络六自由度追踪与未知物体3D重建

相关论文:https://arxiv.org/abs/2303.14158

项目主页:https://bundlesdf.github.io/

代码:coming soon

BundleSDF是一种近乎实时的方法，用于从单眼RGBD视频序列中对未知物体进行六自由度跟踪，同时对物体与场景进行神经网络3D物体重建

本方法适用于任意刚性物体，即使在视觉纹理基本不存在的情况下也是如此，物体对象仅在第一帧中被分割，不需要其他信息，并且不需要对交互代理做出任何假设前提

方法的关键是神经对象场，它与姿势图优化过程同时学习，以便将信息稳健地积累到一致的3D表示中，同时捕获物体的几何和外观，自动维护一个动态的姿态内存帧池，以促进这些线程之间的通信

本方法可以处理具有较大姿势变化、部分和完全遮挡、无纹理表面和镜面高光的具有挑战性的输入序列

在HO3D、YCBInEOAT和BEHAVE数据集上展示了测试结果，表明本方法明显优于现有方法

演示视频:https://youtu.be/5PymzKbKv8w

<img src="https://img.saraba1st.com/forum/202306/03/200626mlmd38bmbodbgjmd.jpg" referrerpolicy="no-referrer">

<strong>pipeline_c.jpg</strong> (635.16 KB, 下载次数: 0)

下载附件

2023-6-3 20:06 上传

特征在连续的分割图像之间进行匹配，以获得粗略的姿态估计，其中一些姿态帧存储在内存池中，供以后使用和完善，姿态图是从内存池的子集动态创建的，在线优化结合当前姿势对图中的所有姿势进行细化，然后将这些更新的姿势存储回内存池中

最后，内存池中的所有姿势帧都用于学习神经对象场(在分离的线程中)，该对象场对对象的几何形状和视觉纹理进行建模，同时调整它们先前估计的姿势

<img src="https://img.saraba1st.com/forum/202306/03/201151uuddzbd3ref35cnr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-200931.jpg</strong> (248 KB, 下载次数: 0)

下载附件

2023-6-3 20:11 上传

<img src="https://img.saraba1st.com/forum/202306/03/201151ydjs0j1dmxz5vn77.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-200958.jpg</strong> (125.72 KB, 下载次数: 0)

下载附件

2023-6-3 20:11 上传

<img src="https://img.saraba1st.com/forum/202306/03/201151smx6zryie85kh1ek.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-201129.jpg</strong> (166.09 KB, 下载次数: 0)

下载附件

2023-6-3 20:11 上传

本方法不仅适用于更具挑战性的动态场景，也适用于通常考虑的静态场景(例如对准物体的移动相机镜头)与那些专门为静态场景设计的方法相比，依然取得了更好或相当的结果

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 481#       发表于 2023-6-3 20:43

qa-from-hf

通过人工反馈不断改进摘要式QA问答

相关论文:https://arxiv.org/abs/2305.12473

github项目地址:https://github.com/lil-lab/qa-from-hf

<img src="https://img.saraba1st.com/forum/202306/03/202837d83msf1t9rfj3s0j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-201823.jpg</strong> (120.87 KB, 下载次数: 0)

下载附件

2023-6-3 20:28 上传

研究通过人工用户反馈不断改进摘要式问题解答(QA)系统，设计和部署了一种迭代方法，其中寻求信息的用户提出问题，收到模型预测的答案，并提供反馈

进行了数以千计的用户交互实验，涵盖各种不同的设定，以加深对随着时间的推移模型从反馈中学习的理解，实验显示，随着时间的推移，用户反馈可以有效地改进摘要式QA模型，包括在不同的数据环境中，以及相关领域的自适应能力的巨大潜力

<img src="https://img.saraba1st.com/forum/202306/03/202832rdjajynsptltch0l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-202813.jpg</strong> (251.95 KB, 下载次数: 0)

下载附件

2023-6-3 20:28 上传

用户交互示例：每个示例都是组合的，用户问题、上下文段落、模型预测的答案和用户反馈

<img src="https://img.saraba1st.com/forum/202306/03/203735ea97us79x5xcuh32.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-203715.jpg</strong> (37.38 KB, 下载次数: 0)

下载附件

2023-6-3 20:37 上传

将用户反馈映射到特定操作以进行奖励价值设定，[i, j] 是交互过程中预测的跨度，假设已经进行了一次预测

<img src="https://img.saraba1st.com/forum/202306/03/204148ymc0emessbey48b0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-204131.jpg</strong> (191.81 KB, 下载次数: 0)

下载附件

2023-6-3 20:41 上传

每轮交互数据训练后交互数据和模型在测试集上的表现统计，测试集是九轮中收集的所有测试数据的联合

<img src="https://img.saraba1st.com/forum/202306/03/204254xioq0i2iinmnntit.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-204219.jpg</strong> (53.09 KB, 下载次数: 0)

下载附件

2023-6-3 20:42 上传

第一轮到第九轮中模型的综合改进

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 482#       发表于 2023-6-3 21:10

 本帖最后由 Machinery 于 2023-6-3 21:12 编辑 

adaptive-retrieval

使用自适应检索与LLM(大型语言模型)的参数知识相辅相成

github项目实现库:https://github.com/AlexTMallen/adaptive-retrieval

LLM在参数中存储了大量的事实知识(parametric factual knowledge)，但最近的研究表明LLM很难学习频率较低的事实，并且在他们不知道的时候经常会产生幻觉

为了回答这些问题，构建了一个新的大型开放域QA数据集PopQA，问题以维基数据为基础，并从相关的长尾流行度分布中采样，以实现细粒度分析，然后以零/少样本的方式测试10个不同种类的LLM

<img src="https://img.saraba1st.com/forum/202306/03/210043gpfbvgpsql0bpsse.jpg" referrerpolicy="no-referrer">

<strong>20230603_205922.jpg</strong> (137.15 KB, 下载次数: 0)

下载附件

2023-6-3 21:00 上传

<img src="https://img.saraba1st.com/forum/202306/03/210043cz6ii6zqz6b1p96c.jpg" referrerpolicy="no-referrer">

<strong>20230603_205924.jpg</strong> (83.6 KB, 下载次数: 0)

下载附件

2023-6-3 21:00 上传

实验发现问题主题实体的流行度和准确性之间存在很强的相关性，这表明LLM会记住流行的事实知识，而不会记住不那么流行的事实知识

<img src="https://img.saraba1st.com/forum/202306/03/210216pvlyzvxeb4ybjwvf.jpg" referrerpolicy="no-referrer">

<strong>20230603_210028.jpg</strong> (72.89 KB, 下载次数: 0)

下载附件

2023-6-3 21:02 上传

在长尾分布中，缩放LLM的规模可能没有我们认为的那么有用：GPT3-003达芬奇版本在不太受欢迎的实体问题上的表现几乎与GPT-Neo 2B一样差，先验知识学习分析经常使用NQ/TriviaQA，这可能会导致夸大缩放规模的有效性

<img src="https://img.saraba1st.com/forum/202306/03/210303mazgmifmgal3kui1.jpg" referrerpolicy="no-referrer">

<strong>20230603_210258.jpg</strong> (85.09 KB, 下载次数: 0)

下载附件

2023-6-3 21:03 上传

使用非参数记忆(检索到的文本块辅助)增强LM在很大程度上有帮助，GPT-Neo 1.3B版本在检索到的上下文的协助下优于普通GPT3-003达芬奇版本，即使对于GPT-3，检索也能提高10%的准确率

<img src="https://img.saraba1st.com/forum/202306/03/210531avnjnvva8lqrbfbj.jpg" referrerpolicy="no-referrer">

<strong>20230603_210512.jpg</strong> (67.83 KB, 下载次数: 0)

下载附件

2023-6-3 21:05 上传

检索增强的LM(红线和绿线)特别有助于解决LM不太受欢迎的实体的问题，相反，由于可能的检索错误，较大的模型(例如 GPT-3)在众所周知的事实中的表现优于一般的检索增强模型

综上所述，LLM现在确实记忆了很多知识，但还不足以完全取代非参数记忆，尤其是长尾分布的相关领域知识，为此，提出了一种简单而有效的自适应检索提升策略

自适应检索根据主题流行度和关系类型决定何时不进行检索，这种方法不仅提高了性能(高达 5%)，而且远远减少了推理时间延迟和API成本(例如，将 GPT-3 API的成本减半)

<img src="https://img.saraba1st.com/forum/202306/03/211040jyjbiy8yyj4co4dw.png" referrerpolicy="no-referrer">

<strong>teaser_llm.png</strong> (54.47 KB, 下载次数: 0)

下载附件

2023-6-3 21:10 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 483#       发表于 2023-6-3 22:14

TextDiffuser

犹如文本画家的扩散模型

项目主页:https://jingyechen.github.io/textdiffuser/

github项目仓库:https://github.com/microsoft/unilm/tree/master/textdiffuser

演示Demo:https://huggingface.co/spaces/microsoft/TextDiffuser

数据集:coming soon
扩散模型目前在渲染准确和连贯的文本方面遇到了困难，为了解决这个问题，提出了TextDiffuser，专注于生成具有与背景一致的具有视觉吸引力的文本的图像

TextDiffuser包括两阶段：首先，Transformer模型生成从文本提示中提取的关键字布局，然后扩散模型生成以文本提示和生成的布局为条件的图像

此外，研究组还贡献了第一个带有OCR注释的大型文本图像数据集MARIO-10M，其中包含1000万个具有文本识别、检测和字符级分割注释的图像-文本对

进一步制作了MARIO-Eval基准，作为评估文本渲染质量的综合工具，通过实验和用户研究，表明TextDiffuser可以灵活可控地单独使用文本提示或与文本模板图像一起使用来创建高质量的文本图像，并进行文本修复以用文本重建不完整的图像

<img src="https://img.saraba1st.com/forum/202306/03/220417zrpl8v9dsrs7drl2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-220342.jpg</strong> (206.1 KB, 下载次数: 0)

下载附件

2023-6-3 22:04 上传

TextDiffuser从文本提示或与模板图像一起生成准确且连贯的文本图像，并进行文本修复以重建不完整的图像

<img src="https://img.saraba1st.com/forum/202306/03/220612s6ii81dwk07rb76c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-220540.jpg</strong> (137.3 KB, 下载次数: 0)

下载附件

2023-6-3 22:06 上传

TextDiffuser两阶段框架

在布局生成的第一个阶段，基于Transformer的编码器-解码器模型生成字符级分割掩码，指示来自文本提示的图像中关键字的布局

在第二个图像生成阶段，扩散模型生成以噪声特征、分割掩码、特征掩码和掩码特征(从左到右)为条件的图像以及文本提示

特征掩码可以覆盖整个或部分图像，对应于整体图像和部分图像的生成。 扩散模型通过去噪和字符感知损失逐步学习去噪特征，实例中的扩散模型在潜在空间中运行，但我们在示范中使用图像像素以获得更好的可视化效果

<img src="https://img.saraba1st.com/forum/202306/03/221101wpg77ytd6iv7ir6v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-220946.jpg</strong> (164.28 KB, 下载次数: 0)

下载附件

2023-6-3 22:11 上传

MARIO-10M数据集中的图像

<img src="https://img.saraba1st.com/forum/202306/03/221122irwnn00d63n0undn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-221008.jpg</strong> (205.91 KB, 下载次数: 0)

下载附件

2023-6-3 22:11 上传

<img src="https://img.saraba1st.com/forum/202306/03/221122ri3hylpe6i2peeyy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-221018.jpg</strong> (368.95 KB, 下载次数: 0)

下载附件

2023-6-3 22:11 上传

<img src="https://img.saraba1st.com/forum/202306/03/221122jzkvt393w2g7zxgu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-221043.jpg</strong> (317.96 KB, 下载次数: 0)

下载附件

2023-6-3 22:11 上传

相关评估

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 484#       发表于 2023-6-3 22:33

self-guidance

自指导扩散的可控图像生成

项目主页:https://dave.ml/selfguidance/

<img src="https://img.saraba1st.com/forum/202306/03/222200z5glxvzl6mpm622m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-222146.jpg</strong> (264.84 KB, 下载次数: 0)

下载附件

2023-6-3 22:22 上传

<img src="https://img.saraba1st.com/forum/202306/03/222200ey1yy11vsyjmcyc7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-221959.jpg</strong> (249.14 KB, 下载次数: 0)

下载附件

2023-6-3 22:22 上传

自指导是一种可控图像生成方法，它仅使用预训练扩散模型的注意力和激活来指导采样

无需任何额外的模型或训练，就可以移动对象或调整对象大小，甚至用真实图像中的项目替换它们，而无需更改场景的其余部分，您还可以借用其他图像的外观或将场景重新排列成所需的布局

图像的许多方面很难或者不可能通过文本传达，自指导，这是一种通过指导扩散模型的内部表示来更好地控制生成图像的方法，可以从这些表示中提取对象的形状、位置和外观等属性，并用于控制采样生成

<img src="https://img.saraba1st.com/forum/202306/03/222344rfspthhj3etrj3jh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-222330.jpg</strong> (133.53 KB, 下载次数: 0)

下载附件

2023-6-3 22:23 上传

自指导的工作方式类似于分类器指导，但使用预训练模型本身中存在的信号，不需要额外的模型或训练

<img src="https://img.saraba1st.com/forum/202306/03/222444nf89otrp9sho56pp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-222434.jpg</strong> (394.58 KB, 下载次数: 0)

下载附件

2023-6-3 22:24 上传

使用自指导仅更改对象的一个属性，我们可以移动或调整该对象的大小而无需修改图像的其余部分

<img src="https://img.saraba1st.com/forum/202306/03/222548qcwwbwc7gzl5g4xy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-222536.jpg</strong> (167.01 KB, 下载次数: 0)

下载附件

2023-6-3 22:25 上传

引导生成对象的外观与真实图像中的对象相匹配，可以创建描绘现实生活中对象的场景，类似DreamBooth，但无需任何微调，仅使用一张图像

<img src="https://img.saraba1st.com/forum/202306/03/222656dm7coulhzmd8mc1z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-222636.jpg</strong> (184.88 KB, 下载次数: 0)

下载附件

2023-6-3 22:26 上传

还可以对真实图像中的对象进行空间操作

<img src="https://img.saraba1st.com/forum/202306/03/222808evvzcckuv4yqt3fc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-222757.jpg</strong> (235.92 KB, 下载次数: 0)

下载附件

2023-6-3 22:28 上传

通过引导对象形状重建图像布局，可以为给定场景采样新外观

<img src="https://img.saraba1st.com/forum/202306/03/222853njybn1oivff7e8ff.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-222844.jpg</strong> (177.36 KB, 下载次数: 0)

下载附件

2023-6-3 22:28 上传

通过引导样本从一个图像中获取对象形状并从另一个图像中获取对象外观，可以将图像重新排列到其他场景的布局中，还可以仅通过引导外观来对场景的新布局进行采样

<img src="https://img.saraba1st.com/forum/202306/03/222950oggzl56666fo9he9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-222939.jpg</strong> (374.26 KB, 下载次数: 0)

下载附件

2023-6-3 22:29 上传

可以通过拼贴来自不同图像的单个图像目标对象来创建新场景，例如，如果由于这些图像的布局不兼容，对象无法在其原始位置组合，可以仅借用它们的外观，并使用新图像指定布局以生成合成

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 485#       发表于 2023-6-3 22:44

StableRep

来自文本到图像模型的合成图像使模型成为更强大的视觉表示学习者

相关论文:https://arxiv.org/abs/2306.00984

研究了使用由文本到图像模型生成的合成图像来学习视觉表示的潜力，鉴于此类模型在生成高质量图像方面的出色性能，这是一个自然而然的问题

特别考虑了Stable Diffusion，它是领先的开源文本到图像模型之一，实验表明

1.当生成模型配置了适当的无分类器指导尺度时，在合成图像上训练自监督方法可以匹配或击败真实图像对应的性能

2.将同一文本提示生成的多个图像视为彼此的positive(正面)样本，开发了一种多正面样本对比学习方法，称之为StableRep

3.仅使用合成图像，在大规模数据集上，StableRep 学习的表示超过了SimCLR和CLIP使用同一组文本提示和相应的真实图像学习的表示的性能

4.当我们进一步添加语言监督时，使用 20M 合成图像训练的StableRep比使用 50M 真实图像训练的 CLIP 获得更好的准确性

<img src="https://img.saraba1st.com/forum/202306/03/223935z2rv7hihb2hwhl22.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-223918.jpg</strong> (93.7 KB, 下载次数: 0)

下载附件

2023-6-3 22:39 上传

传统学习方法与合成数据学习方法

<img src="https://img.saraba1st.com/forum/202306/03/224313bhj7dp42a64s4pcj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-224010.jpg</strong> (44.53 KB, 下载次数: 0)

下载附件

2023-6-3 22:43 上传

<img src="https://img.saraba1st.com/forum/202306/03/224313vs5gwysqmigclg5g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-224154.jpg</strong> (270.45 KB, 下载次数: 0)

下载附件

2023-6-3 22:43 上传

<img src="https://img.saraba1st.com/forum/202306/03/224313kh99z3mvcemcj9vo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-224219.jpg</strong> (126.06 KB, 下载次数: 0)

下载附件

2023-6-3 22:43 上传

<img src="https://img.saraba1st.com/forum/202306/03/224313p7ivi7h7vsuv777h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-224232.jpg</strong> (37.51 KB, 下载次数: 0)

下载附件

2023-6-3 22:43 上传

<img src="https://img.saraba1st.com/forum/202306/03/224313fz11gz73dj3xgfzf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-224245.jpg</strong> (223.78 KB, 下载次数: 0)

下载附件

2023-6-3 22:43 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 486#       发表于 2023-6-3 23:39

CelebBasis

通过Celeb Basis在扩散模型中输入任何人类概念

项目空间:https://celeb-basis.github.io/

github代码仓库:https://github.com/ygtxr1997/CelebBasis

只使用一张面部照片，只有1024个可学习参数，只需要3分钟微调，兼容Textural-Inversion，生成与其他新(人)概念互动的图片

<img src="https://img.saraba1st.com/forum/202306/03/232857wesaw7rrz9mazmsz.jpg" referrerpolicy="no-referrer">

<strong>d495d213-da62-490a-82b1-f8162aaea766.jpg</strong> (185.1 KB, 下载次数: 0)

下载附件

2023-6-3 23:28 上传

<img src="https://img.saraba1st.com/forum/202306/03/232857e9js9lc0iik70thh.jpg" referrerpolicy="no-referrer">

<strong>1bf7dc26-df16-4f97-9cb2-e0e320b0e946.jpg</strong> (94.81 KB, 下载次数: 0)

下载附件

2023-6-3 23:28 上传

文本嵌入空间具有一些很好的插值特性，启发了研究组为人类生成一个定义空间的想法

<img src="https://img.saraba1st.com/forum/202306/03/232905ka37ap662uef2fec.jpg" referrerpolicy="no-referrer">

<strong>a387df3b-7b02-4fd8-853d-64b5f4f30f11.jpg</strong> (79.51 KB, 下载次数: 0)

下载附件

2023-6-3 23:29 上传

首先，收集了大约1500个名人名字作为初始集合，然后根据文本到图像扩散模型(stable-diffusion)的合成质量和相应的名称提示，手动将初始名称过滤为691个，之后每个过滤的名称被标记化并编码为celeb embedding group，最后进行主成分分析来构建一个紧致的向量正交基(compact orthogonal basis)

<img src="https://img.saraba1st.com/forum/202306/03/233503zq81vna22ngnncva.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-233454.jpg</strong> (65.85 KB, 下载次数: 0)

下载附件

2023-6-3 23:35 上传

在训练期间(左方)，优化Celeb Basis的系数帮助修复面部编码，在推理过程中(右方)，结合学习到的个性化权重和共享名人基础来生成具有输入身份的图像

<img src="https://img.saraba1st.com/forum/202306/03/233902mffws1zspw04mpqw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-233709.jpg</strong> (678.29 KB, 下载次数: 0)

下载附件

2023-6-3 23:39 上传

<img src="https://img.saraba1st.com/forum/202306/03/233902sgm1lm025kmq5g2k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-233739.jpg</strong> (477.72 KB, 下载次数: 0)

下载附件

2023-6-3 23:39 上传

<img src="https://img.saraba1st.com/forum/202306/03/233902k338oh5be3ncz81j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-233841.jpg</strong> (496.96 KB, 下载次数: 0)

下载附件

2023-6-3 23:39 上传

注:

<img src="https://img.saraba1st.com/forum/202306/03/234051uxp9sdp7sxevfd4z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-234038.jpg</strong> (65.42 KB, 下载次数: 0)

下载附件

2023-6-3 23:40 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 487#       发表于 2023-6-4 00:01

Bytes Are All You Need

使用Transformers直接读取文件字节

相关论文:https://arxiv.org/abs/2306.00238

github项目仓库:https://github.com/apple/ml-cvnets/tree/main/examples/byteformer

现代深度学习方法通常会将输入转换为特定模式的形式，例如，图像分类的最常见深度学习方法是将图像文件字节解码为RGB张量，然后传入神经网络

相反，在本文中研究了直接在推理时对文件字节进行分类，而无需解码文件，使用文件字节作为模型输入可以开发可以操作多种输入模式的模型

ByteFormer在直接对TIFF文件字节训练和测试时，在ImageNet上达到77.33%的Top-1分类准确率，其性能与DeiT-Ti相似(在RGB图像上运行时准确率为72.2%)

ByteFormer无需修改或调整超参数即可达到95.42%的Speech Commands v2数据集中的WAV文件分类准确率(相比之下,最新技术的准确率为98.7%)

此外，我们证明了ByteFormer在隐私保护推理中有应用，ByteFormer能够对某些混淆输入表示进行推理，而不损失准确性，ByteFormer能够对一台假想的隐私保护相机进行推理，该相机通过持续遮蔽90%的像素通道来避免形成完整图像，同时仍达到71.35%的ImageNet准确率(注:本论文来自Apple)

<img src="https://img.saraba1st.com/forum/202306/03/235023jwgoiwiwzxxziyqi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-234957.jpg</strong> (195.81 KB, 下载次数: 0)

下载附件

2023-6-3 23:50 上传

ByteFormer(BF)与DeiT

A.从硬盘中读取文件并使用标准图像编码方法将文件转换为RGB张量，之后从RGB表示中创建patch embedding再进行分词表示
B.直接将文件比特转换为分词表示
C.类似B，但使用混淆函数ϕ.
D.使用自定义相机代表捕获的隐私保护表示，并从该表示中执行embedding

<img src="https://img.saraba1st.com/forum/202306/04/000100ar9f3r633w4z85hw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230603-235949.jpg</strong> (96.04 KB, 下载次数: 0)

下载附件

2023-6-4 00:01 上传

<img src="https://img.saraba1st.com/forum/202306/04/000100i5aeibsltzi4ipu5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230604-000033.jpg</strong> (137.93 KB, 下载次数: 0)

下载附件

2023-6-4 00:01 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 488#       发表于 2023-6-5 03:56

InternLM

能力逐步增强的多语言模型

github技术报告:https://github.com/InternLM/InternLM-techreport

InternLM是上海人工智能实验室与商汤科技（同等贡献）联合香港中文大学、复旦大学、上海交通大学合作开发的多语种大语言模型

InternLM具有104B参数，在1.6T Token的大型语料库上经过多阶段渐进过程进行预训练，然后进行微调以符合人类偏好

还开发了名为Uniscale-LLM的训练系统，用于高效的大型语言模型训练，对多项基准的评估表明，InternLM在知识理解、阅读理解、数学和编码等多个方面都达到了最先进的性能

凭借如此全面的能力，InternLM在不借助外部工具的情况下，于MMLU、AGIEval、C-Eval和GAOKAO-Bench在内的综合多基准测试中取得了出色的成绩

在这些基准测试中，InternLM 不仅明显优于开源模型，与ChatGPT相比也获得了更优越的性能，此外，InternLM展示了对中文语言和中国文化的出色理解能力，这使其成为支持面向中文的语言应用程序的合适基础模型，技术报告对结果进行了详细研究，并提供了跨越不同知识领域和任务的基准和示例

预训练语料数据集分布

<img src="https://img.saraba1st.com/forum/202306/05/034445psp1vpdvyrfvcky1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-034417.jpg</strong> (54.43 KB, 下载次数: 0)

下载附件

2023-6-5 03:44 上传

InternLM 的开发包括三个主要阶段，即数据集准备、模型预训练和对齐，具体来说，数据准备阶段是构建大规模高质量的语料库； 预训练阶段是基于上述语料库训练基础语言模型； 最后对齐阶段是对齐模型，使其能够可靠地遵循人类指令并产生有用和安全的答案

值得注意的是，InternLM在预训练中引入了多阶段方法，修改了训练数据的组合以及训练超参数的配置，从而有效地引导模型能力的增长朝着期望方向发展

在训练过程中，将整个过程分为多个阶段，每个阶段都有其优化目标，通过控制不同比例的数据来定义，选择合适的数据集来评估实现这些目标的进展如果某个特定阶段的表现不符合预期，可以从该阶段结束的地方继续训练，无需重新开始，从而提高训练效率

为了确保有效的数据利用，在调整数据比例时需要确保不会对相同的数据进行重采样，此外，为了进一步提高训练效率，将不同长度的句子变成固定长度的序列，使用特殊符号来划定不同的句子

全文最有价值的三段<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202306/05/035023fbatlnxl8lll2lyl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-035011.jpg</strong> (744.62 KB, 下载次数: 0)

下载附件

2023-6-5 03:50 上传

相关评估结果

<img src="https://img.saraba1st.com/forum/202306/05/035237v7zmuy461o4osjyo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-035101.jpg</strong> (106.28 KB, 下载次数: 0)

下载附件

2023-6-5 03:52 上传

<img src="https://img.saraba1st.com/forum/202306/05/035236qzlogbtclzjpkpka.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-035111.jpg</strong> (60.3 KB, 下载次数: 0)

下载附件

2023-6-5 03:52 上传

<img src="https://img.saraba1st.com/forum/202306/05/035237ayymfpzpae11ll6i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-035125.jpg</strong> (190.68 KB, 下载次数: 0)

下载附件

2023-6-5 03:52 上传

<img src="https://img.saraba1st.com/forum/202306/05/035237h99sz9h4h1ihoy3m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-035134.jpg</strong> (232.96 KB, 下载次数: 0)

下载附件

2023-6-5 03:52 上传

<img src="https://img.saraba1st.com/forum/202306/05/035237osrsjdscd6dxz23c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-035204.jpg</strong> (133.48 KB, 下载次数: 0)

下载附件

2023-6-5 03:52 上传

<img src="https://img.saraba1st.com/forum/202306/05/035237tehgeeuxfxkbhg8t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-035214.jpg</strong> (625.99 KB, 下载次数: 0)

下载附件

2023-6-5 03:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 489#       发表于 2023-6-5 04:53

ChartT5

通过在图表和表格数据对上进行跨模态预训练增强视觉和语言任务中的图表理解

构建跨模态的人工智能，使其能理解图表并表达隐藏在图表背后的关键信息，是视觉语言社区一个有吸引力的挑战

观察图表中的表格数据的能力是自动理解图表的关键，提出了ChartT5，一个视觉语言模型，它可以通过在图表和表格数据对上进行跨模态预训练来学习如何解释图表中的表格信息

具体来说，提出了两个新的预训练目标:Masked Header Prediction (MHP)和Masked Value Prediction (MVP)，以帮助模型具备不同的技能来解释表格信息

进行了广泛的实验来验证所提出的预训练策略的有效性，包括图表问答和图表摘要生成，特别是在ChartQA基准测试上，ChartT5对比最先进的非预训练方法有8%的性能提升

<img src="https://img.saraba1st.com/forum/202306/05/043657e6xyt65nt5ny1czz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-043644.jpg</strong> (37.78 KB, 下载次数: 0)

下载附件

2023-6-5 04:36 上传

ChartQA数据集的样本，正确的图表在右上角

<img src="https://img.saraba1st.com/forum/202306/05/043850u7ng5q880m3mtznj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-043823.jpg</strong> (76.31 KB, 下载次数: 0)

下载附件

2023-6-5 04:38 上传

ChartT5框架，给定输入图表图像和提取的OCR Token，ChartT5预测输出中表的MASK值

<img src="https://img.saraba1st.com/forum/202306/05/044052dft8frx4bu88f7rs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-044028.jpg</strong> (87.92 KB, 下载次数: 0)

下载附件

2023-6-5 04:40 上传

预训练语料库中不同数量的条形图、折线图和饼图三种图表类型的分布

研究组从现有的图表问答语料库中收集合成数据，预训练语料库包含495K个图表表对，涵盖了多种图表类型

给定图表表对，提出MaskedHeader预测和Masked Value预测使模型使用图表信息恢复不完整的表格，具体来说，这个目标旨在用剩余的表格信息以及图表图像区域和场景文本来预测一个被屏蔽的表格标记

与应用于自然语言文本的传统屏蔽语言建模相比，我们基于两个假设调整了表屏蔽策略：

1.我们只屏蔽表头或数字表值之一，解释这两种类型的信息可能需要不同的技能，预测表格标题需要检索正确的场景文本，而预测数值表格值更多地取决于对视觉元素和场景文本进行数学推理的能力，因此，最好将它们格式化为两个单独的预训练目标

2.将屏蔽率从15%增加到45%，因为屏蔽表标记对周围表值将使模型预测文本的依赖性较小

<img src="https://img.saraba1st.com/forum/202306/05/045159jym5xjkywzuvkvw8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-045056.jpg</strong> (115.02 KB, 下载次数: 0)

下载附件

2023-6-5 04:51 上传

<img src="https://img.saraba1st.com/forum/202306/05/045159sz42r2wkwh2uo32r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-045121.jpg</strong> (90.02 KB, 下载次数: 0)

下载附件

2023-6-5 04:51 上传

消融实验

<img src="https://img.saraba1st.com/forum/202306/05/045316augzwazhkmhwwuw5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-045253.jpg</strong> (78.17 KB, 下载次数: 0)

下载附件

2023-6-5 04:53 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 490#       发表于 2023-6-5 06:08

LaVIN

便宜又快速：大型语言模型的高效视觉语言指令调优

github项目代码库:https://github.com/luogen1996/LaVIN

一种新的、实用的用于视觉语言指令调优(vision-language instruction tuning)的方法，称为混合模态自适应(Mixture-of-Modality Adaptation/MMA)

MMA是一个端到端的优化机制,它通过轻量级的适配器(lightweight adapters)将图像编码器(image encoder)和语言模型(LLM)连接起来

同时提出了一种新的路由算法(routing algorithm)，可以帮助模型根据单模态指令或多模态指令自动切换推理路径(reasoning paths)

基于MMA，开发了一个大型的视觉语言指令模型LaVIN，在各种指令遵循任务中，LaVIN显示出比现有的多模态语言模型更高的训练效率和更强的推理能力

<img src="https://img.saraba1st.com/forum/202306/05/060655hemzi7xieemciwgm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-051109.jpg</strong> (189.24 KB, 下载次数: 0)

下载附件

2023-6-5 06:06 上传

MMA微调的LaVIN构架

<img src="https://img.saraba1st.com/forum/202306/05/060700yohjwfleiew3ole9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-051203.jpg</strong> (76.71 KB, 下载次数: 0)

下载附件

2023-6-5 06:07 上传

<img src="https://img.saraba1st.com/forum/202306/05/060706t9rgzxrv0gmzvkfk.gif" referrerpolicy="no-referrer">

<strong>demo.gif</strong> (272.35 KB, 下载次数: 0)

下载附件

2023-6-5 06:07 上传

<img src="https://img.saraba1st.com/forum/202306/05/060904wffcz3efehamb6e6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-060847.jpg</strong> (209.36 KB, 下载次数: 0)

下载附件

2023-6-5 06:09 上传

<img src="https://img.saraba1st.com/forum/202306/05/060821b8rr037ztrk65krk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-060808.jpg</strong> (137.41 KB, 下载次数: 0)

下载附件

2023-6-5 06:08 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 491#       发表于 2023-6-5 19:24

CRATE

通过稀疏率降低的白盒Transformer架构

github实现仓库:https://github.com/Ma-Lab-Berkeley/CRATE

相关论文:https://arxiv.org/abs/2306.01129

相关文章:https://hub.baai.ac.cn/view/27317

<img src="https://img.saraba1st.com/forum/202306/05/191717etvuuv1ulvoza1av.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-191557.jpg</strong> (677.58 KB, 下载次数: 0)

下载附件

2023-6-5 19:17 上传

因此可设计一类白盒的类transformer深度网络体系架构，同时在数学上是完全可解释的

实验显示这些网络确实学习优化设计的目标:它们压缩和稀疏化大规模真实世界视觉数据集，如ImageNet的表示，并达到非常接近完全设计的transformer如ViT的性能，在ImageNet这样的大数据集上表现也很接近Sota的transformer ViT，构架如下图所示

<img src="https://img.saraba1st.com/forum/202306/05/192053hoel4vgol8pooble.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-192040.jpg</strong> (531.37 KB, 下载次数: 0)

下载附件

2023-6-5 19:20 上传

<img src="https://img.saraba1st.com/forum/202306/05/192309jhotcs7ocvavsa73.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-192215.jpg</strong> (104.72 KB, 下载次数: 0)

下载附件

2023-6-5 19:23 上传

在ImageNet上进行预训练时，CRATE在具有不同模型规模的各种数据集上的Top 1精度，对于ImageNet/ImageNetRealL，直接评估 top-1 精度，对于其他数据集，使用在ImageNet上预训练的模型作为初始化，并通过微调评估迁移学习性能

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 492#       发表于 2023-6-5 20:05

FineGrainedRLHF

良好细粒度的人类反馈为语言模型训练提供了更好的奖励回报

项目主页:https://finegrainedrlhf.github.io/

github代码仓库:https://github.com/allenai/FineGrainedRLHF

<img src="https://img.saraba1st.com/forum/202306/05/195401r49we4zjhojtdojw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-195324.jpg</strong> (208.53 KB, 下载次数: 0)

下载附件

2023-6-5 19:54 上传

本文提出了Fine-Grained RLHF，这是一个框架，可以从两个方面对具有细粒度的奖励函数进行训练和学习

RLHF之前的工作重点是收集人类对语言模型(LM)输出整体质量的偏好，然而，这种类型的整体反馈提供的信息有限，通过引入细粒度的人类反馈(例如，哪个子句不相关，哪个句子不真实，哪个句子有毒)作为显式的训练信号

奖励在两个方面是细粒度的：
(a) 密度：在生成每个片段（例如，一个句子）后提供奖励，类似于OpenAI 的“逐步过程奖励”，这种方法比整体反馈提供更多信息，因此对RL(强化学习)更有效

(b) 与不同反馈类型相关的多个奖励模型：采用多个奖励模型来捕获不同类型的反馈(例如，事实不准确、不相关和信息不完整)，有趣的是，观察到这些奖励模型既相互补充又相互竞争，通过调整奖励模型的权重，我们可以控制不同类型反馈之间的平衡，并根据特定需求为不同的任务定制模型，例如，一些用户可能更喜欢简短明了的输出，而另一些用户可能会寻求更长更详细的响应。

<img src="https://img.saraba1st.com/forum/202306/05/200253sfatyj6fwfq6sx7f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230605-200243.jpg</strong> (89.23 KB, 下载次数: 0)

下载附件

2023-6-5 20:02 上传

代码库即将开源，数据集与论文已经开放

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 493#       发表于 2023-6-6 01:39

 本帖最后由 Machinery 于 2023-6-6 01:40 编辑 

MetaVL

将上下文学习能力从语言模型转移到视觉语言模型

相关论文:https://arxiv.org/abs/2306.01311

大型语言模型已经显示出通过对一些演示进行调节的上下文学习能力来适应新任务的能力，然而，在视觉语言领域，大多数大规模预训练视觉语言(VL)模型不具备进行上下文学习的能力

如何为VL模型启用上下文学习？在本文中研究了一个有趣的假设：我们能否将上下文学习能力从语言领域迁移到 VL 领域？

具体来说，首先对语言模型进行元训练，以对NLP任务执行上下文学习(如MetaICL)；然后通过附加一个视觉编码器来转移这个模型来执行VL任务

实验表明，情境学习能力确实可以跨模态转移：本文的模型远远提高了VL任务的上下文学习能力，甚至可以显着补偿模型的大小造成的性能损失

在 0VQA、OK-VQA和GQA基准上，本文方法可以在胜过基线模型的同时参数减少20倍

<img src="https://img.saraba1st.com/forum/202306/06/012513fenvn43wb7vlzybg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-012433.jpg</strong> (115.55 KB, 下载次数: 0)

下载附件

2023-6-6 01:25 上传

MetaVL的训练步骤，包括训练元语言编码器-解码器和将视觉特征映射到语言嵌入空间，同时保持元训练语言编码器-解码器冻结

为了对自回归语言模型进行元训练，在每次迭代中，从不同元训练语言任务的集合中随机选择一个元学习任务，并从其训练拆分中随机抽取k+1个数据标签示例。 然后，模型由 (x1, y1, x2, y2, ..., xk+1) 的串联进行监督，它将作为单个输入馈送到模型，用于预测标签 (yk+1) 作为训练目标，即元训练步骤旨在最大化：P(yk+1|x1, y1, · · ·, xk, yk, xk+1)

在推理过程中，相同的上下文设置(来自训练的 k 个示例)从目标数据集中采样，用作 (x1, y1)(x2, y2) ···,(xk, yk)(x) 和给模型预测标签y

在不同的自然语言数据集上训练的元训练语言模型在上下文中给出的数据很少的情况下能对未见任务表现出良好的性能

<img src="https://img.saraba1st.com/forum/202306/06/013541p8886a468al985g9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-013440.jpg</strong> (196.41 KB, 下载次数: 0)

下载附件

2023-6-6 01:35 上传

<img src="https://img.saraba1st.com/forum/202306/06/013542pi5dzjqim9sszst1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-013501.jpg</strong> (275.67 KB, 下载次数: 0)

下载附件

2023-6-6 01:35 上传

相关实例与评估成绩

<img src="https://img.saraba1st.com/forum/202306/06/013755qjidomeij8o4x9zw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-013642.jpg</strong> (422.61 KB, 下载次数: 0)

下载附件

2023-6-6 01:37 上传

<img src="https://img.saraba1st.com/forum/202306/06/013755knr7pk8q7pkntqrk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-013650.jpg</strong> (491.01 KB, 下载次数: 0)

下载附件

2023-6-6 01:37 上传

<img src="https://img.saraba1st.com/forum/202306/06/013755vxg6xx63rv3776vv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-013709.jpg</strong> (442.2 KB, 下载次数: 0)

下载附件

2023-6-6 01:37 上传

<img src="https://img.saraba1st.com/forum/202306/06/013755f43m5t6vvb7vb8cv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-013726.jpg</strong> (191.07 KB, 下载次数: 0)

下载附件

2023-6-6 01:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 494#       发表于 2023-6-6 01:49

 本帖最后由 Machinery 于 2023-6-6 01:50 编辑 

potat1-text-to-video-synthesis-colab

开源文本到视频生成，可以使用的Colab演示，1024x576分辨率生成模型Potat1，模型基座为国内modelscope社区modelscope-damo-text-to-video-synthesis

github项目仓库:https://github.com/camenduru/text-to-video-synthesis-colab

potat1的hugface仓库:https://huggingface.co/camenduru/potat1

<img src="https://img.saraba1st.com/forum/202306/06/014549ergirr3jyrr44cyn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-014501.jpg</strong> (394.45 KB, 下载次数: 0)

下载附件

2023-6-6 01:45 上传

<img src="https://img.saraba1st.com/forum/202306/06/014949cgw2029cwjc22q52.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-014933.jpg</strong> (245.99 KB, 下载次数: 0)

下载附件

2023-6-6 01:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 495#       发表于 2023-6-6 02:11

 本帖最后由 Machinery 于 2023-6-6 02:13 编辑 

SAM-HQ

以高质量分割任何内容

相关论文:https://arxiv.org/abs/2306.01567

github项目仓库:https://github.com/SysCV/SAM-HQ

最近的Segment Anything Model (SAM)代表了扩展分割模型的一大飞跃，可以进行强大的零样本性能和灵活的提示预测分割图片

尽管接受了11亿个分割掩码的训练，但SAM的掩码预测质量在许多情况下仍不尽如人意，尤其是在处理具有复杂结构的物体时

本文提出了HQ-SAM，使SAM具备准确分割任何对象的能力，同时保持SAM原有的提示设计、效率和零样本泛化能力

研究组精心设计重用并保留了SAM的预训练模型权重，同时只引入了最少的额外参数和计算，设计了一个可学习的高质量输出令牌，它被注入到SAM的掩码解码器中，并负责预测高质量掩码

同时并不是仅将其应用于掩码解码器功能，而是首先将它们与早期和最终ViT功能融合以改进掩码细节

为了训练引入的可学习参数，从多个来源组成构造了一个44K细粒度掩码数据集，HQ-SAM在引入的44K掩码数据集上进行训练，在8个GPU上训练了4小时

在9个不同的分割数据集上展示了HQ-SAM的有效性，这9个数据集涵盖了不同的下游任务，其中7个数据集是在零样本转移下进行评估的

相关构架与评估

<img src="https://img.saraba1st.com/forum/202306/06/021054qiuiraz9c5haquu1.png" referrerpolicy="no-referrer">

<strong>sam-hf-framework.png</strong> (719.49 KB, 下载次数: 0)

下载附件

2023-6-6 02:10 上传

<img src="https://img.saraba1st.com/forum/202306/06/021054rs27g2z777ncduji.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-020653.jpg</strong> (339.75 KB, 下载次数: 0)

下载附件

2023-6-6 02:10 上传

实际使用

<img src="https://img.saraba1st.com/forum/202306/06/021123ipbpemviebmllbb7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-020536.jpg</strong> (555.79 KB, 下载次数: 0)

下载附件

2023-6-6 02:11 上传

<img src="https://img.saraba1st.com/forum/202306/06/021123cvvytxvlqhxvcqv9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-020603.jpg</strong> (569.05 KB, 下载次数: 0)

下载附件

2023-6-6 02:11 上传

<img src="https://img.saraba1st.com/forum/202306/06/021123ovgydv7kk4bb7sbf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-020619.jpg</strong> (543.13 KB, 下载次数: 0)

下载附件

2023-6-6 02:11 上传

<img src="https://img.saraba1st.com/forum/202306/06/021123cg9ula35mpml57k5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-020511.jpg</strong> (580.31 KB, 下载次数: 0)

下载附件

2023-6-6 02:11 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 496#       发表于 2023-6-6 03:30

 本帖最后由 Machinery 于 2023-6-6 03:31 编辑 

VIC

无词汇图像分类

github项目代码库:https://github.com/altndrr/vic

<img src="https://img.saraba1st.com/forum/202306/06/024620vkdqfptfzpvkkzuf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-024532.jpg</strong> (71.23 KB, 下载次数: 0)

下载附件

2023-6-6 02:46 上传

最近在大型视觉语言模型方面取得的进展彻底改变了图像分类范式，尽管展示了令人印象深刻的零样本能力，但在测试时假定了一个预定义的类别集，也就是词汇

然而，这样的假设在语义环境未知且变化的情况下可能不实际，因此，本文正式提出了一个新的任务，称为无词汇图像分类(VIC)，其目的是将输入图像分配到一个不受约束的语言诱导的语义空间中的类别，而无需知道词汇

VIC是一个具有挑战性的任务，因为语义空间极大，包含数百万个概念，细粒度类别难以区分

在这项工作中，首先通过实证验证，实现这个语义空间的最有效方式是通过外部视觉语言数据库获得与图像语义相关的内容

然后，提出了从外部数据库中搜索类别(CaSED)的方法，它利用预训练的视觉语言模型和外部视觉语言数据库以无需训练的方式解决VIC

CaSED首先从数据库中检索到的图像语义上最相似的标题中提取一组候选类别，然后根据相同的视觉语言模型将最匹配的候选类别分配给图像

实验结果证明，CaSED在准确性方面超过其他复杂的视觉语言框架，参数更少，为未来的研究方向铺平了道路

1.探索了无词汇图像分类的任务，其目标是通过一组不受约束的语义概念为图像分配一个类别，克服现有的基于VLM的图像分类方法的基本假设，通过将此任务形式化，并提出可用作未来工作参考的具体评估指标

2.提出了 CaSED，这是第一个解决VIC问题的方法，这要归功于采用大型字幕数据集，值得注意的是，CaSED 无需训练，不需要任何额外参数，也不需要微调文本和视觉编码器

3.大规模评估表明，CaSED在VIC上始终优于更复杂的VLM，例如 BLIP-2 ，而且需要的参数量少的多

<img src="https://img.saraba1st.com/forum/202306/06/031324vhh751ah2k50szms.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-031312.jpg</strong> (34.87 KB, 下载次数: 0)

下载附件

2023-6-6 03:13 上传

初步研究的结果，在十个不同的数据集中，将语义描述与真实的类名匹配的top-1精度，比较了BLIP-2(VQA)和BLIP-2(字幕)，最接近的字幕和字幕质心，即检索的字幕的平均表示，额外强调了零样本CLIP的上限，将大的语义空间表示为VLD并从中检索字幕会产生与真实标签语义上更为相似的输出，而与启用VQA的VLM进行查询相比，其参数数量要少10倍

<img src="https://img.saraba1st.com/forum/202306/06/032358pvnos6tugnnoyv6v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-032225.jpg</strong> (89.28 KB, 下载次数: 0)

下载附件

2023-6-6 03:23 上传

<img src="https://img.saraba1st.com/forum/202306/06/032358n4too37767ojlfjb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230606-032240.jpg</strong> (138.9 KB, 下载次数: 0)

下载附件

2023-6-6 03:23 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 497#       发表于 2023-6-7 02:19

GPTeam

多代理模拟

一组完全可定制的 AI 代理，具有独立的个性、记忆和指令，项目受“Generative Agents”论文的启发

项目说明主页:https://blog.langchain.dev/gpteam-a-multi-agent-simulation/

 github项目地址:https://github.com/101dotxyz/GPTeam

<img src="https://img.saraba1st.com/forum/202306/07/021501zh69dc1w1w1fqwfg.jpg" referrerpolicy="no-referrer">

<strong>20230607_021455.jpg</strong> (38.23 KB, 下载次数: 0)

下载附件

2023-6-7 02:15 上传

每个代理都在此循环中并行运行，当它们观察到新事件时，它们会分配一个“重要性分数”并存储在它们的内存中

<img src="https://img.saraba1st.com/forum/202306/07/021549jsdrdj3bonda7odp.jpg" referrerpolicy="no-referrer">

<strong>20230607_021543.jpg</strong> (33.1 KB, 下载次数: 0)

下载附件

2023-6-7 02:15 上传

当一个代理执行一个计划时，它们会根据这个逻辑收集相关的记忆，并填充它们的prompt提示

<img src="https://img.saraba1st.com/forum/202306/07/021656bsms12ds1s0cyeym.jpg" referrerpolicy="no-referrer">

<strong>20230607_021652.jpg</strong> (29.69 KB, 下载次数: 0)

下载附件

2023-6-7 02:16 上传

代理还通过reflections，回想他们的记忆并得出更高层次的结论来产生自己的计划

<img src="https://img.saraba1st.com/forum/202306/07/021841yii993yki1mey3wl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-021759.jpg</strong> (231.98 KB, 下载次数: 0)

下载附件

2023-6-7 02:18 上传

<img src="https://img.saraba1st.com/forum/202306/07/021950kksrl9379wfstff7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-021933.jpg</strong> (525.48 KB, 下载次数: 0)

下载附件

2023-6-7 02:19 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 498#       发表于 2023-6-7 02:52

VAC-NVS

可调节视觉外观的通用新视角合成方法

提出了一种通用的新颖的场景视图生成方法，它允许在确保多视图一致性的同时改变场景的视觉外观，本方法建立在Generalizable NeRF Transformer(GNT)上，这是一种基于Transformer的新颖视图合成方法

具体来说，引入了一个潜在的外观变量来控制渲染视图的视觉外观，通过使用可泛化的NeRF模型和这个潜在的外观变量，能够渲染新的视图并改变在训练我们的模型时未见场景的外观，而无需在目标外观处观察场景

<img src="https://img.saraba1st.com/forum/202306/07/025042lh1tg4yi74fdt7tz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-022619.jpg</strong> (55.44 KB, 下载次数: 0)

下载附件

2023-6-7 02:50 上传

不仅允许更改观察场景的视觉外观，还能同时保留底层的几何形状

注:本文工作仅限于现实的天气和光照条件，不考虑进行可定制的随意外观变化，此外，视图合成仅限于所基于的神经渲染方法的性能

基于一个通用的Transformer架构，在不同外观条件下对合成场景进行训练，这允许以一致的方式呈现训练集中未包含的3D场景的新视角，以及修改其外观以匹配目标条件，或者进行不同条件之间的平滑插值连续生成

<img src="https://img.saraba1st.com/forum/202306/07/025058c3zfkgiiofaoxoxg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-023656.jpg</strong> (1007.61 KB, 下载次数: 0)

下载附件

2023-6-7 02:50 上传

上方：CARLA生成的输入图像

下方：由我们的方法生成的图像，其中来自一个条件的图像与对应于另一个条件的潜在外观变量一起作为输入给出

可以看到，本方法能够更改图像的整体的视觉外观以匹配所需的条件，同时还可以进行局部更改，例如从白天到晚上打开灯或从白天在窗户上添加阳光反射，还观察到，从白天到晚上更具挑战性，可能导致模糊和细节丢失

<img src="https://img.saraba1st.com/forum/202306/07/025327aojfxjsaxfx5ifmr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-025314.jpg</strong> (165.04 KB, 下载次数: 0)

下载附件

2023-6-7 02:53 上传

将本方法与在渲染图像上应用的2D样式转换进行比较，本方法能够在保留场景内容的同时彻底改变外观，并提供多视图一致的渲染，观察到由于 Instruct-pix2pix未在特定数据集上进行训练，因此它不会更改场景外观以对应于我们夜间条件描述的视觉外观，Instruct-pix2pix和pix2pix-HD提供的视图不一致，而CyEDA提供明显更一致的结果

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 499#       发表于 2023-6-7 03:27

Orca

从GPT-4的复杂解释追溯中进行渐进式学习

本文作者(微软)正在与法律团队合作，以确认符合LLaMA的发布政策情况下，公开发布模型权重的差异文件

地址:https://aka.ms/orca-lm

最近的研究侧重于通过模仿学习增强较小模型的能力，利用大型基础模型(LFM)生成的输出，许多问题会影响这些模型的质量，包括来自浅层LFM输出的有限模仿信号；小规模同质训练数据；最值得注意的是，缺乏严格的评估导致高估了小模型的能力，因为它们倾向于学习模仿风格，而不是LFM的推理过程

为了应对这些挑战，在本文中提出了Orca，一个学习模仿LFM推理过程的130亿参数模型，Orca从GPT-4 的丰富信号中学习，包括解释追溯，循序渐进的思维过程，和其他复杂的说明，由ChatGPT作为教师协助指导

为了促进这种渐进式学习，通过明智的抽样和选择来挖掘大规模和多样化的模仿数据，Orca在Big-Bench Hard (BBH)等复杂的零样本推理基准测试中超过Vicuna-13B等传统的最先进指令调优模型近100%，在AGIEval上超过42%

此外，Orca在BBH基准测试中与ChatGPT持平，并在SAT、LSAT、GRE和GMAT等专业和学术考试中表现出竞争力，零样本测试，没有CoT，同时落后于GPT-4

经过研究表明，从逐步解释中学习，无论这些解释是由人类还是更高级的AI模型生成的，都是可以提高模型能力和技能的有前途的方向

<img src="https://img.saraba1st.com/forum/202306/07/032654u269kgg9ftet81s9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-032426.jpg</strong> (194.41 KB, 下载次数: 0)

下载附件

2023-6-7 03:26 上传

<img src="https://img.saraba1st.com/forum/202306/07/032654ndbt0t101o10d0hc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-032440.jpg</strong> (57.25 KB, 下载次数: 0)

下载附件

2023-6-7 03:26 上传

<img src="https://img.saraba1st.com/forum/202306/07/032654o88qp8884xb481qx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-032525.jpg</strong> (220.85 KB, 下载次数: 0)

下载附件

2023-6-7 03:26 上传

<img src="https://img.saraba1st.com/forum/202306/07/032654xjzp7f637m0qhzmj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-032617.jpg</strong> (375.18 KB, 下载次数: 0)

下载附件

2023-6-7 03:26 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 500#       发表于 2023-6-7 03:47

 本帖最后由 Machinery 于 2023-6-7 03:50 编辑 

Video-LLaMA

一种用于视频理解的指令微调视听语言模型

相关论文:https://arxiv.org/abs/2306.02858

github项目仓库:https://github.com/DAMO-NLP-SG/Video-LLaMA

Video-LLaMA，这是一种多模态框架，它使大型语言模型(LLM)能够理解视频中的视觉和听觉内容，Video-LLaMA从冻结的预训练视觉编码器，音频编码器和冻结的LLM中引导跨模态训练

与之前专注于静态图像理解的视觉LLM不同，Video-LLaMA解决了视频理解中的两个挑战:捕捉视觉中的时间场景变化，与集成视听信号

<img src="https://img.saraba1st.com/forum/202306/07/034743xgp9a249gghdm9vv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-034652.jpg</strong> (231.54 KB, 下载次数: 0)

下载附件

2023-6-7 03:47 上传

对于第一个挑战，提出了视频Q-former以将预训练图像编码器扩展到视频编码器，并引入视频到文本生成任务来学习视频语言对应关系

对于第二个挑战，利用ImageBind作为预训练的音频编码器，它在将不同的模态对齐到一个共同的嵌入空间方面表现非常出色，然后引入音频Q-former来学习听觉查询标记

为了将视觉和音频编码器的输出与LLM的嵌入空间对齐，使用了大规模的视觉字幕数据集和大量视觉指令调整数据集训练Video-LLaMA

Video-LLaMA展示了感知和理解视频内容的能力，生成基于视频中存在的视觉和听觉信息的有意义的响应，这凸显了Video-LLaMA作为视听AI助手的光明前途

<img src="https://img.saraba1st.com/forum/202306/07/034731uao80341o07r17wa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-034636.jpg</strong> (200.06 KB, 下载次数: 0)

下载附件

2023-6-7 03:47 上传

Video-LLaMA 建立在很酷的MiniGPT-4(图像编码器：ViT-G/14+Q-Former，语言解码器：Vicuna-13B)之上

通过引入一个两层的视频Q-Former和一个帧嵌入层(应用于每个帧的查询标记)，使MiniGPT-4的图像编码器能够处理视频输入

为了使Vicuna-13B能够理解视频表示，在Video-LLaMA上使用了Webvid-2M视频字幕数据集进行了预训练以完成视频到文本生成任务，同时还将图像文本对(来自LLaVA的约 595K 图像说明数据集)添加到预训练数据集中，以增强对静态视觉概念的理解

预训练后，进一步微调Video-LLaMA，使用来自VideoChat的基于视频的训练数据(7K视频详细描述数据集+4K数量的视频对话数据集)

请注意，只有新添加的层和线性投影层在预训练和指令调整阶段是可训练的，这些组件充当视频和文本之间的“适配器”

<img src="https://img.saraba1st.com/forum/202306/07/034034wzb555gdeffafllh.png" referrerpolicy="no-referrer">

<strong>architecture.png</strong> (341.81 KB, 下载次数: 0)

下载附件

2023-6-7 03:40 上传

<img src="https://img.saraba1st.com/forum/202306/07/034034s4qc4nc7mh4himdm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-034016.jpg</strong> (221.41 KB, 下载次数: 0)

下载附件

2023-6-7 03:40 上传

<img src="https://img.saraba1st.com/forum/202306/07/034750lrbme4vm9vzf49on.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-034709.jpg</strong> (312.61 KB, 下载次数: 0)

下载附件

2023-6-7 03:47 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 501#       发表于 2023-6-7 03:58

Drag3D

DragGAN遇见GET3D

github项目地址:https://github.com/ashawkey/Drag3D

该项目将DragGAN的思想扩展到GET3D中，以启用交互式生成和拖动编辑的表格纹理

测试环境
Ubuntu 20 + V100 + CUDA 11.6 + torch 1.12.0
Windows 10 + 3070 + CUDA 12.1 + torch 2.1.0

<img src="https://img.saraba1st.com/forum/202306/07/035832hollglqpaqq27l33.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-035725.jpg</strong> (105.33 KB, 下载次数: 0)

下载附件

2023-6-7 03:58 上传

<img src="https://img.saraba1st.com/forum/202306/07/035832jw294jll28cw4804.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-035751.jpg</strong> (78.4 KB, 下载次数: 0)

下载附件

2023-6-7 03:58 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 502#       发表于 2023-6-7 04:42

PLANNER

通过潜在语言扩散模型生成多样化的段落

相关论文:https://arxiv.org/abs/2306.02531

自回归文本模型有时会生成重复和低质量的输出，因为在生成步骤中错误会累积，这个问题通常归因于曝光偏差(exposure bias)，模型的训练方式与推理过程中的使用方式之间的差异所导致的问题

去噪扩散模型提供了一种替代方法，使模型可以在其中重新访问和修改其输出，然而它们的计算成本可能很高，并且先前对文本的修改努力导致模型产生的输出不如自回归模型流畅，尤其是对于较长的文本和段落在本文中

在本文中提出了PLANNER，这是一种将潜在语义扩散与自回归生成相结合的模型，可以在对段落进行全局控制的同时生成流畅的文本

该模型通过结合自回归“解码”与带有“规划”模块的模块，该模块使用潜在扩散以粗到细的方式生成语义段落嵌入

所提出的方法在各种条件生成任务上进行了评估，语义生成、文本完成和摘要的结果表明其在高效生成高质量长格式文本方面的有效性

<img src="https://img.saraba1st.com/forum/202306/07/042308v6oi44d5hutdxt58.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-042230.jpg</strong> (157.83 KB, 下载次数: 0)

下载附件

2023-6-7 04:23 上传

左图：即使在top-p采样下(K=50，p=0.92)，经过微调的GPT-2大型模型(774M，FT)仍然被自我强化的重复(突出显示的文本)所吸引

右图：不同的生成方法生成512个文本后的n-gram分布，提出的方法导致更多样化的生成，以及对错误的提示具有鲁棒性

建议将扩散技术应用于潜在语义空间，而不是对原始文本或相应的词嵌入进行扩散 (Rombach et al., 2022; Lovelace et al., 2022)，为实现这一目标，通过学习固定数量的连续语义标记，这些标记在段落级别对显着信息进行编码

然后可以使用这些标记来重建原始文本，潜在扩散可以额外地以外部信号为条件来生成语义标记，最后将解码器获得的语义标记映射回原始文本空间

这个过程结合了非自回归语义扩散方法和自回归解码技术，语义扩散过程处理“规划”，以从粗到精的方式修改语义，而解码器通过将语义翻译成原始文本来处理“解码”，在控制含义方面灵活性较低

研究组将提出的方法称为PLANNER(用于嵌入表示的段落级扩散模型)，为文本段落提出了一个潜在语义扩散模型，该模型结合了非自回归语义扩散和自回归生成。 这使模型能生成流畅的文本，同时能够从扩散模型继承全局控制语义，研究了段落扩散模型的良好潜在空间的基本要求，评估了提出的方法在各种条件生成任务上的有效性，由于去噪扩散的迭代改进，与自回归和文本扩散的基线相比，本方法具有更少的重复性和更多样化的生成，同时保持良好的流畅性和相关性

<img src="https://img.saraba1st.com/forum/202306/07/043715kmy5kdxmm0wbl3jb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-043613.jpg</strong> (111.31 KB, 下载次数: 0)

下载附件

2023-6-7 04:37 上传

左图：变分段落嵌入器学习将段落编码为固定数量的潜在代码
右图：应用基于Transformer block的潜在扩散模型来生成潜在代码，解码器最终将它们翻译成文本

(BOS：句子开始标记，EOS：句子结束标记)

评估结果

<img src="https://img.saraba1st.com/forum/202306/07/044211eu7q7rmtdzrtcqsd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-043949.jpg</strong> (129.59 KB, 下载次数: 0)

下载附件

2023-6-7 04:42 上传

<img src="https://img.saraba1st.com/forum/202306/07/044211wknbk6q6gbed5jbg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-044011.jpg</strong> (175.34 KB, 下载次数: 0)

下载附件

2023-6-7 04:42 上传

<img src="https://img.saraba1st.com/forum/202306/07/044211joviqu6q3nu5vz25.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-044033.jpg</strong> (147.74 KB, 下载次数: 0)

下载附件

2023-6-7 04:42 上传

<img src="https://img.saraba1st.com/forum/202306/07/044211spn57pop5zn53rmr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-044151.jpg</strong> (863.61 KB, 下载次数: 0)

下载附件

2023-6-7 04:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 503#       发表于 2023-6-7 05:05

 本帖最后由 Machinery 于 2023-6-7 05:09 编辑 

HeadSculpt

用文字制作3D头像

项目空间:https://brandonhan.uk/HeadSculpt/

github实现代码仓库(coming soon):https://github.com/BrandonHanx/HeadSculpt

<img src="https://img.saraba1st.com/forum/202306/07/045859pm6vi3o6ihidvozz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-045640.jpg</strong> (485.95 KB, 下载次数: 0)

下载附件

2023-6-7 04:58 上传

<img src="https://img.saraba1st.com/forum/202306/07/045859jj7h37quh1s73t67.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-045506__01.jpg</strong> (410.58 KB, 下载次数: 0)

下载附件

2023-6-7 04:58 上传

最近，利用大型视觉语言和图像扩散模型的文本引导的3D生成方法激增，在生成高质量纹理和几何图形方面取得了显著进步

然而，现有方法在两个方面仍然难以创建高保真3D头部头像：

1.它们主要依赖于预训练的文本到图像扩散模型，同时缺少必要的3D意识和头部先验，这使得它们生成的化身容易出现不一致和几何扭曲 

2.它们在细粒度编辑方面存在不足，这主要是由于预训练的2D图像扩散模型的继承所限制的，当涉及到3D头像时，这种限制变得更加明显

在这项工作中，通过引入一种称为HeadSculpt的多功能从粗到精的工作流来应对这些挑战，该工作流可用于根据文本提示制作(即生成和编辑)3D头像

具体来说，首先通过利用基于landmark的控制和可学习的表征头部后视图外观的文本嵌入，为扩散模型配备3D意识，从而实现3D一致的头部头像生成

进一步提出了一种新颖的身份感知编辑分数蒸馏策略，以使用高分辨率可微分渲染技术优化纹理网格，这使得在遵循编辑指令时能够保持身份

通过全面的实验和与现有方法的比较，展示了 HeadSculpt卓越的保真度和编辑能力

<img src="https://img.saraba1st.com/forum/202306/07/050855sl6nmnfn6qiyqt5b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230607-050802.jpg</strong> (489.91 KB, 下载次数: 0)

下载附件

2023-6-7 05:08 上传

使用HeadSculpt 获得的生成和编辑结果示例，它可以使用简单的描述或说明为任何类型的头部头像创建和精细编辑具有复杂几何形状和纹理的高质量头部头像。 符号表示以下提示前缀：*“[文本]的头像”和†“[文本]的数码单反肖像”。 灰色的是提示后缀，蓝色的是编辑说明

<img src="https://img.saraba1st.com/forum/202306/07/050544lg1115sv6qqf78q7.png" referrerpolicy="no-referrer">

<strong>pipeline.png</strong> (699.38 KB, 下载次数: 0)

下载附件

2023-6-7 05:05 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 504#       发表于 2023-6-8 02:54

localizing steps of procedural activities in narrated how-to videos(通过操作演示的视频旁白进行对齐定位按流程进行的活动步骤)

通过视频旁白学习教学指导文章进行自动标注教学视频对应时间段

相关论文:https://arxiv.org/abs/2306.03802

HT-step基准挑战:https://eval.ai/web/challenges/challenge-page/2082/overview

HT-step基准:给定一篇指导性文档，由一系列连续的步骤描述组成，以及一个进行演示活动的视频，文档时间定位(temporal article grounding)是指检测视频中出现这些步骤的时间片段

HT-step基准是来自HowTo100M数据集的视频时间注释的集合，这些注释是从WikiHow文章中提取的一系列步骤，每个步骤都包含一个标题句和一段更详细地描述该步骤的段落，文章的某些步骤可能根本没有出现、部分出现、执行顺序不同或重复多次

<img src="https://img.saraba1st.com/forum/202306/08/024006ihejztpiqa555a1a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-022143.jpg</strong> (256.97 KB, 下载次数: 0)

下载附件

2023-6-8 02:40 上传

本方法无需任何手动标注即可将文档的步骤描述与视频帧、旁白和步骤描述三方内容对齐，具体来说，主要通过两条路径融合信息来对步骤描述与视频对齐
1.步骤描述与帧的直接对齐
2.步骤描述与旁白的间接对齐,然后旁白与视频的对应关系

本文方法可以一次对文章中的所有步骤进行全局的时间定位，利用顺序信息，并使用迭代更新和积极过滤的步骤伪标签进行训练

为了验证该模型,论文引入了一个新的评估基准，正是上文的HT-Step挑战，该基准使用从wikiHow文章提取的步骤手动对HowTo100M的124小时子集进行注释

实验结果表明,该方法的多模态对齐与其他基准和先前的工作相比取得了显著提高，此外，该方法内用于匹配旁白和视频的模块，达成了HTM-Align narration-video对齐基准上的最新Sota

<img src="https://img.saraba1st.com/forum/202306/08/025320x28148zzgare7eee.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-025119.jpg</strong> (74.8 KB, 下载次数: 0)

下载附件

2023-6-8 02:53 上传

<img src="https://img.saraba1st.com/forum/202306/08/025320taao8vvo94kh1ut9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-025139.jpg</strong> (129.08 KB, 下载次数: 0)

下载附件

2023-6-8 02:53 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 505#       发表于 2023-6-8 03:17

ChatDB

使用数据库作为象征记忆来增强LLM

项目主页:https://chatdatabase.github.io/

代码:待整理

具有记忆的大型语言模型(LLM)在计算上是通用的，然而，主流的LLM并没有充分利用记忆，而且设计深受生物大脑的影响

由于它们的近似性质倾向于累积错误，传统的神经记忆机制无法支持LLM模拟复杂推理

在本文中，通过从现代计算机体系结构中寻求灵感，用象征记忆来增强LLM，以进行复杂的多跳推理

这种象征记忆框架被实际构成为一个LLM和一组SQL数据库，其中LLM生成SQL指令来操作SQL数据库

在需要复杂推理的合成数据集上验证了所提出的记忆框架的有效性

<img src="https://img.saraba1st.com/forum/202306/08/030445m9v37xvtbfzo9v3n.png" referrerpolicy="no-referrer">

<strong>teaser_crop3.png</strong> (123.03 KB, 下载次数: 0)

下载附件

2023-6-8 03:04 上传

ChatDB的整体框架，LLM 控制器控制对记忆的读写操作，存储器存储历史信息并提供相关历史信息以协助响应处理用户输入，在ChatDB中，本文专注于使用数据库作为象征记忆来扩充 LLM

虽然大型语言模型在语言组织和知识推理方面的能力越来越强，但它们的主要问题之一是处理长上下文，例如，GPT-4使用32K的序列长度，而Claude使用100K，因此下列问题是将LLM链接到日常和工业应用软件应用程序时的一个实际问题：作为个人聊天机器人，它会忘记您的偏好，因为对于 LLM 来说，每一天都是新的一天

作为业务分析工具，它只能处理在一个小时间窗口内捕获的数据，因为它无法消化长期的历史业务文档，由于神经网络中的分布式知识存储，精确和符号化地维护和操纵神经知识是困难的

数据库可以被认为处于整个学习系统之外，并被动地存储人类指示的信息，另外，之前的工作主要是只关注选择操作，不支持插入，更新，删除相关数据库信息

<img src="https://img.saraba1st.com/forum/202306/08/031146ecojztceca39dldg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-031009.jpg</strong> (69.05 KB, 下载次数: 0)

下载附件

2023-6-8 03:11 上传

红色箭头表示“链式记忆”的流程，以及多个记忆操作之间的联系，数据库表之间的红色箭头表示主键和外键之间的引用关系，从主键指向外键，为了简洁，每个数据库表仅显示前四列，图中示例展示了一个客户(电话号码823451)在2023-01-02购买的商品的退货流程

<img src="https://img.saraba1st.com/forum/202306/08/031705ryuxcfsoyxyluxot.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-031605.jpg</strong> (53.61 KB, 下载次数: 0)

下载附件

2023-6-8 03:17 上传

<img src="https://img.saraba1st.com/forum/202306/08/031705q7fxpos72x7zpoho.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-031624.jpg</strong> (590.62 KB, 下载次数: 0)

下载附件

2023-6-8 03:17 上传

<img src="https://img.saraba1st.com/forum/202306/08/031705fccucdocoamu3cdr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-031634.jpg</strong> (222.45 KB, 下载次数: 0)

下载附件

2023-6-8 03:17 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 506#       发表于 2023-6-8 03:36

 本帖最后由 Machinery 于 2023-6-8 03:41 编辑 

Recognize Anything

一个强大的图片标注模型

项目主页:https://recognize-anything.github.io/

github相关代码库:https://github.com/xinyu1205/Recognize_Anything-Tag2Text

<img src="https://img.saraba1st.com/forum/202306/08/034045cbzx63qclsju6g30.jpg" referrerpolicy="no-referrer">

<strong>20230608_034036.jpg</strong> (170.87 KB, 下载次数: 0)

下载附件

2023-6-8 03:40 上传

Recognize Anything Model(RAM)，图像标注的强大基础模型，RAM可以高精度识别任何常见类别，RAM引入了一种新的图像标注范例，利用大规模图像文本对进行训练而不是手动标注

RAM的开发包括四个关键步骤:
1.通过大规模的自动文本语义解析获得无标注的图像标签
2.训练了一个初步模型用于自动标注统一标题和标记任务，分别由原始文本和解析的标签监督
3.使用数据引擎来生成额外的注释并清除不正确的注释
4.最后使用处理后的数据对模型进行再训练，并使用更小但质量更高的数据集进行微调

研究组在众多基准测试中评估了RAM的标注能力，并观察到令人印象深刻的零样本性能，显着优于CLIP和 BLIP。值得注意的是，RAM甚至超越了完全监督的方式，并展示了与Google API竞争的性能

研究组将开源RAM以促进计算机视觉中大型模型的进步

<img src="https://img.saraba1st.com/forum/202306/08/032732ze3qx2abhwkkbkaf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-032634.jpg</strong> (59.07 KB, 下载次数: 0)

下载附件

2023-6-8 03:27 上传

SAM擅长提供强大的本地化功能，但在识别任务方面存在不足。，相比之下，RAM 表现出卓越的识别能力，在准确性和范围方面都超过了现有模型

<img src="https://img.saraba1st.com/forum/202306/08/032930kg4m5iu5hwrwb055.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-032809__01.jpg</strong> (552.28 KB, 下载次数: 0)

下载附件

2023-6-8 03:29 上传

标注模型识别能力对比，RAM识别了比其他型号的模型更有价值的标签，不会遗漏重要部分，ML-Decoder和Google tagging API倾向于输出冗余标签(例如，“human head”)或不太相关的标签(例如，“property”)标签，BLIP的标签回应是有限的，因为它依赖于字幕生成

<img src="https://img.saraba1st.com/forum/202306/08/033155lua8ixklhal01lxl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-032825__01.jpg</strong> (148.51 KB, 下载次数: 0)

下载附件

2023-6-8 03:31 上传

RAM框架图，通过自动文本语义解析从图像文本对中获得大规模图像标签，通过图像-标签-文本三元组，RAM统一了字幕和标签任务，此外，RAM引入了一个现成的文本编码器，将标签编码成具有语义丰富的上下文的文本标签查询，从而在训练阶段能够泛化到未见类别

<img src="https://img.saraba1st.com/forum/202306/08/033315g8wzwebbwv30dgyi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-032830__01.jpg</strong> (115.75 KB, 下载次数: 0)

下载附件

2023-6-8 03:33 上传

图 4. 不同标注模型的识别范围，Tag2Text可识别 3400多个固定标签，RAM将数字升级到6400+，涵盖比OpenImages V6更有价值的类别，凭借开放数据集的能力，RAM可以识别任何常见类别

相关评估:

<img src="https://img.saraba1st.com/forum/202306/08/033632qd6p7ip0p76aopsj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-033449.jpg</strong> (355.38 KB, 下载次数: 0)

下载附件

2023-6-8 03:36 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 507#       发表于 2023-6-8 05:00

 本帖最后由 Machinery 于 2023-6-8 05:06 编辑 

dift

图像扩散的涌现一致性

项目主页:https://diffusionfeatures.github.io/

github项目仓库:https://github.com/Tsingularity/dift

colab体验地址:https://colab.research.google.com/drive/1tUTJ3UJxbqnfvUMvYH5lxcqt0UdUjdq6?usp=sharing

<img src="https://img.saraba1st.com/forum/202306/08/045536qiiviu5n41ntn181.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-044602.jpg</strong> (160.23 KB, 下载次数: 0)

下载附件

2023-6-8 04:55 上传

寻找图像之间的对应关系是计算机视觉中的一个基本问题，在本文中，实验表明即使在没有任何明确监督的情况下，图像扩散模型中会出现对应关系

提出了一种简单的策略，从扩散网络中提取这种隐含知识作为图像特征，即扩散特征(DIFT)，并使用它们建立真实图像之间的对应关系

没有对特定任务数据或标注进行任何额外的微调或监督的情况下，DIFT能够在识别语义、几何和时间对应方面优于弱监督方法和有竞争力的现成特征方法

特别是对于语义对应，来自Stable Diffusion的DIFT在具有挑战性的SPair-71k基准测试中能够分别优于 DINO和OpenCLIP，19%和14%准确率

在18个类别中的9个类别上的表现优于最先进的监督方法，同时在整体表现上保持同等水平

<img src="https://img.saraba1st.com/forum/202306/08/045549r1t1atts53in6sut.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-045117.jpg</strong> (153.89 KB, 下载次数: 0)

下载附件

2023-6-8 04:55 上传

与相似数据上(预)训练的流行现成特征方法相比，DIFT在遮挡、集群场景、视点变化、姿势变化和实际外观变化下识别出更好的对应关系，优于自监督学习对应特征(DIFT_sd对比 0OpenCLIP；DIFT_adm对比 DINO)超过14个PCK点

<img src="https://img.saraba1st.com/forum/202306/08/045648mivv6jzsediamosv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-045625.jpg</strong> (123.79 KB, 下载次数: 0)

下载附件

2023-6-8 04:56 上传

DIFT还可以轻松地将编辑特征从一个图像传播到不同实例、类别和其他域的图像，而无需任何对应监督

<img src="https://img.saraba1st.com/forum/202306/08/045754qizifd33o1wf1sas.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-045731.jpg</strong> (97.6 KB, 下载次数: 0)

下载附件

2023-6-8 04:57 上传

通过使用较小的时间步长t，DIFT在没有任何此类监督的情况下也显示出几何对应的竞争性能，下面展示了去除异常值后使用DIFT在HPatches上的稀疏匹配结果，可以看到它在具有挑战性的视角和光照变化下效果很好

<img src="https://img.saraba1st.com/forum/202306/08/045915b59iuiptpupx4zqz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-045827.jpg</strong> (182.69 KB, 下载次数: 0)

下载附件

2023-6-8 04:59 上传

DIFT 还在时间对应任务上展示了强大的性能，尽管从未在视频数据上进行过训练或微调。下面展示了使用DIFT在DAVIS上的视频分割传播结果

<img src="https://img.saraba1st.com/forum/202306/08/050228un7v7pv4vd0825nn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230608-045925.jpg</strong> (115.27 KB, 下载次数: 0)

下载附件

2023-6-8 05:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 508#       发表于 2023-6-9 03:29

 本帖最后由 Machinery 于 2023-6-9 03:32 编辑 

FlowCam

通过像素对齐的场景流在没有摄影机姿态的情况下训练通用3D辐射场

项目主页:https://cameronosmith.github.io/flowcam

github代码库(待整理):https://github.com/cameronosmith/FlowCam

FlowCam是一个通用的3D场景表示方法，可以在原始视频数据集上进行自监督表征，没有任何预先计算的摄影机姿态或者SfM(Structure from Motion/运动推断结构)

3D表征是训练通用视觉基础模型的关键要素，那么是什么阻碍了3D深度学习者的ImageNet时刻呢，最关键的瓶颈之一是对于摄影机姿态的运动结构(SfM)的依赖

SfM(运动推断结构)，又称运动恢复结构，是一种摄影测量范围成像技术，用于估计二维图像序列帧中的三维结构，这些图像可能与局部运动信号相结合，它是在计算机视觉和视觉感知领域进行研究的，在生物视觉上，运动推断结指的是人类和其他生物能够从一个移动物体或场景中投射的二维运动场中恢复三维结构的现象

SfM在昂贵的预处理步骤中计算摄影机姿态，对于CO3D数据集，SfM花费了超过5 GPU年的算力，然而，我们需要更大和更多样化的数据集来训练视觉基础模型(几乎所有的视频都需要)，快速扩大数据集大小变得非常棘手

<img src="https://img.saraba1st.com/forum/202306/09/030754oio9i7ciwwbmch7w.jpg" referrerpolicy="no-referrer">

<strong>20230609_030717.jpg</strong> (186.08 KB, 下载次数: 0)

下载附件

2023-6-9 03:07 上传

FlowCam模型提升的关键在于估计3D几何和相机姿势来努力解除瓶颈，进行稳健估计姿势，首先通过通用3D渲染器将光流对应到3D场景流中，然后求解摄像头姿势

<img src="https://img.saraba1st.com/forum/202306/09/031457whm7b2m4o337q8ob.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-031444.jpg</strong> (146.59 KB, 下载次数: 0)

下载附件

2023-6-9 03:14 上传

本文中的姿势估计继承了光流方法的稳健性，它甚至适用于CO3D数据集，这是一个众所周知的相机姿势估计困难数据集，即使是经典的SfM方法也难以解决，在CO3D上，FlowCam甚至优于SLAM方法ORB-SLAM3和Droid-SLAM

FlowCam优于先前关于无姿势新视图合成的工作：对比基线方法regresses camera poses(视频自动编码器/CNN方法)，以及最近使用的latent poses(RUST)在视频重建任务中的方法

之前的工作通常通过黑盒进行回归估计姿势(例如截取两张图像帧进入CNN再进行姿势输出)，相反，本方法采用完全3D结构的方法来进行3D姿态估计，与神经场景表示关系紧密

<img src="https://img.saraba1st.com/forum/202306/09/032336rq4eyqpp4zaq42qq.jpg" referrerpolicy="no-referrer">

<strong>20230609_032323.jpg</strong> (96.6 KB, 下载次数: 0)

下载附件

2023-6-9 03:23 上传

给定一个3D运动场，再通过可微分的加权最小二乘求解器求解姿势，模型预测的加权使之能够对错误的对应关系保持鲁棒性，例如，来自动态物体、镜面反射、遮挡和不正确的光流等干扰

<img src="https://img.saraba1st.com/forum/202306/09/032645m8q37s5bwuqut7fz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-032536.jpg</strong> (109.24 KB, 下载次数: 0)

下载附件

2023-6-9 03:26 上传

FlowCam还可以通过简单的重新渲染损失在OOD(分布外数据)序列上进行微调，比如将RealEstate10K模型应用于Tanks and Temples场景，绘制微调前后的估计姿态

由于SfM是为每个场景实例化新几何体，并且比例不明确，因此每次重建都有不同的比例，这使得3D学习的训练变得困难，而本文方法的模型使用相同的网络来预测所有场景的几何形状和估计姿势，从而规避了这个问题

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 509#       发表于 2023-6-9 03:59

starcoderplus

starcoder微调增强plus豪华典藏版

模型权重下载:https://huggingface.co/bigcode/starcoderplus

StarCoderPlus是StarCoderBase的微调版本，基于来自英语网络的RedefinedWeb数据集的600B Token，再结合来自The Stack (v1.2)的StarCoderData和**数据集，接近15.5B参数的语言模型，接受英语和80多种编程语言的训练

该模型使用Multi Query Attention， 一个包含8192个Token的上下文窗口，并使用Fill-in-the-Middle目标方法对1.6万亿个Token进行训练

<img src="https://img.saraba1st.com/forum/202306/09/035930wwwede5bwukz0ewz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-035900.jpg</strong> (146.7 KB, 下载次数: 0)

下载附件

2023-6-9 03:59 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 510#       发表于 2023-6-9 04:15

AlpacaEval Logo Leaderboard

AlpacaEval基准测试排行榜

项目主页:https://tatsu-lab.github.io/alpaca_eval/

AlpacaEval是一种基于LLM的自动评估，快速、廉价且可靠，它基于AlpacaFarm评估集，测试模型遵循一般用户指令的能力，然后将这些响应与所提供的GPT-4或基于Claude的自动评估的Davinci003参考响应进行比较，从而得出获胜率

AlpacaEval与真实人类标注的结果具有高一致率，并且AlpacaEval上的排行榜排名与基于人类标注者的排行榜排名非常相关

虽然AlpacaEval提供了有用的模型功能比较，但它并不是模型能力的全面或黄金标准评估，其一，详见AlpacaFarm论文，自动标注器的胜率与长度相关，尽管人工注释也显示出这种偏见，但尚不清楚更冗长的答案是否会在下游任务中增加效用

此外，AlpacaFarm评估集虽然多种多样，但主要由简单的说明组成，项目发起者鼓励社区贡献新的、更复杂的评估集，例如用于工具的使用等，最后，AlpacaEval不评估任何模型的安全性

<img src="https://img.saraba1st.com/forum/202306/09/041543mf1sqmv594445fly.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-041530.jpg</strong> (165.06 KB, 下载次数: 0)

下载附件

2023-6-9 04:15 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 511#       发表于 2023-6-9 04:24

M3IT

面向多模态多语言指令微调的大规模数据集

相关论文:https://arxiv.org/abs/2306.04387

数据集地址(待整理):https://huggingface.co/MMInstruction/M3IT

模型:待整理

指令微调显著改进了ChatGPT等大型语言模型 (LLM)，使它们能够在各种任务中与人工指令保持一致

然而，由于缺乏高质量的指令数据集，开放视觉语言模型(VLM)的进展受到限制，为了应对这一挑战并促进视觉语言领域的研究，本文中引入了多模态、多语言的指令调整 (M3IT) 数据集，旨在优化VLM与人类指令的对齐

M^3IT数据集包含40个精心策划的数据集，总共包括240万个实用样例和400个手动编写的任务指令，重新格式化为视觉到文本的结构

关键任务通过先进的翻译系统翻译成80种语言，确保更广泛的可访问性，M3IT在任务覆盖率方面超越了以前的数据集，指令编号和实例规模

此外，还开发了Ying-VLM，这是一种在M3IT数据集上训练的VLM模型，展示了它在回答需要世界知识的复杂问题、泛化到未见的视频任务以及理解未见的中文指令方面的潜力

为了鼓励进一步研究，研究组开源了数据集和经过训练的模型

<img src="https://img.saraba1st.com/forum/202306/09/042358y4kqosdb1341bay9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-042219.jpg</strong> (64.99 KB, 下载次数: 0)

下载附件

2023-6-9 04:23 上传

<img src="https://img.saraba1st.com/forum/202306/09/042358ul9r2pgv3666g6m6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-042233.jpg</strong> (300.69 KB, 下载次数: 0)

下载附件

2023-6-9 04:23 上传

<img src="https://img.saraba1st.com/forum/202306/09/042358p76zettrkdyaqn7h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-042243.jpg</strong> (147.99 KB, 下载次数: 0)

下载附件

2023-6-9 04:23 上传

<img src="https://img.saraba1st.com/forum/202306/09/042358cxpvxx0967fr727r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-042307.jpg</strong> (311.89 KB, 下载次数: 0)

下载附件

2023-6-9 04:23 上传

<img src="https://img.saraba1st.com/forum/202306/09/042358k3x3ixd3q3dqbyxp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-042314.jpg</strong> (163.91 KB, 下载次数: 0)

下载附件

2023-6-9 04:23 上传

<img src="https://img.saraba1st.com/forum/202306/09/042400n6tbr2ppo31oveke.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-042324.jpg</strong> (59.07 KB, 下载次数: 0)

下载附件

2023-6-9 04:24 上传

<img src="https://img.saraba1st.com/forum/202306/09/042400i4dlcx718xlxqwmq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-042331.jpg</strong> (176.85 KB, 下载次数: 0)

下载附件

2023-6-9 04:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 512#       发表于 2023-6-9 04:36

MeZO

只使用前向传递微调语言模型

github项目仓库:https://github.com/princeton-nlp/MeZO

一种内存高效的零阶优化器，可以在保持性能的同时使用前向传递微调大型语言模型，MeZO可以在1x 80GB A100 GPU上训练30B参数的模型

微调语言模型(LM)在各种下游任务中取得成功，但随着LM参数规模的增长，反向传播需要大量显存，Zeroth-order(ZO)方法原则上可以仅使用两次前向传递来估计梯度，但其理论上在优化大型模型时速度极其缓慢

在这项工作中，提出了一种显存高效的零阶优化器 (MeZO)，采用经典的ZO-SGD方法进行直接的操作，从而微调LM，使其具有与推理相同的内存占用

例如，使用单个A100 80GB GPU，MeZO可以训练一个300亿参数的模型，而使用反向传播进行微调在相同预算下只能训练一个2.7B的LM

通过跨模型类型(MASK和自回归LM)、模型规模(高达66B)和下游任务(分类、多项选择和生成)进行综合实验

结果表明：
1.MeZO显著优于上下文学习和线性探测
2.MeZO实现了与跨多个任务的反向传播微调相当的性能，同时性能要求减少12倍
3.MeZO兼容全参数和参数高效调优技术，如LoRA和前缀调优(PEFT)
4.MeZO可以有效地优化不可微分的目标(例如，最大化准确率或F1)

<img src="https://img.saraba1st.com/forum/202306/09/043611xo7yo2o3hsh7isat.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-043547.jpg</strong> (460.83 KB, 下载次数: 0)

下载附件

2023-6-9 04:36 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 513#       发表于 2023-6-9 05:11

Asymmetric_VQGAN

为StableDiffusion设计更好的非对称VQGAN

github项目地址:https://github.com/buxiangzhiren/Asymmetric_VQGAN

与传统方法上在像素空间学习的扩散模型不同，StableDiffusion通过VQGAN在潜在空间学习扩散模型，确保效率和质量，它不仅支持图像生成任务，还支持对真实图像进行图像编辑，例如图像修复和局部编辑

然而，观察到StableDiffusion中使用的普通VQGAN会导致大量信息丢失，即使在未编辑的图像区域也会导致失真伪影

为此，提出了一种具有两种简单设计的新型非对称 VQGAN，首先，除了来自编码器的输入之外，解码器还包含一个条件分支，该分支合并了来自任务特定先验的信息，例如修复中未屏蔽的图像区域

其次，解码器比编码器重得多，允许更详细的恢复，同时仅略微增加总推理成本，这种非对称VQGAN的训练成本很低，只需要重新训练一个新的非对称解码器，同时保持vanilla VQGAN编码器和StableDiffusion不变

非对称VQGAN可广泛用于基于StableDiffusion的修复和局部编辑方法，大量实验表明，它可以显着提高修复和编辑性能，同时保持原始的文本到图像生成能力

<img src="https://img.saraba1st.com/forum/202306/09/051008a5hhw39ui2bz5319.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-050929.jpg</strong> (261.58 KB, 下载次数: 0)

下载附件

2023-6-9 05:10 上传

<img src="https://img.saraba1st.com/forum/202306/09/051009fh087qrrd9vhr0ob.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-050945.jpg</strong> (550.75 KB, 下载次数: 0)

下载附件

2023-6-9 05:10 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 514#       发表于 2023-6-9 05:24

Youku-mPLUG

用于预训练和基准测试的1000万大规模中文视频语言数据集

相关论文:https://arxiv.org/abs/2306.04362

github项目仓库:https://github.com/X-PLUG/Youku-mPLUG

为推动视觉语言预训练(VLP)和多模态大型语言模型(LLM)在中文社区的发展，本文率先发布了最大的公共中文高质量视频语言数据集Youku-mPLUG，该数据集来自于优酷，中国著名的视频网站之一，对安全性、多样性和质量有严格的标准

Youku-mPLUG 包含从45个不同类别的4亿个原始视频中筛选出的1000万个中文视频文本对，用于大规模预训练

此外，为了便于对视频语言模型进行全面评估，研究组还精心构建了最大的人工注释中文基准，涵盖了跨模态检索、视频字幕和视频类别分类这三种流行的视频语言任务

Youku-mPLUG可以让研究人员在未来进行更深入的多模态研究，开发更好的应用，此外，还发布了流行的视频语言预训练模型ALPRO和mPLUG-2，以及研究组相关研究中提出的模块化解码器模型mPLUG-video，在Youku-mPLUG上进行了预训练

实验表明，在Youku-mPLUG 上预训练的模型在视频类别分类上获得了高达23.1%的提升，此外mPLUG-video还在这些基准测试中取得了最先进的Sota结果，视频类别分类的top-1准确率为 80.5%，视频字幕的CIDEr得分为68.9

最后，研究组还基于frozen Bloomz将mPLUG-video进行了扩展，使用了其可训练参数量仅为1.7%的拓展训练作为中文多模态LLM，并展示了令人印象深刻的指令和视频理解能力

与其他相似数据集的对比

<img src="https://img.saraba1st.com/forum/202306/09/052111gy1qxom6gyizztf6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-052044.jpg</strong> (235.01 KB, 下载次数: 0)

下载附件

2023-6-9 05:21 上传

一个标准的数据集样本

<img src="https://img.saraba1st.com/forum/202306/09/052130hwoxoorex6xegxwx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-052101.jpg</strong> (225.66 KB, 下载次数: 0)

下载附件

2023-6-9 05:21 上传

数据集样本分布

<img src="https://img.saraba1st.com/forum/202306/09/052225b4w56fyz6ay56ely.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-052213.jpg</strong> (143.75 KB, 下载次数: 0)

下载附件

2023-6-9 05:22 上传

mPLUG-video框架

<img src="https://img.saraba1st.com/forum/202306/09/052301cmnw2jlitt6i33h3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-052244.jpg</strong> (99.88 KB, 下载次数: 0)

下载附件

2023-6-9 05:23 上传

视频理解对比

<img src="https://img.saraba1st.com/forum/202306/09/052350ri26jp2ao2opxa6p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-052340.jpg</strong> (245.89 KB, 下载次数: 0)

下载附件

2023-6-9 05:23 上传

<img src="https://img.saraba1st.com/forum/202306/09/052428vs07t1155y11st6s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230609-052414.jpg</strong> (274.55 KB, 下载次数: 0)

下载附件

2023-6-9 05:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 515#       发表于 2023-6-9 19:10

MusicGen

简单可控的音乐生成

项目主页:https://ai.honu.io/papers/musicgen/

github项目仓库:https://github.com/facebookresearch/audiocraft

hugface演示Demo:https://huggingface.co/spaces/facebook/MusicGen

可用的Colab地址:https://colab.research.google.com/drive/1fxGqfg96RBUvGxZ1XXN07s3DthrKUl4-?usp=sharing

META所发布的MusicGen，开源代码(MIT授权许可)与模型(CC-BY NC授权许可)

Audiocraft框架为MusicGen提供代码和模型运行，MusicGen是一种简单且可控的音乐生成模型，构架由一个单级自回归Transformer模型组成，在32kHz EnCodec分词器上训练，具有4个以50Hz采样的码本

<img src="https://img.saraba1st.com/forum/202306/09/190815xdwxqvw8m0dvdqv6.jpg" referrerpolicy="no-referrer">

<strong>20230609_190806.jpg</strong> (119.56 KB, 下载次数: 0)

下载附件

2023-6-9 19:08 上传

MusicGen建立在EnCodec音频分词器之上，与之前的工作不同，MusicGen是一种单级transformer LM模型，它使用高效的Token交错模式，因此无需级联多个模型(例如，分层或上采样)

与MusicLM等现有方法不同，MusicGen不需要自监督的语义表示，它一次生成所有4个码本。通过在码本之间引入一个小的延迟，表明可以并行预测它们，因此每秒音频只有50个自回归步骤

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 516#       发表于 2023-6-10 06:16

 本帖最后由 Machinery 于 2023-6-10 06:20 编辑 

Imagen Editor/EditBench

增强和评估文本引导图像修复

【Note that due to concerns in relation to responsible AI, we are not releasing Imagen Editor to the public. EditBench on the other hand is released in full for the benefit of the research community.】<img src="https://static.saraba1st.com/image/smiley/face2017/003.png" referrerpolicy="no-referrer">

项目博客:https://ai.googleblog.com/2023/06/imagen-editor-and-editbench-advancing.html

EditBench项目仓库:https://imagen.research.google/editor/

文本引导图像编辑(TGIE/text-guided image editing)是一项生成下游任务，涉及编辑生成和拍摄的视觉效果，而不是完全重置，当重新创建视觉效果非常耗时或不可行时(例如，调整度假照片中的对象或完善从头生成的可爱小狗的细粒度细节)，快速、自动化和可控的编辑是一种方便的解决方案

此外，TGIE代表了改进基础模型本身训练的重要机会，多模态模型需要不同的数据才能正确训练，而TGIE编辑可以生成和重组高质量和可扩展的合成数据，这可能是最重要的，可以提供优化训练数据沿任何给定分布轴的方法

Imagen Editor ，它是遮罩(MASK)修复任务的最先进解决方案，用户在叠加层或“遮罩”(通常在绘图类型界面中生成)旁边提供文本说明，指示他们想要修改的图像区域

以及EditBench，这是一种衡量图像编辑模型质量的方法，EditBench超越了常用的粗粒度“此图像是否与此文本匹配”的普通方法，并深入到各种类型的属性、对象和场景，以更细粒度地了解模型性能，特别是，它在不忽视图像质量的情况下，非常强调图文对齐的忠实度

Imagen Editor是一个基于扩散的模型，在Imagen上进行了微调以进行编辑，它的目标是改进语言输入的表示、细粒度控制和高保真输出

Imagen Editor 从用户那里获取三个输入，所有三个输入都会引导输出样本：要编辑的图像，用于指定编辑区域binary掩码，文本提示

Imagen Editor依靠三种核心技术进行高质量的文本引导图像修复

1.与应用随机框和笔划掩码的先前修复模型(例如，调色板、上下文注意、门控卷积)不同，Imagen Editor采用对象检测器掩码策略和对象检测器模块构成，在训练期间生成对象掩码，对象蒙版基于检测到的对象而不是随机补丁，并允许在编辑文本提示和蒙版区域之间进行更有原则的对齐，根据经验，该方法有助于模型避免当遮罩区域较小或仅部分覆盖对象(例如 CogView2)时忽略文本提示的普遍问题

<img src="https://img.saraba1st.com/forum/202306/10/054718qtuv6nnioz4gmfnk.png" referrerpolicy="no-referrer">

<strong>image9.png</strong> (303.19 KB, 下载次数: 0)

下载附件

2023-6-10 05:47 上传

随机生成的遮罩(左图)通常包含背景或与物体边界相交的区域，这些区域可以仅从图像上下文合理地填充，物体遮罩(右图)更难仅从图像上下文填充，这促使模型在训练过程中更多地依赖文本输入

2.接下来，在训练和推理期间，Imagen Editor通过调节全分辨率(在本文中为1024×1024)、输入图像和掩码的通道级联(类似于 SR3、Palette 和 GLIDE)来增强高分辨率编辑，对于基本扩散64×64模型和64×64→256×256的超分辨率模型，应用参数化下采样卷积(例如，步长卷积)，凭借相关经验发现这对高保真度至关重要

<img src="https://img.saraba1st.com/forum/202306/10/055129vt6jbffdv8ebzbrz.png" referrerpolicy="no-referrer">

<strong>image4.png</strong> (79.51 KB, 下载次数: 0)

下载附件

2023-6-10 05:51 上传

Imagen针对图像编辑进行了微调，所有扩散模型，即基础模型和超分辨率(SR)模型，都以高分辨率1024×1024图像和遮罩输入为条件，为此引入了新的卷积图像编码器

3.最后，在推理时，应用无分类器指导(CFG/classifier-free guidance)将样本偏向特定条件，在本例中为文本提示，CFG在文本条件和非条件模型预测之间进行插值，以确保生成的图像与文本引导图像修复的输入文本提示之间的强对齐，类似遵循Imagen Video并使用高引导权重和引导振荡(在引导权重值范围内振荡的引导计划)，在基础模型(stage-1 64x diffusion)中，确保与文本的强对齐最为关键，使用在1到30之间振荡的指导权重计划，观察到高指导权重与振荡指导相结合会产生最佳效果，可以达到样本保真度和文本图像对齐之间的权衡

用于文本引导图像修复评估的EditBench数据集包含240张图像，其中120张合成图像和120张自然图像，生成的图像由Parti合成，自然图像来自Visual Genome和Open Images数据集

EditBench捕获各种语言、图像类型和文本提示级别特性(即简单、丰富和完整的字幕)，每个示例都包含 ，遮罩的输入图像，输入文本提示，以及用作自动度量参考的高质量输出图像

为了深入了解不同模型的相对优势和劣势，EditBench旨在测试三个类别的细粒度细节：属性(例如材料、颜色、形状、尺寸、数量)；  对象类型(例如，常见、稀有、文本渲染)；  场景(例如，室内、室外、写实或绘画)

为了解不同的提示规范如何影响模型性能，提供了三种文本提示类型：单一属性(Mask Simple)或蒙版对象的多属性描述(Mask Rich)以及整张图像的描述(Full Image)，Mask Rich特别探讨了模型处理复杂属性绑定和包含的能力

<img src="https://img.saraba1st.com/forum/202306/10/060420ce4nwes5qs8o2n28.png" referrerpolicy="no-referrer">

<strong>image1.png</strong> (176.56 KB, 下载次数: 0)

下载附件

2023-6-10 06:04 上传

由于现有的TGIE自动评估指标(CLIPScore和CLIP-R-Precision)存在内在缺陷，因此将人工评估作为EditBench的黄金标准，在下面的评估部分中，将演示如何将EditBench应用于模型

评估相关:
 评估了Imagen Editor模型使用对象掩码(IM)和随机掩码(IM-RM)，与可比较的模型，Stable Diffusion(SD) 和DALL-E 2(DL2)，Imagen Editor在所有EditBench评估类别中都以可观的优势优于这些模型

对于完整图像提示，单图像人工评估提供二则(是或者否)答案以确认图像是否与说明相匹配，对于Mask Simple提示，单图像人工评估确认对象和属性是否正确呈现并正确绑定(例如，对于红猫，红色桌子上的白猫将是不正确的绑定)，并排人类评估仅使用Mask Rich提示来并排比较IM和其他三个模型(IM-RM、DL2 和 SD)中的每一个，并指出哪个图像与文本说明更匹配(图像对齐)，以及哪个图像最逼真

<img src="https://img.saraba1st.com/forum/202306/10/061414y0g9kz951982s02s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-061338.jpg</strong> (372.22 KB, 下载次数: 0)

下载附件

2023-6-10 06:14 上传

相关评估:

<img src="https://img.saraba1st.com/forum/202306/10/061315xedzowssnfch7h7e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-061248.jpg</strong> (820.03 KB, 下载次数: 0)

下载附件

2023-6-10 06:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 517#       发表于 2023-6-10 07:24

Whisper Web

在浏览器中使用whisper模型进行语音识别

项目地址:https://huggingface.co/spaces/Xenova/whisper-web

使用Transformers.js v2.2.0构建，支持多语言语音识别

注:使用时会自动加载whisper模型

<img src="https://img.saraba1st.com/forum/202306/10/072347cn6cwczy9wzzwaoz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-072129.jpg</strong> (67.71 KB, 下载次数: 0)

下载附件

2023-6-10 07:23 上传

<img src="https://img.saraba1st.com/forum/202306/10/072347tuxcwxmzukbx7ass.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-072155.jpg</strong> (53.46 KB, 下载次数: 0)

下载附件

2023-6-10 07:23 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 518#       发表于 2023-6-10 07:33

 本帖最后由 Machinery 于 2023-6-10 07:38 编辑 

MIMIC-IT

多模态上下文指令微调

项目主页:https://otter-ntu.github.io/

github项目代码&amp;数据库:https://github.com/Luodian/otter

高质量的指令和响应对于大型语言模型在交互式自然语言任务中的零样本性能至关重要，对于涉及复杂视觉场景的交互式视觉语言任务，必须使用大量多样化和创造性的指令-响应对来调整视觉语言模型(VLM)

然而目前视觉-语言指令对数据集，在数量、多样性和创造力方面的可用性仍然有限，这对交互式VLM的推广提出了挑战

本文展示了多模态上下文指令调整数据集MIMIC-IT，这是一个包含280万个多模态指令对的数据集，其中有220万个来自图像和视频的独特指令

每对都伴随着多模态上下文信息，形成旨在增强VLM感知、推理和规划能力的会话上下文，使用被称为Syphus的指令-响应收集过程使用自动标注工作流进行扩展，该工作流将人类专业知识与GPT的功能相结合

使用 MIMIC-IT数据集训练了一个名为Otter的大型 VLM，基于对视觉语言基准进行的广泛评估，观察到Otter在多模态感知、推理和上下文学习方面表现出非凡的熟练程度

人工评估也表明它能有效地符合用户的意图，研究组发布了MIMIC-IT数据集、指令-响应收集工作流代码、基准测试和Otter模型

<img src="https://img.saraba1st.com/forum/202306/10/073317p82d2wa21fb2q12c.png" referrerpolicy="no-referrer">

<strong>68747470733a2f2f692e706f7374696d672e63632f52434770307651312f7379706875732e706e67.png</strong> (207.03 KB, 下载次数: 0)

下载附件

2023-6-10 07:33 上传

Syphus工作流框架

<img src="https://img.saraba1st.com/forum/202306/10/073330dok1q1vc271rx7by.png" referrerpolicy="no-referrer">

<strong>68747470733a2f2f692e706f7374696d672e63632f434b6751325050372f6f747465722d74656173.png</strong> (88.19 KB, 下载次数: 0)

下载附件

2023-6-10 07:33 上传

otter模型微调过程

<img src="https://img.saraba1st.com/forum/202306/10/073241fkpjukibhdn8n4nn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-073207.jpg</strong> (246.3 KB, 下载次数: 0)

下载附件

2023-6-10 07:32 上传

<img src="https://img.saraba1st.com/forum/202306/10/073241xzf2up2f7ubnx4pf.jpg" referrerpolicy="no-referrer">

<strong>62d02a8c-8501-49b7-a5c5-b18135654163.jpg</strong> (173.44 KB, 下载次数: 0)

下载附件

2023-6-10 07:32 上传

<img src="https://img.saraba1st.com/forum/202306/10/073241p17fsbquu01qzt9f.jpg" referrerpolicy="no-referrer">

<strong>20230610_072504.jpg</strong> (226.6 KB, 下载次数: 0)

下载附件

2023-6-10 07:32 上传

注:模型实验演示效果太震撼了，可以去这里看实验演示(https://twitter.com/JingkangY/status/1667087758840709120?s=19)

<img src="https://img.saraba1st.com/forum/202306/10/073705jwnen5trwwhma9ga.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-073549__01.jpg</strong> (183.55 KB, 下载次数: 0)

下载附件

2023-6-10 07:37 上传

<img src="https://img.saraba1st.com/forum/202306/10/073705g9o0fo9zfo5ozszw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-073647.jpg</strong> (186.27 KB, 下载次数: 0)

下载附件

2023-6-10 07:37 上传

<img src="https://img.saraba1st.com/forum/202306/10/073729ah2m2q0vtxqu87m7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-073715.jpg</strong> (188.09 KB, 下载次数: 0)

下载附件

2023-6-10 07:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 519#       发表于 2023-6-10 08:46

omnimotion

一次性地在任何地方追踪任何事物

项目主页(注:主页中也有复现的演示追踪):https://omnimotion.github.io/

代码:coming soon

<img src="https://img.saraba1st.com/forum/202306/10/084511o5siq5fhz4fx55u5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-084205__01.jpg</strong> (718.37 KB, 下载次数: 0)

下载附件

2023-6-10 08:45 上传

OmniMotion可以追踪视频所有帧的所有点的移动情况，即使是遇到行进遮挡的情况下

本文中提出了一种新的测试时优化方法，用于从视频序列中估计密集和长程运动，先前的光流或粒子视频跟踪算法通常在有限的时间窗口内运行，难以对行进遮挡进行跟踪，以及保证估计运动轨迹的全局一致性

OmniMotion是一个完整且全局一致的运动表示估计方法，允许对视频中的每个像素进行准确的长程运动估计追踪，OmniMotion使用准3D规范体积表征视频，并通过局部空间和标准空间之间的双射执行逐像素跟踪

<img src="https://img.saraba1st.com/forum/202306/10/084536o404tx4j4qzz9xet.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-084213__01.jpg</strong> (246.9 KB, 下载次数: 0)

下载附件

2023-6-10 08:45 上传

这种表征能够确保全局一致性，可以保证在行进遮挡时进行跟踪，并对相机和物体运动的任何组合进行建模，对TAP-Vid基准和真实世界镜头的广泛评估表明，本方法在数量和质量上都**优于现有的最先进方法

<img src="https://img.saraba1st.com/forum/202306/10/083542ih5p8dm5vv786i5p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-083432.jpg</strong> (229.08 KB, 下载次数: 0)

下载附件

2023-6-10 08:35 上传

点可视化，被遮挡的点在估计时标记为“+”

<img src="https://img.saraba1st.com/forum/202306/10/083636oxzxas95kosct55a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-083624.jpg</strong> (191.86 KB, 下载次数: 0)

下载附件

2023-6-10 08:36 上传

由于本方法优化了底层的准3D表征，所以可以提取伪深度可视化来显示场景不同部分的估计相对顺序，图中蓝色为近，红色为远

像许多运动估计方法一样，本方法在处理快速和高度弹性运动以及纤薄结构时也会遇到困难，在这些情况下，成对的计算对应可能无法为本方法提供足够可靠的对应来计算准确的全局运动

评估结果:

<img src="https://img.saraba1st.com/forum/202306/10/084611s45dqo04c47q5c44.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-084259__01__01.jpg</strong> (253.5 KB, 下载次数: 0)

下载附件

2023-6-10 08:46 上传

<img src="https://img.saraba1st.com/forum/202306/10/084611wq9yqqc49o9qhoqz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230610-084230__01.jpg</strong> (726.21 KB, 下载次数: 0)

下载附件

2023-6-10 08:46 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 520#       发表于 2023-6-11 03:35

Aquila

BAAI(北京人工智能研究院)发布的Aquila LM，中英双语开源LLM，有7B和33B两种版本，简介如下

github项目说明页:https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README.md

<img src="https://img.saraba1st.com/forum/202306/11/033442lwiiwuui5rzox05z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-033200.jpg</strong> (447.2 KB, 下载次数: 0)

下载附件

2023-6-11 03:34 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 521#       发表于 2023-6-11 04:07

honest_llama

从语言模型中得出真实答案

github代码与评估仓库:https://github.com/likenneth/honest_llama

Inference-Time Intervention(ITI)，这是一种旨在增强大型语言模型(LLM)真实性的技术，ITI 通过在推理过程中改变模型激活来运作，遵循一组跨越有限数量注意力头的方向

这种干预显著提高了LLaMA模型在TruthfulQA基准上的性能，在使用Alpaca的指令微调LLaMA上，ITI提高了其真实性，从32.5%到65.1%

鉴于真实性和乐于助人之间的差异，展示了如何通过调整干预强度来平衡两者，ITI微创且计算成本低，此外，本方法也具有数据效率：虽然像RLHF这样的方法需要大量注释，但ITI仅使用几百个示例来定位真实方向

研究结果表明，LLM可能对某事为真的可能性有一个内部表征，即使它们在表面上经常产生虚假信息

<img src="https://img.saraba1st.com/forum/202306/11/035348daoppwxmo59is94i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-035303.jpg</strong> (73.98 KB, 下载次数: 0)

下载附件

2023-6-11 03:53 上传

没有(红色)和有(蓝色)ITI干预情况下，LLaMA 对同一问题的回答，标准答案分别是：“学者们认为地球是圆的”和“我没有意见”，第一个问题反映了一个普遍的误解； 第二个是幻觉的例子

<img src="https://img.saraba1st.com/forum/202306/11/035847dnkjlcqqqlh05zcz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-035826.jpg</strong> (82.34 KB, 下载次数: 0)

下载附件

2023-6-11 03:58 上传

(A)是LLaMA-7B模型在验证集上的线性探测器精度，按行排序,颜色越深代表精度越高，50%是随机猜测的基线精度，这个图展示了LLaMA-7B模型不同层级和不同头部的线性探测器在验证集上的精度
(B)是LLaMA-7B模型第14层的第18个注意力头的激活值在投影到前两个真实方向后的核密度估计图，蓝色代表真实的QA对，橙色代表错误的QA对，上面和右边显示的是边际分布

相关评估结果:

<img src="https://img.saraba1st.com/forum/202306/11/040501hr77jsgkg011xsek.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-040416.jpg</strong> (232.35 KB, 下载次数: 0)

下载附件

2023-6-11 04:05 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 522#       发表于 2023-6-11 04:21

CodeVQA

通过代码生成进行模块化的视觉问答

github项目代码库:https://github.com/sanjayss34/codevqa

提出了一个框架，将视觉问题回答制定为模块化的代码生成，与之前关于VQA模块化方法的工作相比，本方法不需要额外的训练，也不依赖于预训练语言模型 (LM)、在图像和描述对上预训练的视觉模型，以及用于上下文学习的50个VQA示例

生成的Python程序使用算术和条件逻辑调用和组合可视化模型的输出，与不使用代码生成的少样本基线相比，本方法将COVR数据集的准确性提高了3%，在 GQA数据集上的准确性提高了大约 2%

<img src="https://img.saraba1st.com/forum/202306/11/041714z9lcvrvxrwzcvrzq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-041039.jpg</strong> (115 KB, 下载次数: 0)

下载附件

2023-6-11 04:17 上传

<img src="https://img.saraba1st.com/forum/202306/11/041714x3b99do3u6d7pq1d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-041052.jpg</strong> (138.13 KB, 下载次数: 0)

下载附件

2023-6-11 04:17 上传

<img src="https://img.saraba1st.com/forum/202306/11/041714hn3vlc9vq21rr39c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-041200.jpg</strong> (87.61 KB, 下载次数: 0)

下载附件

2023-6-11 04:17 上传

CodeVQA概述，CodeVQA首先使用上下文示例提示 Codex，这些示例将给定问题分解为Python代码，通过仅使用问题，Codex生成一个可执行程序，该程序使用条件逻辑、算术等组成预定义的视觉模块

视觉模块进行查询，通过为图像添加标题并使用LM 根据标题回答问题来回答问题，get_pos检索对象的位置，在这里，CodeVQA将问题正确识别为查询和空间比较的结合，并得出正确答案

<img src="https://img.saraba1st.com/forum/202306/11/042122dqkemmnqmop88e8v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-041948.jpg</strong> (145.47 KB, 下载次数: 0)

下载附件

2023-6-11 04:21 上传

<img src="https://img.saraba1st.com/forum/202306/11/042122emfm7k9ekeerig6i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-042039.jpg</strong> (72.75 KB, 下载次数: 0)

下载附件

2023-6-11 04:21 上传

<img src="https://img.saraba1st.com/forum/202306/11/042122v1ktkl2ft8f9tdik.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-042101.jpg</strong> (241.9 KB, 下载次数: 0)

下载附件

2023-6-11 04:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 523#       发表于 2023-6-11 04:39

 本帖最后由 Machinery 于 2023-6-11 04:41 编辑 

Matte Anything

一种交互式自然图像抠图模型，可以生成带有各种简单提示的高质量alpha透明通道遮罩

github项目代码仓库:https://github.com/hustvl/Matte-Anything

自然图像抠图算法旨在通过trimap指导预测透明度图(alpha-matte)然而，trimaps的制作往往需要大量的劳动力，这限制了matting算法的大规模广泛应用

<img src="https://img.saraba1st.com/forum/202306/11/043902vgy4iizvgovicgzb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-043723.jpg</strong> (100.71 KB, 下载次数: 0)

下载附件

2023-6-11 04:39 上传

【所谓Trimap，就是将图像中的像素归为3类:确定的前景、确定的背景和未知区域，未知区域中的像素既受到前景像素的影响，也受到背景像素的影响】

<img src="https://img.saraba1st.com/forum/202306/11/043908w3ml4amr776327ni.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-043739.jpg</strong> (38.12 KB, 下载次数: 0)

下载附件

2023-6-11 04:39 上传

为了解决这个问题，本文提出了Matte Anything模型 (MatAny)，这是一种交互式自然图像抠图模型，可以产生具有各种简单提示的高质量alpha-matte

MatAny的关键是通过轮廓和透明度预测自动生成Trimap，利用特定于任务的视觉模型来增强自然图像抠图的性能

<img src="https://img.saraba1st.com/forum/202306/11/043914u3a44aycnnncmcmj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-043751.jpg</strong> (86.16 KB, 下载次数: 0)

下载附件

2023-6-11 04:39 上传

具体来说，使用segment anything模型来预测具有用户交互的高质量轮廓，并使用开放词汇(OV)检测器来预测任何对象的透明度，随后，预训练图像抠图模型生成带有伪Trimap的alpha抠图

<img src="https://img.saraba1st.com/forum/202306/11/043920c1cu0gz1w5hwugxc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-043802.jpg</strong> (230.73 KB, 下载次数: 0)

下载附件

2023-6-11 04:39 上传

MatAny是迄今为止交互方式支持最多、性能最好的交互式抠图算法，它由无需任何额外训练的视觉模型组成，针对几种当前的图像抠图算法评估了MatAny的性能，结果证明了本方法的巨大潜力

<img src="https://img.saraba1st.com/forum/202306/11/043651npj7dfrpaim7iii7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-043559__01.jpg</strong> (72.97 KB, 下载次数: 0)

下载附件

2023-6-11 04:36 上传

<img src="https://img.saraba1st.com/forum/202306/11/043651r7228ekflb36zbbc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-043613.jpg</strong> (72.2 KB, 下载次数: 0)

下载附件

2023-6-11 04:36 上传

<img src="https://img.saraba1st.com/forum/202306/11/043651ro9w6lzqwbzstllq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-043542.jpg</strong> (122.79 KB, 下载次数: 0)

下载附件

2023-6-11 04:36 上传

<img src="https://img.saraba1st.com/forum/202306/11/043651twqozxtxbq99pzqq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-043525.jpg</strong> (158.58 KB, 下载次数: 0)

下载附件

2023-6-11 04:36 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 524#       发表于 2023-6-11 05:02

Tülu

骆驼能走多远？探索开源指令调整数据集的情况

github项目仓库:https://github.com/allenai/open-instruct

在这项工作中，探索了一系列开放指令跟随数据集上指令调整语言模型的最新进展，尽管最近声称开放模型可以与最先进的专有模型相提并论，但这些声明通常伴随着有限的评估，因此难以全面比较模型并确定各种资源的效用

本文提供了大量参数从6.7B到65B的指令调优模型，在12个指令数据集上进行训练，范围从手动策划(例如OpenAssistant)到合成和蒸馏(例如Alpaca)，并根据它们的事实知识系统地评估它们、推理、多语言、编码和开放式指令遵循能力，通过一系列自动、基于模型和基于人类的指标，进一步介绍Tülu

实验表明，不同的指令调整数据集可以发现或增强特定技能，而没有一个数据集(或组合)在所有评估中提供最佳性能。有趣的是，研究发现基于模型和人类偏好的评估未能反映基于基准的评估所暴露的模型能力的差异，这表明需要在这项工作中进行系统评估

评估表明，任何给定评估中的最佳模型平均能够达到ChatGPT性能的83%和GPT-4性能的68%，这表明需要进一步构建更好的基础模型和指令调整数据以缩小差距

发布了相关的指令调整模型，包括完全微调的65BTülu，以及相关代码、数据和评估框架

<img src="https://img.saraba1st.com/forum/202306/11/045224ylt5ko44zd4o3o2p.jpg" referrerpolicy="no-referrer">

<strong>20230611_045208.jpg</strong> (96.86 KB, 下载次数: 0)

下载附件

2023-6-11 04:52 上传

研究了12个以不同方式创建的代表性指令数据集，对包括OPT、Pythia和LLaMa(7B-65B)在内的每个模型及其组合进行了微调，然后使用基准测试、GPT4和人类评估进行了综合征的系统评估

<img src="https://img.saraba1st.com/forum/202306/11/045325k9fqpkc7qvjvh6jv.jpg" referrerpolicy="no-referrer">

<strong>20230611_045318.jpg</strong> (210.99 KB, 下载次数: 0)

下载附件

2023-6-11 04:53 上传

最佳结果是......混合，不同的数据集似乎可以提升特定技能，但是没有一个数据集能在所有评估中提供最佳性能，但是如果您组合最好的数据集，您会看到最好的整体性能

<img src="https://img.saraba1st.com/forum/202306/11/045437tgsiocdxysmcmdsv.jpg" referrerpolicy="no-referrer">

<strong>20230611_045424.jpg</strong> (41.99 KB, 下载次数: 0)

下载附件

2023-6-11 04:54 上传

评估表明LLaMa基础模型在指令调优后确实优于 OPT/Pythia，没有好的基础模型，指令调优就无法走得太远

现在让我们使用我们拥有的最佳模型+数据来追逐 ChatGPT，Tülu，名称来自一种不同骆驼杂交的杂交骆驼，Tülu包含7-65B的模型，这些模型是使用LLaMa在7个数据集的组合上全参数微调获得的

指令调优**改进了vanilla LLaMa模型，特别是Tülu 65B，研究组构建的最强大的模型，它优于较小规模的模型或在单个数据集上训练的模型，尽管如此，它与ChatGPT仍有可见差距

也许自动指标无法完全通过模型来衡量模型的生成，进一步与18位专家对4个模型的332个提示进行了人工评估，结果证实了更大模型、更多样化数据以及ChatGPT卓越性能的优势

总的来说，研究结果证实了构建更好的基础模型和收集更多样化的指令数据集的必要性，无论是为了提高特定技能还是为了调整模型行为，虽然已经做的够多了，但依然需要努力

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 525#       发表于 2023-6-11 19:40

 本帖最后由 Machinery 于 2023-6-11 19:42 编辑 

SlimPajama

RedPajama预训练数据集的627B Token清理和去重版本

项目博客:https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama

SlimPajama 627B数据集:https://huggingface.co/datasets/cerebras/SlimPajama-627B

<img src="https://img.saraba1st.com/forum/202306/11/194019a7znw96o668jno55.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-192723.jpg</strong> (107.36 KB, 下载次数: 0)

下载附件

2023-6-11 19:40 上传

SlimPajama，最大的广泛去重、多语料库、开源数据集，用于训练大型语言模型，SlimPajama是通过清理和删除来自Together的1.21T Token RedPajama数据集创建的

通过过滤低质量数据和重复数据，能够删除约49.6%的字节，将数据集大小从1210B减少到627B，相信SlimPajama可以提供高质量和高效的计算运行效率来训练627B Token，当进行上采样时，预计SlimPajama在以万亿Token规模进行训练时的性能等于或优于RedPajama-1T

除了数据之外，还将发布为创建SlimPajama而构建的工具，使用现成的开源代码无法将MinHashLSH重复数据删除方案应用于RedPajama等万亿Token数据集

研究组对现有解决方案进行了多项改进，以生成一个基础架构，该基础架构可以以分布式、多线程和内存高效的方式对万亿Token数据集执行MinHashLSH重复数据删除，同时开源了这一基础套装，使社区能够在未来轻松创建更高质量、广泛删除重复数据的数据集

1.SlimPajama 627B——用于 LLM 培训的最大的广泛去重、多语料库、开放数据集，根据Apache 2.0许可发布

2.发布验证和测试集，每个500M Token，已针对训练数据进行去污染

3.从头开始复制或预处理其他数据集的方法库，目前来说，这些是第一个能够以万亿令牌规模对文本数据进行清理和MinHashLSH去重的开源工具

最新的研究(Penedo et al. 2023)表明，数据质量和数据数量一样重要，虽然训练多个epoch的数据可以有利于提高效果，但这应该是有选择地进行，而不是数据集中重复数据的副作用的效果

因此研究组决定深入地对RedPajama数据集进行重复数据删除，以产生信息密度更高的数据集，这意味着，使用SlimPajama数据集时，与其他数据集相比，您可以在相同的计算预算下获得更高的准确率

<img src="https://img.saraba1st.com/forum/202306/11/194026zp8td57d1kkjkz17.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230611-193017.jpg</strong> (57.29 KB, 下载次数: 0)

下载附件

2023-6-11 19:40 上传

SlimPajama数据集预处理工作流

预处理工作流由多个阶段组成，例如NFC重整化、清理、去除重复数据、文档交错、文档混和、切分为训练集和保持集、训练集与保持集的重复数据删除，所有这些步骤都已在图中显示，可以使用位于github的脚本(地址:https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama)执行序列级混组和上采样，这里的所有步骤都假设整个数据集无法容纳在可用的RAM中并分布在多个流程中

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 526#       发表于 2023-6-12 02:54

 本帖最后由 Machinery 于 2023-6-12 02:57 编辑 

The kitchen sink model(厨房水槽模型)

通过神经核表面重建重建高保真数字孪生资产

项目博客:https://developer.nvidia.com/blog/recreate-high-fidelity-digital-twins-with-neural-kernel-surface-reconstruction/

<img src="https://img.saraba1st.com/forum/202306/12/025345zk9gmtduj0d33rtb.jpg" referrerpolicy="no-referrer">

<strong>953b5255-127c-42e3-a3b4-886543d010bb.jpg</strong> (35.56 KB, 下载次数: 0)

下载附件

2023-6-12 02:53 上传

从点云重建光滑表面是创建真实世界对象和场景的数字孪生的基本步骤，表面重建算法出现在各种应用中，例如工业模拟、视频游戏开发、建筑设计、医学成像和机器人技术。 

神经核表面重建(NKSR)是NVIDIA的新算法，用于从大型点云重建高保真表面，NKSR可以在几秒钟内处理数百万个点，并在广泛的基准测试中达到最先进的质量，NKSR是传统泊松曲面重建的绝佳替代品，可提供更多细节和更快的运行时间

NKSR利用一种称为神经核场的新型3D深度学习方法来实现高质量的重建。神经核场由NVIDIA多伦多人工智能实验室于2022年首次推出，它预测了一组依赖于数据的基函数，用于解决封闭形式的表面重建问题，这种新方法实现了前所未有的泛化(用于训练对象和重建场景(以及对不同规模的场景和对象进行多模态训练。有关该方法的更多技术细节，请访问项目页面(https://research.nvidia.com/labs/toronto-ai/NKSR/)

除了代码发布，研究组还将推出The kitchen sink model(厨房水槽模型)，这是一个在不同规模的数据集上训练的综合模型，通过合并对象级和场景级数据，确保了模型在不同场景中的通用性，为了证明其有效性，NVIDIA已成功地将厨房水槽模型应用于各种数据集

<img src="https://img.saraba1st.com/forum/202306/12/025539ich7et87knncf2to.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-025511.jpg</strong> (112.61 KB, 下载次数: 0)

下载附件

2023-6-12 02:55 上传

<img src="https://img.saraba1st.com/forum/202306/12/025539fzgsuhypynpsypyg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-025517.jpg</strong> (218.48 KB, 下载次数: 0)

下载附件

2023-6-12 02:55 上传

显示了使用稀疏输入点云的房间级重建结果，本方法生成的平滑和准确的几何图形优于其他基线结果

<img src="https://img.saraba1st.com/forum/202306/12/025419qjumuulmqnrcppjr.jpg" referrerpolicy="no-referrer">

<strong>de869f8e-8cac-4e11-b2ec-47cfaae8b137.jpg</strong> (31.02 KB, 下载次数: 0)

下载附件

2023-6-12 02:54 上传

<img src="https://img.saraba1st.com/forum/202306/12/025419tsbyspyekdhigszf.jpg" referrerpolicy="no-referrer">

<strong>ec1e8d7d-7ead-4f7e-90e1-362d66235bc3.jpg</strong> (53.43 KB, 下载次数: 0)

下载附件

2023-6-12 02:54 上传

展示了本方法在赛道上的应用，以及周边场景，这些场景是使用配备激光雷达传感器的自动驾驶汽车拍摄的，这两个场景都跨越几公里长，但依然能够在GPU上高效地处理它们

<img src="https://img.saraba1st.com/forum/202306/12/025724xjza5w1wwoqzmwnw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-025247__01.jpg</strong> (588.66 KB, 下载次数: 0)

下载附件

2023-6-12 02:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 527#       发表于 2023-6-12 03:56

HQ-50K

用于图像恢复的大规模、高质量数据集

github项目仓库:https://github.com/littleYaang/HQ-50K

本文介绍了一个新的大规模图像恢复数据集，称为 HQ-50K，包含五万张张具有丰富纹理细节和语义多样性的高质量图像

从五个不同的角度分析了现有的图像恢复数据集，包括数据规模、分辨率、压缩率、纹理细节和语义覆，然而，所有这些数据集在某些方面都存在缺陷，相比之下，HQ-50K在数据管理与构造过程中考虑了所有这五个方面并满足所有要求

还提出了一种新的退化感知混合专家(DAMoE)模型，使单个模型能够处理多种损坏类型和未知级别，广泛的实验表明，HQ-50K在各种图像恢复任务中(例如超分辨率、去噪、去除块效应、去雨痕等

在HQ-50K上训练的DAMoE，优于现有的为多个恢复任务和级别设计的最先进的统一模型

<img src="https://img.saraba1st.com/forum/202306/12/034058m5mf996v6zmv76v6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-034010.jpg</strong> (82.98 KB, 下载次数: 0)

下载附件

2023-6-12 03:40 上传

HQ-50K与现有恢复数据集的五个方面的比较，本文数据集同时考虑了所有五个方面，而现有数据集在某些方面总是存在缺陷

<img src="https://img.saraba1st.com/forum/202306/12/034357hzyjj0unnfyj3cnc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-034312__01.jpg</strong> (416.6 KB, 下载次数: 0)

下载附件

2023-6-12 03:43 上传

数据集对比与DAMoE框架结构

<img src="https://img.saraba1st.com/forum/202306/12/034744n9dglhoxv0fokgw0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-034728.jpg</strong> (101.46 KB, 下载次数: 0)

下载附件

2023-6-12 03:47 上传

(a)H-MoE层将任务特定的专家应用于不同的任务，对于一项任务，不同级别的退化(degradation)等级共享同一个专家
(b)S-MoE层通过门控网络处理多任务输入，这意味着所有专家对所有任务都是可见的

<img src="https://img.saraba1st.com/forum/202306/12/035543w7vqqy6veoococqo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-035249__01.jpg</strong> (375.22 KB, 下载次数: 0)

下载附件

2023-6-12 03:55 上传

<img src="https://img.saraba1st.com/forum/202306/12/035543vs2salmwzooizk0q.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-035413__01.jpg</strong> (406.11 KB, 下载次数: 0)

下载附件

2023-6-12 03:55 上传

<img src="https://img.saraba1st.com/forum/202306/12/035543nkoloz6l8vl6l5ol.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230612-035421__01.jpg</strong> (292.88 KB, 下载次数: 0)

下载附件

2023-6-12 03:55 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 528#       发表于 2023-6-13 02:52

Mind2Web

迈向通用网络代理浏览者，自行互联网机器

项目主页:https://osu-nlp-group.github.io/Mind2Web/

训练数据集:https://huggingface.co/datasets/osunlp/Mind2Web

github项目代码仓库:https://github.com/OSU-NLP-Group/Mind2Web

Mind2Web是一个用于开发和评估网络通用代理浏览者的数据集，它可以按照语言说明在任何网站上完成复杂任务，Mind2Web包含来自31个领域的137个网站的2350个任务

可以反映网络上多样化和实际的用例
通过真实世界的网站提供具有挑战性但逼真的环境
测试跨任务和环境的泛化能力

现有的网络代理浏览者数据集要么使用模拟网站，要么只涵盖一组有限的网站和任务，因此不适合通用网络代理者，基于Mind2Web，对使用大型语言模型(LLM)构建通用网络代理浏览者进行了初步探索

虽然现实世界网站的原始HTML通常太大而无法提供给LLM当做上下文，但可以通过首先用一个小的LM过滤，就可以显著提高LLM的有效性和效率

以前的解决方案展示了不错的性能水平，即使部分工作可以泛化到从见网站或整个领域上，但仍有很大的改进空间以实现真正的可泛化代理浏览

通过平衡任务和网站分布可以更好的测试不同级别的泛化能力，大致分为三种:
Cross Task Generalization
Cross Website Generalization 
Cross Domain Generalization

对于每项任务，提供以下信息：
Task Description/用自然语言句子描述任务

Action Sequence/描述完成任务要执行的操作顺序

每个动作都是一对(Operation, Target Element)，其中Target Element网页中用户选择与之交互的元素， 是Operation要对该元素执行的动作

支持四种常见操作:Click, Hover, Type and Select

Webpage Snapshots作为环境提供不同格式的快照：
MHTML包含网页的原始HTML代码
DOM Snapshot包含带有DOM、布局和样式信息的快照
Image包含网页的屏幕截图
HAR包含用于重现的所有网络流量
Trace包含注释的完整交互跟踪

数据是通过Amazon Mechanical Turk(AMT)平台收集的，分三个阶段完成：
第1阶段，任务提案:首先要求工作人员提出可以在给定网站上执行的任务，之后手动审查提出的任务，并选择可行且有趣的任务用于第2阶段的注释
第2阶段，任务演示:要求工作人员在网站上演示如何执行任务，研究组与Playwright(https://playwright.dev/)一起开发了一个注释工具，它可以记录交互轨迹并在每个步骤拍摄网页快照
第3阶段，任务验证:作者验证了所有任务，以确保所有操作都是干净的，并且任务描述正确地反映了带注释的操作

<img src="https://img.saraba1st.com/forum/202306/13/025206x1z3q7b132tobzqw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-023524__01.jpg</strong> (788.92 KB, 下载次数: 0)

下载附件

2023-6-13 02:52 上传

<img src="https://img.saraba1st.com/forum/202306/13/025206s3y93323a3z1cm8i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-023524__02.jpg</strong> (451.84 KB, 下载次数: 0)

下载附件

2023-6-13 02:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 529#       发表于 2023-6-13 03:21

svg llm

利用大型语言模型进行可扩展矢量图形驱动的图像理解

相关论文:https://arxiv.org/abs/2306.06094

github项目仓库(待整理，之后应该会包含代码模型与数据集):https://github.com/mu-cai/svg-llm

最近，大型语言模型(LLM)在自然语言理解和生成方面取得了重大进展，然而，它们在计算机视觉方面的潜力在很大程度上仍未被开发

在本文中，介绍了一种新的探索性方法，使LLM能够使用可缩放矢量图形(SVG)格式处理图像，通过利用SVG表示的基于XML的文本描述而不是光栅图像，弥合视觉和文本模式之间的差距，允许 LLM 直接理解和操作图像，而无需参数化视觉组件

本方法仅使用LLM就可以促进简单的图像分类、生成和上下文学习，展示了本方法在判别和生成任务中的前景，强调其对分布变化的鲁棒性，通过利用LLM的上下文学习能力实现的实质性改进，以及在人类指导下的图像理解和生成能力

<img src="https://img.saraba1st.com/forum/202306/13/031345v4y4hyeghrdgwysq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-031243__01.jpg</strong> (244.01 KB, 下载次数: 0)

下载附件

2023-6-13 03:13 上传

(a)展示了一个SVG表示，一个高尔夫球场，SVG代码中的每个几何形状代表二维图形中的不同对象或线条，例如，红色多边形代表图形图像中的旗帜
(b)提供了一个示例，表明 LLM 能够以交互方式理解和生成不同元素之间的形状、颜色和关系

<img src="https://img.saraba1st.com/forum/202306/13/031832mp5cc3j7r3cg77vl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-031814.jpg</strong> (133.68 KB, 下载次数: 0)

下载附件

2023-6-13 03:18 上传

使用LLM的SVG生成进行的上下文学习和图像生成功能:
(a)随着人类的反馈，LLM 逐渐在数字分类上表现更好
(b)LLM为SVG提供图像识别和参考分割的能力
(c)通过人工反馈，内容生成性能变得更好
(d)LLM可以识别和操作汉堡包的特定部分，例如移除或替换它们

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 530#       发表于 2023-6-13 03:45

mm-ovod

开放词汇对象检测(OVOD/open-vocabulary object detection)，构建一个模型，可以检测超出训练中所见类别集的对象，从而使用户能够在推理时指定感兴趣的类别，而无需重新训练模型

项目主页:https://www.robots.ox.ac.uk/~vgg/research/mm-ovod/

github项目仓库(coming soon):https://www.robots.ox.ac.uk/~vgg/research/mm-ovod/

本文采用标准的两阶段对象检测器架构，并探索三种指定新类别的方法：通过语言描述、通过图像示例或通过两者的组合

首先，提示大型语言模型(LLM)为对象类生成信息丰富的语言描述，并构建强大的基于文本的分类器

其次，在图像模态上使用视觉聚合器，可以摄取任意数量的图像作为输入，形成基于视觉的分类器

最后，提供了一种简单的方法来融合来自语言描述和图像样本的信息，从而产生多模态分类器

在评估具有挑战性的LVIS开放词汇基准时，实验证明
1.本文基于文本的分类器优于所有以前的OVOD作品
2.基于视觉的分类器在之前的工作中表现与基于文本的分类器一样好
3.使用多模态分类器比单独使用任何一种模态表现更好

<img src="https://img.saraba1st.com/forum/202306/13/032958nsx05om89mo5393o.jpg" referrerpolicy="no-referrer">

<strong>d389a157-d667-4f97-9945-4168c6ca0eb6.jpg</strong> (185.64 KB, 下载次数: 0)

下载附件

2023-6-13 03:29 上传

模型构架
开放词汇对象检测(OVOD)通过提供合适的分类向量(分类器)，使感兴趣的类别能够在推理时由用户定义

本方法支持三种方式来提供此类分类器，在指定感兴趣的类别时，可以使用三种方法中的任何一种来执行开放式词汇对象检测:
1. 冻结的大型语言模型(LLM)被提示用来提供对感兴趣类别的视觉描述，使用视觉语言模型(VLM)的文本编码器为其生成embedding(例如CLIP模型)，这些embedding的向量平均值的OVOD产生了一个基于文本的分类器(图中右上)

2. 给定感兴趣类别的图像样本，来自VLM的图像编码器为每个样本生成embedding，训练有素的基于Transformer的视觉聚合器将多个embedding组合到一个用于OVOD的基于视觉的分类器中(图中左上)

3. 将视觉和文本这两种模式结合起来，添加基于L2正则化文本和基于视觉的分类器，产生了OVOD多模态分类器(中间的+)

<img src="https://img.saraba1st.com/forum/202306/13/033817c3dek8j6bepl866d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-033652.jpg</strong> (167.98 KB, 下载次数: 0)

下载附件

2023-6-13 03:38 上传

LVIS Open-Vocabulary对象检测基准的结果，LVIS定义的稀有类(APr)和跨所有类(mAP)的性能，(Extra Data: ✗)为没有使用额外数据的情况下

<img src="https://img.saraba1st.com/forum/202306/13/034200haoocreek46skezy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-034143.jpg</strong> (101.18 KB, 下载次数: 0)

下载附件

2023-6-13 03:42 上传

消融实验，比较了所有三种方法在LVIS OVOD基准上的检测性能：
(1)橙色——基于视觉的分类器
(2)蓝色——基于文本的分类器
(3)灰色——多模态分类器

分别在上表的底部和顶部显示了使用和不使用额外图像级数据训练的模型的结果，在除一种情况外的所有情况下，添加视觉聚合器都可以改进OVOD性能

<img src="https://img.saraba1st.com/forum/202306/13/034525fnltbnhnejztkbaa.jpg" referrerpolicy="no-referrer">

<strong>b2ab454d-628c-4559-b8fd-bda1bd72eec0.jpg</strong> (523.56 KB, 下载次数: 0)

下载附件

2023-6-13 03:45 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 531#       发表于 2023-6-13 04:06

FinGPT

开源金融大语言模型

相关论文:https://arxiv.org/abs/2306.06031

github FinGPT项目仓库:https://github.com/AI4Finance-Foundation/FinGPT

github自然语言处理金融新闻信息示范仓库:https://github.com/AI4Finance-Foundation/FinNLP

大型语言模型(LLM)显示出在不同领域彻底改变自然语言处理任务的潜力，引发了人们对金融处理的极大兴趣，访问高质量的财务数据是金融LLM(FinLLM)面临的第一个挑战

虽然像BloombergGPT这样的专有模型已经利用了其独特的数据积累，但这种特权访问需要一种开源替代方案来使互联网规模的金融数据普及化

在本文中，提出了一种用于金融领域的开源大型语言模型FinGPT，与专有模型不同，FinGPT采用以数据为中心的方法，为研究人员和从业者提供可访问且透明的资源来开发他们的FinLLM

本文强调了自动数据管理工作流和轻量级低秩自适应(Lora)技术在构建FinGPT中的重要性，此外还展示了几个潜在的应用程序作为用户的灵感源，例如机器人建议、算法交易和低代码开发

<img src="https://img.saraba1st.com/forum/202306/13/040557s2j011rt0jqn1rv7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-040341.jpg</strong> (483.15 KB, 下载次数: 0)

下载附件

2023-6-13 04:05 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 532#       发表于 2023-6-13 04:20

 本帖最后由 Machinery 于 2023-6-13 04:21 编辑 

Corr2Cause

大型语言模型可以从相关性推断因果关系吗？

github项目代码仓库:https://github.com/causalNLP/corr2cause

hugface Corr2Cause数据集下载:https://huggingface.co/datasets/causalnlp/corr2cause

因果推理是人类智能的标志之一，虽然CausalNLP(因果自然语言处理)领域近年来引起了很多兴趣，但目前NLP中现有的因果推理数据集主要依赖于从经验知识(例如常识知识)中发现因果关系

在本文中提出了第一个基准数据集来测试大型语言模型(LLM)的纯因果推理技能，具体来说，制定了一个新任务Corr2Cause，它采用一组相关语句并确定变量之间的因果关系

策划了构造一个包含超过40万个样本的大规模数据集，在这些数据集上评估了17个现有的LLM之后

通过实验，确定了目前LLM在因果推理技能方面的一个关键缺点，并表明这些模型在任务上只实现了几乎接近随机效果的实际性能

当我们尝试通过微调将LLM重新用于此技能时，这个缺点有所缓解，但我们发现这些模型仍然无法泛化，LLM使用变量名和文本表达式执行因果推理时这些查询与训练集中的查询分布类似，但在对抗的扰动测试中这些查询生成的分布外挑战失败

Corr2Cause对LLM来说是一项具有挑战性的任务，将有助于指导未来关于提高LLM纯推理技能和泛化能力的研究

<img src="https://img.saraba1st.com/forum/202306/13/041440q9iz8n1rkr91tvxi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-041307.jpg</strong> (103 KB, 下载次数: 0)

下载附件

2023-6-13 04:14 上传

<img src="https://img.saraba1st.com/forum/202306/13/041503yld6yw9hg9b6zwdd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-041451.jpg</strong> (211.27 KB, 下载次数: 0)

下载附件

2023-6-13 04:15 上传

整体性能，报告了F1(主要指标)、精度、召回率和准确性，对于主要指标F1分数，使用粗体突出显示整体最佳性能，并使用下划线突出显示每个类别模型中的最佳性能

<img src="https://img.saraba1st.com/forum/202306/13/041712ti67xt6odis8xoi8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230613-041642.jpg</strong> (173.31 KB, 下载次数: 0)

下载附件

2023-6-13 04:17 上传

微调模型后在原始测试集与扰动对抗测试数据集的性能表现

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 533#       发表于 2023-6-14 04:02

I-JEPA

一种学习高度语义化图像表征的方法，而不依赖于手工制作的数据增强的数据集

本文介绍了基于图像的联合嵌入预测架构(I-JEPA/Image-based Joint-Embedding Predictive Architecture)，这是一种用于从图像进行自监督学习的非生成类方法

在I-JEPA以及更普遍的联合嵌入预测架构(JEPA)模型上的工作基于这样一个事实，即人类仅通过被动观察就可以了解有关世界的大量背景知识，据推测，这种常识信息是实现智能行为的关键，例如新概念的样本有效获取、基础知识和规划等

人工智能研究人员试图设计学习算法，捕捉关于世界的常识背景知识，然后将其编码为算法可以稍后访问的数字表征，为了有效，系统必须以自监督的方式学习这些表征，也就是说，直接从图像或声音等未标记的数据中学习，而不是从手动组装的标记数据集中学习

在高层次上，JEPA旨在根据同一输入的其他部分的表示来预测输入的一部分(例如图像或一段文本)的表征，因为它不涉及将图像的多个视图/增强表示折叠到单个点，所以希望JEPA可以避免与另一种广泛使用的方法(称为基于不变性的预训练/invariance-based pretraining)相关的偏差和问题

同时，通过在高抽象层次上预测表示而不是直接预测像素值，希望可以直接学习有用的表示，同时避免最近生成方法的局限性

相比之下，生成式学习架构通过删除或扭曲模型输入的部分内容来学习——例如，擦除照片的一部分或隐藏文本段落中的某些单词，然后他们尝试预测损坏或丢失的像素或单词，然而，生成式方法的一个显著缺点是模型试图填补每一点缺失的信息，即使世界本质上是不可预测的

因此，生成式方法可能容易犯一个人永远不会犯的错误，因为它们过于关注不相关的细节，而不是捕捉高级可预测的概念，例如众所周知的，生成模型很难准确生成人手，以及经常添加额外的数字或犯其他明显的错误

自监督学习的通用架构，其中系统学习捕获输入之间的关系，目标是将高能分配给不兼容的输入，并将低能分配给兼容的输入

<img src="https://img.saraba1st.com/forum/202306/14/034412pnxhtwphtp0zxrb8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-034355.jpg</strong> (51.51 KB, 下载次数: 0)

下载附件

2023-6-14 03:44 上传

(A.)联合嵌入(不变)架构通过学习为兼容的输入x、y输出相似的嵌入，为不兼容的输入输出不同的嵌入

(B.)生成式架构学习则是直接从兼容信号x重建信号y，同时使用附加的(也可能是潜在的)变量z为条件的解码器网络以促进重建

(C.)联合嵌入预测架构学习从兼容信号x预测信号y的嵌入，使用附加的(可能是潜在的)变量z为条件的预测网络来促进预测

I-JEPA背后的想法很简单：从单个内容块(context block)中，预测同一图像中多种目标块(various target blocks)的表征

引导I-JEPA生成语义表征的核心设计是掩码策略，具体来说，至关重要的是对具有足够大规模(语义)的目标块进行采样，以及使用信息量足够大规模(空间分布)的内容块

<img src="https://img.saraba1st.com/forum/202306/14/034753gjdrejev6wfo683z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-034716.jpg</strong> (52.8 KB, 下载次数: 0)

下载附件

2023-6-14 03:47 上传

基于图像的联合嵌入预测架构(I-JEPA)使用单个内容块来预测源自同一图像的各种目标块的表征，内容编码器是一个视觉Transformer(ViT)，它只处理可见的内容补丁，而预测器是一个窄ViT，它以目标的位置标记(在图中以颜色显示)为条件，采用内容编码器输出并预测特定位置的目标块的表征，目标表征对应于目标编码器的输出，其权重在每次迭代时通过内容编码器权重的指数移动平均值进行更新

I-JEPA中的预测器可以看作是一个原始的(和受限的)世界模型，它能够从部分可观察的上下文中模拟静态图像中的空间不确定性，更重要的是，这个世界模型是语义的，因为它预测图像中不可见区域的高级信息，而不是像素级细节

<img src="https://img.saraba1st.com/forum/202306/14/035343hxk777f3f97fkkb9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-035334.jpg</strong> (81.97 KB, 下载次数: 0)

下载附件

2023-6-14 03:53 上传

上图为预测器如何学习对世界的语义建模，对于每张图像，蓝色框外的部分被编码并作为内容提供给预测器，预测器输出它期望在蓝色框内的区域中的表示，为了可视化预测，研究组训练了一个生成模型，该模型生成由预测器输出表征的内容的草图，并在蓝色框中显示样本输出，很明显，预测器识别出了应该填充哪些部分的语义(狗的头顶、鸟的腿、狼的腿、建筑物的另一侧等)

为了了解模型捕获的内容，还训练了一个随机解码器，将I-JEPA预测的表征映射回像素空间，这显示了当被探测以在蓝框中进行预测时模型的输出，这种定性评估表明该模型正确地捕获了位置不确定性并生成了具有正确姿势的高级对象部分，简而言之，I-JEPA能够学习对象部分的高级表征，而不会丢弃它们在图像中的局部位置信息

<img src="https://img.saraba1st.com/forum/202306/14/035710jzhldln9hn99kjkw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-035657.jpg</strong> (81.95 KB, 下载次数: 0)

下载附件

2023-6-14 03:57 上传

更高的效率和强大的性能，I-JEPA预训练在计算上也很高效，它不涉及与应用更多计算密集型数据扩充来生成多个视图相关的任何开销，目标编码器只需要处理图像的一个视图，内容编码器只需要处理内容块

根据经验，发现 I-JEPA 在不使用手工图片增强数据集的情况下学习了强大的现成语义表征，它还在 ImageNet-1K线性探测和半监督评估上优于像素和Token重建方法

<img src="https://img.saraba1st.com/forum/202306/14/035859uldddd4orddozda4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-035845.jpg</strong> (107.44 KB, 下载次数: 0)

下载附件

2023-6-14 03:58 上传

I-JEPA还可以与以前的预训练方法竞争，后者依赖于语义任务上的手工数据增强，与这些方法相比I-JEPA在对象计数和深度预测等低级视觉任务上取得了更好的性能，通过使用更少的刚性归纳偏差的简单模型，I-JEPA可以适用于更广泛的任务集

根据经验，当与Vision Transformers结合使用时，发现I-JEPA具有高度可扩展性，例如，在72小时内使用16个A100 GPU在ImageNet上训练的ViT-Huge/14，在从线性分类到对象计数和深度预测的各种任务中实现强大的下游性能

I-JEPA 展示了学习有竞争力的现成图像表示的架构的潜力，而无需通过手工制作转换的图像编码的额外知识，推进JEPA构架从更丰富的模态中学习更通用的世界模型将特别有趣，例如，使人们能够从短的上下文中对视频中的未来事件进行长期空间和时间预测，并将这些条件预测使用到音频或文字提示上

期待着将JEPA方法扩展到其他领域，例如图像-文本配对数据和视频数据等，将来，JEPA 模型可以在视频理解等任务中有令人兴奋的应用。 这是朝着应用和扩展自监督方法学习通用世界模型迈出的重要一步

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 534#       发表于 2023-6-14 04:36

 本帖最后由 Machinery 于 2023-6-14 04:39 编辑 

UniSim

闭环式自动驾驶传感器环境模拟器

项目主页:https://waabi.ai/unisim/

代码、模型与应用等待整理

严格的自主测试系统对于安全的自动驾驶汽车技术(SD/self-driving vehiclesV)成为现实至关重要，它需要生成超过世界上可以安全收集的安全关键场景，因为许多场景很少发生在实际的道路上

为了准确评估性能，需要在闭环中的这些场景中测试SDV，SDV需要和其他参与者在每个时间步中相互交互

以前记录的驾驶日志为构建这些新场景提供了丰富的资源，但对于闭环评估，我们需要根据新场景配置和SDV的决策修改传感器数据，因为可能会添加或删除Actor以及轨迹，现有Actor和SDV将与原始日志不同

UniSim，一种神经传感器模拟器，它采用配备传感器的车辆捕获的单个记录日志，并将其转换为逼真的闭环多传感器模拟场景，UniSim构建神经特征网格以重建场景中的静态背景和动态Actor，并将它们合成在一起以在新视点模拟LiDAR和相机数据，在新位置添加或删除Actor

为了更好地处理外推视图，结合了动态对象的可学习先验，并利用卷积网络来完成未见区域，实验表明UniSim可以在下游任务上模拟真实传感器数据而没有太多的域间隙(domain gap)

借助UniSim，首次展示了在安全关键场景下对自治系统的闭环评估，就好像它在现实世界中一样

<img src="https://img.saraba1st.com/forum/202306/14/042604wzqjrezfd5z9b9d9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-042539.jpg</strong> (202.98 KB, 下载次数: 0)

下载附件

2023-6-14 04:26 上传

UniSim从数据收集平台获取记录的相机和LiDAR数据，并创建可操作的数字孪生，UniSim为新场景模拟真实的、时间一致的传感器数据，从而实现闭环自主评估

自主系统与场景进行反应性交互，接收新的传感器数据，并改变车道，图中所有图像和激光雷达均由UniSim模拟，UniSim可以准确地重建原始传感器数据，几乎没有伪影

<img src="https://img.saraba1st.com/forum/202306/14/043025byuvy6ky4fkuk861.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-043002.jpg</strong> (109.29 KB, 下载次数: 0)

下载附件

2023-6-14 04:30 上传

通过使用分解表征，UniSim可以执行Actor移除并修复静态场景，创建一个全新的区域来构建安全关键场景

<img src="https://img.saraba1st.com/forum/202306/14/043338gqq08446jqe366e0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-043327.jpg</strong> (108.56 KB, 下载次数: 0)

下载附件

2023-6-14 04:33 上传

随着Actor的移除，可以操纵现有Actor的轨迹来创建新的场景，比如让原本左转的白色轿车改变主意，转向其他车道，就此完成了一个未见的情况，并创建了一个逼真的反事实场景

还可以将自动驾驶车辆的视点向左、向右移动，或者改变摄像头传感器的位置，升高或降低等

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 535#       发表于 2023-6-14 05:04

Oyster

从LiDAR点云实现无监督目标检测

项目主页:https://waabi.ai/oyster/

在本文中，研究了自动驾驶场景中3D点云的无监督目标检测问题，提出了一种简单而有效的方法，该方法利用近距离区域中点云密集的点聚类与时间一致性来过滤掉嘈杂的无监督检测信号，使用CNN的平移等变换来扩展自动伪标签到长距离，还有自监督以自我改进等

OYSTER不对数据收集施加约束(例如重复通过同一位置收集相关数据)，能够在没有监督微调的情况下以零样本方式检测对象(即使是稀疏、遥远的区域)，并持续自监督多轮迭代自我训练的情况下进行改进

为了更好地衡量自动驾驶场景中的模型性能，还提出了一种基于碰撞距离的新的以规划为中心的感知指标，证明了OYSTER在PandaSet和Argoverse 2传感器数据集上的性能明显优于无监督基线，表明自我监督与对象先验伪标签相结合可以在实际情况中发现对象

<img src="https://img.saraba1st.com/forum/202306/14/045503q8sjhmguypp8rsry.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-045449.jpg</strong> (106.99 KB, 下载次数: 0)

下载附件

2023-6-14 04:55 上传

OYSTER有两个训练阶段：初始引导阶段和自我提升阶段

<img src="https://img.saraba1st.com/forum/202306/14/045632ipk5zvm1m5ilmdxv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-045612.jpg</strong> (74.57 KB, 下载次数: 0)

下载附件

2023-6-14 04:56 上传

初始bootstrapping阶段利用了近距离点云往往密集且具有清晰的对象簇这一事实，因此可以通过点聚类获得合理的近距离边界框种子伪标签，由于卷积网络的平移等方差特性，发现在近距离标签上训练的CNN检测器可以借助数据增强以零样本方式泛化到更长距离

<img src="https://img.saraba1st.com/forum/202306/14/045818fw7wbw7v7d7v77jr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-045619.jpg</strong> (68.48 KB, 下载次数: 0)

下载附件

2023-6-14 04:58 上传

自我改进阶段利用对象轨迹的时间一致性作为自我监督信号，提出了一个Track、Refine、Retrain、Repeat框架：
Track:给定跨时间的噪声检测，使用无监督的离线跟踪器来查找各种长度的对象轨迹
Refine:舍弃短轨迹，提炼长轨迹，为下一轮自训练准备伪标签
Retrain:一个对象轨迹在时间上应该具有相同的物理对象大小，因此优化过程使用轨迹级信息来更新长轨迹中的伪标签
Repeat:在更新的伪标签上训练一个新的检测器，将其输出转储为新的伪标签

<img src="https://img.saraba1st.com/forum/202306/14/050312w6ih6vxxkc1kgkc6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-050217.jpg</strong> (409.23 KB, 下载次数: 0)

下载附件

2023-6-14 05:03 上传

与点聚类的比较，上图是无监督对象检测器(OYSTER)在没有任何标签的情况下训练的鸟瞰图(BEV)输出，还可视化用于初始引导的点聚类输出，以启动自监督训练过程，请注意，模型输出和真实标签是类别不可知的边界框

相关评估

<img src="https://img.saraba1st.com/forum/202306/14/050336bxukudh4j4xzbr48.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-050244.jpg</strong> (886.1 KB, 下载次数: 0)

下载附件

2023-6-14 05:03 上传

如上图所示，迭代自监督训练方法从非常嘈杂的伪标签开始，但过程中设法消除误报、发现漏检并提高无监督边界框的质量

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 536#       发表于 2023-6-14 05:20

 本帖最后由 Machinery 于 2023-6-14 05:22 编辑 

LLM-Blender

通过成对排序比较和生成融合聚合大型语言模型的输出提升效果

github项目代码库:https://github.com/yuchenlin/LLM-Blender

<img src="https://img.saraba1st.com/forum/202306/14/051052nowt3otw8tw8cp98.jpg" referrerpolicy="no-referrer">

<strong>20230614_051043.jpg</strong> (277.48 KB, 下载次数: 0)

下载附件

2023-6-14 05:10 上传

尽管某些LLM可能表现出更好的全领域性能，但不同示例的𝘰𝘱𝘵𝘪𝘮𝘢𝘭 LLM可能会有很大差异，如何聚合多个LLM如何利用他们不同的优势来产生更好的结果？

<img src="https://img.saraba1st.com/forum/202306/14/051123bj2l11v4lv66vz4z.jpg" referrerpolicy="no-referrer">

<strong>20230614_051118.jpg</strong> (392.77 KB, 下载次数: 0)

下载附件

2023-6-14 05:11 上传

LLM-Blender是一个LLM两阶段聚合学习框架
1.PairRanker是一个比较成对结果的模块，联合编码学习一对候选者的结果并判断哪个更好
2.GenFuser融合top-K输出以产生更好的输出

<img src="https://img.saraba1st.com/forum/202306/14/051438g8tv5a1taxjl13ey.jpg" referrerpolicy="no-referrer">

<strong>20230614_051426.jpg</strong> (301.32 KB, 下载次数: 0)

下载附件

2023-6-14 05:14 上传

<img src="https://img.saraba1st.com/forum/202306/14/051438zom25l75oyommmkm.jpg" referrerpolicy="no-referrer">

<strong>20230614_051429.jpg</strong> (134.24 KB, 下载次数: 0)

下载附件

2023-6-14 05:14 上传

<img src="https://img.saraba1st.com/forum/202306/14/051438c9phjb3a9ypycjp7.jpg" referrerpolicy="no-referrer">

<strong>20230614_051431.jpg</strong> (150.42 KB, 下载次数: 0)

下载附件

2023-6-14 05:14 上传

PairRanker在LLM-Blender中的关键作用是成对推理，与通过对每个候选人单独评分进行推断的先前方法不同，LLM-Blender比较每对候选人并汇总结果以对每个输入的LLM进行排名，从而产生最佳相关性

GenFuser则是一个seq2seq(序列到序列)模型，它结合了PairRanker按排名的输入和topK候选者nker，然后学习生成最终输出，本文中PairRanker使用DeBERTa(304m参数)，GenFuser使用Flan-T5(3b参数)

为了评估聚合结果，创建了MixInstruct数据集，混合多个开放的指令跟随数据集，例如Alpaca、ShareGPT等，使用像Viccuna这样的11种流行的开源LLM作为候选者来集成和应用各种指标，包括基于GPT的评估

<img src="https://img.saraba1st.com/forum/202306/14/051916nhcvjpjqfm3310mv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-051756.jpg</strong> (150.28 KB, 下载次数: 0)

下载附件

2023-6-14 05:19 上传

LLM-Blender通过为每个输入排序和融合多个LLM，显著提高了最终性能，Blender的结果在69%的例子中排名前三(Vicuna=53%)，在BARTScores等指标中也实现了最佳的总体GPT排名和得分

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 537#       发表于 2023-6-14 05:33

FLARE(Forward-Looking Active REtrieval augmented generation)

主动检索增强生成

github项目实现仓库(代码&amp;数据集):https://github.com/jzbjyb/FLARE

尽管大型语言模型(LM)具有非凡的理解和生成语言的能力，但它们往往会产生幻觉并产生与事实不符的输出

通过从外部知识资源中检索信息来增强LM是一种很有前途的解决方案，大多数现有的检索增强LM设定都采用检索和生成，但设定中仅根据输入检索一次信息

然而，在涉及生成长文本的更一般情况下，这是有限制的，在这种情况下，在整个生成过程中不断收集信息是必不可少的

过去也有一些尝试在生成输出时多次检索信息，这主要是使用先前的上下文作为查询以固定间隔检索文档

<img src="https://img.saraba1st.com/forum/202306/14/053312ovcz0jk8nnof8ov0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-053221.jpg</strong> (153.73 KB, 下载次数: 0)

下载附件

2023-6-14 05:33 上传

在本文中，通过主动决定在整个生成过程中何时检索和检索什么内容，提出了前瞻性主动检索增强生成(FLARE)，这是一种通用的检索增强生成方法，它迭代地使用对即将到来的句子的预测来预测未来的内容，然后将其用作查询来检索相关文档以重新生成句子，如果句子包含低置信度标记

在4个长期知识密集型生成任务/数据集上全面测试 FLARE和基线，FLARE在所有任务上都取得了优异或有竞争力的表现，证明了方法的有效性

<img src="https://img.saraba1st.com/forum/202306/14/053324k9ziiu1iaazxaix1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-053249.jpg</strong> (88.9 KB, 下载次数: 0)

下载附件

2023-6-14 05:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 538#       发表于 2023-6-14 05:40

Cap3D

使用预训练模型构建可扩展3D标题数据集

hugface数据集地址:https://huggingface.co/datasets/tiange/Cap3D

介绍了Cap3D，这是一种为3D对象生成描述性文本的自动方法，这种方法利用来自图像字幕、图像文本对齐和LLM的预训练模型来整合来自3D资产的多个视图的字幕，完全避免了耗时且成本高昂的手动注释过程

<img src="https://img.saraba1st.com/forum/202306/14/054019gsjhfjbhqvjqvcff.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-053848.jpg</strong> (256.34 KB, 下载次数: 0)

下载附件

2023-6-14 05:40 上传

<img src="https://img.saraba1st.com/forum/202306/14/054020srro1bh16hbohhob.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-053907.jpg</strong> (164.37 KB, 下载次数: 0)

下载附件

2023-6-14 05:40 上传

<img src="https://img.saraba1st.com/forum/202306/14/054020hplcrfrwvih3hcat.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-053856.jpg</strong> (91.44 KB, 下载次数: 0)

下载附件

2023-6-14 05:40 上传

通过将Cap3D应用于最近推出的大规模3D数据集Objaverse，产生660k的3D文本对，评估使用了来自同一数据集的41k人工标注进行，表明Cap3D在质量、成本和速度方面优于人工编写的描述

通过有效的提示工程，Cap3D在从ABO数据集收集的17k注释上生成几何描述方面的性能可与人类相媲美

最后，在Cap3D和人类标注数据集上微调文本到3D模型，并展示Cap3D的表现，对SOTA模型进行基准测试，包括Point-E、Shape-E和DreamFusion

<img src="https://img.saraba1st.com/forum/202306/14/054027nhbfbh8rbubzsbh1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-053925.jpg</strong> (300.3 KB, 下载次数: 0)

下载附件

2023-6-14 05:40 上传

<img src="https://img.saraba1st.com/forum/202306/14/054027alja6vklmmz6k6a0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-053938.jpg</strong> (145.95 KB, 下载次数: 0)

下载附件

2023-6-14 05:40 上传

<img src="https://img.saraba1st.com/forum/202306/14/054027kp6s0s66ff16ss1z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-053946.jpg</strong> (235.71 KB, 下载次数: 0)

下载附件

2023-6-14 05:40 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 539#       发表于 2023-6-14 06:02

LongMem

用长期记忆增强语言模型

github项目代码库:https://github.com/Victorwz/LongMem

由于输入长度限制，现有的大型语言模型 (LLM) 只能提供固定大小的输入，从而阻止它们利用过去输入中丰富的长上下文信息

<img src="https://img.saraba1st.com/forum/202306/14/055938bkzutvzv855k5kzt.jpg" referrerpolicy="no-referrer">

<strong>20230614_055726.jpg</strong> (73.51 KB, 下载次数: 0)

下载附件

2023-6-14 05:59 上传

为了解决这个问题，研究组提出了一个框架，使用长期记忆增强的语言模型(LongMem)，使LLM能够记住长期的历史

同时设计了一种新颖的解耦网络架构，将原始骨干LLM冻结为记忆编码器，将自适应残差侧网络作为记忆检索器和读取器

这种解耦记忆设计可以轻松缓存和更新长期过去的记忆检索上下文，而不会遭受记忆陈旧的困扰，通过记忆增强适应训练得到增强，LongMem可以记住过去很久的上下文，并将长期记忆用于语言建模

所提出的记忆检索模块可以处理其记忆库中的无限长度上下文，以有益于各种下游任务。 通常LongMem 可以将long-form memory扩大到 65k tokens，从而将多样本额外演示示例缓存为长期记忆用于上下文学习

实验表明，本方法在具有挑战性的长上下文建模基准ChapterBreak上优于强大的长上下文模型，并且在LLM上的记忆增强上下文学习方面取得了显着改进

而且所提出的方法在帮助语言模型记忆和利用长格式内容方面是有效的

<img src="https://img.saraba1st.com/forum/202306/14/055943exwxnkjv1jxnsxvu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-055845__01.jpg</strong> (227.13 KB, 下载次数: 0)

下载附件

2023-6-14 05:59 上传

LONGMEM框架，“MemAug”代表记忆增强层

<img src="https://img.saraba1st.com/forum/202306/14/060217crrrrg3ucr9r6dh0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-060100.jpg</strong> (85.82 KB, 下载次数: 0)

下载附件

2023-6-14 06:02 上传

将大的文本语料库分批处理，以确保每个文档中的每个连续片段都分布在连续的批次中

评估结果:

<img src="https://img.saraba1st.com/forum/202306/14/060239sqtjym5p5jx7vmmj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-060132.jpg</strong> (140.9 KB, 下载次数: 0)

下载附件

2023-6-14 06:02 上传

<img src="https://img.saraba1st.com/forum/202306/14/060239xh9urazprbbijrpr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-060140.jpg</strong> (143.27 KB, 下载次数: 0)

下载附件

2023-6-14 06:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 540#       发表于 2023-6-14 06:05

manga-ocr

日语文本的OCR识别，主要用于日本漫画

github项目代码库:https://github.com/kha-white/manga-ocr

<img src="https://img.saraba1st.com/forum/202306/14/060453koobivno4ngs4ugz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-060431.jpg</strong> (129.68 KB, 下载次数: 0)

下载附件

2023-6-14 06:04 上传

<img src="https://img.saraba1st.com/forum/202306/14/060453eyf0zaobp6lpy7dd.jpg" referrerpolicy="no-referrer">

<strong>20230614_060445.jpg</strong> (114.45 KB, 下载次数: 0)

下载附件

2023-6-14 06:04 上传

<img src="https://img.saraba1st.com/forum/202306/14/060600md2u2q1u15dbq8uu.jpg" referrerpolicy="no-referrer">

<strong>20230614_060547.jpg</strong> (431.3 KB, 下载次数: 0)

下载附件

2023-6-14 06:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 541#       发表于 2023-6-14 06:09

<img src="https://static.saraba1st.com/image/smiley/face2017/066.png" referrerpolicy="no-referrer">还有五篇左右待更，先摸了

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 542#       发表于 2023-6-14 07:21

Fill-Up

通过生成模型平衡长尾数据

项目主页:https://alex4727.github.io/Fill-Up/

github项目代码库:https://github.com/alex4727/Fill-Up

提出了一种基于填充操作的长尾数据认知进行的两阶段训练方法，借助来自textual-inverted token的合成图像和平衡Softmax损失，从头开始训练的本方法在标准长尾基准测试中取得了最先进的结果

现代文本到图像合成模型已经达到了照片级真实感的卓越水平，可以从任意文本描述中生成高质量图像，鉴于令人印象深刻的合成能力，一些研究在利用生成的数据进行图像识别方面展示了有希望的结果

然而，使用现有方法直接补充现实世界中数据匮乏的情况(例如，少样本或长尾场景)会导致边际性能提升，因为它们无法彻底反映真实数据的分布

通过广泛的实验，本文提出了一种新的图像合成流程，使用textual-inverted的长尾情况，研究表明，从文本倒置文本标记生成的图像可以有效地与真实域对齐，显著增强了标准ResNet50骨干网的识别能力

还发现可以通过用合成图像填充不平衡数据来成功缓解现实世界的数据不平衡情况

<img src="https://img.saraba1st.com/forum/202306/14/071830ic2w73yo3k2dc33d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-071819.jpg</strong> (159.56 KB, 下载次数: 0)

下载附件

2023-6-14 07:18 上传

使用textual-inverted作为主要的图片增强方法，因为其性能在多种方式对比中最佳

<img src="https://img.saraba1st.com/forum/202306/14/072034ugkssg8n8svg84s4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-071939.jpg</strong> (337.75 KB, 下载次数: 0)

下载附件

2023-6-14 07:20 上传

<img src="https://img.saraba1st.com/forum/202306/14/072034z7hl0h0u87mu0h07.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-071951.jpg</strong> (243.91 KB, 下载次数: 0)

下载附件

2023-6-14 07:20 上传

<img src="https://img.saraba1st.com/forum/202306/14/072035a9wcng0zttq50hkk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-072004.jpg</strong> (394.68 KB, 下载次数: 0)

下载附件

2023-6-14 07:20 上传

<img src="https://img.saraba1st.com/forum/202306/14/072035ut9wuk95q9bc9yub.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230614-072017.jpg</strong> (490.36 KB, 下载次数: 0)

下载附件

2023-6-14 07:20 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 543#       发表于 2023-6-15 03:45

TART

用于以任务无关推理的即插即用Transformer模块

github项目代码库:https://github.com/HazyResearch/TART

TART是一个用于训练和部署的任务无关(task-agnostic)的推理模块调用包，它可以与任意基础模型组成，以提高分类任务的上下文学习性能，TART本身与任何任务、模型和领域无关

大型语言模型(LLM)展示了上下文学习能力，使同一模型无需任何特定任务训练即可执行多项任务，相比之下，传统的适应方法(例如微调)会针对每个特定任务修改基础模型

然而，即使在呈现相同示例时，上下文学习在大部分时候也始终不如特定任务的直接微调方法，虽然大多数现有方法(例如提示工程等)侧重于LLM的学习表征来弥补迁移性能差距，但本文经过分析验证表明LLM表征通常包含足够的信息来做出良好的预测

因此，通过专注于LLM的推理能力，发现并证明由于LLM无法执行简单的概率推理任务而存在这种性能差距

这就提出了一个有趣的问题：LLM 是否真的能够学习如何以任务无关(task-agnostic)的方式进行推理？

TART，使用经过综合训练的基于Transformer的推理模块来普遍提高LLM的推理能力，TART仅使用合成逻辑回归任务以任务不可知的方式训练此推理模块，并将其与任意真实世界的预训练模型组合，无需任何额外训练

通过单个推理模块，TART提高了不同模型系列(GPT-Neo、Pythia、BLOOM)、不同模型大小(100M - 6B)、不同任务(14个NLP二元分类任务)、甚至不同模态(音频和视觉)的模型性能

此外，在RAFT Benchmark上，TART提高了GPT-Neo(125M)的性能，使其优于BLOOM(176B)，并且与GPT-3(175B)的差距在4%以内

<img src="https://img.saraba1st.com/forum/202306/15/033519hvvezvrvvjse47v0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-033335.jpg</strong> (59.37 KB, 下载次数: 0)

下载附件

2023-6-15 03:35 上传

任务适应策略的分类
(左图)跨三个需求的不同适应策略的比较：任务不可知、质量、可扩展性
(右图)跨适应策略的参数更新，上色区域表示适应策略导致的参数变化

<img src="https://img.saraba1st.com/forum/202306/15/033653jiyldbicd5zvdd2r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-033627.jpg</strong> (54.31 KB, 下载次数: 0)

下载附件

2023-6-15 03:36 上传

Tart构架
(左图)推理模块训练过程：推理模块在生成的合成逻辑回归任务序列上进行训练
(右图)端到端框架：Tart将预训练的LLM与推理模块组合在一起，使用LLM嵌入输入文本，这些嵌入与训练标签一起作为序列传递给推理模块，推理模块生成最终预测

<img src="https://img.saraba1st.com/forum/202306/15/034432erxarpx4y8hc1h1a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-034354.jpg</strong> (78 KB, 下载次数: 0)

下载附件

2023-6-15 03:44 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 544#       发表于 2023-6-15 04:03

SyncDiffusion

通过同步联合扩散进行连贯的无缝蒙太奇式宽图像生成

项目主页:https://syncdiffusion.github.io/

github项目代码仓库:https://github.com/KAIST-Geometric-AI-Group/SyncDiffusion

预训练图像扩散模型的卓越功能不仅用于生成固定大小的图像，还用于创建全景图，然而，简单地拼接多个图像通常会导致可见的不连贯接缝

最近的技术试图通过在多个窗口中执行联合扩散并对重叠区域中的潜在特征进行平均来解决这个问题，然而，这些专注于无缝蒙太奇生成的方法通常会通过在单个图像中混合不同场景来产生不连贯的输出

为了克服这个限制，提出了SyncDiffusion，这是一个即插即用的模块，使用通过感知相似性损失的梯度下降来同步多个扩散

具体来说，在每个去噪步骤使用预测的去噪图像计算感知损失的梯度，为实现连贯的蒙太奇式图像提供有意义的指导

实验结果表明，与以前的方法相比，本方法产生了明显更连贯的输出(在用户研究中评价为66.35%对33.65%)，同时仍保持高保真度(由GIQA评估)和与输入的提示具有兼容性(由CLIP分数衡量)

<img src="https://img.saraba1st.com/forum/202306/15/035432sdd9r1ourrxbrarb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-035314__01.jpg</strong> (714.8 KB, 下载次数: 0)

下载附件

2023-6-15 03:54 上传

Blended Latent Diffusion(上图)、MultiDiffusion(中间)和本文方法SYNCDIFFUSION(下)

使用提示“a photo of a rock concert”生成的全景图的比较，Blended Latent Diffusion应用于图像外推时，通常会产生可见的接缝和重复的图案

MultiDiffusion创建无缝全景图，但无法实现图像的全局一致性，相比之下，SYNCDIFFUSION通过增加去噪输出预测的感知相似性来同步整个全景图的画面，产生更连贯的全景图输出

<img src="https://img.saraba1st.com/forum/202306/15/040017qo51o75dr81rlamr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-035849.jpg</strong> (385.33 KB, 下载次数: 0)

下载附件

2023-6-15 04:00 上传

定性比较，Blended Latent Diffusion往往会出现可见的接缝和重复的模式，MultiDiffusion生成无缝结果，但缺乏连贯性，例如将落日的天空与蓝天混合，并显示紫色、粉红色和蓝色背景的组合，相比之下，SYNCDIFFUSION得到显著改善连贯性的全景图

相关对比生成结果、失败案例、与特殊参数的权衡应用结果:

<img src="https://img.saraba1st.com/forum/202306/15/040309iqdn1ok5tyiobljj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-040125.jpg</strong> (267.52 KB, 下载次数: 0)

下载附件

2023-6-15 04:03 上传

<img src="https://img.saraba1st.com/forum/202306/15/040310hz7u8b7g6g6hlagg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-040137.jpg</strong> (305.41 KB, 下载次数: 0)

下载附件

2023-6-15 04:03 上传

<img src="https://img.saraba1st.com/forum/202306/15/040310jlxu92ll9qucltxc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-040149.jpg</strong> (470.38 KB, 下载次数: 0)

下载附件

2023-6-15 04:03 上传

<img src="https://img.saraba1st.com/forum/202306/15/040310unx1m7omsmtstdux.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-040158.jpg</strong> (540.42 KB, 下载次数: 0)

下载附件

2023-6-15 04:03 上传

<img src="https://img.saraba1st.com/forum/202306/15/040310gtzea643wtzwh8ax.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-040206.jpg</strong> (446.88 KB, 下载次数: 0)

下载附件

2023-6-15 04:03 上传

<img src="https://img.saraba1st.com/forum/202306/15/040310rsj77i1po7y7p78n.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-040215.jpg</strong> (384.04 KB, 下载次数: 0)

下载附件

2023-6-15 04:03 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 545#       发表于 2023-6-15 04:18

localrf

渐进优化的局部神经辐射场(NERF)，用于稳健的场景与视角视图合成

项目主页:https://localrf.github.io/

github项目仓库:https://github.com/facebookresearch/localrf

相关数据集:https://drive.google.com/drive/folders/1kGY-VijIbXNsNb7ghEywi1fvkH4BaIEz?usp=share_link

提出了一种算法，用于从一个随意拍摄的视频中重建大型场景的NERF，这项任务有两个核心挑战

首先，大多数现有的NERF重建方法都依赖于SFT(Structure-from-Motion)算法中准确预估的相机姿态，在实际视频中这经常导致失败

其次，使用具有有限表征能力的单一全局辐射场不能扩展到无边界场景中的更长轨迹，为了处理未知姿态，通过以渐进的方式进行联合预测以估计NERF的相机姿态

渐进优化显着提高了重建的稳健性，为了处理大型无边界场景，通过动态分配新的局部辐射场，在时间窗口内用相关帧训练

这进一步提高了稳健性(例如，即使在适度的姿势漂移下也表现良好)并允许扩展到更大的场景

对Tanks and Temples数据集和收集的户外数据集Static Hikes的广泛评估表明，本方法与最先进的方法相比毫不逊色

<img src="https://img.saraba1st.com/forum/202306/15/041240g1yujgzig4jyugji.jpg" referrerpolicy="no-referrer">

<strong>572d3df2-20f5-4501-a0d2-520ade95cc7c.jpg</strong> (200.83 KB, 下载次数: 0)

下载附件

2023-6-15 04:12 上传

<img src="https://img.saraba1st.com/forum/202306/15/041240rm2e9g33blttq4se.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-041141.jpg</strong> (368.75 KB, 下载次数: 0)

下载附件

2023-6-15 04:12 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 546#       发表于 2023-6-15 04:25

WizardCoder-15B-v1.0

新SOTA代码生成模型

WizardLM的github项目地址:https://github.com/nlpxucan/WizardLM

原版:https://huggingface.co/WizardLM/WizardCoder-15B-V1.0

GGML量化版:https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML

GPTQ量化版:https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ

来自WizardLM AI的基于StarCoder微调的新代码模型WizardCoder-15B-v1.0在HumanEval基准测试中达到了57.3的pass@1通过率，比开源的前SOTA LLM高出22.3

<img src="https://img.saraba1st.com/forum/202306/15/042308emuomoufhrn5gu93.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-042226.jpg</strong> (296.69 KB, 下载次数: 0)

下载附件

2023-6-15 04:23 上传

作为对比

<img src="https://img.saraba1st.com/forum/202306/15/042532kcvxv2i8wvv2xnaa.jpg" referrerpolicy="no-referrer">

<strong>20230615_042440.jpg</strong> (198.23 KB, 下载次数: 0)

下载附件

2023-6-15 04:25 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 547#       发表于 2023-6-15 04:52

BehindTheScenes

通过单视图重建的密度场

项目主页:https://fwmb.github.io/bts/

github项目地址:https://github.com/Brummi/BehindTheScenes

从单个图像中推断出有意义的几何场景表征图是计算机视觉中的一个基本问题，基于传统深度图预测的方法只能推断图像中可见的区域

目前，神经辐射场(NeRF)可以捕获包括颜色在内的真实3D，但又由于过于复杂而无法从单个图像生成NERF

作为替代方案，本文引入了一个神经网络，它可以从单个图像中预测隐式的密度场，通过将图像的每个棱台体(frustum)区块中的每个位置映射到体积密度

可以通过仅使用视频数据的自监督方法进行训练，通过不在隐式体积表征中存储颜色，而是在训练期间直接从可用视图中采样颜色，与NeRF相比，BehindTheScenes的场景表示变得不那么复杂，所以可以训练神经网络来预测它

因此可以应用体积渲染来执行深度预测和新视图合成在实际实验，本方法能够为输入图像中被遮挡的区域预测有意义的几何形状

此外，还展示了本方法在深度预测和新视图合成的三个相关数据集上的潜力

BehindTheScenes构架与实现方法描述:

<img src="https://img.saraba1st.com/forum/202306/15/045121lswswbl9dhvbugvp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-044704.jpg</strong> (250.44 KB, 下载次数: 0)

下载附件

2023-6-15 04:51 上传

<img src="https://img.saraba1st.com/forum/202306/15/045121c1rz0vgx9zxg7w2g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-044736.jpg</strong> (388.74 KB, 下载次数: 0)

下载附件

2023-6-15 04:51 上传

评估结果:

<img src="https://img.saraba1st.com/forum/202306/15/045100hn90t4jdfh4cppjf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-044952.jpg</strong> (521.52 KB, 下载次数: 0)

下载附件

2023-6-15 04:51 上传

KITTI上的新视图合成，合成中只使用一个输入帧，并从中预测密度和样本颜色，这意味着输入图像中被遮挡的区域没有有效的颜色样本:

<img src="https://img.saraba1st.com/forum/202306/15/045146eez6i07ibxpcujkf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-045018.jpg</strong> (327.15 KB, 下载次数: 0)

下载附件

2023-6-15 04:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 548#       发表于 2023-6-15 05:27

WebGLM

迈向具有人类偏好的高效网络增强问答系统

项目地址:https://github.com/THUDM/WebGLM

WebGLM，一种基于通用语言模型(GLM)的网络增强型问答系统，目标是通过网络搜索和检索功能增强预训练的大型语言模型(LLM)，同时高效地进行实际部署

为了实现这一目标，本文中使用LLM增强检索器、自举生成器和人类偏好感知评分器的策略开发了WebGLM

具体来说，确定并解决了WebGPT系统(如OpenAI)的局限性，使用WebGLM具有准确性、效率和成本效益等优势

此外，还提出了评估网络增强QA系统的系统标准，进行了多维的人类评估和定量消融研究，表明所提出的WebGLM设计优于现有系统

具有100亿参数的GLM(10B)的WebGLM被证明比类似大小的WebGPT(13B)表现更好，甚至在人类评估中与 WebGPT (175B) 相当

WebGLM具有以下三种特点:
LLM-augmented Retriever:增强相关网络内容的检索，以更好地帮助准确回答问题

Bootstrapped Generator:生成对问题的类似人类的回答，利用GLM的强大功能提供完善的答案

Human Preference-aware Scorer:通过优先考虑人类偏好来估计生成的响应的质量，确保系统产生有用且引人入胜的内容

演示效果:

<img src="https://img.saraba1st.com/forum/202306/15/052231f0fqcudp9uj5jcc0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-052130.jpg</strong> (102.45 KB, 下载次数: 0)

下载附件

2023-6-15 05:22 上传

<img src="https://img.saraba1st.com/forum/202306/15/052231ot1tyhrdbw9ztitc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-052108.jpg</strong> (141.47 KB, 下载次数: 0)

下载附件

2023-6-15 05:22 上传

工作流程框架、评估结果与对比实验:

<img src="https://img.saraba1st.com/forum/202306/15/052650ydyxzdv3tdxl77vx.png" referrerpolicy="no-referrer">

<strong>main_process.png</strong> (118.84 KB, 下载次数: 0)

下载附件

2023-6-15 05:26 上传

<img src="https://img.saraba1st.com/forum/202306/15/052650pvvvyyv3xv8j8vuy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-052336.jpg</strong> (82.92 KB, 下载次数: 0)

下载附件

2023-6-15 05:26 上传

<img src="https://img.saraba1st.com/forum/202306/15/052650y9v58kcjszv8xefx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-052356.jpg</strong> (187.18 KB, 下载次数: 0)

下载附件

2023-6-15 05:26 上传

<img src="https://img.saraba1st.com/forum/202306/15/052650lqugn5byzuv3amqy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-052430.jpg</strong> (226.11 KB, 下载次数: 0)

下载附件

2023-6-15 05:26 上传

<img src="https://img.saraba1st.com/forum/202306/15/052650kfmg4a4g6qq0go40.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-052456.jpg</strong> (212.09 KB, 下载次数: 0)

下载附件

2023-6-15 05:26 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 549#       发表于 2023-6-15 08:10

baichuan-7B

Baichuan-7B是由百川智能开发的一个开源的大规模预训练模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096

github项目地址:https://github.com/baichuan-inc/baichuan-7B

<img src="https://img.saraba1st.com/forum/202306/15/081039lzvdm6dg0vzg440m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-080744__01__01.jpg</strong> (663.84 KB, 下载次数: 0)

下载附件

2023-6-15 08:10 上传

<img src="https://img.saraba1st.com/forum/202306/15/081039hosjzqfhjfbyjyeo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-080744__01.jpg</strong> (814.71 KB, 下载次数: 0)

下载附件

2023-6-15 08:10 上传

<img src="https://img.saraba1st.com/forum/202306/15/081039kkgtownit772tpit.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230615-080744__02.jpg</strong> (269.82 KB, 下载次数: 0)

下载附件

2023-6-15 08:10 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 550#       发表于 2023-6-16 06:33

 本帖最后由 Machinery 于 2023-6-16 06:35 编辑 

Rerender A Video

零样本文本引导的视频到视频转换

项目主页:https://anonymous-31415926.github.io/

注:* Code: According to the Anonymity Policy, we will release the code and data upon the publication of the paper.

大型文本到图像扩散模型在生成高质量图像方面展现了令人印象深刻的熟练程度，然而，当将这些模型应用于视频领域时，确保视频的帧序列之间的内容保持时间一致性仍然是一项艰巨的挑战

本文提出了一种新颖的零样本文本引导视频到视频转换框架使图像模型适应视频，该框架包含两个部分：关键帧转译和全视频转译

第一个部分使用自适应扩散模型生成关键帧，并应用层次交叉帧约束来增强形状、纹理和颜色的一致性，第二部分则通过时间感知图像块匹配和帧混合将关键帧传播到其他帧

本框架可以以低成本(既无需重新训练或优化)实现全局样式和局部纹理的内容保持时间一致性，该方法与现在已有的图像扩散技术完全兼容，例如组合使用LoRA定制特定主题，或者使用ControlNet引入额外的空间引导生成等

广泛的实验结果证明了本框架在渲染高质量和时间一致性的视频方面优于现有方法

<img src="https://img.saraba1st.com/forum/202306/16/061834pake0nnzxdlxdn0x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230616-061821.jpg</strong> (103.83 KB, 下载次数: 0)

下载附件

2023-6-16 06:18 上传

本框架提出了新颖的层次交叉帧约束，以生成连贯的视频帧内容，其中的关键思想是应用光流条件进行密集交叉帧约束，之前的渲染帧作为当前帧的低级别参考，第一个渲染帧作为锚点来调节渲染过程以防止偏离初始内容

层次交叉帧约束框架在扩散采样的不同阶段实现，除了全局风格一致性(交叉帧注意力)之外，本方法还增强了早期的形状(形状感知交叉帧潜在融合)、中期的纹理(像素感知交叉帧潜在融合)和最后阶段的颜色(颜色感知自适应潜在调整)的一致性，这种创新的轻量级修改实现了全局和局部内容的时间一致性

<img src="https://img.saraba1st.com/forum/202306/16/063023e7k0twkxmn4nxmmm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230616-063000.jpg</strong> (440.84 KB, 下载次数: 0)

下载附件

2023-6-16 06:30 上传

四种最近的零样本方法进行了比较：vid2vid-zero、FateZero、Pix2Video、Text2Video-Zero

FateZero成功重建了输入帧，但无法调整以匹配提示语句，另一方面，vid2vid-zero和Pix2Video过度修改了输入帧，导致视频帧之间出现明显的形状失真和不连续性，以及，虽然FateZero生成的每一帧都呈现出了高质量，但在局部纹理之上缺乏连贯性

最后，本文提出的方法在输出质量、内容和提示匹配以及时间一致性方面表现出明显的优势

<img src="https://img.saraba1st.com/forum/202306/16/063023ll1z6vrjk2ec6tzk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230616-062944.jpg</strong> (138.59 KB, 下载次数: 0)

下载附件

2023-6-16 06:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 551#       发表于 2023-6-16 07:11

SqueezeLLM

密集和稀疏量化的语言模型

相关论文:https://arxiv.org/abs/2306.07629

github项目地址:https://github.com/SqueezeAILab/SqueezeLLM

SqueezeLLM，一种新的模型量化方法，结合了一种称为密集和稀疏量化的新方法，实现了高效的LLM服务，同时在相同的大小和速度限制下实现更高质量的生成效果

<img src="https://img.saraba1st.com/forum/202306/16/065412nigl5gg3gipy55nk.jpg" referrerpolicy="no-referrer">

<strong>20230616_065243.jpg</strong> (81.99 KB, 下载次数: 0)

下载附件

2023-6-16 06:54 上传

基于此，研究组提出了仅优化权重的低位精度量化，并结合了两个新颖的想法来避免模型性能下降，首先是基于灵敏度的非均匀量化，以及密集和稀疏分解，上图为不同方法组合的困惑度，其中包括前SOTA方法的效果

非均匀的量化允许以更准确的方式表示参数，并且非常适合生成推理，因为乘法/加法是在FP16中执行的，然而简单的非均匀量化效率不高

<img src="https://img.saraba1st.com/forum/202306/16/065819mwy5iwwwlis4flfj.jpg" referrerpolicy="no-referrer">

<strong>20230616_065717.jpg</strong> (83.82 KB, 下载次数: 0)

下载附件

2023-6-16 06:58 上传

识别每一层中的敏感参数并对它们进行更多加权以确保它们在非均匀量化后受到的扰动最小是很重要的

LLM模型包含大量对量化产生负面影响的离群的关键值 ，直接用低位精度分割大范围的权重会导致大量错误，反之亦然，离群的关键值会使这个影响范围非常大

<img src="https://img.saraba1st.com/forum/202306/16/070142k75465ljz7adku69.jpg" referrerpolicy="no-referrer">

<strong>20230616_070125.jpg</strong> (96.27 KB, 下载次数: 0)

下载附件

2023-6-16 07:01 上传

LLM模型中有总数&lt;0.5%的异常值，通过从权重矩阵中提取这些关键的异常值并将它们有效地表示为稀疏矩阵可以解决这个问题，而剩余的数值是一个没有离群关键值的密集矩阵，可以更准确地量化到低位精度

<img src="https://img.saraba1st.com/forum/202306/16/070513v3tf333fcucuu249.jpg" referrerpolicy="no-referrer">

<strong>20230616_070507.jpg</strong> (96.88 KB, 下载次数: 0)

下载附件

2023-6-16 07:05 上传

对LLaMA-7B/13B/30B进行了广泛的测试，发现SqueezeLLM始终优于现有的量化方法，尤其是在低位精度的情况下，上图展示了不同LLaMA变体的困惑值(越低越好)

<img src="https://img.saraba1st.com/forum/202306/16/070645y6ss3b0xouskeu01.jpg" referrerpolicy="no-referrer">

<strong>20230616_070625.jpg</strong> (96.56 KB, 下载次数: 0)

下载附件

2023-6-16 07:06 上传

SqueezeLLM也适用于Vicuna等指令跟随模型，在MMLU基准上评估了生成的结果质量，观察到SqueezeLLM应用于4位Vicuna-13B时，在6.5GB显存占用下，准确率为41%，而Vicuna-7B在16位时使用13GB显存占用，准确率为39%

<img src="https://img.saraba1st.com/forum/202306/16/071039rrmdvkfz7vnvdkse.jpg" referrerpolicy="no-referrer">

<strong>20230616_071034.jpg</strong> (43.34 KB, 下载次数: 0)

下载附件

2023-6-16 07:10 上传

还使用了GPT4与基线进行对量化模型的生成质量进行排名的对比，SqueezeLLM始终优于当前的SOTA方法，包括GPTQ和AWQ

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  s1234y  
##### 552#       发表于 2023-6-16 19:47

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61290617&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-6-15 08:10</a>
 baichuan-7B  Baichuan-7B是由百川智能开发的一个开源的大规模预训练模型。基于Transformer结构，在大约1.2 ...</blockquote>
这玩意没有sft过，还得自己用各种羊驼的instructions去sft下<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">


*****

####  Machinery  
##### 553#       发表于 2023-6-16 20:01

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61313453&amp;ptid=2126390" target="_blank">s1234y 发表于 2023-6-16 19:47</a>
这玩意没有sft过，还得自己用各种羊驼的instructions去sft下</blockquote>
确实，这个是预训练模型不是已经微调好开箱即用的…需要自行搞些语料微调或者等官方出调好的版本…

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 554#       发表于 2023-6-17 01:11

LMFlow/LMFlow Benchmark/Robin V2

LMFlow是一个可扩展、方便和高效的工具箱，用于微调大型机器学习模型，项目目标是开发一套用户友好、快速可靠，并对整个社区开放的全流程微调代码库

github项目地址:https://github.com/OptimalScale/LMFlow

LMFlow Benchmark是一套开源的开源 LLM 的自动评估框架，开发了基于负对数似然(NLL)的评估基准，针对常见NLP评测中另一个类似且常用的指标是困惑度(PPL)的问题进行了改进

相关博客说明:https://blog.gopenai.com/lmflow-benchmark-an-automatic-evaluation-framework-for-open-source-llms-ef5c6f142418

Robin V2是一系列基于LLaMA微调的模型，继承了Robin模型的系列工作，进行了深度微调并取得了令人满意的结果

Robin-7B V2在OpenLLM标准测试中得分为51.7，Robin-13B V2达到了59.1，排名第六，超越了部分60B模型的效果，而Robin-33b V2和Robin-65b V2的成绩更加惊人，在测试中以64.1和65.2的成绩稳居榜首

<img src="https://img.saraba1st.com/forum/202306/17/010929ak2kh2o562m6f66b.jpg" referrerpolicy="no-referrer">

<strong>20230617_010912.jpg</strong> (375.69 KB, 下载次数: 0)

下载附件

2023-6-17 01:09 上传

<img src="https://img.saraba1st.com/forum/202306/17/010929c2q1777khyvcvkv3.jpg" referrerpolicy="no-referrer">

<strong>20230617_010914.jpg</strong> (39.19 KB, 下载次数: 0)

下载附件

2023-6-17 01:09 上传

项目模型与相关说明可以在(https://github.com/OptimalScale/LMFlow)项目页面获取

<img src="https://img.saraba1st.com/forum/202306/17/011058b8yz45q98rpd8ty1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-011010.jpg</strong> (352.78 KB, 下载次数: 0)

下载附件

2023-6-17 01:10 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 555#       发表于 2023-6-17 01:47

 本帖最后由 Machinery 于 2023-6-17 01:48 编辑 

Infinigen

一个逼真的自然世界3D场景生成器

相关论文:https://arxiv.org/abs/2306.09310

项目网站:https://infinigen.org/

使用引导:https://infinigen.org/docs/category/installation-instructions

github项目代码库:https://github.com/princeton-vl/infinigen

Infinigen是完全程序化的：从形状到纹理的每项资产都是通过随机数学规则从头开始生成的，不使用外部源并允许无限变化和组合

Infinigen可以提供广泛的自然界物体和场景覆盖，包括植物、动物、地形以及火、云、雨和雪等自然现象生成，Infinigen可用于为广泛的计算机视觉任务生成无限多样的训练数据，包括对象检测、语义分割、光流和3D重建任务等

<img src="https://img.saraba1st.com/forum/202306/17/013950zdbyk9ckhpbz1az3.jpg" referrerpolicy="no-referrer">

<strong>20230617_013947.jpg</strong> (307.96 KB, 下载次数: 0)

下载附件

2023-6-17 01:39 上传

Infinigen由Princeton Vision &amp; Learning Lab开发，针对计算机视觉研究进行了优化，可以生成多样化的高质量3D训练数据，Infinigen本身基于Blender，是免费和开源的(使用GPL授权许可)

<img src="https://img.saraba1st.com/forum/202306/17/014327xr7r9qgnaajcq2nu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-014230.jpg</strong> (204.39 KB, 下载次数: 0)

下载附件

2023-6-17 01:43 上传

<img src="https://img.saraba1st.com/forum/202306/17/014624rtlcgpx2fdlafcap.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-014512.jpg</strong> (583.57 KB, 下载次数: 0)

下载附件

2023-6-17 01:46 上传

Infinigen针对计算机视觉研究进行了优化，尤其是3D视觉，Infinigen不使用凹凸/法线贴图、全透明或其他伪造几何细节的技术，Infinigen的所有几何细节都是真实的，确保准确的3D ground truth结果

Infinigen可以为各种计算机视觉任务自动生成高质量的标注，包括光流、3D场景流、深度、表面法线、全景分割、遮挡边界等，因为用户可以完全访问渲染过程，所以很容易定制标注

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 556#       发表于 2023-6-17 02:05

AssistGPT

可以计划、执行、检查和学习的 通用多模态助手

项目主页:https://showlab.github.io/assistgpt/

github项目地址库(coming soon):https://github.com/showlab/assistgpt

<img src="https://img.saraba1st.com/forum/202306/17/020042m25n8aaras2nkf5n.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-015929.jpg</strong> (279.14 KB, 下载次数: 0)

下载附件

2023-6-17 02:00 上传

AssistGPT可以以交错的语言和代码格式进行推理，给定查询输入和视觉输入，AssistGPT可以用语言规划问题解决的路径，使用结构化代码调用各种强大的工具

Inspector是整个助手系统中的一部分，可以管理视觉输入和中间结果，从而协助Planner调用工具，与此同时，Learner可以评估推理过程并收集上下文中的示例学习

<img src="https://img.saraba1st.com/forum/202306/17/020452max99w49iak79wv9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-020347.jpg</strong> (292.42 KB, 下载次数: 0)

下载附件

2023-6-17 02:04 上传

<img src="https://img.saraba1st.com/forum/202306/17/020452cweac8c5d7yw1eys.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-020435.jpg</strong> (447.03 KB, 下载次数: 0)

下载附件

2023-6-17 02:04 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 557#       发表于 2023-6-17 03:42

 本帖最后由 Machinery 于 2023-6-17 03:43 编辑 

ChessGPT

桥接策略学习和语言建模

相关论文:https://arxiv.org/abs/2306.09200

github项目仓库:https://github.com/waterhorse1/ChessGPT

在解决决策任务时，人类通常依赖来自两个关键来源的信息：
1.历史策略数据，提供对环境的交互重放进行学习
2.自然语言形式的分析见解，揭示宝贵的思维过程的考虑因素

尽管如此，大多数先前的研究通常只关注其中一面，要么专门使用历史回放来直接学习策略或价值函数，要么只利用语料库进行语言模型训练

本文中认为一个强大的自治的代理应该同时涵盖这两个方面，因此提出了ChessGPT，这是一种GPT模型，通过在国际象棋游戏中集成来自这两个来源的数据来桥接策略学习和语言建模

具体来说则是构建了一个与国际象棋相关的大规模游戏相关语言数据集，展示了两个模型示例ChessCLIP和ChessGPT，分别集成了策略学习和语言建模

最后提出了一个完整的评估框架来评估语言模型的国际象棋能力，实验结果验证了本方法的模型和数据集的有效性

通过从互联网上收集所有与国际象棋相关的资料，引入了一个大规模游戏相关语言数据集，主要分为四类:

1.游戏数据集，包括涉及全球人类玩家和不同技能水平的国际象棋引擎的在线国际象棋比赛重放数据
2.语言数据集，主要以自然语言的形式记录与国际象棋相关的知识、分析、讨论和新闻
3.混合游戏语言数据集，包含对齐的游戏数据和人类自然语言元素(如游戏分析或评论)
4.指令调整和对话数据集，由与国际象棋相关的指令数据和对话数据组成

ChessCLIP和ChessGPT框架相关技术实现细节:

<img src="https://img.saraba1st.com/forum/202306/17/034122p2a57vb97trk2oap.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-033756.jpg</strong> (867.5 KB, 下载次数: 0)

下载附件

2023-6-17 03:41 上传

相关实验的对比评估成绩:

<img src="https://img.saraba1st.com/forum/202306/17/034143azd0qtp6tq6ttavl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-033933.jpg</strong> (147.99 KB, 下载次数: 0)

下载附件

2023-6-17 03:41 上传

<img src="https://img.saraba1st.com/forum/202306/17/034143tkql83ppg8t80890.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-034004.jpg</strong> (123.76 KB, 下载次数: 0)

下载附件

2023-6-17 03:41 上传

<img src="https://img.saraba1st.com/forum/202306/17/034143rbb83j1jb28jj18c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-034102.jpg</strong> (75.26 KB, 下载次数: 0)

下载附件

2023-6-17 03:41 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 558#       发表于 2023-6-17 05:02

QR Code Conditioned ControlNet Models for Stable Diffusion 1.5

用于生成二维码艺术图像的controlnet模型

模型权重:https://huggingface.co/DionTimmer/controlnet_qrcode-control_v1p_sd15

插件演示Demo:https://huggingface.co/spaces/huggingface-projects/QR-code-AI-art-generator

<img src="https://img.saraba1st.com/forum/202306/17/050055sz85tcf0tnbfn40j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-050039.jpg</strong> (238.59 KB, 下载次数: 0)

下载附件

2023-6-17 05:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 559#       发表于 2023-6-17 06:00

 本帖最后由 Machinery 于 2023-6-17 06:02 编辑 

Rosetta Neurons(罗塞塔神经元)

在模型园中挖掘通用的神经元

github项目实现代码库:https://github.com/yossigandelsman/rosetta_neurons

<img src="https://img.saraba1st.com/forum/202306/17/054322iu36u66h2u56cziw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-054303.jpg</strong> (133.38 KB, 下载次数: 0)

下载附件

2023-6-17 05:43 上传

挖掘“罗塞塔神经元”，在不同模型的神经元中匹配到存在表达相同概念(例如对象轮廓、对象部分和颜色)的类似神经元

这些概念在没有任何监督或手动标注的情况下自然出现，通过使用注意力热力图以及一种新颖的技术(activation invasions)将这个结果可视化

针对各种视觉任务训练的不同神经网络是否共享一些共同的表征？在这篇论文中，展示了被称之为“罗塞塔神经元”的拥有共同特征的存在，跨越一系列具有不同架构、不同任务(生成和判别)和不同类型的监督(分类监督、文本监督、自监督)模型

同时提出了一种算法，用于在几种流行的视觉觉模型中挖掘罗塞塔神经元的字典：诸如Class Supervised-ResNet50、DINO-ResNet50、DINO-ViT、MAE、CLIP-ResNet50、BigGAN、StyleGAN-2、StyleGAN-XL等模型

研究结果表明，某些视觉概念和结构可以在本质上嵌入自然世界中，而不用管具体任务或架构如何，之后通过不同的模型学习，且无需使用任何语义标签，由于分析中也包含生成模型，所以可以直接将共通的概念可视化展示，罗塞塔神经元既可以促进模型到模型的转换，也支持各种基于反转的操作，包括跨类的对齐、转移、缩放等操作，甚至无需专门训练就可以产生不错的效果

<img src="https://img.saraba1st.com/forum/202306/17/055940blplewdfofchphdf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-055822.jpg</strong> (380.01 KB, 下载次数: 0)

下载附件

2023-6-17 05:59 上传

<img src="https://img.saraba1st.com/forum/202306/17/055940oqz8kkw6uxxq6yky.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-055828.jpg</strong> (297.9 KB, 下载次数: 0)

下载附件

2023-6-17 05:59 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 560#       发表于 2023-6-17 08:30

 本帖最后由 Machinery 于 2023-6-17 08:33 编辑 

Scalable Image Captioners

很有意思的一篇文章，谷歌出品，更有意思了，随意翻了一下<img src="https://static.saraba1st.com/image/smiley/face2017/066.png" referrerpolicy="no-referrer">

Image Captioners(图像字幕)也是可扩展的视觉学习者

相关文章:https://hub.baai.ac.cn/view/27423

来自网络的大规模图像文本对的比对预训练方法(传统CLIP架构)是视觉骨干模型最流行的大规模预训练策略之一，尤其是在当前大型多模态模型流行的背景下

同时，此类数据的Image Captioners(直接的图像字幕说明对)通常被认为是一种较差的预训练策略，在本文中对这两种预训练策略进行了公平比较，通过仔细匹配训练数据、计算和模型容量，使用标准的编码器-解码器Transformer架构，发现仅仅使用Image Captioners进行规模拓展训练的传统模型就非常有效

在分类任务上，使用生成式字幕方法的视觉编码器完全可以与经过对比的预训练编码器竞争，同时在视觉和语言相关任务上超越后者(CLIP)

进一步分析了模型架构和规模的影响，以及预训练数据对表示质量的影响，并发现Image Captioners完全可以在这些轴上表现出相同或更好的可拓展性能

总的来说，纯图像字幕是一种比以前认为的更强大的预训练策略，甚至比CLIP更强大

<img src="https://img.saraba1st.com/forum/202306/17/082628kws8hzewshpf93h3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-075726.jpg</strong> (67.13 KB, 下载次数: 0)

下载附件

2023-6-17 08:26 上传

1.比对模型(左方)使用两个独立的Transformer编码器从图像-文本对中提取向量表征，然后在潜在的大batch中进行向量比对匹配
2.Cap方法(中间)使用Transformer编码器-解码器架构并以自回归地方式预测文本Token，在模型训练期间，通过将预期输出移动一个标记并应用因果自注意掩码(教师强制)来并行预测所有标记
3.在并行解码(右边)中，通过仅以图像作为条件，Transformer解码器必须一次性预测所有标记，CapPa单模型通过在自回归和并行解码方案之间切换来训练

D：模型宽度 M：图像标记的数量 N：文本标记的数量，V：词汇量大小

进一步提出了CapPa预训练程序：解码器训练在标准自回归预测 (Cap) 和并行预测(Pa)之间交替进行，CapPa的混合训练显着提高了视觉主干的分类准确性

<img src="https://img.saraba1st.com/forum/202306/17/082657mhybxt8ex5mtje88.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-081548.jpg</strong> (223.94 KB, 下载次数: 0)

下载附件

2023-6-17 08:26 上传

结果证实了这个发现，即虽然Cap模型在零样本分类精度方面初期通常落后于比对模型，但随着规模的增加，差距变得越来越小，更重要的是，使用Cap方法作为视觉主干模型在可以少样本分类中匹配或优于同类CLIP模型，并在转移到具有大型Token数据集的分类任务时获得有竞争力的性能

将视觉编码器与随机初始化的Transformer解码器结合使用时，Cap预训练编码器在字幕、OCR和VQA任务上的表现优于CLIP，这表明Cap视觉主干可能更适合多模态下游任务

而将Cap视觉编码器应用于预训练、部分冻结的语言解码器相结合时，这些好处会更加明显

<img src="https://img.saraba1st.com/forum/202306/17/082716wnqxjlisxzoccncn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-081721.jpg</strong> (207.85 KB, 下载次数: 0)

下载附件

2023-6-17 08:27 上传

<img src="https://img.saraba1st.com/forum/202306/17/082716ntc8mlnrlntvnnnm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230617-081806.jpg</strong> (164.63 KB, 下载次数: 0)

下载附件

2023-6-17 08:27 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 561#       发表于 2023-6-18 00:55

 本帖最后由 Machinery 于 2023-6-18 00:56 编辑 

TryOnDiffusion

两个UNet的换装故事

项目主页:https://tryondiffusion.github.io/

<img src="https://img.saraba1st.com/forum/202306/18/001446v6iamjadffqmcfqq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230618-001432.jpg</strong> (108.01 KB, 下载次数: 0)

下载附件

2023-6-18 00:14 上传

给定两张图片，其中一张为试穿者，另一个为不固定的人穿着的需要试穿的衣服的图像，目标是根据给定的这两张图片，生成逼真的拥有细节保留的服装穿在试穿者身上的可视化效果

最关键的挑战就是合成的图片中服装需要保留逼真的细节的同时，还需要扭曲服装的形状与姿态以适应试穿者的身体姿势和形状变化

以前的方法要么侧重于保留服装细节而没有有效的姿势和形状变化，要么允许以所需的形状和姿势试穿但缺少服装的微妙细节

在本文中，提出了一种基于扩散模型的架构，它联合了两个UNet模块(称为并行UNet/Parallel-UNet)，这使模型能够在单个网络中保留服装细节并且成功扭曲服装以实现适应试穿者显著的姿势和身体变化

Parallel-UNet背后的关键思想为：
1.服装通过交叉注意机制隐式形变
2.服装形变和人物融合作为一个统一的过程的一部分发生，而不是两个单独的任务的序列

实验结果表明，TryOnDiffusion在定性和定量对比上都达到了最先进的性能

<img src="https://img.saraba1st.com/forum/202306/18/002738tf6yece976vpvj1v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230618-002653.jpg</strong> (46.65 KB, 下载次数: 0)

下载附件

2023-6-18 00:27 上传

框架工作流程，在预处理步骤中，将输入图片中的目标人物从人物图像中分割出来，用以创建一个与“服装无关RGB”图像，而目标衣物从服装图像中分割出来，并为人物和服装图像计算姿势

这些结果被输入128×128 Parallel-UNet模型中用以创建128x128的试穿图像，该图像与试穿条件输入一起作为下一步的输入发送到256×256 Parallel-UNet中，之后256×256 Parallel-UNet的输出被发送到标准的超分辨率扩散模型中以创建1024×1024分辨率的图像

<img src="https://img.saraba1st.com/forum/202306/18/003622rel9tvyyj6tt9ttu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230618-003534.jpg</strong> (93.15 KB, 下载次数: 0)

下载附件

2023-6-18 00:36 上传

128×128型号的Parallel-UNet架构，人物-UNet(上方)将与服装无关RGB和噪声图像输入，由于两个输入都是按像素对齐的，所以可以在UNet处理开始时直接沿着对应的维度通道将它们连接起来，服装-UNet(下方)将分割后的服装图像作为输入

服装特征通过交叉注意力融合到目标图像中，为了保存模型参数，在32×32的上采样块之后提前停止了服装-UNet，此时人物-UNet中的最终交叉注意力模块已经完成，人物和服装的姿势首先被送入线性层以分别计算姿势embedding，再通过注意力机制将姿势embedding融合到人物-UNet中，除此之外，FiLM也被用于在所有尺度上调节两个UNet的特征

<img src="https://img.saraba1st.com/forum/202306/18/005534vxcnrlhbhon6hnit.jpg" referrerpolicy="no-referrer">

<strong>232c153f-be0e-4ddf-9be2-ce0207ba7536.jpg</strong> (901.23 KB, 下载次数: 0)

下载附件

2023-6-18 00:55 上传

TryOnDiffusion当然也有一些限制，首先，本方法在预处理过程中分割图和姿势估计出现错误的情况下会出现服装泄漏伪影，幸运的是，近年来这些变得相当准确，而且这种情况并不经常发生

其次，通过与服装无关的RGB表征人物身份并不理想，因为有时它可能只保留部分身份信息，例如，纹身或特定肌肉结构在身份信息中无法有效保留

第三，训练和测试的数据集大多具有干净均匀的背景，因此尚不知道该方法如何在更复杂的背景下执行

第四，本方法不保证绝对合身，因为目前方法只关注试穿的视觉效果

最后，这项工作主要集中在上半身服装上，还没有来得及尝试全身试穿相关工作，留待以后解决

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 562#       发表于 2023-6-18 03:08

Human Preference Score v2

用于评估文本到图像合成的人类偏好的可靠基准

github项目地址:https://github.com/tgxs002/HPSv2

最近的文本到图像生成模型可以从文本输入生成高保真图像，但是这些生成图像的质量无法通过现有的评估指标进行准确评估，为了解决这个问题，在本文中引入了人类偏好数据集 v2 (HPD v2)，这是一个大规模数据集，可从各种来源捕获人类对图像的偏好

HPD v2数据集包含798090个在430060张图像对上进行的人工标注偏好选择，使其成为同类型中最大的数据集，通过过滤整理收集的文本提示和图像以消除潜在的偏见，这是以前数据集中的常见问题

在HPD v2上微调CLIP，获得了Human Preference Score v2(HPS v2)，这是一种可以更准确地预测文本生成图像的人类偏好的评分模型

实验表明，HPS v2 在各种图像分布中的泛化能力优于以前的指标，并且可以对文本到图像生成模型的算法改进做出响应，使其成为这些模型的首选自动评估指标

还研究了文本到图像生成模型的评估提示设计，以使评估稳定、公平且易于使用，最后使用HPS v2建立了文本到图像生成模型的评价基准，其中包括一组来自学术界、社区和工业界的最新文本到图像模型的排名

<img src="https://img.saraba1st.com/forum/202306/18/030648o6pkh8jx06kxjpcp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230618-030329.jpg</strong> (120.64 KB, 下载次数: 0)

下载附件

2023-6-18 03:06 上传

<img src="https://img.saraba1st.com/forum/202306/18/030648l2r2zkf6ommfj6js.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230618-030633.jpg</strong> (317.2 KB, 下载次数: 0)

下载附件

2023-6-18 03:06 上传

<img src="https://img.saraba1st.com/forum/202306/18/030648p9xcwzjpsvjaecvw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230618-030409.jpg</strong> (143.22 KB, 下载次数: 0)

下载附件

2023-6-18 03:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 563#       发表于 2023-6-19 01:49

CondTopDown

重新思考人群姿态估计：克服检测信息瓶颈和歧义情况

github项目仓库:https://github.com/amathislab/CondTopDown

在姿态估计时，因为个体之间的频繁交互导致的姿势估计算法结果错误是目前的一项困难挑战

当前的工作流程中要么使用对象检测器和姿态估计器(称之为自上而下[top-down]的方法），要么则首先定位所有身体部位，之后将它们连接起来作为条件以预测个体的姿势（称之为自下而上[bottom-up]的方法）

然而，当估计姿态时的个体密切互动的场合，由于个体经常在画面中出现重叠，top-down的方法定义不明确，而bottom-up的方法更是常常错误地推断出与远处身体部位的联系

<img src="https://img.saraba1st.com/forum/202306/19/013933vsw9tsssg6s6dqe9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230619-013711.jpg</strong> (292.1 KB, 下载次数: 0)

下载附件

2023-6-19 01:39 上传

因此，本文提出了一种新颖的工作流程方法BUCTD(bottom-up conditioned top-down pose estimation)，它结合了自下而上和自上而下两种方法的优势

具体来说，通过使用bottom-up的模型作为检测器，除了估计的边界框外，它还能提供一个初步估计的姿态，将此作为条件提供给基于注意力的top-down模型生成最终结果

本方法在动物和人体姿势估计基准方面的性能和效率优秀，在CrowdPose和OCHuman上，BUCTD表现出了明显优于以前方法的SOTA性能

在CrowdPose上实现了78.5AP，在OCHuman上实现了47.2AP，分别比现有技术提高了8.6%和4.9%，此外，本方法在COCO等非拥挤基准的姿态数据集上也具有出色的性能，同时也提高了涉及小鼠、鱼和猴子的多动物基准测试中的性能

模型架构:

<img src="https://img.saraba1st.com/forum/202306/19/013939pcif3f8lec3l8v97.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230619-013848.jpg</strong> (587.49 KB, 下载次数: 0)

下载附件

2023-6-19 01:39 上传

BUCTD方法的二阶段框架流程：条件自上而下(CTD)姿态估计器与CoAM(CTD with Conditional Attention Module)

<img src="https://img.saraba1st.com/forum/202306/19/014951gmsr6yfny3wr64i6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230619-014816__01.jpg</strong> (751.16 KB, 下载次数: 0)

下载附件

2023-6-19 01:49 上传

<img src="https://img.saraba1st.com/forum/202306/19/014951srkzsjw7zrceg7sg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230619-014840__01.jpg</strong> (409.75 KB, 下载次数: 0)

下载附件

2023-6-19 01:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 564#       发表于 2023-6-19 02:38

NAVI

具有高质量3D形状和姿态标注的[未知类别]图像集

项目主页:https://navidataset.github.io/

github数据集地址:https://github.com/google/navi

最近的神经3D重建技术的最新发展已经达到可以从随意拍摄的图像的集合中重建高质量的3D对象与场景

目前可以使用SfM技术提供基准真相级(GT)的摄像机姿态用以帮助在相对简单的图像集合上进行处理

但值得注意的是，SfM和相关的估计技术往往在真实的自然场景图像集合中会失败，例如具有不同背景和照明情况的图像结果

为了推进3D重建系统的研究进展，本文提出了“NAVI”：一个新的数据集，包含特意处理的[未知类别]的图像集合数据，具有高质量的3D扫描结果图以及对图像2D-3D进行对齐的处理，同时提供近乎完美的基准真相级的摄像机参数

这些2D-3D对齐使研究组能够提取准确的衍生标注，例如密集的像素对应关系、深度和图片分割等，同时演示了NAVI图像集在不同情况上的使用，并表明NAVI可以进行更彻底的评估，而现有数据集通常无法做到这一点

NAVI 数据集由带有高质量3D形状和姿态标注的拍摄的图像集合组成，该数据集由36个对象的多视角图像和真实自然场景图像组成，总共约10K张图像，以下是数据集的一些关键方面：

1.真实自然场景，除了典型的多视图对象图像外NAVI还提供真实自然场景图像集合，其中对象目标是在不同的背景、照明和相机状况下拍摄的

2.未知类别，NAVI数据集中的对象与类别无关，玩具和装饰品的图像集合没有任何关于类别的特别形状

3.近乎完美的3D几何图形，通过使用高质量的3D扫描仪来获取3D形状基准真相

4.近乎完美的相机姿态，通过手动2D-3D对齐以及严格的验证获得高质量的3D相机姿态标注

5.密集像素对应关系、深度等衍生标注，鉴于近乎完美的3D形状和相机参数，可以轻松导出其他高质量标注，如密集的像素级对应关系、单目深度、前景分割等

<img src="https://img.saraba1st.com/forum/202306/19/023351j9b9r4rfkkaxshy9.jpg" referrerpolicy="no-referrer">

<strong>ae5f884f-c70b-48b3-89ec-019baa6e3771.jpg</strong> (100.09 KB, 下载次数: 0)

下载附件

2023-6-19 02:33 上传

<img src="https://img.saraba1st.com/forum/202306/19/023351c8o0wz9v88aooevn.jpg" referrerpolicy="no-referrer">

<strong>7d55b4c8-49d6-45c2-8360-a1282be0bfde.jpg</strong> (103.07 KB, 下载次数: 0)

下载附件

2023-6-19 02:33 上传

<img src="https://img.saraba1st.com/forum/202306/19/023351kxt5rt1x2brb1e23.jpg" referrerpolicy="no-referrer">

<strong>cdc97a9e-16a0-4836-b0b3-4ad9ff07f710.jpg</strong> (107.01 KB, 下载次数: 0)

下载附件

2023-6-19 02:33 上传

<img src="https://img.saraba1st.com/forum/202306/19/023351dbxvpsaxzbxmyoxr.jpg" referrerpolicy="no-referrer">

<strong>9ae10689-562c-4be6-8b02-814dbcc0ffea.jpg</strong> (131.86 KB, 下载次数: 0)

下载附件

2023-6-19 02:33 上传

<img src="https://img.saraba1st.com/forum/202306/19/023351ekv1w4wtiee79dul.png" referrerpolicy="no-referrer">

<strong>NAVI_depths_masks.png</strong> (493.07 KB, 下载次数: 0)

下载附件

2023-6-19 02:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 565#       发表于 2023-6-19 03:13

 本帖最后由 Machinery 于 2023-6-19 03:22 编辑 

openllama llm

 Meta AI的LLaMA大型语言模型的开源许可复现模型

通过还原相关类似的配置，在开源预训练数据集的 1T Token上训练的3B、7B、13B模型，提供预训练OpenLLaMA模型的PyTorch和JAX权重，以及与原始LLaMA模型的比较的评估结果

以两种不同的格式发布权重：一种用于EasyLM框架的EasyLM格式，以及一种用于Hugging Face Transformer库的PyTorch格式，EasyLM训练框架和权重检查点均使用Apache 2.0许可

openlm-research主页:https://huggingface.co/openlm-research

相关评估结果:

<img src="https://img.saraba1st.com/forum/202306/19/031640xb50mi5yg340biab.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230619-031619.jpg</strong> (122.53 KB, 下载次数: 0)

下载附件

2023-6-19 03:16 上传

注:We removed the task CB and WSC from our benchmark, as our model performs suspiciously well on these two tasks. We hypothesize that there could be a benchmark data contamination in the training set.<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  marrrk  
##### 566#       发表于 2023-6-19 15:07

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61338648&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-6-19 03:13</a>

openllama llm

 Meta AI的LLaMA大型语言模型的开源复现模型，最近完全训练完了</blockquote>
方便加下微信不？想有偿请教一些ai方面的问题，多谢！


*****

####  Machinery  
##### 567#       发表于 2023-6-19 17:40

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61344343&amp;ptid=2126390" target="_blank">marrrk 发表于 2023-6-19 15:07</a>
方便加下微信不？想有偿请教一些ai方面的问题，多谢！</blockquote>
这个就没必要了吧…开这个贴的主要目标是做一个开源模型项目的信息汇总<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">如果是AI方面的比较专业的问题，应该需要咨询一些比较专业的人士或者在相关项目或者群了解，这样问题比较容易解决

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 568#       发表于 2023-6-20 01:30

GAIA-1

用于生成驾驶场景的帧序列预测方法

相关博客:https://wayve.ai/thinking/introducing-gaia1/

代码:(无，等待项目更新)

演示视频:https://youtu.be/cwJ_TV9Daqo

GAIA-1是一种新的生成式AI模型，可以通过利用视频、文本和动作相关输入来创造逼真的驾驶状态的场景视频

能提供对场景中智能车辆行为和场景特征的细粒度控制，使其可以成为研究、模拟和训练目的的理想选择，为自动驾驶领域的研发创新提供了许多可能

GAIA-1是一种多模态方法，通过对Wayve庞大的真实世界中的英国城市驾驶数据集进行训练，模型学习了如何预测视频序列中的后续帧，从而无需任何标签即可实现自回归预测能力，这类似于在大型语言模型(LLM)中看到的方法

GAIA-1不仅仅是一个标准的生成视频模型，同时也是一个真实的世界模型，可以学习理解和理清驾驶的重要概念，包括汽车、卡车、公共汽车、行人、骑自行车的人、道路布局、建筑物和交通信号灯等场景事物的规则与生成状态

使GAIA-1与众不同的是它能够对自我车辆行为和其他基本场景特征进行细粒度控制，无论是改变场景中智能车辆的行为还是修改整体场景动态，模型都能提供无与伦比的灵活性，使其成为加速自动驾驶基础模型开发的宝贵工具

GAIA-1通过对各种驾驶数据的广泛训练，综合了现实世界的固有结构和模式，使其能够生成非常逼真和多样化的驾驶场景，这一成就代表着实现具身人工智能的重要一步，人工系统不仅可以与世界互动，还可以理解和再现其规则和行为

当用几秒钟的起始上下文内容提示GAIA-1时，它可以生成几种不同的可能的未来，这表明即使基于相同的视频提示，GAIA-1也明白未来可能会发生许多不同的事情

<img src="https://img.saraba1st.com/forum/202306/20/012951gq1r6vogqo6o61fv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230619-011516.jpg</strong> (128.75 KB, 下载次数: 0)

下载附件

2023-6-20 01:29 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 569#       发表于 2023-6-20 02:56

 本帖最后由 Machinery 于 2023-6-20 02:57 编辑 

CommonClaim数据集

一个包含20000个布尔值形式标记的常识语句的数据集，每个语句都由两个人标记为common-knowledge-true、common-knowledge-false或neither

数据集地址(common_claim.csv文件):https://github.com/thestephencasper/common_claim

数据集来自论文(Explore, Establish, Exploit: Red Teaming Language Models from Scratch相关连接:https://arxiv.org/abs//2306.09896)，by the MIT Algorithmic Alignment Group

<img src="https://img.saraba1st.com/forum/202306/20/025336kngvqggn7nim47q4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-025323.jpg</strong> (569.82 KB, 下载次数: 0)

下载附件

2023-6-20 02:53 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 570#       发表于 2023-6-20 04:02

 本帖最后由 Machinery 于 2023-6-20 04:04 编辑 

RPT

通过感觉运动预训练进行机器人动作学习

项目主页:https://robotic-pretrained-transformer.github.io/

代码:coming soon

提出了一种自监督学习的机器人感觉运动预训练方法，训练出的模型称之为RPT，RPT是一个在感觉运动标记序列上运行的Transformer

给定一系列相机图像、运动知觉的机器人状态和过去的动作，将交错的序列编码为Tokens，屏蔽掉随机的子集，并训练模型来预测被屏蔽掉的内容

作为假设，如果机器人能够预测缺失的内容，它就已经相当于获得了一个能够使其行动的物理世界的良好模仿模型

RPT旨在对潜在的视觉表征进行操作，这使得预测易于处理，并且能够扩展到10倍大的模型，并在真实机器人上进行10Hz推理

结合使用运动规划和基于模型的抓取算法，在9个月内收集了20000个真实世界轨迹的数据集，实验发现，对这些数据的预训练始终优于从头开始的训练，可以提高块堆叠任务2倍的性能，并且具有良好的模型可拓展性

<img src="https://img.saraba1st.com/forum/202306/20/035442doibi70in0bd0nhk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-035356.jpg</strong> (54.11 KB, 下载次数: 0)

下载附件

2023-6-20 03:54 上传

<img src="https://img.saraba1st.com/forum/202306/20/035442btgie0e3utqgtt6t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-035406.jpg</strong> (52.14 KB, 下载次数: 0)

下载附件

2023-6-20 03:54 上传

给定一系列图像、感觉运动的机器人状态和过去的机器人动作，将交错的序列编码为Tokens，通过屏蔽掉一个随机子集来训练一个模型来预测屏蔽掉的内容，使用高屏蔽率对所有模态和时间执行随机掩蔽，这会鼓励模型学习跨模态与时空表征

<img src="https://img.saraba1st.com/forum/202306/20/040255hauvva90nwnf97un.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-040231.jpg</strong> (131.35 KB, 下载次数: 0)

下载附件

2023-6-20 04:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 571#       发表于 2023-6-20 04:33

 本帖最后由 Machinery 于 2023-6-20 04:34 编辑 

CLIPSonic

使用未标记的视频和预训练语言视觉模型进行文本到音频的合成

项目主页:https://salu133445.github.io/clipsonic/

最近的工作使用大量的文本音频对数据研究了从文本到音频的合成，但是，很难获得带有高质量文本注释的录音

在本文中，通过使用未标记的视频和预训练的语言视觉模型来处理文本到音频的合成，利用视觉模态作为桥梁学习所需的文本与音频的对应关系

训练了一个条件扩散模型来生成相关视频的音轨，给定一个由预训练的CLIP模型编码的视频帧，在测试时，首先探索了零样本模态迁移，并使用CLIP编码的文本查询来调节扩散模型，但是观察到图像查询的性能明显下降

为了缩小这一差距，进一步的采用预训练的先验扩散模型在给定CLIP文本嵌入的情况下生成CLIP图像嵌入

结果表明了所提出方法的有效性，并且预训练的扩散先验可以减少模态间的转移差距，虽然本文专注于文本到音频的合成，但所提出的模型还可以从图像查询生成音频，并且在主观听力测试中显示出与最先进的图像到音频合成模型相比具有竞争力的性能

这项研究提供了一种接近文本到音频合成的新方向，可以利用视频中自然发生的视听对应和预训练语言视觉模型的帮助

<img src="https://img.saraba1st.com/forum/202306/20/043117irwwsxzeg4wjpp84.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-042336.jpg</strong> (55.13 KB, 下载次数: 0)

下载附件

2023-6-20 04:31 上传

<img src="https://img.saraba1st.com/forum/202306/20/043117l5jv5x00o3jv6ju6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-042427__01.jpg</strong> (183.47 KB, 下载次数: 0)

下载附件

2023-6-20 04:31 上传

CLIPSonic模型，在训练期间，CLIPSonic学习给定视频帧中的图像的情况下合成视频的音轨，在推理时，以“[label]的照片”的形式提供文本查询以进行文本到音频合成，使用预训练的扩散先验模型来缩小文本查询(用于推理)和图像查询(用于训练)之间的差距，生成的梅尔频谱图通过预训练的BigVGAN模型反转回波形

<img src="https://img.saraba1st.com/forum/202306/20/043130qjd3pl3jimpjwv4w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-042036.jpg</strong> (57.95 KB, 下载次数: 0)

下载附件

2023-6-20 04:31 上传

文本到音频模型:
CLIPSonic-ZS(零样本模态迁移):CLIPSonic模型在零样本设置中使用CLIP文本嵌入查询
CLIPSonic-PD(预训练扩散先验):CLIPSonic使用预训练的扩散先验模型生成的CLIP图像嵌入进行查询
CLIPSonic-SD(监督扩散先验):CLIPSonic使用在目标数据集上训练的扩散先验模型生成的CLIP图像嵌入进行查询
CLIP-TTA:基线模型，将CLIP文本嵌入转换为梅尔频谱图
CLAP-TTA:基线模型，将CLAP文本嵌入转换为梅尔频谱图

图像到音频模型:
CLIPSonic-IQ(图像查询):CLIPSonic模型使用 CLIP 图像嵌入进行查询
SpecVQGAN:Iashin和Rahtu(2021)提出的图像到音频合成模型
im2wav:Sheffer和Adi(2023)提出的最先进的图像到音频合成模型

评估结果:

<img src="https://img.saraba1st.com/forum/202306/20/043148pf54xv6kcx2txgyf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-042447__01.jpg</strong> (137.92 KB, 下载次数: 0)

下载附件

2023-6-20 04:31 上传

<img src="https://img.saraba1st.com/forum/202306/20/043242in6nb6s6qgaya4qm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-043215.jpg</strong> (153.49 KB, 下载次数: 0)

下载附件

2023-6-20 04:32 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 572#       发表于 2023-6-20 06:01

AvatarBooth

AvatarBooth是一个文本到3D的生成模型，可以使用文本描述创建一个可动画化的3D模型

项目主页:https://zeng-yifei.github.io/avatarbooth_page/

github项目仓库(coming soon):https://github.com/zeng-yifei/AvatarBooth

还可以通过使用手机中的4~6张照片生成自定义模型，或者从扩散模型生成角色设计

以及使用任何魔法词语来改变最终的角色生成效果，并且可以固定角色本身的相关身份

<img src="https://img.saraba1st.com/forum/202306/20/055406w2fhl6eap2s3fmhm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-055335.jpg</strong> (121.55 KB, 下载次数: 0)

下载附件

2023-6-20 05:54 上传

<img src="https://img.saraba1st.com/forum/202306/20/055406inkpmpphpryuugg6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-055351.jpg</strong> (110.3 KB, 下载次数: 0)

下载附件

2023-6-20 05:54 上传

与以前只能根据简单文本描述合成化身的方法不同，本方法可以从随意获取的面部或身体的图像创建个性化的化身，同时仍然支持基于文本的生成和编辑

AvatarBooth通过分别对人脸和身体使用双微调扩散模型来精确控制化身生成，这使模型能够捕捉到面部外观、服装和配饰的复杂细节，从而生成高度逼真的化身

此外，在优化过程中引入姿势一致约束后，可以增强扩散模型合成头部图像的多视图一致性，从而消除不受控制的人体姿势的干扰

同时提出了一种多分辨率渲染策略，有助于对3D头像生成进行从粗到细的细粒度监督，从而提高本方法的性能，生成的化身模型还可以使用额外的文本描述进行进一步编辑，并由运动序列驱动行动

实验表明，AvatarBooth在文本提示或特定图像的渲染和几何质量方面优于以前的文本到3D方法

<img src="https://img.saraba1st.com/forum/202306/20/055955c0889z80hh9hfbyf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-055855.jpg</strong> (362.44 KB, 下载次数: 0)

下载附件

2023-6-20 05:59 上传

<img src="https://img.saraba1st.com/forum/202306/20/055955erhpthubclpvdqvl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-055904.jpg</strong> (346.83 KB, 下载次数: 0)

下载附件

2023-6-20 05:59 上传

<img src="https://img.saraba1st.com/forum/202306/20/055955bkk335xm9my6t0kb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-055937.jpg</strong> (62.75 KB, 下载次数: 0)

下载附件

2023-6-20 05:59 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 573#       发表于 2023-6-20 06:23

MagicBrush

用于指令引导图像编辑的手动标注数据集

项目主页:https://osu-nlp-group.github.io/MagicBrush/

github项目仓库:https://github.com/OSU-NLP-Group/MagicBrush

训练数据集地址(测试数据集请查看项目主页):https://huggingface.co/datasets/osunlp/MagicBrush

日常生活中广泛需要文本引导的图像编辑，从个人使用到Photoshop等专业应用程序，然而，现有方法要么是零样本训练的，要么是在包含大量噪声的自动合成数据集上训练的

因此，它们仍然需要大量手动调整才能在实践中产生理想的结果，为了解决这个问题，引入了MagicBrush，第一个用于指令引导真实图像编辑的大规模手动标注数据集

涵盖多种场景：单轮、多轮、提供Mask和无Mask编辑，MagicBrush包含超过10K个手动标注的三元组样本(源图像、指令、目标图像)，支持训练大规模文本引导图像编辑模型

通过使用MagicBrush微调的InstructPix2Pix进行了测试，结果表明新模型可以根据人类评估生成更好的图像

进一步进行了广泛的实验，以从多个维度评估当前的图像编辑基线，包括定量、定性和人类评估

<img src="https://img.saraba1st.com/forum/202306/20/062347wkg4kv9vk8zm662x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-062017.jpg</strong> (325.25 KB, 下载次数: 0)

下载附件

2023-6-20 06:23 上传

<img src="https://img.saraba1st.com/forum/202306/20/062347fbbny12xutmtuu16.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-062036.jpg</strong> (127.8 KB, 下载次数: 0)

下载附件

2023-6-20 06:23 上传

<img src="https://img.saraba1st.com/forum/202306/20/062347i0lszlkvp0dkspxf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-062053.jpg</strong> (358.08 KB, 下载次数: 0)

下载附件

2023-6-20 06:23 上传

评估结果

<img src="https://img.saraba1st.com/forum/202306/20/062348ld6nj3ja77qhj2yg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230620-062132.jpg</strong> (1016.09 KB, 下载次数: 0)

下载附件

2023-6-20 06:23 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  夜游宫  
##### 574#       发表于 2023-6-20 08:02

<img src="https://static.saraba1st.com/image/smiley/face2017/125.png" referrerpolicy="no-referrer">日新月异啊


*****

####  Machinery  
##### 575#       发表于 2023-6-21 03:09

 本帖最后由 Machinery 于 2023-6-21 03:10 编辑 

Vid2Avatar

通过自监督场景分解从真实自然场景视频中重建3D人体化身

项目主页:https://moygcc.github.io/vid2avatar/

github项目仓库:https://github.com/MoyGcc/vid2avatar

演示视频:https://youtu.be/EGi47YeIeGQ

<img src="https://img.saraba1st.com/forum/202306/21/031015eyypgt8inin8z6ps.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-031003.jpg</strong> (266.98 KB, 下载次数: 0)

下载附件

2023-6-21 03:10 上传

Vid2Avatar是一种从单目的真实自然场景视频中学习重建人类3D化身的方法

重建视频中自然移动的人类是困难的，解决它需要准确地将人类与任意背景分开，此外，它还需要学习从短视频序列中重建详细的3D人物表面纹理，使其更具挑战性

尽管存在这些挑战，本文中的方法不需要任何基准真实的监督或者从穿着衣服的人体扫描的大型数据集中提取先验知识，也不依赖任何外部分割模块，相反，它通过对场景中的人和背景进行联合建模，通过两个独立的NERF场进行参数化，直接在3D中解决场景分解和表面重建的任务

具体来说则是在规范空间中定义了时间一致的人体表征，并对背景模型、规范人体形状和纹理以及每帧的人体姿态参数进行了全局优化

一种从粗到精的立体渲染和采样策略，以干净地分离动态人体和静态背景，从而产生详细而稳健的3D人体几何重建

在公开可用的数据集上评估方法的结果，展示了对现有技术的改进

<img src="https://img.saraba1st.com/forum/202306/21/030249hz100bbrag73zazm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-030150.jpg</strong> (101.3 KB, 下载次数: 0)

下载附件

2023-6-21 03:02 上传

Vid2Avatar架构，对场景中的人物和背景进行隐式建模，通过两个独立的NERF场进行参数化，两个NERF场是从图像中联合学习的，用以合成整个场景

为了减轻接触身体的场景部分的模糊性并更好地描绘表面，利用规范空间中动态更新的人体形状来规范光线不透明度

研究组还创建了一个名为SynWild的新数据集来评估真实自然场景中的单目视频的人体表面重建，更多详细信息和数据集的下载链接将很快在项目空间中提供

<img src="https://img.saraba1st.com/forum/202306/21/030606odwfvogm7doco9yc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-030525.jpg</strong> (308.45 KB, 下载次数: 0)

下载附件

2023-6-21 03:06 上传

<img src="https://img.saraba1st.com/forum/202306/21/030606fp2kgudz0uwq0h70.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-030543.jpg</strong> (332.97 KB, 下载次数: 0)

下载附件

2023-6-21 03:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 576#       发表于 2023-6-21 03:18

KnowLM

知识丰富的大型语言模型框架KnowLM，自带预训练和指令微调代码(支持多机多GPU设置)，以及名为ZhiXi(智析)的双语信息抽取LLM

github项目地址:https://github.com/zjunlp/KnowLM

智析13b差异权重:https://huggingface.co/zjunlp/zhixi-13b-diff

<img src="https://img.saraba1st.com/forum/202306/21/031851ma0wzeda393aciid.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-031245.jpg</strong> (439.53 KB, 下载次数: 0)

下载附件

2023-6-21 03:18 上传

<img src="https://img.saraba1st.com/forum/202306/21/031851tbb1ucx73j5qj11b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-031306.jpg</strong> (71.5 KB, 下载次数: 0)

下载附件

2023-6-21 03:18 上传

<img src="https://img.saraba1st.com/forum/202306/21/031851lsn808ad8i63m2aa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-031402.jpg</strong> (804.27 KB, 下载次数: 0)

下载附件

2023-6-21 03:18 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 577#       发表于 2023-6-21 04:08

VectorFusion

通过提炼的基于像素的扩散模型使用文本生成SVG(可缩放矢量图形)

项目主页:https://ajayj.com/vectorfusion/

扩散模型在文本到图像的合成中显示出令人印象深刻的结果，使用大量带字幕图像的数据集，扩散模型可以学会生成高度多样化的对象和场景的图像

然而，设计人员经常使用矢量表示的图像，如可缩放矢量图形(SVG)，并且用于数字图标、图形和贴图等各种用途

矢量图形可以缩放到任意大小并且紧凑，在本文中展示了在图像像素表示上训练的文本条件扩散模型也可用于生成可导出的SVG

本项工作中并没有使用带字幕说明的SVG相关的大型数据集，相反，受最近关于文本到3D合成的工作的启发，通过将文本到图像的扩散样本矢量化，并使用分数蒸馏采样损失进行微调

通过优化可微分的矢量图形光栅器，本方法从预训练的扩散模型中提取了抽象语义知识，在约束矢量表征过程中，还可以生成连贯的像素艺术和草图类型的图片，VectorFusion比之前优化CLIP的相关项目相比产生了更连贯的图形

<img src="https://img.saraba1st.com/forum/202306/21/040425z2nzxii2xgfhon1y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-040249.jpg</strong> (432.51 KB, 下载次数: 0)

下载附件

2023-6-21 04:04 上传

<img src="https://img.saraba1st.com/forum/202306/21/040425cogs77xszroerxlo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-040323.jpg</strong> (596.25 KB, 下载次数: 0)

下载附件

2023-6-21 04:04 上传

<img src="https://img.saraba1st.com/forum/202306/21/040425rtv8354ctnt865e4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-040359.jpg</strong> (657.42 KB, 下载次数: 0)

下载附件

2023-6-21 04:04 上传

<img src="https://img.saraba1st.com/forum/202306/21/040839ioq8p86qoyjjrpzm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230621-040714.jpg</strong> (560.08 KB, 下载次数: 0)

下载附件

2023-6-21 04:08 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 578#       发表于 2023-6-22 05:24

 本帖最后由 Machinery 于 2023-6-22 05:27 编辑 

MotionGPT

微调LLM可以成为通用动作生成器

项目主页:https://qiqiapink.github.io/MotionGPT/

github项目仓库:https://github.com/qiqiApink/MotionGPT

从给定的动作描述生成真实的人体动作已经取得了显著的进步，近期的相关工作在直接从文本动作描述生成运动方面取得了令人瞩目的成果，但通常只支持单一模态的控制信号

这限制了它们在真实数字人类行业中的应用，本文介绍了一种通用运动动作生成器(MotionGPT)，可以使用多模态控制信号，例如文本和单帧姿态

通过将多模态信号视为大型语言模型(LLM)中的特殊输入Token来生成连续的人体运动，具体来说，首先将多模态控制信号量化为离散的编码，然后将它们制定为统一的提示指令，之后要求LLM生成运动的答案

MotionGPT仅调整了0.4%的LLM参数即可使用，同时展示了具有多模态控制信号的统一人体运动生成模型，据目前所知，MotionGPT是第一种通过多模态控制信号生成人体运动的方法

<img src="https://img.saraba1st.com/forum/202306/22/052352w45yh3hb0ygr68hy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-051128.jpg</strong> (135.92 KB, 下载次数: 0)

下载附件

2023-6-22 05:23 上传

<img src="https://img.saraba1st.com/forum/202306/22/052352na16uns6n6zvahn4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-051147.jpg</strong> (192.17 KB, 下载次数: 0)

下载附件

2023-6-22 05:23 上传

<img src="https://img.saraba1st.com/forum/202306/22/052352q4jsvspsbpsphus4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-052332.jpg</strong> (56.56 KB, 下载次数: 0)

下载附件

2023-6-22 05:23 上传

相关评估结果:

<img src="https://img.saraba1st.com/forum/202306/22/052627zdr9wrctivtcejrv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-052554.jpg</strong> (254.65 KB, 下载次数: 0)

下载附件

2023-6-22 05:26 上传

多模态控制演示:

<img src="https://img.saraba1st.com/forum/202306/22/052703uwu3wststupl2stm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-052646.jpg</strong> (314.56 KB, 下载次数: 0)

下载附件

2023-6-22 05:27 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 579#       发表于 2023-6-22 06:04

RepoFusion

通过训练代码模型了解代码仓库上下文以提升能力

hugface项目仓库:https://huggingface.co/RepoFusion

尽管大型语言模型(LLM)在GitHub Copilot等编码助手中应用取得了巨大成功，但这些模型很难理解代码仓库中存在的上下文内容(例如相关导入、父类、具有相似名称的文件等)，因而生成不准确的代码

当将这些助手用于模型在训练期间未见过的代码仓库(例如专有软件或正在进行的代码项目)时，这种效果更加明显

最近的相关工作表明了在模型推理过程中使用代码仓库中的上下文内容是不错的研究方向，在这项工作中，扩展了这个想法并提出了RepoFusion，这是一个通过训练模型学习合并的相关代码库上下文内容的框架

通过单行代码补完实验表明，使用代码仓库上下文训练的模型显著优于CodeGen-16B-multi等更大的代码模型(73倍大)，并且与使用Fill-in-the-Middle建模目标训练的StarCoderBase模型(70倍大)的性能非常匹配

这些结果是一个新颖且令人信服的证明，说明了使用代码仓库上下文可以带来可观的性能收益，之后进行了广泛的消融研究，以调查设计选择的影响，例如上下文类型、上下文数量、上下文长度和框架内的初始化等

发布了Stack-Repo，这是一个包含200个Java代码仓库的数据集，具有宽松的许可证和相似重复数据删除增强的文件，这些文件通过三种类型的存储库上下文进行了增强，此外，也将发布RepoFusion检查点权重

<img src="https://img.saraba1st.com/forum/202306/22/054300y9sweneh18eeqhez.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-054227.jpg</strong> (299.06 KB, 下载次数: 0)

下载附件

2023-6-22 05:43 上传

RepoFusion，给定代码仓库中的多个相关上下文(称为Repo Contexts)，RepoFusion将周围上下文(以灰色突出显示)附加到每个Repo Contexts中，然后分别对它们进行编码，组合它们以产生预测目标的结果

<img src="https://img.saraba1st.com/forum/202306/22/054802yaa6q0xrjo2c2ecg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-054743.jpg</strong> (76.9 KB, 下载次数: 0)

下载附件

2023-6-22 05:48 上传

图 2：使用不同的策略以从prompt proposal contexts(PPC)中处理生成生成Repo Contexts(RC):
(a)T-rank:截断第i个排名的PPC以生成第i个RC
(b)T-rand:将截断的第i个PPC放置在RepoFusion的RC序列中的随机位置j处
(c)NT-Rank:每个PPC产生所需数量的RC，以在不截断的情况下耗尽其所有Token长度
(d)NT-Prior-Last:为先前的PPC保留最后的r RC，并像NT-Rank中那样填充其余的RC

评估结果:

<img src="https://img.saraba1st.com/forum/202306/22/060427gjslzg2pgl9q05gf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-060417.jpg</strong> (271.32 KB, 下载次数: 0)

下载附件

2023-6-22 06:04 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 580#       发表于 2023-6-22 06:41

BayLing(百聆)

通过大型语言模型的交互式翻译桥接跨语言对齐和指令遵循

BayLing，一种遵循指令的LLM，具有人性化的多语言能力、擅长多轮交互式翻译

项目主页:https://nlp.ict.ac.cn/bayling/

模型差异权重:https://huggingface.co/ICTNLP/bayling-13b-diff

github项目仓库: https://github.com/ictnlp/BayLing

演示demo(需要申请):https://nlp.ict.ac.cn/bayling/demo/

相关论文:https://arxiv.org/abs/2306.10968

大型语言模型（LLM）在语言理解和生成方面表现出了非凡的能力，从基础LLM到指令跟随LLM，指令调整在使LLM符合人类偏好方面发挥着至关重要的作用

然而，现有的LLM通常专注于英语，导致非英语语言的表现较差，为了提高非英语语言的性能，需要为基础LLM收集特定语言的训练数据，并构建特定语言的指令进行指令调优，这两个任务都是繁重的工作

为了最大限度地减少人力工作量，建议通过交互式翻译任务将语言生成和指令跟踪的能力从英语转移到其他语言

为此研究组开发了BayLing，以LLaMA为基础的LLM，可以构建交互式翻译指令以用于合成语料指导调优的指令跟随LLM

<img src="https://img.saraba1st.com/forum/202306/22/063724mnglh5hkcv0kckvv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-063313.jpg</strong> (219.72 KB, 下载次数: 0)

下载附件

2023-6-22 06:37 上传

<img src="https://img.saraba1st.com/forum/202306/22/063724csm94zpra90ydk9m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-063340.jpg</strong> (151.84 KB, 下载次数: 0)

下载附件

2023-6-22 06:37 上传

<img src="https://img.saraba1st.com/forum/202306/22/063724wj19ssirhos1e99s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-063431__02.jpg</strong> (797.82 KB, 下载次数: 0)

下载附件

2023-6-22 06:37 上传

<img src="https://img.saraba1st.com/forum/202306/22/063724gkwsgnwy6ggkilln.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-063431__03.jpg</strong> (341.21 KB, 下载次数: 0)

下载附件

2023-6-22 06:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 581#       发表于 2023-6-22 07:36

vllm

简介如图所示

github项目地址:https://github.com/vllm-project/vllm

<img src="https://img.saraba1st.com/forum/202306/22/073540nzgw6bkk6bc2g42p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-073343.jpg</strong> (141.22 KB, 下载次数: 0)

下载附件

2023-6-22 07:35 上传

<img src="https://img.saraba1st.com/forum/202306/22/073540rxmmy0mybnzuonum.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230622-073416.jpg</strong> (240.76 KB, 下载次数: 0)

下载附件

2023-6-22 07:35 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  lvseqiji  
##### 582#       发表于 2023-6-22 12:20

没人说这事情吗，推上几方人爆料GPT4内幕，https://twitter.com/swyx/status/1671272883379908608

GPT4并不是一个模型，而是8个200B规模的不同模型胶水起来的。

—— 来自 Sony XQ-BQ72, Android 13上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.2-play


*****

####  宵待草  
##### 583#       发表于 2023-6-22 12:26

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61382142&amp;ptid=2126390" target="_blank">lvseqiji 发表于 2023-6-22 12:20</a>

没人说这事情吗，推上几方人爆料GPT4内幕，https://twitter.com/swyx/status/1671272883379908608</blockquote>
听起来像是做 boosting，对机器学习挺常见的吧


*****

####  Machinery  
##### 584#       发表于 2023-6-22 15:43

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61382142&amp;ptid=2126390" target="_blank">lvseqiji 发表于 2023-6-22 12:20</a>
没人说这事情吗，推上几方人爆料GPT4内幕，https://twitter.com/swyx/status/1671272883379908608</blockquote>
混合专家模型在业界一直有这个线路，甚至国内也有不少大厂做过，不算什么新闻…

比如https://zhuanlan.zhihu.com/p/588651760

主要区别就是openai用的是220b的大型专家而之前的moe趋向于小型专家

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  cyp909  
##### 585#       发表于 2023-6-22 22:57

提问
1. 现在有可以ghs特化的本地可部署的模型了吗，离喂刘备训练lora还有多远
2. 现阶段有无必要买2080ti魔改版来玩，还是说目前本地可部署模型在ghs上不如claude一根

—— 来自 Xiaomi 22041211AC, Android 13上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4


*****

####  icer  
##### 586#       发表于 2023-6-22 23:25

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61389366&amp;ptid=2126390" target="_blank">cyp909 发表于 2023-6-22 22:57</a>

提问

1. 现在有可以ghs特化的本地可部署的模型了吗，离喂刘备训练lora还有多远

2. 现阶段有无必要买2080ti ...</blockquote>
现在单卡就能部署的llm全是玩具，性能完全不行


*****

####  Machinery  
##### 587#       发表于 2023-6-23 00:11

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61389366&amp;ptid=2126390" target="_blank">cyp909 发表于 2023-6-22 22:57</a>
提问
1. 现在有可以ghs特化的本地可部署的模型了吗，离喂刘备训练lora还有多远
2. 现阶段有无必要买2080ti ...</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/125.png" referrerpolicy="no-referrer">你这个属于下游任务，难点在于不会有人去整这方面的语料去做预训练模型，预训练不行，微调也无从说起，claude是因为预训练有喂进相关语料才能诱导出来，即使如此，在线llm依然加了大量过滤和道德倾向锁定

可以这么说，真正的问题从来不在于lora或者微调之类的方面，也和模型大小无关，完全无关技术，实际上7b或者13b就完全能很好的搞定这种事

如果你真的要尝试…<img src="https://static.saraba1st.com/image/smiley/face2017/245.png" referrerpolicy="no-referrer">提个关键词吧，rwkv-world

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 588#       发表于 2023-6-23 04:32

Fast Segment Anything

快速SAM分割

github项目代码库:https://github.com/CASIA-IVA-Lab/FastSAM

最近提出的SAM模型对许多计算机视觉任务产生了重大影响，但其巨大的计算成本使其无法在行业场景中得到更广泛的应用

计算成本主要来自高分辨率输入的Transformer架构的固有消耗，在本文中，提出了一种具有可比性能的加速替代方法

将任务重新表述为实例分割生成和提示之后，发现具有实例分割能力的常规CNN检测器也可以很好地完成任务

具体来说，通过将此任务转换为经过充分研究的实例分割任务，并且在仅使用SAM模型作者发布的SA-1B数据集的1/50的情况下直接训练现有的实例分割方法，实现了与SAM方法相当的性能，同时运行速度提高了50倍

<img src="https://img.saraba1st.com/forum/202306/23/042359wp884scda9dajn11.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-042318.jpg</strong> (202.98 KB, 下载次数: 0)

下载附件

2023-6-23 04:23 上传

FastSAM和SAM的对比分析:
A.FastSAM和SAM在单个NVIDIA GeForce RTX 3090上的速度比较
B.在BSDS500数据集上进行边缘检测的比较
C.Fast-SAM和SAM对COCO数据集上的AR@1000框体目标检测评估，SAM和FastSAM为使用PyTorch进行推理，FastSAM(TRT)则使用TensorRT进行推理

<img src="https://img.saraba1st.com/forum/202306/23/042817ydrnygryz2pb6i9d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-042730.jpg</strong> (147.32 KB, 下载次数: 0)

下载附件

2023-6-23 04:28 上传

FastSAM框架，包含两个阶段：全实例分割(AIS)和提示引导选择(PGS)，使用YOLOv8-seg来分割图像中的所有对象或区域，然后再使用多种提示来识别感兴趣的特定对象，提示主要涉及点提示、框提示、文字提示，其中文本提示基于CLIP

评估结果:

<img src="https://img.saraba1st.com/forum/202306/23/043201w77j183lffpx1vuw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-043049.jpg</strong> (205.29 KB, 下载次数: 0)

下载附件

2023-6-23 04:32 上传

<img src="https://img.saraba1st.com/forum/202306/23/043201etxhsxkllozqxqx2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-043101.jpg</strong> (144.9 KB, 下载次数: 0)

下载附件

2023-6-23 04:32 上传

<img src="https://img.saraba1st.com/forum/202306/23/043201paiczbco2ucxoz6a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-043120.jpg</strong> (265.82 KB, 下载次数: 0)

下载附件

2023-6-23 04:32 上传

<img src="https://img.saraba1st.com/forum/202306/23/043201w3kc80kk840k4kco.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-043130.jpg</strong> (341.64 KB, 下载次数: 0)

下载附件

2023-6-23 04:32 上传

失败案例:

<img src="https://img.saraba1st.com/forum/202306/23/043225esl929269wqlbmwb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-043147.jpg</strong> (148.04 KB, 下载次数: 0)

下载附件

2023-6-23 04:32 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 589#       发表于 2023-6-23 04:47

MPT-30B

简介请看附图

博客项目说明:https://www.mosaicml.com/blog/mpt-30b

mpt-30b模型权重:https://huggingface.co/mosaicml/mpt-30b

mpt-30b-instruct模型权重:https://huggingface.co/mosaicml/mpt-30b-instruct

mpt-30b-chat:https://huggingface.co/mosaicml/mpt-30b-chat

mpt-30b-chat的Demo演示:https://huggingface.co/spaces/mosaicml/mpt-30b-chat

<img src="https://img.saraba1st.com/forum/202306/23/044411a24ap864328nuh8h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-044150.jpg</strong> (208.2 KB, 下载次数: 0)

下载附件

2023-6-23 04:44 上传

<img src="https://img.saraba1st.com/forum/202306/23/044411azwekcgwcvwji8ki.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-044229.jpg</strong> (352.38 KB, 下载次数: 0)

下载附件

2023-6-23 04:44 上传

<img src="https://img.saraba1st.com/forum/202306/23/044411ljrvo7gcgc1g17rg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-044310.jpg</strong> (180.9 KB, 下载次数: 0)

下载附件

2023-6-23 04:44 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 590#       发表于 2023-6-23 05:38

 本帖最后由 Machinery 于 2023-6-23 05:40 编辑 

ViTEraser

利用视觉Transformer的力量通过SegMIM预训练去除场景图片中的文本内容

github项目代码仓库(预定公开，待整理):https://github.com/shannanyinxiang/ViTEraser

场景文本去除(STR)旨在用视觉连贯的背景替换自然场景中的文本笔划痕迹，最近的STR方法依赖于迭代细化或显式的文本掩码，导致定位文本的复杂性和准确性要求更高

此外，大多数现有的STR方法利用卷积神经网络(CNN)进行特征表征，而视觉Transformers(ViT)的潜力在很大程度上仍未被开发

在本文中提出了一种简单而有效的基于ViT的文本橡皮擦，称为ViTEraser，遵循简洁的编码器-解码器Transformer框架，不同类型的ViT也可以轻松集成到ViTEraser中，以增强长程依赖和全局推理

具体来说，编码器通过ViT blocks和patch embedding层将输入的图像进行分层次的映射到隐藏空间，而解码器通过ViT blocks和patch分割层逐渐将隐藏特征上采样到文本擦除的图像

由于ViTEraser实际上已经隐式集成了文本定位和修复，提出了一种新颖的端到端预训练方法，称为SegMIM，该方法将编码器和解码器分别专注于文本框分割和掩模图像建模任务

实验结果表明，带有SegMIM的ViTEraser在STR任务上大幅实现了新的SOTA性能成绩，此外，篡改场景文本检测的扩展实验任务也证明了ViTEraser对其他任务的通用性

<img src="https://img.saraba1st.com/forum/202306/23/052601nkzgb2g18siujnbl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-052531.jpg</strong> (135.81 KB, 下载次数: 0)

下载附件

2023-6-23 05:26 上传

ViTEraser与现有的STR相关范式的比较

ViTEraser方法重新审视了传统的单步一阶段框架，使用ViT进行特征建模和提出的SegMIM预训练对其进行了改进，ViTEraser在SCUT-EnsText上实现了SOTA性能

<img src="https://img.saraba1st.com/forum/202306/23/052919md7i9jxer99gf967.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-052838__01.jpg</strong> (196.33 KB, 下载次数: 0)

下载附件

2023-6-23 05:29 上传

ViTEraser 整体架构，ViTEraser遵循单阶段范式，但装备了ViT，用于增强远程上下文建模，从而产生一种简单而有效的STR方法，无需渐进式细化和文本定位子模型

<img src="https://img.saraba1st.com/forum/202306/23/053316lx9j1kxg5kddaqgs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-053240.jpg</strong> (47.36 KB, 下载次数: 0)

下载附件

2023-6-23 05:33 上传

<img src="https://img.saraba1st.com/forum/202306/23/053454u56nsvx6vscmm07n.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-053423.jpg</strong> (200.7 KB, 下载次数: 0)

下载附件

2023-6-23 05:34 上传

<img src="https://img.saraba1st.com/forum/202306/23/053459q69zsu91ja4sxjmm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-053440.jpg</strong> (93.63 KB, 下载次数: 0)

下载附件

2023-6-23 05:34 上传

SegMIM预训练流程:给定随机掩码图像，文本框分割和掩码图像建模任务分别在编码器和解码器之上完成，经过大规模场景文本检测数据集的端到端预训练，网络全面学习文本和背景的判别性和生成性特征，达到强大的全局推理能力

相关评估结果:

<img src="https://img.saraba1st.com/forum/202306/23/053703vn5fwgzunrlgw4wn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-053626.jpg</strong> (665.35 KB, 下载次数: 0)

下载附件

2023-6-23 05:37 上传

<img src="https://img.saraba1st.com/forum/202306/23/053703mlx8y7l675y8n8zx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230623-053650.jpg</strong> (388.63 KB, 下载次数: 0)

下载附件

2023-6-23 05:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  cyp909  
##### 591#       发表于 2023-6-23 07:10

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61390320&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-6-23 00:11</a>
你这个属于下游任务，难点在于不会有人去整这方面的语料去做预训练模型，预训练不行，微调也无从 ...</blockquote>
我看那个pygmalion ai号称最自由最适合nsfw的语言模型，跟rwkv这个比哪个强啊
以及这个可以自己喂刘备训练吗

—— 来自 Xiaomi 22041211AC, Android 13上的 [S1Next-鹅版](https://github.com/ykrank/S1-Next/releases) v2.5.4


*****

####  Machinery  
##### 592#       发表于 2023-6-23 16:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61391677&amp;ptid=2126390" target="_blank">cyp909 发表于 2023-6-23 07:10</a>
我看那个pygmalion ai号称最自由最适合nsfw的语言模型，跟rwkv这个比哪个强啊
以及这个可以自己喂刘备训 ...</blockquote>
Pygmalion是英文模型，rwkv是中文模型，rwkv有微调教程

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 593#       发表于 2023-6-24 04:58

DreamEdit/DreamEditBench

个性化主题(Subject)驱动的图像编辑与修改

项目主页:https://dreameditbenchteam.github.io/

github项目代码库:https://github.com/DreamEditBenchTeam/DreamEdit

DreamEditBench数据集:https://huggingface.co/datasets/tianleliphoebe/DreamEditBench

个性化主题图像生成方法一般使用几张图片作为样本(通常为3-5张，可以更多或更少)生成包含定制主题的看起来直观又自然的平滑图像

<img src="https://img.saraba1st.com/forum/202306/24/044704dkppowmfk2dlkefm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230624-044649.jpg</strong> (155.83 KB, 下载次数: 0)

下载附件

2023-6-24 04:47 上传

然而，之前的作品无法精确控制目标主体的背景和位置，在本文中提出了两个新颖的主题驱动子任务，即主题替换(将图片的主体修改为自定义的主体)和主题添加(将个性化主体自然的加入已经存在的图像中)

新任务在多个方面都具有挑战性：用定制的主题替换主题可以改变其形状、纹理和颜色，而将目标主题添加到所提供场景中的指定位置需要更为精密的上下文感知与参考

为了克服这两项新任务，研究组手动策划构造了一个新的数据集DreamEditBench，其中包含22个不同类型的主题和440个不同难度级别的源图像，聘请经过培训的评估员进行标准人工评估

还设计了一种创新方法，DreamEditor，可以通过执行迭代生成来解决这些任务，从而能够顺利适应定制主题

在本项目中，通过进行自动和人工评估，同时了解了DreamEditor和DreamEditBench上基线的相关性能

对于主题替换，发现现有模型对原始主体的形状和颜色很敏感，当源对象和目标对象高度不同时，模型失败率将急剧增加

对于主题添加，发现了现有模型无法轻松地将定制主题平滑自然地融合到背景中，导致生成的图像中出现明显的伪影

<img src="https://img.saraba1st.com/forum/202306/24/045355hdp5bbbv3v53pl5u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230624-045339.jpg</strong> (202.76 KB, 下载次数: 0)

下载附件

2023-6-24 04:53 上传

多重迭代编辑生成

<img src="https://img.saraba1st.com/forum/202306/24/045604yk0hfh409um0zeqf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230624-045449__01__02.jpg</strong> (370.6 KB, 下载次数: 0)

下载附件

2023-6-24 04:56 上传

DreamEditor框架与评估结果

<img src="https://img.saraba1st.com/forum/202306/24/045622eahrq2q3h6hsy98a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230624-045449__01__01.jpg</strong> (700.38 KB, 下载次数: 0)

下载附件

2023-6-24 04:56 上传

与其他模型的对比结果

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 594#       发表于 2023-6-24 06:26

Deep Language Networks

使用变分推理对堆叠LLM构架进行联合提示(prompt)训练

github项目地址:https://github.com/microsoft/deep-language-networks

相关论文:https://arxiv.org/abs/2306.12509

将大规模语言模型(Large Language Models/LLMs)视为神经网络中的随机语言层，其中可学习的参数为每个层中的自然语言提示(natural language prompts)

通过将两个这样的层堆叠起来，将前一层的输出馈送到下一层，这个堆叠的架构称之为深层语言网络(Deep Language Network/DLN)，首先展示了如何有效地进行单层语言网络(DLN-1)的提示优化，然后展示了如何训练双层DLN(DLN-2)联合学习两个提示

通过将第一层的输出视为潜变量来边缘化(marginalize)，设计了联合提示训练的变分推理算法，双层DLN(DLN-2)的性能高于单层，有时甚至可以与少样本GPT-4输出结果相当，即使网络中的每个LLM规模更小，功能更弱

生成的知识提示可以被视为一个固定的仅前向DLN-2，在第一层，LLM生成相关知识，而在第二层，另一个LLM将生成的知识作为输入并生成最终答案，像 ReAct、Reflexicon和Self-Consistency这样的提示技术都可以被视为具有不同提示初始化的DLN-1的集合

<img src="https://img.saraba1st.com/forum/202306/24/060103ol9l942j8zili9c9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230624-060032.jpg</strong> (159.19 KB, 下载次数: 0)

下载附件

2023-6-24 06:01 上传

左图是单层语言网络(DLN-1)执行情感分析任务的流程图示，输入和可训练的提示使用模板合并，然后提供给语言模型生成答案

右图是具有残差连接的双层语言网络(DLN-2)，执行日期理解任务，学习了两个提示，在此例中，隐藏的模板扩展了Chain-Of-Thought可学习的前缀(learnable prefix)，之后将第一层的输出隐藏，视为隐藏变量h，模板可以视为网络的超参数，即可使用变分推理来学习π0和π1

<img src="https://img.saraba1st.com/forum/202306/24/061734yumeo4fe4164kkzp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230624-061626.jpg</strong> (103.34 KB, 下载次数: 0)

下载附件

2023-6-24 06:17 上传

与DaVinci-003和GPT-4上的基线相比，浅层1层语言网络(DLN-1)的三个随机种子的平均测试准确率，对于可训练系统(即APE和DLN-1)，置信区间为95%，DaVinci-003为蓝色，GPT-4为粉色

<img src="https://img.saraba1st.com/forum/202306/24/062054o0jj6flwsf5v0nvj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230624-062029.jpg</strong> (112.37 KB, 下载次数: 0)

下载附件

2023-6-24 06:20 上传

DLN-2测试三个随机种子的平均准确率，结果来说优于GPT-4零样本基线，相比GPT-4零样本基线，DLN-2提供了超过20%的绝对提升效果

<img src="https://img.saraba1st.com/forum/202306/24/061922jxc4r4b0c3lm24ml.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230624-061848.jpg</strong> (125.18 KB, 下载次数: 0)

下载附件

2023-6-24 06:19 上传

Hyperbaton上DLN-1的最终提示，不仅包括说明，还包括训练集中的示例，这些样本是通过提示优化自动选择的，在某种程度上来说，这种方法结合了上下文学习和提示优化

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 595#       发表于 2023-6-25 05:56

 本帖最后由 Machinery 于 2023-6-25 05:57 编辑 

Zeroscope_v2 XL

基于Modelscope的无水印视频模型，能够根据文本生成1024x576的高质量平滑自然的视频内容

zeroscope_v2_576w模型:https://huggingface.co/cerspense/zeroscope_v2_576w

zeroscope_v2_XL模型:https://huggingface.co/cerspense/zeroscope_v2_XL

基于Modelscope的无水印视频模型，能够生成1024x 576的高质量视频，该模型训练集使用24帧、1024x576分辨率的9923个视频剪辑片段和29769个标记帧，配置使用偏移噪声进行训练

 Zeroscope_v2_XL被专门设计用于使用kabachuha的1111 text2video扩展中的vid2vid来上采样拓展由Zeroscope_v2_576w模型制作的内容

利用此模型作为升级器可以在更高分辨率下实现卓越的整体构图，从而允许在过渡到高分辨率渲染之前在576x320(或448x256)中进行更快的生成

当以1024x576渲染30帧内容时，zeroscope_v2_XL会使用约15.3GB的gpu vram

<img src="https://img.saraba1st.com/forum/202306/25/055243q0k969b19wkkohvu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230625-055225.jpg</strong> (139.43 KB, 下载次数: 0)

下载附件

2023-6-25 05:52 上传

<img src="https://img.saraba1st.com/forum/202306/25/055633y6g3jg4z4y363ru6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230625-055540.jpg</strong> (234.61 KB, 下载次数: 0)

下载附件

2023-6-25 05:56 上传

<img src="https://img.saraba1st.com/forum/202306/25/055633jyx8jq05kx9v9x9j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230625-055549.jpg</strong> (104.61 KB, 下载次数: 0)

下载附件

2023-6-25 05:56 上传

<img src="https://img.saraba1st.com/forum/202306/25/055633xwmyjndw6sminrn3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230625-055608.jpg</strong> (153.92 KB, 下载次数: 0)

下载附件

2023-6-25 05:56 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 596#       发表于 2023-6-25 20:03

ChatGLM2-6B

开源LLM chatglm6b的第二代版本，简介如图

github项目仓库:https://github.com/THUDM/ChatGLM2-6B

<img src="https://img.saraba1st.com/forum/202306/25/200323hhgyvhwdl1vddvdj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230625-200040__01.jpg</strong> (674.81 KB, 下载次数: 0)

下载附件

2023-6-25 20:03 上传

<img src="https://img.saraba1st.com/forum/202306/25/200323u8xfxw11xfa5wfxl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230625-200040__02.jpg</strong> (1007.75 KB, 下载次数: 0)

下载附件

2023-6-25 20:03 上传

<img src="https://img.saraba1st.com/forum/202306/25/200323uk9md53akv0dvw9e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230625-200040__03.jpg</strong> (417.84 KB, 下载次数: 0)

下载附件

2023-6-25 20:03 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  苦悩の梨  
##### 597#       发表于 2023-6-25 23:19

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61428919&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-6-25 20:03</a>
ChatGLM2-6B

开源LLM chatglm6b的第二代版本，简介如图</blockquote>
实测迅雷P2P可以拉满带宽，给抱脸省点流量钱<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">


*****

####  ZetaGo  
##### 598#       发表于 2023-6-26 00:16

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61107895&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-6-4 00:01</a>
Bytes Are All You Need

使用Transformers直接读取文件字节</blockquote>
脑洞大开


*****

####  Machinery  
##### 599#       发表于 2023-6-26 05:05

 本帖最后由 Machinery 于 2023-6-26 05:07 编辑 

PanoHead

形状感知的360°完全3D合成头像

项目主页:https://sizhean.github.io/panohead

相关论文:https://arxiv.org/abs/2303.13071

github项目代码库:https://github.com/SizheAn/PanoHead

开箱可用的colab文件仓库:https://github.com/camenduru/PanoHead-colab

近年来，3D人体的头部合成和重建在计算机视觉和计算机图形学领域引起了越来越多的兴趣，现有的SOTA 3D人体头部合成的3D生成对抗网络(GAN)要么仅限于近正面视图，要么难以在大视角下保持3D一致性

 PanoHead，这是第一个3D感知生成模型，能够仅使用自然场景的真实非结构化图像进行训练，实现360°高质量视图一致的完全图像合成，具有不同的外观和详细的几何形状

其核心思想为，通过提升最新3D GAN的表征能力，并在广泛分布视图可对齐的真实自然场景的图像上进行训练弥合了数据对齐差距

具体来说，提出了一种新颖的两阶段自适应图像对齐，用于进行稳健的3D GAN训练，同时进一步引入了一种三重网格神经体积表征，有效地解决了广泛采用的三平面公式中的正面和背面特征纠缠的问题

PanoHead将2D图像分割的先验知识灌输到3D场景结构的对抗性学习中，从而实现不同背景下的完美头部细节合成，受益于这些设计，本方法显著优于以前的3D GAN，可以生成具有精确几何形状和多样化外观的高质量3D头部，即使是合成长波浪发型和非洲发型，也可以从任意姿态渲染

此外，PanoHead也可以从单个输入图像重建完整的3D头部，以实现个性化的逼真3D化身的头像

PanoHead模型框架:

<img src="https://img.saraba1st.com/forum/202306/26/050413b4zva5v1zzzttvil.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-050237.jpg</strong> (218.63 KB, 下载次数: 0)

下载附件

2023-6-26 05:04 上传

对比评估结果:

<img src="https://img.saraba1st.com/forum/202306/26/050439o7kv2jjkhccedk00.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-050318.jpg</strong> (286.07 KB, 下载次数: 0)

下载附件

2023-6-26 05:04 上传

<img src="https://img.saraba1st.com/forum/202306/26/050439hldd9lymccvzbjpl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-050334.jpg</strong> (88.42 KB, 下载次数: 0)

下载附件

2023-6-26 05:04 上传

实例效果图:

<img src="https://img.saraba1st.com/forum/202306/26/050457xf6weq3hnh83wms8.jpg" referrerpolicy="no-referrer">

<strong>20230626_045540.jpg</strong> (218.07 KB, 下载次数: 0)

下载附件

2023-6-26 05:04 上传

<img src="https://img.saraba1st.com/forum/202306/26/050457g8m5oh2h7sm6shas.jpg" referrerpolicy="no-referrer">

<strong>20230626_045620.jpg</strong> (204.77 KB, 下载次数: 0)

下载附件

2023-6-26 05:04 上传

<img src="https://img.saraba1st.com/forum/202306/26/050457d7b1sst99u1gegue.jpg" referrerpolicy="no-referrer">

<strong>bd46605a-459c-4b62-8965-e7dd71499a8c.jpg</strong> (112.2 KB, 下载次数: 0)

下载附件

2023-6-26 05:04 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 600#       发表于 2023-6-26 05:36

unlimited-replicant

NovelAi系列模型的竞品

总结一下，用A6000*2花了1000 GPU小时在训练集与模型符合日本法律的条件下从Stable diffusion2.x代训练的二次元风格模型，可以直接像Stable diffusion2.x一样使用webui加载使用

hugface权重仓库:https://huggingface.co/alfredplpl/unlimited-replicant

<img src="https://img.saraba1st.com/forum/202306/26/053558dpxkks97yh3nnpuu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-053255.jpg</strong> (474.07 KB, 下载次数: 0)

下载附件

2023-6-26 05:35 上传

<img src="https://img.saraba1st.com/forum/202306/26/053605pctduk8saajzitcs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-053351__02.jpg</strong> (247.92 KB, 下载次数: 0)

下载附件

2023-6-26 05:36 上传

<img src="https://img.saraba1st.com/forum/202306/26/053609pvp3avxj7vvzxza3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-053351__01.jpg</strong> (406.65 KB, 下载次数: 0)

下载附件

2023-6-26 05:36 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 601#       发表于 2023-6-26 05:48

 本帖最后由 Machinery 于 2023-6-26 05:50 编辑 

orca_mini_3b

使用OpenLLaMa-3B模型在解释微调(explain tuned)数据集上进行训练的模型

模型权重下载地址:https://huggingface.co/psmathur/orca_mini_3b

数据集使用使用WizardLM、Alpaca和Dolly-V2数据集的指令和输入并应用Orca论文中的研究方法构建(WizardLM数据集~70K、Alpaca数据集~52K、Dolly-V2数据集~15K)

利用Orca研究论文中提供的所有15条系统指令生成自定义数据集，与原始数据集使用的普通指令调整方法相反，这有助于学生模型(此模型)从教师模型(ChatGPT/gpt-3.5-turbo-0301版本)学习思维过程

使用了Lambda Labs的8个A100(80G) GPU，训练持续约4小时，成本为48美元

<img src="https://img.saraba1st.com/forum/202306/26/054828sfoz4qj001eglh0f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-054645.jpg</strong> (160.04 KB, 下载次数: 0)

下载附件

2023-6-26 05:48 上传

研究组主页中的其他项目模型:https://huggingface.co/TheBloke

<img src="https://img.saraba1st.com/forum/202306/26/055010crj88ehe57a8tat6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-054931.jpg</strong> (111.41 KB, 下载次数: 0)

下载附件

2023-6-26 05:50 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 602#       发表于 2023-6-26 06:50

Latopia

vits(RVC)与diffusion-svc语音训练合成与学习GUI工具

github项目地址:https://github.com/ddPn08/Latopia

<img src="https://img.saraba1st.com/forum/202306/26/064921ouijp6udkguzyu1u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-064825.jpg</strong> (36.03 KB, 下载次数: 0)

下载附件

2023-6-26 06:49 上传

<img src="https://img.saraba1st.com/forum/202306/26/064921x9xo39xvn2qpcen3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-064840.jpg</strong> (43.15 KB, 下载次数: 0)

下载附件

2023-6-26 06:49 上传

todo，将预定为轻量用户实装一个webui

<img src="https://img.saraba1st.com/forum/202306/26/064921vm5m3za45aubpc4n.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230626-064909.jpg</strong> (84.75 KB, 下载次数: 0)

下载附件

2023-6-26 06:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 603#       发表于 2023-6-26 07:43

SeaThru-NeRF

散射介质中的神经辐射场(NERF)合成

项目主页:https://sea-thru-nerf.github.io

项目代码:coming soon

用于新视图生成的神经辐射场(NeRF)研究随着新模型和扩展的出现而呈爆炸式增长，然而，一个尚未解答的问题是，这些模型在水下或有雾的场景合成中会发生什么，答案是其中的介质会强烈影响合成的物体外观

到目前为止，NeRF及其变体模型都忽略了这些情况，然而，由于NeRF框架基于体积渲染，因此一旦进行适当建模，它其实可以拥有解释介质效果的能力

通过为散射介质中的NeRF合成开发了一种新的渲染模型，该模型基于SeaThru图像形成模型，并提出了一种用于学习场景信息和介质参数的合适架构

使用模拟和真实场景展示了本文方法的优势，正确渲染了新颖的具有真实感的水下场景视图，更令人兴奋的是，这些场景的合成视图结果都非常清晰，去除了相机和场景之间的介质，并重建了被介质严重遮挡的远处物体的外观和深度

通过将散射图像形成模型合并到NeRF渲染方程中，能够将场景分为“干净的”和“后向散射”部分，因此可以在有或没有参与介质的情况下渲染逼真的新颖视图，并在没有介质的情况下进行颜色恢复，就像图像是在晴朗的空气中拍摄的一样

库拉索岛的场景合成结果：原始图像(左)被增亮和白平衡(WB)以实现可视化，显示更多细节，而远离相机的区域(右上角)被严重的反向散射遮挡和衰减，这在恢复图像中被有效去除

模拟散射效果

对比结果:

水下成像模型

可感知介质的渲染:

SeaThru-NeRF模型构架:

附加结果对比(色彩恢复任务)

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  fufusako  
##### 604#       发表于 2023-6-26 13:36

这个帖pc端看不了，手机端看复制粘贴跳转又很不方便，难受哇


*****

####  Machinery  
##### 605#       发表于 2023-6-26 19:47

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61438741&amp;ptid=2126390" target="_blank">fufusako 发表于 2023-6-26 13:36</a>
这个帖pc端看不了，手机端看复制粘贴跳转又很不方便，难受哇</blockquote>
pc端倒是也能看，好像要权限高点才行…不是我设的

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 606#       发表于 2023-6-27 05:56

BYOD

大型语言模型的自监督评估

github项目仓库:https://github.com/neelsjain/BYOD

相关论文:https://arxiv.org/abs/2306.13651

随着大型语言模型(LLM)的兴起及其在不同领域的普遍部署，测量语言模型实际上的行为与数据至关重要，例如，在部署需要面向客户的聊天机器人时必须确保该模型不会以脏话回应公司客户的请求

当前的评估使用带有人工策划标注、小型、特定领域数据集来解决这个问题，这些评估集通常是从狭窄且简化的分布中采样的，数据源可能会在不知不觉中泄漏到训练集中导致误导性的评估

为了绕过这些缺点，通过分析LLM对输入文本转换的敏感性或不变性，提出了一个对LLM进行自监督评估的框架，自监督评估可以直接在真实自然场景中监察收集或实时模型部署期间流式传输的数据集上的LLM行为

除了对语法结构和标记化错误的敏感性之外，还展示了用于测量闭卷知识、毒性和长期上下文依赖的自我监督评估策略，当与类似的人工标注基准进行比较时，发现自我监督评估和人工监督评估之间存在很强的相关性，自监督范式补充了当前依赖标注数据的评估策略

<img src="https://img.saraba1st.com/forum/202306/27/055518m8skg3d3kqkyy3ai.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230627-054923.jpg</strong> (55.05 KB, 下载次数: 0)

下载附件

2023-6-27 05:55 上传

在自我监督评估流程中，首先从语料库创建文本对，每个文本对包含原始文本和扰动文本，比如在上图中，通过添加“not”来创建否定的扰动文本，然后将这些文本对输入到网络，并通过检查每对文本对的输出结果参考(如困惑度、概率分布或文本类型等)进行比较

<img src="https://img.saraba1st.com/forum/202306/27/055557vtgfgf3zgtq1tc3g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230627-055543.jpg</strong> (63.06 KB, 下载次数: 0)

下载附件

2023-6-27 05:55 上传

(左图)显示了Pythia 1.4B模型和Pythia 7B模型的否定、毒性、词序、分词稳健性和远程上下文敏感性的知识敏感性得分

(右图)显示了Pythia指令模型版本(Dolly-V2)与普通模型(Pythia)之间的差异，从左侧，可以看到较大的模型在灵敏度方面比较小的模型更好，从右侧，可以看到指令模型在除了分词稳健性指标之外的所有指标的灵敏度方面都更好

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 607#       发表于 2023-6-27 06:11

DreamEditor

使用NERF进行文本驱动的3D场景编辑

相关论文:https://arxiv.org/abs/2306.13455

<img src="https://img.saraba1st.com/forum/202306/27/060913vagrm981rigimmgr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230627-060330.jpg</strong> (204.23 KB, 下载次数: 0)

下载附件

2023-6-27 06:09 上传

NERF在视图合成和场景重建方面取得了令人印象深刻的进步，然而由于构造几何与纹理信息使用的方式为隐式编码，编辑这些NERF仍然具有挑战性

在本文中提出了DreamEditor，这是一种新颖的框架，使用户能够使用文本提示直接对NERF进行受控编辑，通过将场景表示为基于网格的NERF，DreamEditor允许在特定区域内进行定位编辑

DreamEditor利用预训练的文本到图像扩散模型的文本编码器，根据文本提示的语义自动识别要编辑的区域，随后DreamEditor优化编辑区域，并通过分数蒸馏采样将其几何形状和纹理与文本提示对齐

大量实验证明，DreamEditor可以根据给定的文本提示准确编辑现实场景的神经场，同时保证不相关区域的一致性，DreamEditor生成的高度逼真的纹理和几何形状，在定量和定性评估方面均显著超越了以前的作品

<img src="https://img.saraba1st.com/forum/202306/27/060920hyv4vvvgx29auyb2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230627-060647.jpg</strong> (114.7 KB, 下载次数: 0)

下载附件

2023-6-27 06:09 上传

图 2.DreamEditor框架概述，本方法通过优化现有NERF以符合目标文本提示来编辑3D场景

编辑过程涉及三个步骤：第一步，将原始NERF蒸馏为基于网格的神经场，第二步，根据文本提示，DreamEditor自动识别基于网格的NERF需要编辑区域，第三步，使用SDS损失来优化编辑区域的颜色特征𝑓𝑐、几何特征𝑓𝑔和顶点位置𝑣，从而改变各个区域的纹理和几何形状

相关示例与评估结果:

<img src="https://img.saraba1st.com/forum/202306/27/060932jzeu873mmzm1z8tm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230627-060703.jpg</strong> (244.72 KB, 下载次数: 0)

下载附件

2023-6-27 06:09 上传

<img src="https://img.saraba1st.com/forum/202306/27/060932x55blb7tw2gppu7g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230627-060715.jpg</strong> (110.7 KB, 下载次数: 0)

下载附件

2023-6-27 06:09 上传

<img src="https://img.saraba1st.com/forum/202306/27/060932ryzo3aujvocnwyn0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230627-060755.jpg</strong> (201.79 KB, 下载次数: 0)

下载附件

2023-6-27 06:09 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 608#       发表于 2023-6-28 03:45

 本帖最后由 Machinery 于 2023-6-28 03:51 编辑 

superhuman-ai-consistency

对于超级AI系统的逻辑一致性评估

github项目仓库:https://github.com/ethz-spylab/superhuman-ai-consistency

<img src="https://img.saraba1st.com/forum/202306/28/034344cifipimcupyuyui3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-034216.jpg</strong> (120.47 KB, 下载次数: 0)

下载附件

2023-6-28 03:43 上传

如果机器学习模型在各种推理或决策任务中达成了超越人类的能力，考虑到人类作为这些模型最终的测试者，在相关的基准答案上评估效果会非常糟糕，我们该如何评估这些模型？

在本文中，提出了一个通过逻辑一致性检查评估这些超级模型的框架，评估的前提是，虽然超人决策的正确性可能无法评估，但如果模型的决策无法满足某些逻辑的、人类可解释的规则，仍然可以从模型的结果中发现错误

通过在三个相关任务实例上测试这些框架，这些任务的本身的决策正确性由于模型的超人能力不好评估或因为缺失某些基准答案而难以评估：比如评估国际象棋位置、预测未来事件和做出法律判断

实验表明，无论模型是否强大，在这些任务上的表现如何，都可以发现决策中的逻辑不一致情况

例如：国际象棋AI将相反的落棋位置价值估值分配给实际上在语义层面相同的棋盘；GPT-4预测的体育记录结果随时间非单调的变化(non-monotonically)；又或者，AI法官仅在将重罪添加到被告的犯罪记录后才会向被告分配保释的判定

实验的代码可在以下目录中找到，数据被打包在v1.0.0版本中(地址:https://github.com/ethz-privsec/superhuman-ai-consistency/releases/tag/v1.0.0)：
RL testing:用于测试国际象棋人工智能是否不一致的代码和数据
LLMs forecasting future events:GPT-4和GPT-3.5 生成的实验数据
Legal AI testing:用于测试合法AI是否不一致的代码和数据

<img src="https://img.saraba1st.com/forum/202306/28/034430ljhbftfuzw8i0j9w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-034225.jpg</strong> (116.25 KB, 下载次数: 0)

下载附件

2023-6-28 03:44 上传

国际象棋AI实验:
部分游戏AI可以说是在目前情况下远远超出人类水平的，国际象棋，这是复杂决策任务的典型示例，人类可以轻松评估端到端性能(即通过模型是否获胜判断)，但不能评估单个模型决策效果，尽管如此，国际象棋规则实际上隐含着几个简单的不变性，即使对于业余棋手来说，这些不变性也是显而易见和可验证的，这是本框架的完美应用

在实验中测试了Leela Chess Zero，一个知名的开源国际象棋引擎，可以达到超人类的游戏水平，但同时也发现严重违反了各种逻辑一致性约束的问题:

1.强制走法:对于只有一个合法走法的棋盘位置，执行此走法对游戏结果没有影响，因此，强制移动之前和之后的位置必须具有相同的评估结果
2.棋盘变换：对于没有棋子和易位的位置，棋盘方向的任何变化(例如旋转棋盘或在任何轴上对棋盘进行镜像转换)都不应该会影响游戏结果
3.位置镜像：镜像玩家的位置，使得白方获得黑方的棋子设置，反之亦然，同时游戏状态的其余部分固定(例如易位权等)，必须进行语义上相同位置的走法
4.推荐走法：如果采取模型预测的最强走法，模型对位置的评估应该保持相似，事实上，国际象棋引擎通常旨在衡量双方玩家在最佳玩法下的预期游戏结果，因此任何最佳举动都不应该影响这一衡量标准

<img src="https://img.saraba1st.com/forum/202306/28/034450gs1ov9c9sghk19nx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-034234.jpg</strong> (86.54 KB, 下载次数: 0)

下载附件

2023-6-28 03:44 上传

LLM预测未来事件:
预测和建模未来是一项重要的任务，其基本事实本质上是未知的：俗话说，“预测很难，尤其是对未来的预测。 ”

在实验中，测试了GPT-4和gpt-3.5-turbo预测未来事件的能力，并给出事件是否发生的概率估计，结果发现严重违反了各种逻辑一致性约束的问题：

否定:对于任何事件A，模型应该可以预测A和¬A的相反概率
释义:模型应该对多个等效事件预测相同的概率
单调性:已知在时间上单调的数字或数量，例如体育记录或完成给定情况的人数，具有单调的模型预测结果
贝叶斯规则:对于A和B两个事件，模型预测的A发生概率、B发生概率、A给定B发生的条件概率和B给定A发生的条件概率，应该满足贝叶斯定理

<img src="https://img.saraba1st.com/forum/202306/28/034519wbvgxpigbvxzal92.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-034242.jpg</strong> (58.81 KB, 下载次数: 0)

下载附件

2023-6-28 03:45 上传

法律AI实验:
对复杂的法律案件做出裁决可能会耗时较长且成本高昂，而且裁决的“正确性”常常受到质疑，评估法律决策的正确性或公平性的困难将延伸到用于协助或自动化法律决策的AI工具身边

展示了如何揭示用于预测法律判决的两种不同语言模型中明显的逻辑不一致：
(1)评估违反欧洲人权公约行为的BERT模型
(2)gpt-3.5-turbo提示根据被告的犯罪记录预测保释决定

发现了以下违反逻辑一致性约束的问题：
释义：测试改变法律案件的措辞是否会改变模型的决定
部分排序：虽然法律判决的“正确性”很难评估，但仍然可以通过明确的方法对不同的结果进行“排序”，我们在这里考虑一个极端的例子，我们测试如果被告犯下更多罪行，保释决定模型是否可以有利地改变其决定

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 609#       发表于 2023-6-28 04:16

FunQA

迈向非同寻常的视频理解的数据集

项目主页:https://funqa-benchmark.github.io/

github项目仓库:https://github.com/Jingkang50/FunQA

一些令人惊讶的视频，例如有趣的短视频片段、创意表演或视觉错觉，会引起人们的极大关注，欣赏这些视频不仅仅是对视觉刺激反应的需求，还取决于人类理解和欣赏这些视频中经常出现的违反常识的行为的能力

FunQA，这是一个具有挑战性的视频QA数据集，专门设计用于评估和增强基于反直觉和有趣视频的视频推理深度，与大多数专注于不太令人惊讶的视频背景(例如烹饪或教学视频)的视频QA基准不同

FunQA涵盖了三种以前未探索的令人惊讶的视频类型:HumorQA、CreativeQA、MagicQA，对于每个子数据集，研究组构建了严格的QA任务，旨在评估模型在反直觉时间戳定位、详细视频描述以及反直觉推理方面的能力

还提出了更高级别的任务，例如为视频指定合适且生动的标题，并对视频创意进行评分，FunQA基准测试由源自4.3K数量的视频剪辑片段的312K个自由文本QA对组成，总共有约24小时以上的视频长度

<img src="https://img.saraba1st.com/forum/202306/28/041611qwxxxykwysspb9pg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-035924.jpg</strong> (70.75 KB, 下载次数: 0)

下载附件

2023-6-28 04:16 上传

<img src="https://img.saraba1st.com/forum/202306/28/041611x24ej3rn21y4phbs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-041218.jpg</strong> (411.71 KB, 下载次数: 0)

下载附件

2023-6-28 04:16 上传

<img src="https://img.saraba1st.com/forum/202306/28/041708yi9w9ukt9hrrrht9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-041257__01.jpg</strong> (148.79 KB, 下载次数: 0)

下载附件

2023-6-28 04:17 上传

<img src="https://img.saraba1st.com/forum/202306/28/041611sgugzztzayyoyofe.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-041319.jpg</strong> (166.08 KB, 下载次数: 0)

下载附件

2023-6-28 04:16 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 610#       发表于 2023-6-28 05:21

Symbolic Chain-of-Thought Distillation

小模型也能一步步“思考”

github项目仓库(coming soon):https://github.com/allenai/cot_distillation

<img src="https://img.saraba1st.com/forum/202306/28/051455h4dojzsdiytwjysy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-050938.jpg</strong> (152.77 KB, 下载次数: 0)

下载附件

2023-6-28 05:14 上传

Chain-of-Thought提示可以带来显著的性能提升，但似乎只有足够大的模型(超过50B参数)才会出现明显的好处

在本文中，引入了符号Chain-of-Thought蒸馏(SCoTD)，使参数总量较小的模型(125M-1.3B参数)仍然可以从Chain-of-Thought提示中受益

为了实现这一目标，需要从更大的教师模型中合理化采样来训练较小的学生模型

多个常识性基准测试的实验表明：
1.SCoTD增强了学生模型在监督和少样本设置中的性能，特别是对于挑战测试集
2.从教师模型采样每个实例的多种推理链样本是至关重要的
3.在蒸馏之后，尽管参数少了几个数量级，但学生模型的Chain-of-Thought被人类评估判断为与教师相当

测试了关于Chain-of-Thought样本的哪些属性很重要的几个假设，例如多样性、与教师的似然性、开放性

<img src="https://img.saraba1st.com/forum/202306/28/051518pg3gezx314vw14xt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-050953.jpg</strong> (196.82 KB, 下载次数: 0)

下载附件

2023-6-28 05:15 上传

少样本Chain- of-thought模型GPT-3(code davinci-002，教师模型)、OPT-1.3B(未蒸馏的学生模型)和OPT-1.3B+SCoTD(本文方法的模型)，使用符号Chain- of-thought蒸馏训练，这个过程显著提高了学生模型在各种设置中的任务准确性，人工评估表明，即使未经蒸馏的学生模型正确地回答了多项选择题，人类依然倾向于OPT-1.3B+SCoTD模型

<img src="https://img.saraba1st.com/forum/202306/28/051525od5kictvtvqgatcd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-050921.jpg</strong> (55.11 KB, 下载次数: 0)

下载附件

2023-6-28 05:15 上传

对于三个常识性QA任务，随着学生接受来自教师模型(x轴)的更多Chain- of-thought的训练，准确率(y轴)显著提高，有时也需要对Chain- of-thought进行过采样，以将学生模型的表现提高到超出受监督的仅标签基线的水平，例如OpenbookQA基准

<img src="https://img.saraba1st.com/forum/202306/28/051535wqsqswoxozvveiy7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-051302.jpg</strong> (56.74 KB, 下载次数: 0)

下载附件

2023-6-28 05:15 上传

使用不同数量的训练实例在CSQA上的性能，从仅使用DTrain的x的20%到使用全套(X轴)，橙色线是仅标签的基线

底部蓝线(标有1x)的也是SCoTD，但每个实例只有1个采样，以上是每个实例分别有5、10、20、30个不同采样(基本原理/解释/依据)的SCoTD

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 611#       发表于 2023-6-28 05:39

 本帖最后由 Machinery 于 2023-6-28 05:41 编辑 

Let's Do a Thought Experiment

(注:非常有意思的文章，随意翻下)

利用反事实推理(假设)来改进道德推理

相关论文:https://arxiv.org/abs/2306.14308

尽管语言模型在许多其他任务中表现出色，但在道德推理方面仍然举步维艰，特别是MMLU(多任务语言理解)中的道德场景任务是包括GPT-3在内的许多语言模型中表现最差的任务之一

在这项工作中，研究组提出了一个新的提示框架——思维实验，来教导语言模型使用反事实推理(假设推理)进行更好的道德推理

实验结果表明，思维实验框架从模型中引出了反事实的问题和答案，与其他零样本基线相比，将道德场景任务的准确性提高9-16%

最有趣的是，与数学推理任务不同，零样本思维链(CoT)推理并不是开箱即用的，甚至相比直接零样本来说，准确性反而降低了约4%，而自我一致性这种方法也毫无帮助

(Model. We used Flan-PaLM 540B (Chung et al., 2022) with the temperature of 0.7 across all experiments.)

进一步观察到，通过5个小例子的形式进行最少的人类监督，任务的准确性可以提高到约80%

<img src="https://img.saraba1st.com/forum/202306/28/053649gg1eegvc4v3wg34x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-053626.jpg</strong> (207.11 KB, 下载次数: 0)

下载附件

2023-6-28 05:36 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 612#       发表于 2023-6-28 06:33

Kosmos-2

为世界奠定多模态大型语言模型的基础

github项目仓库:https://github.com/microsoft/unilm/tree/master/kosmos-2

演示Demo:https://44e505515af066f4.gradio.app/

Kosmos-2，一种多模态大语言模型(MLLM)，支持感知对象描述(例如边界框)，以及理解视觉世界文本的能力

具体来说，将指代表达表示为Markdown中的链接，即“[文本范围](边界框)”，其中对象描述是标记位置的文本序列

与多模态语料库一起构建大规模的基础图像文本对数据(GrIT)来训练模型，除了MLLM的现有功能(例如，感知一般模态、遵循指令以及执行上下文学习)之外，Kosmos-2还同时将功能集成，应用于下游应用程序中

<img src="https://img.saraba1st.com/forum/202306/28/063336idozdn5ala5ezsmw.jpg" referrerpolicy="no-referrer">

<strong>20230628_062146.jpg</strong> (235.77 KB, 下载次数: 0)

下载附件

2023-6-28 06:33 上传

<img src="https://img.saraba1st.com/forum/202306/28/063336zabpba66i1hvis1l.jpg" referrerpolicy="no-referrer">

<strong>20230628_062454.jpg</strong> (168.02 KB, 下载次数: 0)

下载附件

2023-6-28 06:33 上传

<img src="https://img.saraba1st.com/forum/202306/28/063336gho2gqqioxdi9vst.jpg" referrerpolicy="no-referrer">

<strong>20230628_062455.jpg</strong> (314.93 KB, 下载次数: 0)

下载附件

2023-6-28 06:33 上传

<img src="https://img.saraba1st.com/forum/202306/28/063336xfahasdd8fhsd8jf.jpg" referrerpolicy="no-referrer">

<strong>20230628_062457.jpg</strong> (190.38 KB, 下载次数: 0)

下载附件

2023-6-28 06:33 上传

<img src="https://img.saraba1st.com/forum/202306/28/063336wrcvywizv74r34k4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230628-063208.jpg</strong> (315.53 KB, 下载次数: 0)

下载附件

2023-6-28 06:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 613#       发表于 2023-6-29 05:03

XGen-7B

在8K长度的输入序列上训练的7B LLM系列模型

博客项目说明:https://blog.salesforceairesearch.com/xgen/

github项目代码库:https://github.com/salesforce/xgen

模型权重下载:xgen-7b-8k-base(基本8K型号):https://huggingface.co/Salesforce/xgen-7b-8k-base
xgen-7b-8k-inst(8K指令微调型号):https://huggingface.co/Salesforce/xgen-7b-8k-inst
xgen-7b-4k-base(4K基本型号):https://huggingface.co/Salesforce/xgen-7b-4k-base

三种型号的差异:

<img src="https://img.saraba1st.com/forum/202306/29/050053ldimoyg2z85rrh55.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-045529.jpg</strong> (167.49 KB, 下载次数: 0)

下载附件

2023-6-29 05:00 上传

两阶段预训练数据集相关信息:

<img src="https://img.saraba1st.com/forum/202306/29/050152so3ogs4yefeg71j3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-050135.jpg</strong> (184.37 KB, 下载次数: 0)

下载附件

2023-6-29 05:01 上传

开源LLM项目，一系列XGen-7B的开源7B参数LLM，使用标准密集注意力训练了8K Token的序列长度，预训练数据集1.5T Token，同时使用了开源指令数据集对模型进行了微调，相关总结如下:
1.在标准NLP基准上，与模型大小相似的最先进的开源LLM(例如 MPT、Falcon、LLaMA、Redpajama、OpenLLaMA系列模型)相比，XGen取得了相当以及更好的结果
2.对长序列建模基准的针对性评估显示了我们的8K-seq模型相对于2K-seq和4K-seq模型的优势
3.XGen-7B在文本基准(例如MMLU、QA等)和代码基准(HumanEval)任务中都提供了同样强大的性能
4.根据TPU-v4的Google Cloud定价，1T Token的培训成本为15万美元

XGen-7B为什么选择8K序列长度？
随着LLM变得无处不在，它们在长序列上的应用已成为重点关注的焦点，特别是对于总结文本(可能与表格和图像等其他数据源交错)、编写代码和预测蛋白质序列等应用，这些应用需要模型有效地考虑长距离结构依赖性，大容量上下文允许预先训练的LLM查看客户数据(例如LLM在培训中未使用或见过的文档)并输出有用的响应结果信息解决问题

然而，大多数开源LLM(例如LLaMA、MPT、 Falcon)都是用最大2K Token序列长度进行训练的，这是建模长序列的一个关键限制，ALiBi等推理时间解决方案尚未针对较大模型(例如MPT-7b-StoryWriter-65k+)进行评估

最近的工作关于模型扩展的研究表明，对于给定的计算预算，最佳性能不一定是通过最大的模型实现的，而是通过在更多数据上训练的较小模型(以Token数量衡量)实现的，为了提高服务(包括本地部署)期间的推理效率，较小的模型通常也是首选，鉴于这些原因，便有了XGen的7B LLM

标准基准评估结果:

<img src="https://img.saraba1st.com/forum/202306/29/050244qm7vsomvjsv87vsa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-045919__01.jpg</strong> (428.96 KB, 下载次数: 0)

下载附件

2023-6-29 05:02 上传

长序列基准相关评估结果:

<img src="https://img.saraba1st.com/forum/202306/29/050259kdq8fzdepjkcji7i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-050000.jpg</strong> (376.81 KB, 下载次数: 0)

下载附件

2023-6-29 05:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 614#       发表于 2023-6-29 05:23

control_v1p_sd15_qrcode_monster

生成具有创意的可扫二维码的控制网模型(SD1.5)

模型权重下载:https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster

演示Demo:https://huggingface.co/spaces/monster-labs/Controlnet-QRCode-Monster-V1

该模型旨在生成可扫描的创意二维码，请注意，并非所有生成的创意二维码图片都可以正常扫描，但依然可以通过尝试不同的参数和提示以获得所需的结果

QR码作为条件图像传递，模块大小为16px，使用较高的纠错级别可以使其更易于阅读(有时，如果尺寸较小，较低的级别可能更容易阅读)

使用提示来指导二维码生成，输出将高度依赖于给定的提示，有些似乎很容易被二维码处理流程接受，有些则需要仔细调整才能获得良好的结果

 Controlnet引导比例
 较高的数值：生成的二维码将更具可读性
 较低的数值：生成的二维码会更有创意

<img src="https://img.saraba1st.com/forum/202306/29/052305s6qxyrrauanbc5hc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-052246.jpg</strong> (246.65 KB, 下载次数: 0)

下载附件

2023-6-29 05:23 上传

<img src="https://img.saraba1st.com/forum/202306/29/052305n8r7vksk8rszsy9r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-052046.jpg</strong> (361.9 KB, 下载次数: 0)

下载附件

2023-6-29 05:23 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 615#       发表于 2023-6-29 06:26

PoseDiffusion

通过扩散辅助的束调整解决摄像机姿态估计问题

项目主页:https://posediffusion.github.io/

github项目代码仓库:https://github.com/facebookresearch/PoseDiffusion

摄像机位置姿态估计是一个长期存在的计算机视觉问题，迄今为止通常依赖于经典方法，例如手工的关键点匹配、RANSAC和束调整等

在本文中，通过使用概率扩散框架，解决运动结构(SfM/Structure from Motion)问题，对给定的输入图像的相机姿态的条件分布进行建模

这种对老问题的新视角看法有以下几处优点，扩散框架的性质可以反映束调整的迭代过程，该公式允许无缝集成对极几何(epipolar geometry)的几何约束，可以在典型的困难估计场景中表现出色，例如具有宽基线的稀疏视，该方法可以预测任意数量图像的内在和外在

PoseDiffusion方法相比经典的SfM工作流程和两个真实世界数据集上的学习方法有了显著改进，甚至可以无需进一步训练即可跨数据集进行泛化

<img src="https://img.saraba1st.com/forum/202306/29/062227g8dfdvgyvhcngzmf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-062146.jpg</strong> (119.91 KB, 下载次数: 0)

下载附件

2023-6-29 06:22 上传

给定一组输入帧，模型一步步采样p(x|I)，在时间步长t=10处使用几何引导采样，对在视频中观察到的预测质量有显著改进

<img src="https://img.saraba1st.com/forum/202306/29/062306duhp2h20e3uuzhh6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-062257.jpg</strong> (100.26 KB, 下载次数: 0)

下载附件

2023-6-29 06:23 上传

给定图像和摄像机姿态的多视图数据集的情况下对训练进行监督，以学习扩散模型Dθ到模型p(x|I)，在推理过程中，反向扩散过程是通过辛普森对极误差(Sampson Epipolar Error)优化姿态之间的几何一致性来引导的

<img src="https://img.saraba1st.com/forum/202306/29/062314uogyfqtkozgjtfl4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-062237.jpg</strong> (169.16 KB, 下载次数: 0)

下载附件

2023-6-29 06:23 上传

CO3Dv2数据集上姿态估计的定性评估样本，给定输入图像I(第一行)，PoseDiffusion(第二行)，RelPose(第三行)、 COLMAP+SPSG(第四行)和基准真实进行比较，缺少摄像头的表明出现了姿态估计故障

<img src="https://img.saraba1st.com/forum/202306/29/062358cosch8slz455472c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-062348.jpg</strong> (105.27 KB, 下载次数: 0)

下载附件

2023-6-29 06:23 上传

还提供了一段视频，以全景、飞行环绕视角进行观察

<img src="https://img.saraba1st.com/forum/202306/29/062435rfkh34hiv7xhs4is.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-062408.jpg</strong> (120.44 KB, 下载次数: 0)

下载附件

2023-6-29 06:24 上传

采样迭代更加可视化

<img src="https://img.saraba1st.com/forum/202306/29/062440avho1z0b1nf7fokr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-062420.jpg</strong> (33.3 KB, 下载次数: 0)

下载附件

2023-6-29 06:24 上传

新视图合成，为了说明PoseDiffusion摄像机姿态估计的质量，使用PoseDiffusion预测的外在参数和内在参数来训练NeRF模型，结果如上所示

<img src="https://img.saraba1st.com/forum/202306/29/062502vewcjspsoqxmaoea.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-062453.jpg</strong> (95.38 KB, 下载次数: 0)

下载附件

2023-6-29 06:25 上传

摄像机姿势不确定性，利用扩散模型进行相机姿态估计的固有优势之一是其概率性质，众所周知，少样本视图摄像机姿态估计是一个非确定性问题，其中多个姿态组合对于一组图像来说可能都是合理的，在上面提供了一个可视化来验证PoseDiffusion可以提供几个合理的姿势集(对于相同的输入帧I可以提供不同的合理的姿态估计输出x，预测同一帧的摄像机用相同的颜色表示)

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 616#       发表于 2023-6-29 06:42

ARIES

针对同行评审进行的科学论文编辑结果对比的数据集

github项目仓库:https://github.com/allenai/aries

相关论文:https://arxiv.org/abs/2306.12587

根据同行评审反馈修改科学论文是一项具有挑战性的任务，不仅需要深厚的科学知识和推理能力，还需要能够识别高层反馈中隐含的要求，并从多种可能的方式中选择最好的方式来更新稿件作为回应

通过为大型语言模型引入了这项任务，发布了ARIES，这是一个同行评审及其相应论文编辑修改的数据集，以支持训练和评估大型语言模型在这方面的性能

研究了该任务的两个不同方面：编辑评论对齐和编辑结果生成，并评估了多个基线模型的效果，包括 GPT-4，发现模型甚至很难识别与评论相对应的编辑，特别是在要求编辑的评论以间接方式表达请求的情况下，或者编辑涉及评论的启发而不是精确的请求的情况下

当负责生成编辑时，GPT-4通常能够成功地解决表面上的评论，但它严格遵循反馈的措辞而不是潜在的意图，并且包含的​​技术细节比人工编写的编辑结果要更少

<img src="https://img.saraba1st.com/forum/202306/29/064136ugbxtttnkb6l6ymm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-063644.jpg</strong> (114.82 KB, 下载次数: 0)

下载附件

2023-6-29 06:41 上传

任务概述，在评论-编辑对齐中，模型会收到来自源论文和修订后的目标论文的审阅评论和一组候选编辑结果，并且它必须将评论与与其关联的编辑对齐，在编辑生成中，模型会收到评论和源论文，并且必须生成解决评论的编辑，其中可以使用占位符来弥补缺失的信息

相关任务与部分评估结果:

<img src="https://img.saraba1st.com/forum/202306/29/064145i9mcnc9t92mimtmm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-063825.jpg</strong> (117.25 KB, 下载次数: 0)

下载附件

2023-6-29 06:41 上传

<img src="https://img.saraba1st.com/forum/202306/29/064145tjdki1966kkuu6dl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-063919.jpg</strong> (105.84 KB, 下载次数: 0)

下载附件

2023-6-29 06:41 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 617#       发表于 2023-6-29 07:55

OpenFlamingo V2

新模型和增强的训练设置

项目主页:https://laion.ai/blog/open-flamingo-v2/

演示Demo:https://huggingface.co/openflamingo

模型权重下载:https://huggingface.co/openflamingo

大约三个月前，Laion宣布推出OpenFlamingo ，这是一项复现与开源类DeepMind Flamingo模型的开源项目

而今天，开源了第二代版本，5个经过训练的OpenFlamingo系列模型，涵盖3B、4B和9B不同的参数量

<img src="https://img.saraba1st.com/forum/202306/29/075202tew2w5hzxo2sh341.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-074415.jpg</strong> (304.28 KB, 下载次数: 0)

下载附件

2023-6-29 07:52 上传

这些模型基于Mosaic的MPT-1B和7B版本以及Together.xyz的RedPajama-3B，这意味着它们基于开源模型构建，其许可证限制比基于LLaMA的基础模型限制更少

当对7个基准数据集的性能进行平均评估时，OpenFlamingo模型获得了相应Flamingo模型80%以上的性能，OpenFlamingo-3B和OpenFlamingo-9B还仅使用32个上下文样本就获得了超过60%的微调SOTA性能

同时改进了开源训练和评估代码，在评估套件中添加了对完全分片数据并行(FSDP/Fully Sharded Data Parallel)和新数据集(TextVQA、VizWiz、HatefulMemes和Flickr30k)的支持

OpenFlamingo模型可以处理任意交错的图像和文本序列并输出文本，这使得模型能够接受上下文中的示例并解决诸如字幕说明、视觉问答和图像分类等任务

遵循Flamingo模型范例,增强一个预训练的、冻结的语言模型的层，使其在解码时跨模态关注视觉特征，遵循 Flamingo，冻结视觉编码器和语言模型,但在web抓取的图像-文本序列上训练连接模块，具体来说，使用了LAION-2B和Multimodal C4数据集的混合

4B规模的模型还接受了实验性的从ChatGPT生成的(图像、文本)序列的训练，其中图像是从LAION中提取的，将会尽快发布这些序列数据集

相关评估结果:

<img src="https://img.saraba1st.com/forum/202306/29/075213h8neeuih91nb4ubr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-075105.jpg</strong> (512.67 KB, 下载次数: 0)

下载附件

2023-6-29 07:52 上传

<img src="https://img.saraba1st.com/forum/202306/29/075213avy1uwduw5yzwrdk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-075035.jpg</strong> (216.17 KB, 下载次数: 0)

下载附件

2023-6-29 07:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 618#       发表于 2023-6-29 08:40

ViNT

视觉导航的基础模型(Foundation Model)

项目主页:https://visualnav-transformer.github.io/
github项目仓库:https://github.com/PrieureDeSion/visualnav-transformer

<img src="https://img.saraba1st.com/forum/202306/29/083619zq77c7xixcxioxxz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-083540.jpg</strong> (79.19 KB, 下载次数: 0)

下载附件

2023-6-29 08:36 上传

ViNT是一种以目标为条件的多样导航策略模型，在多样化的跨任务具身训练数据上进行训练，可以零样本控制许多不同的机器人，还可以有效地微调或适应新的机器人和下游任务

通用预训练模型(“基础模型”)使从业者能够为单个机器学习问题生成通用的解决方案，其数据集比从头开始学习所需的数据集要小得多，此类模型通常在监督较弱的大型且多样化的数据集上进行训练，消耗的训练数据比任何单个下游应用程序可用的训练数据多得多

在本文中提出了视觉导航Transformer(ViNT)，这是一个基础模型，旨在将通用预训练模型的成功带入基于视觉的机器人导航，ViNT经过训练，可实现一般目标，可与任何导航数据集一起使用，并采用灵活的基于Transformer的架构来学习导航功能可供性(affordance)，并能够有效适应各种下游导航任务，ViNT在许多现有导航数据集上进行了训练，其中包括来自各种不同机器人平台的数百小时的机器人导航，并表现出正向的迁移能力，优于在单一数据集上训练的专业模型

ViNT可以通过基于扩散的子目标建议进行增强，以探索新的环境，并且在配备远程启发式技术时可以解决公里级的导航问题，ViNT还可以采用受提示调整启发的技术来适应新任务规范，其中目标编码器被嵌入到相同目标Token空间中的另一种任务模态(例如GPS航路点或路径命令)的编码所取代，这种灵活性和适应各种下游问题领域的能力使ViNT成为移动机器人的有效基础模型

<img src="https://img.saraba1st.com/forum/202306/29/083707kwl1i0dcljilj0im.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-083645.jpg</strong> (81.08 KB, 下载次数: 0)

下载附件

2023-6-29 08:37 上传

ViNT使用基于Transformer的架构，使用EfficientNet CNN对(当前和过去)的视觉观察和目标进行编码，同时以具身无关(embodiment-agnostic)的方式预测时间距离和标准动作

<img src="https://img.saraba1st.com/forum/202306/29/083720aoajz1kzqgfyj7wl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-083654.jpg</strong> (57.85 KB, 下载次数: 0)

下载附件

2023-6-29 08:37 上传

ViNT可以使用基于拓扑图的全局规划器来探索以前未见过的环境，图像到图像扩散模型提出了多种探索目标，这些目标使用ViNT(黄色)进行空间置信选择，并使用目标导向启发式h.进行评分，子目标被添加到拓扑图中并使用ViNT策略执行

<img src="https://img.saraba1st.com/forum/202306/29/083814emkeuyff3f1kkk1i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-083749.jpg</strong> (72.63 KB, 下载次数: 0)

下载附件

2023-6-29 08:38 上传

具有上下文的远程导航，当配备远程启发式技术时，ViNT可以解决远程导航问题，在这里展示了ViNT在以前未见的环境中解决1.5公里导航问题，使用启发式方法，使用预训练的深度模型估计到目标的距离

<img src="https://img.saraba1st.com/forum/202306/29/083822n3da15cp1jcw5w81.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-083758.jpg</strong> (150.1 KB, 下载次数: 0)

下载附件

2023-6-29 08:38 上传

为了进一步展示ViNT的不同探索行为，部署了一个机车机器人，从相同的起点探索办公楼层，但有两个不同的位置目标来指导搜索，从同一位置出发，不同的目标引导机器人到达建筑物的两个不同部分，并且两条轨迹都成功实现了目标

<img src="https://img.saraba1st.com/forum/202306/29/083832kcac98azqwa62bm8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-083446.jpg</strong> (293.41 KB, 下载次数: 0)

下载附件

2023-6-29 08:38 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 619#       发表于 2023-6-29 20:48

 本帖最后由 Machinery 于 2023-6-29 20:51 编辑 

RWKV Runner/RWKV-4-World

RWKV Runner简介请参考下图

github项目地址:https://github.com/josStorer/RWKV-Runner

<img src="https://img.saraba1st.com/forum/202306/29/204740rtft1s2m2z3mm877.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-203929.jpg</strong> (255.24 KB, 下载次数: 0)

下载附件

2023-6-29 20:47 上传

<img src="https://img.saraba1st.com/forum/202306/29/204740f00x0hlufujzhnhv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-203943.jpg</strong> (155.23 KB, 下载次数: 0)

下载附件

2023-6-29 20:47 上传

<img src="https://img.saraba1st.com/forum/202306/29/204740crehyarrbbea44cr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-204005__01.jpg</strong> (545.14 KB, 下载次数: 0)

下载附件

2023-6-29 20:47 上传

<img src="https://img.saraba1st.com/forum/202306/29/204740e99rvj7prk0jezlw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-204148.jpg</strong> (196.97 KB, 下载次数: 0)

下载附件

2023-6-29 20:47 上传

RWKV-4-World

RWKV-4是一种经过多语言(100多种世界语言)预训练的大型RWKV系列新模型，预训练数据分布为70%英语、15%多语言、15%代码，有多种不同参数版本的模型可供使用

模型权重下载地址:https://huggingface.co/BlinkDL/rwkv-4-world

RWKV-World-7B演示Demo:https://huggingface.co/spaces/BlinkDL/RWKV-World-7B

<img src="https://img.saraba1st.com/forum/202306/29/205112qmd0it9rdrsmumrr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-205018.jpg</strong> (271 KB, 下载次数: 0)

下载附件

2023-6-29 20:51 上传

<img src="https://img.saraba1st.com/forum/202306/29/204754m10w5c22ici2y11i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-204453.jpg</strong> (144.99 KB, 下载次数: 0)

下载附件

2023-6-29 20:47 上传

<img src="https://img.saraba1st.com/forum/202306/29/204754o6abuuj120bllbb2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-204636.jpg</strong> (238.12 KB, 下载次数: 0)

下载附件

2023-6-29 20:47 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 620#       发表于 2023-6-29 22:23

 本帖最后由 Machinery 于 2023-6-29 22:30 编辑 

MobileSAM

在移动设备上部署的快速Sam模型

github项目仓库:https://github.com/ChaoningZhang/MobileSAM

Demo演示:https://huggingface.co/spaces/dhkim2810/MobileSAM

MobileSAM的性能与原始SAM相当(至少在视觉上效果)，并且除了图像编码器上的更改之外，与原始SAM保持完全相同的工作流程

具体来说，通过使用更小的Tiny-ViT(5M)替换了原来的重量ViT-H编码器(632M)实现

在单个GPU上，MobileSAM每帧图像的处理时间总共约为12毫秒：图像编码器上花费约8毫秒，掩码解码器上花费约4毫秒

修改的构架:

<img src="https://img.saraba1st.com/forum/202306/29/222646faio44udlggw0lsb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-222622.jpg</strong> (57.51 KB, 下载次数: 0)

下载附件

2023-6-29 22:26 上传

与原始Sam的处理速度对比:

<img src="https://img.saraba1st.com/forum/202306/29/222229wkqrtb29j22jbimw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-221926.jpg</strong> (112.55 KB, 下载次数: 0)

下载附件

2023-6-29 22:22 上传

实例效果对比:

<img src="https://img.saraba1st.com/forum/202306/29/222247fvavqjkrqe37gk63.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-221945.jpg</strong> (420.06 KB, 下载次数: 0)

下载附件

2023-6-29 22:22 上传

对比FastSAM:

<img src="https://img.saraba1st.com/forum/202306/29/222315ywpimeciv3icdcvs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230629-222026.jpg</strong> (249.58 KB, 下载次数: 0)

下载附件

2023-6-29 22:23 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 621#       发表于 2023-6-30 04:38

MVDiffusion

使用一致性感知扩散进行整体多视角图片生成

项目主页:https://mvdiffusion.github.io/

github项目代码仓库:https://github.com/Tangshitao/MVDiffusio

Demo演示:https://huggingface.co/spaces/tangshitao/MVDiffusion

<img src="https://img.saraba1st.com/forum/202306/30/043756f8qtocsfcscmf8ho.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-043027.jpg</strong> (160.22 KB, 下载次数: 0)

下载附件

2023-6-30 04:37 上传

<img src="https://img.saraba1st.com/forum/202306/30/043756rcw3rnos0yx7330y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-043045.jpg</strong> (229.14 KB, 下载次数: 0)

下载附件

2023-6-30 04:37 上传

MVDiffusion，一种简单而有效的多视图生成方法，适用于像素到像素对应的场景生成，例如全景图，或者多视图的透视裁剪(给定几何形状，深度图或姿态的情况下)

与依赖迭代图像变形和修复的先前的其他模型不同，MVDiffusion同时生成具有全局意识的所有图像，包含高分辨率和丰富的内容，有效解决了先前模型中普遍存在的生成错误累积问题

MVDiffusion特别结合了对应感知注意机制，实现有效的跨视图交互，该机制使用了三个关键模块：第一个模块生成低分辨率图像同时保持全局对应性，第二个模块负责密集图像空间处的插值覆盖，第三个模块将图像升级为高分辨率的超分功能

在全景图像方面，MVDiffusion可以生成高达1024x1024像素的高分辨率真实感图像，对于几何条件的多视图生成，MVDiffusion演示了第一种能够生成场景网格纹理贴图的方法

给定文本描述，MVDiffusion可以生成高分辨率、内容丰富的整体一致的多视图，这可实际使用于全景生成和多视图深度到图像生成等实际任务

任务一实例:全景图生成

<img src="https://img.saraba1st.com/forum/202306/30/043835mye7cmzbc2yerkr2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-043150.jpg</strong> (773.28 KB, 下载次数: 0)

下载附件

2023-6-30 04:38 上传

任务二实例:多视图 深度图到图像生成

<img src="https://img.saraba1st.com/forum/202306/30/043847kqucks1rs3ee6atm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-043210.jpg</strong> (253.43 KB, 下载次数: 0)

下载附件

2023-6-30 04:38 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 622#       发表于 2023-6-30 04:57

lens

迈向语言模型的视觉感知，通过提供自然语言的镜头信息进行计算机视觉理解

github项目代码仓库:https://github.com/ContextualAI/lens

项目演示:https://lens.contextual.ai/

LENS，这是一种利用大语言模型(LLM)的力量来解决计算机视觉问题的模块化方法，通过使用(纯)语言模型对一组独立且高度描述性的视觉模块(也可能是模型)的输出进行推理，这些模块将提供有关图像的详尽信息来进行图像认知

在纯计算机视觉设置(例如零样本和少样本目标识别)以及视觉和语言问题上评估该方法，LENS可以应用于任何现成的LLM

实验发现采用LENS的语言模型在更大、更复杂的系统中表现出很强的竞争力，而无需任何多模态训练

模块与流程图:

<img src="https://img.saraba1st.com/forum/202306/30/045547nnwiswhui9fna9wp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-044317.jpg</strong> (157.49 KB, 下载次数: 0)

下载附件

2023-6-30 04:55 上传

<img src="https://img.saraba1st.com/forum/202306/30/045547y1zr8wswwr1rsrdr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-045214.jpg</strong> (126.1 KB, 下载次数: 0)

下载附件

2023-6-30 04:55 上传

<img src="https://img.saraba1st.com/forum/202306/30/045547lobfffdfma2bf2fv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-045310.jpg</strong> (90.54 KB, 下载次数: 0)

下载附件

2023-6-30 04:55 上传

结果评估:

<img src="https://img.saraba1st.com/forum/202306/30/045646k95io71cbtesfws4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-045400.jpg</strong> (183.84 KB, 下载次数: 0)

下载附件

2023-6-30 04:56 上传

<img src="https://img.saraba1st.com/forum/202306/30/045703vmprny4kr7rrkk7k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-045414.jpg</strong> (288.31 KB, 下载次数: 0)

下载附件

2023-6-30 04:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 623#       发表于 2023-6-30 05:04

fine_tuning521k-ja

日语微调数据集

由原数据集“ign_clean_instruct_dataset_500k”和“GPTeacher”翻译为日语后混合构成，可用于微调本地语言模型，授权信息相关请参阅原页面信息

hugface数据集地址:https://huggingface.co/datasets/shumpei2525/fine_tuning521k-ja

<img src="https://img.saraba1st.com/forum/202306/30/050321fj7wexzzqqebqd77.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-050300.jpg</strong> (274.68 KB, 下载次数: 0)

下载附件

2023-6-30 05:03 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 624#       发表于 2023-6-30 06:31

Detector-Free Structure from Motion

一种无检测器的SfM方法，Image Matching Challenge 2023大赛第一名

项目主页:https://zju3dv.github.io/DetectorFreeSfM/

github项目代码库(coming soom):https://github.com/zju3dv/DetectorFreeSfM

提出了一种无需检测器的运动恢复结构方法，去除了关键点检测的需求，甚至可以在具有挑战性的缺乏明显纹理的场景中重建摄像机姿态信息，从无序图像中恢复准确的摄像机姿态和点云

传统的SfM系统通常依赖于跨多个视图的可重复的成功检测的关键点作为第一步，这对于缺乏显著纹理结构的场景来说很困难，而且糟糕的关键点检测可能会破坏整个SfM系统正常运作

通过使用有效的无检测器匹配框架，成功避免了过早确定关键点，同时解决了无检测器匹配的多视图不一致问题

具体来说，首先根据量化的无检测器匹配重建粗略的SfM模型，然后，通过新颖的迭代细化工作流程来细化SfM模型，该工作流程在基于注意力的多视图匹配模块(以细化特征轨迹)和几何细化模块(以提高重建精度)之间进行迭代

实验表明，所提出的框架在通用基准数据集上优于现有的基于检测器的SfM系统，同时还收集了缺乏显著纹理的SfM数据集，以展示本框架重建纹理贫乏场景的能力

框架工作流程:

<img src="https://img.saraba1st.com/forum/202306/30/063013or3axxrsysxokma6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-062537.jpg</strong> (104.52 KB, 下载次数: 0)

下载附件

2023-6-30 06:30 上传

重建效果评估实例:

<img src="https://img.saraba1st.com/forum/202306/30/063000vp39genp5gaa8a9z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230630-062501.jpg</strong> (133.54 KB, 下载次数: 0)

下载附件

2023-6-30 06:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 625#       发表于 2023-7-1 05:48

 本帖最后由 Machinery 于 2023-7-1 05:51 编辑 

LongChat LM/LongEval

长上下文LLM模型，可选版本7B与13B

博客项目说明:https://lmsys.org/blog/2023-06-29-longchat/

longchat-7b-16k权重仓库:https://huggingface.co/lmsys/longchat-7b-16k

longchat-13b-16k权重仓库:https://huggingface.co/lmsys/longchat-13b-16k

最新系列的聊天机器人模型LongChat-7B和LongChat-13B，具有高达16K Token级别的扩展上下文长度，评估结果表明LongChat-13B的长程检索准确率比MPT-7B-storywriter(65K)/MPT-30B-chat(8K)/ChatGLM2-6B等其他长上下文开源模型高出2倍，LongChat缩小了开源模型和专有闭源长上下文模型(例如Claude-100K和GPT-4-32K)之间的差距

<img src="https://img.saraba1st.com/forum/202307/01/054608kev53p8l02l14pp4.png" referrerpolicy="no-referrer">

<strong>topic_retrieval.png</strong> (375.94 KB, 下载次数: 0)

下载附件

2023-7-1 05:46 上传

在远程主题检索任务上使用LongChat与其他模型进行了比较

LongChat不仅可以处理如此长的上下文长度，而且还可以精确地遵循人类在对话中的指令，并在人类偏好基准MT-Bench(相关项目:https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)中表现出强大的性能

LongChat使用LLaMA模型进行微调，该模型最初是用 2048 上下文长度进行预训练的，之后的长上下文拓展包含一个两阶段过程(注:对最近开源社区的压缩旋转位置嵌入trick具体信息感兴趣的可以查看这里的说明博客:https://kaiokendev.github.io/)

1.压缩旋转位置嵌入(Rotary position embedding/RoPE):LLaMA模型在2048的序列长度上通过旋转位置嵌入进行预训练，这意味着它在预训练阶段没有观察到position_ids &gt; 2048的场景，因此通过适当的将position_ids &gt; 2048压缩到0到2048之间可以正确的让模型处理实际上超出2048序列长度的情况，根据推测这种压缩可以最大限度地重用在预训练阶段学到的模型权重

通过将新目标上下文长度y除以2048来定义压缩比，之后将每position_ids除以该比率并将其输入apply_rotary_pos_emb()函数中进行实际应用

在LongChat中，将模型微调为目标上下文长度16384，因此压缩比为8，例如，position_ids=10000的token变为position_ids=10000/8=1250，相邻token10001变为10001 /8=1250.125，此步骤无需训练

2.使用精选对话数据集进行微调:压缩嵌入后，再对构造的对话数据集执行微调过程，通过重复使用之前用于训练Vicuna所收集的用户共享对话记录，使用FastChat数据工作流程整理数据，并截断这些对话，使其不超过16K，使用标准的下一个Token预测损失来微调模型

分别对7B和13B模型进行了80k和18k轮对话数量的微调，为了节省显存，使用了Pytorch FSDP和Flash Attention，假设A100显卡在云上的价格为3美元/小时，7B 型号的训练费用约为300美元，13B型号的费用约为700美元

评估工具包LongEval，最近，商业和开源模型在其最新版本中不断宣传其支持扩展上下文长度能力(从8K、32K到100K)，但如何验证这些说法呢？，“长上下文能力”一词对于不同的模型提供商来说可能意味着不同的含义

例如，MPT-7B-StoryWriter宣传的65K上下文长度是否与OpenAI的ChatGPT 16K的运行容量相同？这个问题在LongChat模型开发中也出现了：如何快速有效地确认新训练的模型是否可以处理预期的上下文长度？

为了解决这个问题，需要根据需求对LLM处理冗长上下文的任务进行评估，例如长文本序列中的文本生成、检索、摘要和信息关联，受最近讨论的启发，设计了LongEval，一个长上下文测试套件，该套件包含两个不同难度的任务，提供了一种简单而快捷的方法来测量和比较长上下文性能

结果证明在细粒度检索任务中，当上下文长度接近16K时，LongChat-13B-16K的准确率会下降，在初步尝试中推测这是因为它接近最大微调长度，如果在更大的数据集(例如 32K)上进行训练可能可以缓解这个问题，研究组将积极努力在不久的将来版本中解决这个问题

通过比对任务1(粗粒度主题检索)与任务2(细粒度数字检索)，对比了相关模型的性能，细节请参考下图

<img src="https://img.saraba1st.com/forum/202307/01/054735wu85388bu5u2j9up.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-053326.jpg</strong> (434.88 KB, 下载次数: 0)

下载附件

2023-7-1 05:47 上传

同时也在人类偏好基准(MT-bench)和长序列问题答案推理基准上测试了模型的性能，细节请参考下图

<img src="https://img.saraba1st.com/forum/202307/01/054742za13kabhk1wbbz13.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-053839.jpg</strong> (538.98 KB, 下载次数: 0)

下载附件

2023-7-1 05:47 上传

相关总结:

<img src="https://img.saraba1st.com/forum/202307/01/054753wfnwta7v9wf6a6kp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-054400.jpg</strong> (288.99 KB, 下载次数: 0)

下载附件

2023-7-1 05:47 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 626#       发表于 2023-7-1 06:13

Michelangelo

基于形状-图像-文本对齐的潜在表征的条件3D形状生成

项目主页:https://neuralcarver.github.io/michelangelo/

github项目仓库:https://github.com/NeuralCarver/Michelangelo

提出了一种新颖的生成前对齐(alignment-before-generation)方法来解决基于2D图像或文本生成通用3D形状的挑战性任务

直接学习从图像或文本到3D形状的条件生成模型很容易产生与条件不一致的结果，因为3D形状具有附加维度，其分布与2D图像和文本的分布显著不同

为了弥合三种模态之间的领域差距并促进多模态3D形状生成，探索在形状-图像-文本对齐空间中表示3D形状

框架包括两个模型：形状-图像-文本对齐变分自动编码器(SITA-VAE)和条件对齐形状潜在扩散模型(ASLDM)，前一个模型将3D形状编码到与图像和文本对齐的形状潜在空间中，并通过基于Transformer的解码器重建与给定的形状嵌入相对应的具有细粒度的3D NERF，后一个模型学习从图像或文本空间到潜在形状空间的概率映射函数

广泛的实验表明，提出的方法可以生成更高质量和更多样化的3D形状，在语义上更好地符合视觉或纹理条件输入，验证了形状-图像-文本对齐空间对于跨模态3D形状生成的有效性

Michelangelo框架图示:

<img src="https://img.saraba1st.com/forum/202307/01/061301p30xfnd3d7dzh67l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-061041.jpg</strong> (161.08 KB, 下载次数: 0)

下载附件

2023-7-1 06:13 上传

相关样例:

<img src="https://img.saraba1st.com/forum/202307/01/061317naa2ziyd9z9avvxy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-061148.jpg</strong> (127.59 KB, 下载次数: 0)

下载附件

2023-7-1 06:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 627#       发表于 2023-7-1 06:38

One-2-3-45

在45秒内将任何图像转换为3D网格(Mesh)，无需预先的形状优化(Per-Shape Optimization)

项目主页:https://one-2-3-45.github.io/

github项目仓库:https://github.com/One-2-3-45/One-2-3-45

单图像3D重建是一项重要但具有挑战性的任务，需要模型拥有对自然世界的广泛了解，许多现有方法通过在2D扩散模型的指导下优化NERF来解决这个问题，但存在优化时间长、3D结果不一致和几何形状差的问题

在本文中，提出了一种新颖的方法，该方法可以将任何对象的单张图像作为输入，并在单次前馈传递中生成完整的可360度旋转的3D纹理网格

给定单个图像，首先使用视图条件2D扩散生成模型Zero123为输入视图生成多视图，然后将它们作为新的条件提升到3D空间

由于传统的重建方法难以应对不一致的多视图预测，因此研究组在基于SDF的广义神经表面重建方法上构建了3D重建模块，并提出了几种关键的训练策略来实现360度网格的重建

无需昂贵的优化，本方法重建3D形状的时间明显少于现有方法，此外，本方法有利于生成更好的几何形状，更一致的3D结果，并且更紧密地关联于输入图像

在合成数据和自然场景图像上评估了本方法，并证明其在网格质量和运行时间方面的优越性，此外，本方法还可以通过与现成的文本到图像扩散模型集成来无缝支持直接的文本到3D任务

<img src="https://img.saraba1st.com/forum/202307/01/063722b9oihilwyylw29i6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-062936.jpg</strong> (95.52 KB, 下载次数: 0)

下载附件

2023-7-1 06:37 上传

模型构架，主要由三个部分组成:
1.多视图合成：使用视图条件二维扩散模型Zero123，以两阶段方式生成多视图图像，Zero123的输入包括单个图像和关联的摄像机姿态变换，其由关联的球面坐标(Δθ、Δφ、Δr)参数化

 2.姿态估计：根据Zero123生成的四个附近视图来估计输入图像的仰角θ，然后通过将指定的摄像机姿势与输入视图的估计姿态相结合来获得多视图摄像机姿态

3.3D重建：将多视图与摄像机姿态输入基于SDF的广义神经表面重建模块，以进行360°网格重建

<img src="https://img.saraba1st.com/forum/202307/01/063735jglpygwpl1cpy1wc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-063412.jpg</strong> (112.92 KB, 下载次数: 0)

下载附件

2023-7-1 06:37 上传

使用DALL-E 2生成以文本为条件的图像，然后将其提升为3D

<img src="https://img.saraba1st.com/forum/202307/01/063741qt4trdc1ra41h41l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-063451.jpg</strong> (139.13 KB, 下载次数: 0)

下载附件

2023-7-1 06:37 上传

与其他方法的总结性对比

<img src="https://img.saraba1st.com/forum/202307/01/063757hqr4qqe461o60u44.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-063554.jpg</strong> (295.84 KB, 下载次数: 0)

下载附件

2023-7-1 06:37 上传

<img src="https://img.saraba1st.com/forum/202307/01/063757t5ix5y8grwwkz5yy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-063606.jpg</strong> (264.88 KB, 下载次数: 0)

下载附件

2023-7-1 06:37 上传

相关的对比生成效果以及生成实例

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 628#       发表于 2023-7-1 06:53

LLaVAR

增强视觉指令微调以实现更丰富的图像与文本理解

项目主页:https://llavar.github.io/

github项目仓库:https://github.com/SALT-NLP/LLaVAR

演示Demo:https://eba470c07c805702b8.gradio.live/

数据集下载地址:https://llavar.github.io/#data

模型权重下载:https://drive.google.com/drive/folders/19uEwM1VrzX_KqCzzSJAh8RqOHbf4WS5Z?usp=sharing

指令微调释放了大型语言模型(LLM)与人类交互的卓越能力，此外，最近的指令追随数据集使用图像作为视觉输入，收集了基于图像的对于指令的响应结果

然而，视觉指令调整的模型无法很好地理解图像中的文本细节，这项工作通过富含文本的图像(例如电影海报、书籍封面等)增强了当前的视觉指令调整工作流程

具体来说，首先使用公开的OCR工具从LAION数据集中收集了422K富含文本的图像的对应结果，此外还使用识别的文本和图像标题提示纯文本GPT-4，用以生成16K对话，每个对话都包含文本丰富的图像的问答对

通过将收集的数据与之前的多模态指令跟踪数据相结合，训练出的新模型LLaVAR，显著提高了LLaVA模型在基于文本的VQA数据集上的能力(准确率提高了20%)，同时在ScienceQA上实现了91.42%的准确率

基于GPT-4的指令跟随评估还证明了LLaVAR模型在自然图像和富含文本的图像上的改进，通过定性分析，LLaVAR基于结合文本和图像的最新现实世界在线内容，展示了与人类有前途的互动技能(例如推理、写作和阐述)

<img src="https://img.saraba1st.com/forum/202307/01/065302otua1bns6tn0nx6x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-064941__01.jpg</strong> (641.96 KB, 下载次数: 0)

下载附件

2023-7-1 06:53 上传

<img src="https://img.saraba1st.com/forum/202307/01/065302qp6kvdzkxx6kd1xv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230701-064941__02.jpg</strong> (639.07 KB, 下载次数: 0)

下载附件

2023-7-1 06:53 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  ZetaGo  
##### 629#       发表于 2023-7-2 01:04

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61212805&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-6-10 07:24</a>
Whisper Web

在浏览器中使用whisper模型进行语音识别</blockquote>
这个能本地部署吗，可以把内网的会议录音转成文字


*****

####  Machinery  
##### 630#       发表于 2023-7-2 02:23

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61510649&amp;ptid=2126390" target="_blank">ZetaGo 发表于 2023-7-2 01:04</a>
这个能本地部署吗，可以把内网的会议录音转成文字</blockquote>
whisper本来就是本地模型，不过中文精度不怎么好…

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  大江户战士  
##### 631#       发表于 2023-7-2 03:49

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61510649&amp;ptid=2126390" target="_blank">ZetaGo 发表于 2023-7-2 01:04</a>

这个能本地部署吗，可以把内网的会议录音转成文字</blockquote>
中文用阿里的modelscope更好


*****

####  Machinery  
##### 632#       发表于 2023-7-2 05:28

openchat_8192

简介如下图

hugface权重仓库:https://huggingface.co/openchat/openchat_8192

<img src="https://img.saraba1st.com/forum/202307/02/052805n3u11h0wtheuwauu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230702-052727.jpg</strong> (240.97 KB, 下载次数: 0)

下载附件

2023-7-2 05:28 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 633#       发表于 2023-7-2 07:38

UltraLM-13b/CPM-Bee/VisCPM

UltraLM-13b:基于UltraChat论文训练的新模型

相关论文:https://arxiv.org/abs/2305.14233

github项目页:https://github.com/thunlp/UltraChat

hugface权重下载:https://huggingface.co/openbmb/UltraLM-13b

Alpaca-Eval榜单(地址:https://tatsu-lab.github.io/alpaca_eval/)成绩

<img src="https://img.saraba1st.com/forum/202307/02/072722qrggx6x59ojnq9zn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230702-072702.jpg</strong> (221.1 KB, 下载次数: 0)

下载附件

2023-7-2 07:27 上传

CPM-Bee:百亿参数的开源中英文双语基座大模型

github项目地址:https://github.com/OpenBMB/CPM-Bee

hugface transformer权重下载:https://huggingface.co/openbmb/cpm-bee-10b

<img src="https://img.saraba1st.com/forum/202307/02/073106m09c55fzl3s9kfo9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230702-072910.jpg</strong> (496.5 KB, 下载次数: 0)

下载附件

2023-7-2 07:31 上传

VisCPM:基于CPM基础模型的中英双语多模态大模型系列

github项目地址(readme有权重下载地址):https://github.com/OpenBMB/VisCPM

<img src="https://img.saraba1st.com/forum/202307/02/073724tdoygdg30rdy3pty.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230702-073617.jpg</strong> (73.45 KB, 下载次数: 0)

下载附件

2023-7-2 07:37 上传

<img src="https://img.saraba1st.com/forum/202307/02/073554n49fub9gnyh1f5uc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230702-072940__01.jpg</strong> (698.37 KB, 下载次数: 0)

下载附件

2023-7-2 07:35 上传

<img src="https://img.saraba1st.com/forum/202307/02/073554jrisskspriimyzvh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230702-072940__02.jpg</strong> (431.76 KB, 下载次数: 0)

下载附件

2023-7-2 07:35 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  darktide  
##### 634#       发表于 2023-7-2 10:41

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61364631&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-6-21 03:18</a>

KnowLM

知识丰富的大型语言模型框架KnowLM，自带预训练和指令微调代码(支持多机多GPU设置)，以及名为ZhiXi ...</blockquote>
这个个人非常感兴趣，感谢


*****

####  s1234y  
##### 635#       发表于 2023-7-2 17:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61510649&amp;ptid=2126390" target="_blank">ZetaGo 发表于 2023-7-2 01:04</a>
 这个能本地部署吗，可以把内网的会议录音转成文字</blockquote>
asr目前还是讯飞效果最好，尤其是普通话和方言混杂的时候


*****

####  Machinery  
##### 636#       发表于 2023-7-3 03:08

 本帖最后由 Machinery 于 2023-7-3 03:12 编辑 

3D-Speaker

用于解耦语音表征的大规模多设备、多距离、多方言语料数据集

项目主页:https://3dspeaker.github.io/

github项目地址:https://github.com/alibaba-damo-academy/3D-Speaker

训练数据集:https://speech-lab-share-data.oss-cn-shanghai.aliyuncs.com/3D-Speaker/train.tar.gz

测试集:https://speech-lab-share-data.oss-cn-shanghai.aliyuncs.com/3D-Speaker/test.tar.gz

解耦语音话语中的不相关信息是语音界的一个重要研究课题，不同的语音相关任务侧重于提取不同的语音表征，同时最大限度地减少其他不相关信息的影响

本文提出了一个大规模的语音语料库来促进语音表示解耦的研究，3D-Speaker包含超过10,000个说话者，每个说话者都由多个设备同时录制，位于不同的距离，并且一些说话者讲多种方言

多维音频数据的受控组合产生了语音表征纠缠的多种混合矩阵，从而可以激发构建相关系统解耦它们的有趣方法

<img src="https://img.saraba1st.com/forum/202307/03/030833ie829gnpi62e2po2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-030739.jpg</strong> (46.66 KB, 下载次数: 0)

下载附件

2023-7-3 03:08 上传

<img src="https://img.saraba1st.com/forum/202307/03/030833cduegfulfuggz1f7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-030651.jpg</strong> (309.18 KB, 下载次数: 0)

下载附件

2023-7-3 03:08 上传

<img src="https://img.saraba1st.com/forum/202307/03/030826d13f28s69zrhry81.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-030449.jpg</strong> (97.94 KB, 下载次数: 0)

下载附件

2023-7-3 03:08 上传

<img src="https://img.saraba1st.com/forum/202307/03/030826ienp2l14bepn4bez.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-030539.jpg</strong> (87.38 KB, 下载次数: 0)

下载附件

2023-7-3 03:08 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 637#       发表于 2023-7-3 03:57

 本帖最后由 Machinery 于 2023-7-3 04:00 编辑 

CLIPA-v2

在10000美元的预算扩展CLIP训练，实现81.1%精度的零样本ImageNet准确率，再额外附加4000美元可以实现81.8%的准确率

github项目主页:https://github.com/UCSC-VLAA/CLIPA

<img src="https://img.saraba1st.com/forum/202307/03/035651gdmssr8rujjal0o0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-035440.jpg</strong> (102.5 KB, 下载次数: 0)

下载附件

2023-7-3 03:56 上传

CLIPA-v2前代的关联论文(地址:https://arxiv.org/abs/2305.07017)

CLIPA-v2论文(地址https://arxiv.org/abs/2306.15658)

<img src="https://img.saraba1st.com/forum/202307/03/035527pwvau6p2vw9odiao.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-034749.jpg</strong> (74.73 KB, 下载次数: 0)

下载附件

2023-7-3 03:55 上传

CLIP模型最近在计算机视觉领域实现了许多突破，然而其相关的训练成本过高，对其广泛探索造成了重大障碍

在本文中，提出了CLIP训练存在逆缩放定律，即使用的图像/文本编码器越大，可实际应用于训练的图像/文本标记的序列长度可以越短

此外，展示了减少图像/文本标记长度的策略在确定该缩放定律的质量方面发挥着至关重要的作用

这一发现的结果是，可以利用学术级别的研究资源成功地训练CLIP，例如在A100x8 GPU服务器上，CLIP模型在大约2天的时间内实现了63.2%的零样本top-1 ImageNet准确率，在大约3天的时间内达到67.8%，在大约4天的时间达到69.3%

在此工作的基础上，提出了CLIPA-v2的两个关键贡献，从技术上讲，研究组发现这种逆缩放定律也适用于微调阶段，从而可以进一步减少计算需求

根据实际经验，大规模探索了CLIPA训练，将实验扩展到H/14模型，在训练期间模型看到了约130亿个图像-文本对

结果也令人兴奋，仅使用了10000美元的预算，CLIP模型就实现了令人印象深刻的81.1%的零样本ImageNet准确率，比之前最好的 CLIP模型(来自OpenCLIP，80.1%)高出1.0%，同时降低了约39倍的计算成本

此外，通过额外投入4000美元，可以将零样本ImageNet准确率进一步提升至81.8%

<img src="https://img.saraba1st.com/forum/202307/03/040000n8qnehqnw9e2pp8a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-035037__01.jpg</strong> (49.34 KB, 下载次数: 0)

下载附件

2023-7-3 04:00 上传

通过逆缩放定律拓展CLIPA-v1，具体来说，我们从数据、模型和进度等方面探索扩展，通过使用36个图像Token(84×84)和8个文本Token来预训练H/14模型

为了进行微调，使用256(224×224)个图像标记和32个文本Token

<img src="https://img.saraba1st.com/forum/202307/03/035643afve1mchvgdcju2e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-034840.jpg</strong> (48.95 KB, 下载次数: 0)

下载附件

2023-7-3 03:56 上传

微调的逆缩放定律，所有模型都使用128M数量的样本进行微调，其中采用了随机掩码来减少Token数量

相关评估结果:

<img src="https://img.saraba1st.com/forum/202307/03/035705nzyyqbokbkbssws3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-035059.jpg</strong> (221.99 KB, 下载次数: 0)

下载附件

2023-7-3 03:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 638#       发表于 2023-7-3 07:46

 本帖最后由 Machinery 于 2023-7-3 07:49 编辑 

MT-Bench/Vicuna-33B

项目说明博客:https://lmsys.org/blog/2023-06-22-leaderboard/

Chatbot Arena排行榜的最新更新，现在排行榜中将包括更多开放模型和以下三个评估指标(具体评估成绩请参照下图):

1.Chatbot Arena Elo，基于使用Elo评级系统从Chatbot Arena进行的42K人类基准匿名投票模型战斗结果
2.MT-Bench分数，一种基于具有挑战性的多回合回答基准，也使用了GPT-4进行了评分
3.MMLU基准，一种广泛采用的基准

<img src="https://img.saraba1st.com/forum/202307/03/074606pbf67bbi393653ib.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-072737.jpg</strong> (332.79 KB, 下载次数: 0)

下载附件

2023-7-3 07:46 上传

虽然存在一些用于评估大型语言模型(LLM)性能的基准，例如MMLU、HellaSwag和HumanEval等，但这些基准在评估LLM的人类偏好时可能会达不到要求

传统的基准测试经常用简洁的输出(例如多项选择题)来测试LLM的封闭问题回答性能，但这不能并不适合评估基于LLM的聊天助理的通常使用情况，为了填补这一空白，在本次排行榜更新中，更新了MT-bench，旨在使用人类偏好作为主要测试指标

<img src="https://img.saraba1st.com/forum/202307/03/074613p3jxjcqvj20gv7q1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-073928.jpg</strong> (86.33 KB, 下载次数: 0)

下载附件

2023-7-3 07:46 上传

MT-bench是一个具有挑战性的多轮问题测试集，旨在评估模型的对话和指令跟随能力，可以在此处查看 MT-bench 的示例问题和答案(地址:https://huggingface.co/spaces/lmsys/mt-bench)

Chatbot Arena是一个众包匿名模型对战平台，用户可以向聊天机器人提出任何问题并投票选出他们喜欢的答案，这两个基准都旨在使用人类偏好作为主要指标(地址:https://chat.lmsys.org/?arena)

相关评估结果与分析:

<img src="https://img.saraba1st.com/forum/202307/03/074623w7pyzgrx7jppaza7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-074008__01.jpg</strong> (987.77 KB, 下载次数: 0)

下载附件

2023-7-3 07:46 上传

下一步还会将MT-bench拓展到1K规模:

<img src="https://img.saraba1st.com/forum/202307/03/074632tzcx1m1x9d1ww111.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230703-074313.jpg</strong> (144.13 KB, 下载次数: 0)

下载附件

2023-7-3 07:46 上传

新系列的Vicuna-v1.3模型，参数范围从7B到33B，并在扩展的用户共享对话集上进行训练，权重下载地址(地址:https://github.com/lm-sys/FastChat/tree/main#vicuna-weights)

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 639#       发表于 2023-7-4 01:28

AttrPrompt

作为属性训练数据生成器的大型语言模型：多样性和偏见的故事

github项目链接:https://github.com/yueyu1030/AttrPrompt

大型语言模型(LLM)被广泛应用于自动生成各种自然语言处理(NLP)任务所需的训练数据，虽然以往的研究探索了利用生成数据训练模型的不同方法，但这些方法通常依赖于简单的分类提示(simple class-conditional prompts)，这可能会限制生成数据的多样性，并传播LLM内部的系统偏差

因此，通过研究利用多样性的属性提示(例如指定长度和风格等属性)生成训练数据，可以产生属性多样性与属性指定的生成数据

本文的研究侧重于高基数(High-Cardinality)和多元领域的数据集，实验证明，与简单的分类提示相比，属性提示在提升模型性能方面有较大优势

经过数据生成方面全面的研究，涵盖偏差、多样性和效率等重要方面后，观察到三个关键观察结果

1.简单提示生成的人工数据集，存在显著的偏差，如区域偏差(总是容易生成倾向某个区域的语料)
2.属性多样性在提高模型性能方面具有关键作用
3.属性提示所得性能与ChatGPT类似的简单分类提示相当，但仅使用了后者5%的查询成本

两种不同提示方法的对比:

<img src="https://img.saraba1st.com/forum/202307/04/012632cwazwppk7lo7kdp5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-012209.jpg</strong> (74.25 KB, 下载次数: 0)

下载附件

2023-7-4 01:26 上传

AttrPrompt的工作流程:

<img src="https://img.saraba1st.com/forum/202307/04/012655qylq944ukz8lll6j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-012242.jpg</strong> (175.74 KB, 下载次数: 0)

下载附件

2023-7-4 01:26 上传

属性维度与相关值，带星号*的属性是与类相关的属性:

<img src="https://img.saraba1st.com/forum/202307/04/012752hqaqhfazuqg88iy8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-012350.jpg</strong> (147.54 KB, 下载次数: 0)

下载附件

2023-7-4 01:27 上传

不同的多样性与相关测试结果:

<img src="https://img.saraba1st.com/forum/202307/04/012752qsucpknnnsmi1iis.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-012436.jpg</strong> (43.66 KB, 下载次数: 0)

下载附件

2023-7-4 01:27 上传

<img src="https://img.saraba1st.com/forum/202307/04/012753y991wsu91sllwl1u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-012458.jpg</strong> (118.01 KB, 下载次数: 0)

下载附件

2023-7-4 01:27 上传

<img src="https://img.saraba1st.com/forum/202307/04/012753l4yrvh4zlhkw66ww.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-012518.jpg</strong> (131.14 KB, 下载次数: 0)

下载附件

2023-7-4 01:27 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 640#       发表于 2023-7-4 02:22

Statler

用于具身推理的状态维护语言模型

项目主页:https://statler-lm.github.io/

项目代码:coming soon

大型语言模型(LLM)提供了一种很有前途的使用方向，使机器人能够执行复杂的机器人推理任务，然而当前LLM的有限上下文窗口使得长期推理变得困难

人们可能会期望家用机器人执行的具体任务通常要求规划者考虑很久以前获取的信息(例如机器人在之前的环境中遇到的许多物体的情况)

由于机器人动作历史中缺乏可用的任务和环境相关信息，尝试使用LLM的隐式内部表征来捕获世界状态变得复杂，而依赖于通过提示向LLM传达信息的能力的方法则受到其有限的上下文窗口的影响

在本文中提出了Statler，这是一个框架，赋予LLM以一种随着时间的推移而维持的“记忆”形式的世界状态的明确表征方法

Statler的重要组成部分是两个通用LLM实例，一个世界模型读取者，一个世界模型编写者，彼此与世界状态交互并维护世界状态，通过提供对这种世界状态“记忆”的访问，Statler提高了现有LLM在较长时间范围内进行推理的能力，而且不受上下文长度的限制

在三个模拟桌面操作领域和一个真实机器人领域上评估了方法有效性，并表明它提高了基于LLM的机器人推理的SOTA水平

三种不同LLM方法进行的游戏实例:三杯一球游戏

<img src="https://img.saraba1st.com/forum/202307/04/021844rju0iybyoa6yokpo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-015119.jpg</strong> (382.1 KB, 下载次数: 0)

下载附件

2023-7-4 02:18 上传

Statler框架包括响应用户查询的世界模型读取者和负责更新状态表示的世界模型写入者，Statler对其应用领域或子任务数量没有任何限制，Statler可以被视为代码即策略(Code-as-Policies)的扩展，其中状态管理机制被额外嵌入，而不影响代码即策略的基本功能(即分层代码生成)

Statler方法图:

<img src="https://img.saraba1st.com/forum/202307/04/021940zzn279tl9l9ro7m2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-014755.jpg</strong> (197.28 KB, 下载次数: 0)

下载附件

2023-7-4 02:19 上传

与代码既策略(Code-as-Policies)LLM方法的对比:

<img src="https://img.saraba1st.com/forum/202307/04/022040c1x6o6zseiw316xz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-022021.jpg</strong> (302.43 KB, 下载次数: 0)

下载附件

2023-7-4 02:20 上传

<img src="https://img.saraba1st.com/forum/202307/04/022143gnre5rhzamjjl5rj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-022105.jpg</strong> (141.87 KB, 下载次数: 0)

下载附件

2023-7-4 02:21 上传

相关评估与机器人实验:

<img src="https://img.saraba1st.com/forum/202307/04/022150ixouo4udkxnojmxl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-022114.jpg</strong> (432.53 KB, 下载次数: 0)

下载附件

2023-7-4 02:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 641#       发表于 2023-7-4 02:51

 本帖最后由 Machinery 于 2023-7-4 02:52 编辑 

Magic123

使用2D和3D扩散先验从一张图像生成高质量3D对象

github项目地址:https://github.com/guochengqian/Magic123

项目主页:https://guochengqian.github.io/project/magic123

Magic123，这是一种两阶段的从粗到细的优化方法，可使用2D和3D先验从真实自然场景的单张未给出摄像头姿态的图像重建具有详细3D几何形状和高渲染分辨率(1024 × 1024)的高质量3D网格

在第一阶段，优化NERF产生粗略的几何形状，在第二阶段，采用显存高效的可微网格表征来生成具有视觉吸引力的纹理的高分辨率网格

在这两个阶段中，3D内容都是通过参考视图进行监督和由2D和3D扩散先验相结合引导的新颖视图来学习的，通过在2D和3D先验之间引入了一个权衡参数，可以控制生成的几何图形的倾向(更有想象力或更精确)

此外，分别采用了文本反转(textual inversion)和单眼深度正则化来鼓励不同视图之间的外观一致并防止效果退化，Magic123展示了比以前的图像转3D技术更显著改进，这一点通过对合成基准和各种真实世界图像的大量实验进行了验证

Magic123框架流程:

<img src="https://img.saraba1st.com/forum/202307/04/025005mj99txtjhxtzqhx1.jpg" referrerpolicy="no-referrer">

<strong>ab0b7c88-47e3-45ba-8fe3-40c3df970d27.jpg</strong> (119.26 KB, 下载次数: 0)

下载附件

2023-7-4 02:50 上传

质量评估示例与控制参数效果对比:

<img src="https://img.saraba1st.com/forum/202307/04/025042dan05atnxg3aoqu3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-024529.jpg</strong> (145.98 KB, 下载次数: 0)

下载附件

2023-7-4 02:50 上传

<img src="https://img.saraba1st.com/forum/202307/04/025042csazs13nq6c5x16s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-024822.jpg</strong> (184.45 KB, 下载次数: 0)

下载附件

2023-7-4 02:50 上传

<img src="https://img.saraba1st.com/forum/202307/04/025042ffiiiamztx85uaas.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-024857.jpg</strong> (555.48 KB, 下载次数: 0)

下载附件

2023-7-4 02:50 上传

<img src="https://img.saraba1st.com/forum/202307/04/025042yueuitkifnfkuv61.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-024919.jpg</strong> (359.53 KB, 下载次数: 0)

下载附件

2023-7-4 02:50 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 642#       发表于 2023-7-4 03:30

ChatLaw

具有集成外部知识库的开源法律大语言模型

简介如下图

github项目地址:https://github.com/PKU-YuanGroup/ChatLaw

相关文章:https://www.zhihu.com/question/610072848/answer/3101663890

<img src="https://img.saraba1st.com/forum/202307/04/032750b5yt044h576g6tt4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-032625__01.jpg</strong> (520.24 KB, 下载次数: 0)

下载附件

2023-7-4 03:27 上传

<img src="https://img.saraba1st.com/forum/202307/04/032750x6q33qqlqh36j6xl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-032625__02.jpg</strong> (587.4 KB, 下载次数: 0)

下载附件

2023-7-4 03:27 上传

<img src="https://img.saraba1st.com/forum/202307/04/032750px8h6zxj8zaj53cc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-032625__03.jpg</strong> (449.34 KB, 下载次数: 0)

下载附件

2023-7-4 03:27 上传

注:既然开了相似度匹配模型怎么不把数据集也开了<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 643#       发表于 2023-7-4 05:53

 本帖最后由 Machinery 于 2023-7-4 05:56 编辑 

replit-code-instruct-glaive

glaive公司微调的开源代码生成模型replit-code-instruct-glaive，使用的模型底座为replit-code-3B(from csahil28)

模型权重仓库:https://huggingface.co/sahil2801/replit-code-instruct-glaive

code-eval评测记录:https://github.com/abacaj/code-eval

演示Demo(from teknium):https://huggingface.co/spaces/teknium/sahil2801-replit-code-instruct-glaive

instruct-glaive的新微调代码模型在HumanEval的pass@1基准测试中取得了63.5%的新SOTA开源记录，超越了WizardCoder，该模型只有3B个参数，比WizardCoder小5倍

注:与gpt4相比，replit-code-instruct-glaive更擅长直接生成代码而不是解释代码

<img src="https://img.saraba1st.com/forum/202307/04/055047ox55h75nvc7uv5sx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230704-055028.jpg</strong> (270.11 KB, 下载次数: 0)

下载附件

2023-7-4 05:50 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 644#       发表于 2023-7-5 01:16

 本帖最后由 Machinery 于 2023-7-5 01:18 编辑 

JourneyDB

对于生成图像理解的基准数据集

项目主页:https://journeydb.github.io/

github项目仓库:https://github.com/JourneyDB/JourneyDB

JourneyDB数据集地址:https://huggingface.co/datasets/JourneyDB/JourneyDB

<img src="https://img.saraba1st.com/forum/202307/05/011543fuslepmu484mz0du.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-010826.jpg</strong> (306.6 KB, 下载次数: 0)

下载附件

2023-7-5 01:15 上传

最近的视觉语言模型(vision-language models)的最新进展彻底改变了多模态理解，但仍不清楚它们是否具有理解生成图像的能力，与真实数据相比，合成图像在内容和风格上表现出更高程度的多样性，而模型要完全理解这些多样性存在很大的困难

为此，本文提出了一个大规模数据集JourneyDB，可以应用于生成图像中的多模态视觉理解领域，包含精心策划构建的涵盖400万张多样化且高质量的生成图像，以及用于生成这些图像的文本提示

<img src="https://img.saraba1st.com/forum/202307/05/011606t8tp8q8s7to8ozps.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-011146.jpg</strong> (97.09 KB, 下载次数: 0)

下载附件

2023-7-5 01:16 上传

<img src="https://img.saraba1st.com/forum/202307/05/011606fhnsuwskeuubuhg3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-011215.jpg</strong> (239.01 KB, 下载次数: 0)

下载附件

2023-7-5 01:16 上传

进一步设计了4个基准来量化生成图像理解，包括内容和风格解释方面的性能，分别是提示反转(prompt inversion)、风格检索、图像字幕和视觉问答

最后，评估了当前的SOTA多模态模型应用于JourneyDB时的性能，并深入分析了它们在生成内容理解方面的优势和局限性，研究组希望所提出的数据集和基准能够促进生成内容理解领域的研究

<img src="https://img.saraba1st.com/forum/202307/05/011606tezmte3rak83ece8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-011133.jpg</strong> (96.74 KB, 下载次数: 0)

下载附件

2023-7-5 01:16 上传

数据收集程序，为了收集足够的生成图像，研究组调查了Discord上的Midjourney频道以收集可用的图片，然后使用GPT-3.5来注释下游任务，包括
1.将提示分为“风格”和“内容”
2.根据从任务1获得的内容词生成标题
3.生成“风格相关” 问题”和“内容相关问题”，每个问题提供 4 个选项以及答案

相关评估结果:

<img src="https://img.saraba1st.com/forum/202307/05/011626mnrazr6xq0ma4y4p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-011440.jpg</strong> (213.42 KB, 下载次数: 0)

下载附件

2023-7-5 01:16 上传

<img src="https://img.saraba1st.com/forum/202307/05/011626n6az3q43soo67rmj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-011505.jpg</strong> (77.64 KB, 下载次数: 0)

下载附件

2023-7-5 01:16 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 645#       发表于 2023-7-5 01:52

metatrained-demRAG

使用范例检索进行的元训练可实现高效的小样本学习效率

github项目仓库:https://github.com/facebookresearch/

metatrained-demRAG

大型语言模型在少样本NLP任务上显示出令人印象深刻的结果，然而，这些模型是显存和计算密集型的

元训练(Meta-training)允许人们利用较小的模型以领域通用和任务无关的方式进行少样本泛化，然而，这些方法本身会导致模型可能没有足够的参数化或知识来快速适应多样化的任务

为了克服这个问题，本文提出了带有范例检索的元训练，其中使用密集的段落检索器来检索与每个示例语义相似的标记演示，以实现更多样化的监督，通过将外部知识与模型参数分离，可以使用元训练来训练参数高效的模型，这些模型可以很好地泛化到通用领域更多种类的任务性能中

研究组从UnifiedQA和CrossFit构建了一个元训练集，并提出了一个基于UnifiedQA任务的范例库(demonstration bank)

本文是第一个将检索与元训练相结合的工作，通过使用DPR模型检索演示，并同时利用来自多个任务的演示，而不是从目标任务的训练集中随机采样演示

本文方法在QA、NLI和文本分类任务(包括SQuAD、QNLI和TREC)上优于各种目标参数高效和检索增强的小样本方法，同时可以在单个GPU上进行快速的元训练和微调

<img src="https://img.saraba1st.com/forum/202307/05/015210lzthuql5mo3jhh1o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-014230__01.jpg</strong> (73.14 KB, 下载次数: 0)

下载附件

2023-7-5 01:52 上传

方法流程，给定来自许多可能的QA任务之一的输入x，使用密集段落检索器从标记示例组成的范例库Z中检索K个语义相似的演示Z={zk}1,...,K，对BART进行元训练，监督它在给定x和Z的不同QA任务集合中生成(问题和)答案y

数据集与相关评估结果:

<img src="https://img.saraba1st.com/forum/202307/05/015233aqr73nqaw7nn30nn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-014718.jpg</strong> (158.87 KB, 下载次数: 0)

下载附件

2023-7-5 01:52 上传

<img src="https://img.saraba1st.com/forum/202307/05/015233yzk1pb7rbb6z5v6o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-014850.jpg</strong> (121.98 KB, 下载次数: 0)

下载附件

2023-7-5 01:52 上传

<img src="https://img.saraba1st.com/forum/202307/05/015233iji88873j2z3ztwy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-014908.jpg</strong> (213.49 KB, 下载次数: 0)

下载附件

2023-7-5 01:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 646#       发表于 2023-7-5 02:15

sam-pt

当SAM遇到点追踪(Point Tracking)

github项目地址:https://github.com/SysCV/sam-pt

Segment Anything Model(SAM)已成为强大的零样本图像分割模型，其使用交互式提示(例如点)来生成掩码

本文介绍了SAM-PT，这是一种扩展SAM跟踪和分割到任意动态视频中任何内容的能力的方法

SAM-PT利用稳健且稀疏的点选择和传播技术来生成掩码，证明了基于SAM的分割跟踪器也可以在流行的视频对象分割基准(包括 DAVIS、YouTube-VOS和MOSE)中产生强大的零样本性能

与传统的以对象为中心的掩码传播策略相比，独特地使用点传播来利用与对象语义无关的局部结构信息，通过对零样本开放世界未识别视频对象(UVO/zero-shot open-world Unidentified Video Objects)基准的直接评估来强调基于点的跟踪的优点

为了进一步增强本方法，还利用了K-Medoids聚类进行点初始化并跟踪正点和负点以清楚地区分目标对象，采用了多个掩码解码通道进行掩码细化，并设计了跟踪点重新初始化策略以提高跟踪精度

<img src="https://img.saraba1st.com/forum/202307/05/021504khzcdl0myi3sisgi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-020615.jpg</strong> (148.72 KB, 下载次数: 0)

下载附件

2023-7-5 02:15 上传

<img src="https://img.saraba1st.com/forum/202307/05/021504teb9j1bg87s78hu3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-020921.jpg</strong> (122.39 KB, 下载次数: 0)

下载附件

2023-7-5 02:15 上传

<img src="https://img.saraba1st.com/forum/202307/05/021504tufdiz05uh5bjnp3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-021058.jpg</strong> (278.62 KB, 下载次数: 0)

下载附件

2023-7-5 02:15 上传

SAM-PT 是第一个利用稀疏点传播进行视频对象分割(VOS)的方法，SAM-PT的本质是用长期点跟踪器扩展SAM，以零样本的方式有效地预测视频，SAM-PT将视频以及第一帧中目标对象的注释作为输入，这些注释称为“查询点”，表示目标对象(正点)或指定非目标片段(负点)

使用点跟踪器在整个视频中跟踪点，将查询点传播到所有视频帧，生成预测轨迹和遮挡分数，随后用轨迹中的非遮挡点提示SAM，以便独立地输出每个视频帧的分段掩码

重新初始化有助于消除不可靠和遮挡的点，并添加在后续帧中变得可见的对象部分或片段中的点，例如当对象旋转时

相关示例:

<img src="https://img.saraba1st.com/forum/202307/05/021541lcv6k5zuuy0h69u9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-021242.jpg</strong> (590.7 KB, 下载次数: 0)

下载附件

2023-7-5 02:15 上传

<img src="https://img.saraba1st.com/forum/202307/05/021541kpipmj0030hjm1ij.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-021250.jpg</strong> (504.01 KB, 下载次数: 0)

下载附件

2023-7-5 02:15 上传

相关评估:

<img src="https://img.saraba1st.com/forum/202307/05/021519pg0kk91u42kktt4k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-021121.jpg</strong> (144.47 KB, 下载次数: 0)

下载附件

2023-7-5 02:15 上传

<img src="https://img.saraba1st.com/forum/202307/05/021519v2jxcdqrrxjrf2fe.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-021144.jpg</strong> (214 KB, 下载次数: 0)

下载附件

2023-7-5 02:15 上传

<img src="https://img.saraba1st.com/forum/202307/05/021519v87oi688riallgso.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-021223.jpg</strong> (173.4 KB, 下载次数: 0)

下载附件

2023-7-5 02:15 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 647#       发表于 2023-7-5 02:22

非官方的StyleDrop开源复现

github项目代码库:https://github.com/zideliu/StyleDrop-PyTorch

hugface权重下载地址:https://huggingface.co/zideliu/StyleDrop/tree/main

<img src="https://img.saraba1st.com/forum/202307/05/022145qe55di25xqznrkab.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-022108__01.jpg</strong> (745.22 KB, 下载次数: 0)

下载附件

2023-7-5 02:21 上传

<img src="https://img.saraba1st.com/forum/202307/05/022145qyqrd1r8ii0udvpm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-022108__02.jpg</strong> (453.27 KB, 下载次数: 0)

下载附件

2023-7-5 02:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 648#       发表于 2023-7-5 02:50

DisCo

参考现实世界中的人类舞蹈进行解耦的控制生成

项目主页:https://disco-dance.github.io/

github项目仓库:https://github.com/Wangt-CN/DisCo

Demo演示:https://d383ecc864f168ffb5.gradio.live/

生成式AI在计算机视觉领域取得了重大进展，特别是在基于文本描述的图像/视频合成方面，尽管取得了进步，但仍然具有挑战性，特别是在生成以人为中心的内容(例如舞蹈合成)方面等，现有的舞蹈合成方法难以解决合成内容与现实世界舞蹈场景之间的差距

<img src="https://img.saraba1st.com/forum/202307/05/024915frj111frd3bzcc1b.jpg" referrerpolicy="no-referrer">

<strong>teaser.jpg</strong> (165.13 KB, 下载次数: 0)

下载附件

2023-7-5 02:49 上传

在本文中，定义了一个新的问题设定：参考人类舞蹈生成，它侧重于具有三个重要属性的现实世界舞蹈场景:
1.忠实性：合成应保留参考中人类主体前景和背景的外观图像，精确跟随目标姿势
2.泛化性：模型应该泛化到未见的人类主体、背景和姿势
3.组合性：它应该允许来自不同来源的可见/不可见主题、背景和姿势的组合

为了应对这些挑战，引入了一种新方法，DISCO，它包括一种新颖的模型架构，具有解耦控制，以提高舞蹈合成的忠实度和组合性，以及有效的人类属性预训练，可以更好地推广到未来的人类动作

大量的定性和定量结果表明，DISCO可以生成外观多样、动作灵活的高质量人类舞蹈图像和视频

<img src="https://img.saraba1st.com/forum/202307/05/024947cp2sj2a6x7nbsxuy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-024938.jpg</strong> (125.15 KB, 下载次数: 0)

下载附件

2023-7-5 02:49 上传

(a)具有解耦控制的模型架构：提出了条件与交叉注意力和ControlNet的有机集成，具体来说，用人类主体的CLIP图像嵌入替换了T2I扩散模型中的文本条件，这是通过U-Net的交叉注意模块合并的，而背景和人体姿势条件则被输入两个独立的ControlNet分支，通过解耦所有三个条件的控制，DisCo不仅可以实现人类前景和背景的保真度，还可以实现人类主体、背景和舞蹈动作的任意组合

(b)人体属性预训练：设计了一个代理任务，其中模型以单独的前景和背景区域为条件，并且必须重建完整的图像，这样模型在预训练时就学会了更好地对复杂的人脸和衣服进行编码和解码，并将姿势控制学习留给了人类舞蹈合成的微调阶段，至关重要的是，在没有成对人类图像用于姿势控制的约束的情况下，可以利用大规模的人类图像集合来学习不同的人类属性，从而**提高DISCO对未见人类的泛化能力

生成结果与最近的其他方法:

<img src="https://img.saraba1st.com/forum/202307/05/025027w1rfxmfkgqqx1qee.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-024748.jpg</strong> (526.07 KB, 下载次数: 0)

下载附件

2023-7-5 02:50 上传

<img src="https://img.saraba1st.com/forum/202307/05/025027kccdlo6oqvltvdvf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-024759.jpg</strong> (250.81 KB, 下载次数: 0)

下载附件

2023-7-5 02:50 上传

<img src="https://img.saraba1st.com/forum/202307/05/025027t4av1859v0480884.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-024833.jpg</strong> (427.87 KB, 下载次数: 0)

下载附件

2023-7-5 02:50 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 649#       发表于 2023-7-5 03:59

llm-japanese-dataset v0

大型语言模型的日语聊天数据集及其构建方法

相关论文:https://arxiv.org/abs/2305.12720

项目主页:https://llm.msuzuki.me/

相关数据集:https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset

GitHub项目主页:https://github.com/masanorihirano/llm-japanese-dataset

这项研究构建了一个用于调整大型语言模型(LLM)的日语聊天数据集，该数据集包含约840万条记录

最近，LLM得到了发展并受到欢迎，然而，表现优异的LLM通常主要是英语专业，这些LLM有两种支持英语以外的语言的方法：从头开始构建LLM或调整现有模型

然而在这两种方式中，数据集都是必要的部分因此这项研究的重点是支持日语LLM的发展，并制作用于训练或调整LLM的数据集

构建的数据集包含各种任务，例如翻译和知识任务，在实验中评估了使用本项目的数据集调整的现有的LLM，结果表明本数据集对于LLM的可能增益，同时也揭示了一些用英语以外的语言构建LLM涉及到的具体困难

<img src="https://img.saraba1st.com/forum/202307/05/035927szj6nnkj75h5mf17.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-034613.jpg</strong> (147.13 KB, 下载次数: 0)

下载附件

2023-7-5 03:59 上传

<img src="https://img.saraba1st.com/forum/202307/05/035928pls2bxbw3wx1myms.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-034637.jpg</strong> (276.93 KB, 下载次数: 0)

下载附件

2023-7-5 03:59 上传

<img src="https://img.saraba1st.com/forum/202307/05/035928nznocq5fa2u5kmrf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-034651.jpg</strong> (366.79 KB, 下载次数: 0)

下载附件

2023-7-5 03:59 上传

<img src="https://img.saraba1st.com/forum/202307/05/035928oyy8yo3m1hhai11m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-034837.jpg</strong> (214.85 KB, 下载次数: 0)

下载附件

2023-7-5 03:59 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 650#       发表于 2023-7-5 04:27

SketchMetaFace

基于学习的高保真3D角色面部草图绘制界面自动建模

项目主页:https://zhongjinluo.github.io/SketchMetaFace/

github项目仓库:https://github.com/zhongjinluo/SketchMetaFace

3D虚拟形象建模有利于AR/VR、游戏、影视等多种应用场景，角色面部作为虚拟形象的重要组成部分，具有显著的多样性和生动性，然而构建3D人物面部模型通常需要使用商业工具进行大量工作，即使对于经验丰富的艺术家来说也是如此，现有的各种基于草图的工具无法支持业余爱好者建模不同的面部形状和丰富的几何细节

在本文中介绍了SketchMetaFace，一个针对业余用户的草图绘制系统，可在几分钟内对高保真3D脸部进行建模

全程使用精心设计的用户界面和底层算法，首先，采用曲率感知笔画，更好地支持雕刻面部细节的可控性，其次，考虑到将2D草图映射到3D模型的关键问题，开发了一种新颖的基于学习的方法，称为“隐式和深度引导的网格建模”(IDGMM/Implicit and Depth Guided Mesh Modeling)，融合了网格、隐式和深度表征的优点，以高效率获得高质量的结果

此外，为了进一步支持可用性，还提出了从粗到细的2D草图界面设计和数据驱动的笔画建议工具，用户研究证明了本系统在易用性和结果的视觉质量方面优于现有建模工具，实验分析还表明IDGMM在精度和效率之间达到了更好的权衡

<img src="https://img.saraba1st.com/forum/202307/05/042632qvcdbzwcckb9i006.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-042405.jpg</strong> (80.59 KB, 下载次数: 0)

下载附件

2023-7-5 04:26 上传

<img src="https://img.saraba1st.com/forum/202307/05/042632kt2n2ztz0002b7tm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-042528.jpg</strong> (299.19 KB, 下载次数: 0)

下载附件

2023-7-5 04:26 上传

<img src="https://img.saraba1st.com/forum/202307/05/042633o770j230349jj3bb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-042554.jpg</strong> (533.99 KB, 下载次数: 0)

下载附件

2023-7-5 04:26 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 651#       发表于 2023-7-5 04:43

InstructEval

系统性评估各种上下文指令选择方式的性能

相关论文:https://arxiv.org/abs/2307.00259

github项目仓库(待整理):https://github.com/princeton-nlp/InstructEval

上下文学习(ICL)通过使用指令和一小组带标注的示例(称为演示)提示大型语言模型(LLM)来执行任务

最近的研究表明，提示中使用的输入的精确细节会显著影响ICL，从而激励了指令选择算法的开发

然而，指令选择的影响还被严重低估，现有的分析仅限于模型和任务的浅层子集，这限制了其见解的普遍性

本文研究组开发了ICL评估套件来对这些技术进行全面评估，该套件包括来自4个不同模型系列的13个不同规模的开源LLM，涵盖9种不同的任务，代表3个类别的一系列任务类型，使用与ICL相关的5个需求的基准测试来评估7种流行指令选择方法的相对性能

实验发现，使用精心策划的手动编写指令和没有任何特定于任务描述的简单指令通常会比自动指令归纳方法产生更好的ICL性能，这表明后者缺乏通用性，同时发布了用于基准指令选择方法的评估套件，并呼吁在该领域采用更严格和更通用的方法

评估流程等:

<img src="https://img.saraba1st.com/forum/202307/05/043808rtpo48z7hzpo8kwx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-043723.jpg</strong> (135.3 KB, 下载次数: 0)

下载附件

2023-7-5 04:38 上传

<img src="https://img.saraba1st.com/forum/202307/05/044135b3zjgy8abkyzoxiu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-043926.jpg</strong> (141.2 KB, 下载次数: 0)

下载附件

2023-7-5 04:41 上传

<img src="https://img.saraba1st.com/forum/202307/05/044135rcc49bt4q24akcij.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-043947.jpg</strong> (152.12 KB, 下载次数: 0)

下载附件

2023-7-5 04:41 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 652#       发表于 2023-7-5 06:05

LEDITS

使用DDPM反演和语义指导进行真实图像编辑

Demo演示:https://huggingface.co/spaces/editing-images/ledits

<img src="https://img.saraba1st.com/forum/202307/05/060435h1q1n83gh5qnvcj8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-054347__01.jpg</strong> (544.9 KB, 下载次数: 0)

下载附件

2023-7-5 06:04 上传

最近的大规模文本引导扩散模型提供了强大的图像生成功能，目前，人们也付出了巨大的努力来通过仅使用文本作为直观和多功能编辑的方式来修改这些图像

然而，由于编辑技术的固有性质，编辑对于这些生成模型来说是困难的，其中涉及保留原始图像中的某些内容，相反，在基于文本的模型中，即使对文本提示进行微小的修改也常常会导致完全不同的结果，这使得实现准确对应用户意图的一次性生成变得极具挑战性

此外，要使用SOTA方法编辑真实图像，必须首先将图像反转到预先训练的模型域中，添加了另一个影响编辑质量的因素，以及延迟

在这份探索性报告中，提出了LEDITS，一种用于真实图像编辑的组合轻量级方法，将对编辑友好的DDPM反演技术与语义指导相结合，从而将语义指导扩展到真实图像编辑，同时利用DDPM反演的编辑功能

这种方法实现了多种编辑效果，微妙和广泛以及构图和风格的改变，同时不需要对架构进行优化或扩展

<img src="https://img.saraba1st.com/forum/202307/05/060425vvibo53e4a4ibudo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-054428.jpg</strong> (135.49 KB, 下载次数: 0)

下载附件

2023-7-5 06:04 上传

LEDITS概览图

上方为输入图像的反转，首先对原始图像应用DDPM反演以获得反演的潜在(inverted latents)和对应的噪声图

下方则使用反演潜在在语义指导下驱动反向扩散过程，在每个去噪步骤中，根据SEGA逻辑计算噪声估计，使用预计算的噪声图根据DDPM scheme计算更新后的潜在

引入语义指导的概念是为了增强对文本引导扩散模型生成过程的细粒度控制，SEGA通过专门与模型潜在空间中已有的概念进行交互，扩展了无分类器指导中引入的原则

该计算在正在进行的扩散迭代中进行，旨在影响多个方向的扩散过程，更具体地说，SEGA除了文本提示p之外，还使用多个文本描述ei来表示生成图像的给定目标概念

生成实例与对比结果:

<img src="https://img.saraba1st.com/forum/202307/05/060445kjppl3rssal4k0x2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-060245.jpg</strong> (353.83 KB, 下载次数: 0)

下载附件

2023-7-5 06:04 上传

<img src="https://img.saraba1st.com/forum/202307/05/060445hwrrhqlf0hikiler.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230705-060228__01.jpg</strong> (638.8 KB, 下载次数: 0)

下载附件

2023-7-5 06:04 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 653#       发表于 2023-7-6 03:43

 本帖最后由 Machinery 于 2023-7-6 03:51 编辑 

SDXL(预览技术报告)/stable-diffusion-xl-base-0.9

改进潜在扩散模型的高分辨率图像合成

hugface的stable-diffusion-xl-base-0.9权重:https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9

SDXL，一种用于文本到图像合成的新型潜在扩散模型，与之前版本的Stable Diffusion相比，SDXL将构架中的UNet主干扩大了三倍，模型参数的增加主要是由于更多的注意力块和更大的交叉注意力上下文，因为SDXL使用两个文本编码器，还设计了多种新颖的调节方案以在多种图片纵横比上训练SDXL，最后还引入了一种细化模型，该模型用于使用事后的图像到图像技术提高SDXL生成的样本的视觉保真度

用户研究表明，SDXL始终大幅超越所有以前版本的Stable Diffusion，SDXL显著提高了性能，并取得了与黑盒闭源SOTA图像生成器相媲美的结果，本着促进开放研究并提高大型模型训练和评估透明度的精神，将提供对代码和模型权重的访问

<img src="https://img.saraba1st.com/forum/202307/06/034114wsaccopqh7pqo9hv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-030904.jpg</strong> (53 KB, 下载次数: 0)

下载附件

2023-7-6 03:41 上传

左图:比较了SDXL与Stable Diffusion 1.5和2.1之间的用户偏好，虽然SDXL的性能已经明显优于1.5和2.1，但添加额外的细化阶段依然可以继续提高性能

右图:两阶段工作流程的可视化，首先使用SDXL生成大小为128×128的初始潜在变量，之后利用专门的高分辨率细化模型，并使用相同的提示对第一步中生成的潜在变量应用SDEdit，SDXL和细化模型使用相同的自动编码器

<img src="https://img.saraba1st.com/forum/202307/06/034120y3hhtzadhdy70zwg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-031129.jpg</strong> (81.77 KB, 下载次数: 0)

下载附件

2023-7-6 03:41 上传

与原始的Stable Diffusion架构相比，在UNe内使用Transformer Block的异构分布，出于效率原因，省略了最高特征级别的Transformer Block，在较低级别使用2和10个Block， 并完全删除UNet中的最低级别(8×下采样)

比较Stable Diffusion1.x &amp; 2.x和SDXL的架构，选择了更强大的预训练文本编码器用于文本调节，具体来说，将OpenCLIP ViT-bigG与CLIP ViT-L结合使用，其中沿着通道轴连接倒数第二个文本编码器输出，除了使用交叉注意力层来根据文本输入来调节模型之外，并根据(arXiv：2112.10741，2021)进行修改，以及使用OpenCLIP模型的池化文本嵌入来调节模型，这些变化导致UNet中的模型大小为2.6B参数

<img src="https://img.saraba1st.com/forum/202307/06/034657b4xopfnyf9so4484.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-033301__01.jpg</strong> (279.01 KB, 下载次数: 0)

下载附件

2023-7-6 03:46 上传

SDXL与之前版本的Stable Diffusion的输出比较，对于每个提示，使用DDIM采样器和cfg-scale 8.0的50个步骤的相应模型的3个随机样本

<img src="https://img.saraba1st.com/forum/202307/06/034134u52m3g1m3qm515zq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-033827.jpg</strong> (325.47 KB, 下载次数: 0)

下载附件

2023-7-6 03:41 上传

细化阶段，根据经验，发现生成的模型有时会产生局部质量较低的样本，为了提高样本质量，在同一潜在空间中训练一个单独的LDM，该LDM专门用于高质量、高分辨率的数据，并对来自基本模型的样本采用SDEdit引入的噪声去噪过程

在推理过程中，从基础SDXL渲染潜在变量，并使用相同的文本输入，通过细化模型直接在潜在空间中对它们进行扩散和去噪，此步骤是可选的，但可以提高详细背景和人脸的样本质量

为了评估模型的性能(有或没有细化阶段)，进行了一项用户研究，并让用户从以下四种型号中选择自己喜欢的：SDXL、SDXL(带细化器)、Stable Diffusion1.5和Stable Diffusion 2.1

结果表明，具有细化阶段的SDXL是评级最高的选择，并且显著优于Stable Diffusion1.5和2.1(获胜率为：带细化的SDXL：48.44%，SDXL基础：36.93%，Stable Diffusion1.5：7.91%  ，Stable Diffusion2.1：6.71%)

然而，当使用FID和CLIP分数等经典性能指标时，SDXL相对于以前方法的改进并没有反映出来，这可能是因为自动评估指标的局限性

<img src="https://img.saraba1st.com/forum/202307/06/034710o48m34l3nk4i8m7d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-034151__01.jpg</strong> (172.89 KB, 下载次数: 0)

下载附件

2023-7-6 03:47 上传

SDXL的失败案例，尽管与之前版本的稳定扩散相比有了很大的改进，该模型有时仍然难以处理涉及详细空间安排和详细描述的非常复杂的描述

此外，手部也并不总是能正确生成，并且模型有时会遇到两个概念相互渗透的问题，图中所有示例都是使用DDIM采样器和cfg-scale 8.0的50个步骤生成的随机样本

虽然SDXL在生成逼真图像和合成复杂场景方面表现出了令人印象深刻的能力，但重要的是要承认其固有的局限性

了解这些限制对于进一步改进和确保负责任地使用该技术至关重要，首先模型在​​合成复杂的结构时可能会遇到挑战，例如人手，尽管它已经接受了各种数据的训练，但人体解剖学的复杂性给实现一致的准确表示带来了困难，这种限制表明需要进一步的缩放模型规模和更强大的训练技术，例如专门针对细粒度细节的合成

发生这种情况的原因可能是手和类似物体在照片中出现的差异非常大，在这种情况下模型很难提取真实3D形状和物理限制的知识，其次，虽然该模型在生成的图像中实现了显着的真实感水平，但值得注意的是，它并没有达到完美的照片真实感，某些细微差别，例如微妙的灯光效果或微小的纹理变化，可能仍然不存在或在生成的图像中不太忠实地表示，此限制意味着在需要高度视觉保真度的应用程序中仅依赖模型生成的视觉效果时应谨慎行事

此外，该模型的训练过程严重依赖于大规模数据集，这可能会无意中引入社会和种族偏见，因此，模型在生成图像或推断视觉属性时可能会无意中加剧这些偏差

在样本包含多个对象或主体的某些情况下，模型可能会表现出一种称为“概念出血”的现象，此问题表现为不同视觉元素的意外合并或重叠

对于未来的可能改进:

<img src="https://img.saraba1st.com/forum/202307/06/034226xz4qfxs4fy4q4fhd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-033912.jpg</strong> (373.39 KB, 下载次数: 0)

下载附件

2023-7-6 03:42 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 654#       发表于 2023-7-6 04:00

SearchAnything

“SearchAnything”是一个本地语义搜索引擎，由各种AI模型提供支持，可以让您根据语义搜索句子和图像

github项目地址:https://github.com/Immortalise/SearchAnything

应用工作流程:

<img src="https://img.saraba1st.com/forum/202307/06/040027a99wf9xoh1h22nxw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-035925.jpg</strong> (64.5 KB, 下载次数: 0)

下载附件

2023-7-6 04:00 上传

演示视频截图:

<img src="https://img.saraba1st.com/forum/202307/06/040006q8bch70ob7z0zjxl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-035858.jpg</strong> (91.46 KB, 下载次数: 0)

下载附件

2023-7-6 04:00 上传

<img src="https://img.saraba1st.com/forum/202307/06/040006uhlzvgkzl1tlq3lg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-035914.jpg</strong> (57.62 KB, 下载次数: 0)

下载附件

2023-7-6 04:00 上传

Todo列表:

<img src="https://img.saraba1st.com/forum/202307/06/040040pvbm3b3f8mk4c8fk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-035942.jpg</strong> (97.29 KB, 下载次数: 0)

下载附件

2023-7-6 04:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 655#       发表于 2023-7-6 04:22

HyenaDNA

可达到单核苷酸级别良好细粒度分辨率的长距基因组序列建模

相关论文:https://arxiv.org/abs/2306.15794

模型权重下载:https://huggingface.co/LongSafari

<img src="https://img.saraba1st.com/forum/202307/06/042209ruxantn7xyfnufm5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-041954.jpg</strong> (87.83 KB, 下载次数: 0)

下载附件

2023-7-6 04:22 上传

github项目仓库:https://github.com/HazyResearch/hyena-dna

开箱既用的colab:https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL?usp=sharing

项目博客:https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna

<img src="https://img.saraba1st.com/forum/202307/06/042146o4nno0p9kozzklo8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230706-042029.jpg</strong> (87.49 KB, 下载次数: 0)

下载附件

2023-7-6 04:21 上传

基因组(DNA)序列编码大量用于基因调控和蛋白质合成的信息，与自然语言模型类似，研究人员提出了基因组的基础模型，从未标记的基因组数据中学习可概括的特征，然后可以针对下游任务(例如识别作用元件)进行微调

由于注意力的二次缩放原因，之前基于Transformer的基因组模型通常使用512到4k个Token作为上下文(总量小于人类基因组的 0.001%)，极大地限制了DNA中远程相互作用的建模精确性

此外，这些方法依赖分词器来聚合有意义的DNA单元，从而失去单核苷酸级别的细粒度分辨率，其中细微的遗传变异可以通过单核苷酸多态性(SNP/single nucleotide polymorphisms)完全改变蛋白质功能

最近，Hyena(鬣狗)架构模型，基于隐式卷积的大型语言模型被证明可以在质量上匹配通常的Transformer注意力架构，同时允许更长的上下文长度和更低的时间复杂度

利用Hyenas架构新的长程能力，研究组推出了HyenaDNA，这是一种在人类参考基因组上进行预训练的基因组基础模型，在单核苷酸级别的上下文长度高达100万Token的训练集上训练，比之前基于密集注意力的模型增加了500倍

HyenaDNA在序列长度上按次二次复杂度缩放(训练速度比 Transformer快160倍)，使用单核苷酸级别的Token，并且在每一层都有完整的全局上下文，探索了更长的上下文可以带来什么，包括在基因组中首次使用上下文学习来简单地适应新任务，而无需更新预训练的模型权重

根据Nucleotide Transformer的微调基准，HyenaDNA使用参数和预训练数据少几个数量级的模型，在17个数据集中的12个上达到了SotA水平，在GenomicBenchmarks基准上，HyenaDNA在所有8个数据集上，平均超过前SotA 9%

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 656#       发表于 2023-7-7 02:29

counterfactual-evaluation

推理还是背诵？通过反事实任务探索语言模型的能力和局限性

github项目地址(coming soon):https://github.com/ZhaofengWu/counterfactual-evaluation

最近的语言模型在广泛的任务中令人印象深刻的表现，表明它们拥有一定程度的抽象推理能力，这些技能是通用的、可转移的，还是专门针对在预训练期间看到的特定任务的？ 

为了理清这些影响，本文提出了一个基于“反事实”的任务变体的评估框架，这些任务变体偏离了标准任务的默认假设，在一组11项任务中，观察到反事实变体的非凡性能，但仍然发现与默认条件相比，性能持续大幅下降

这表明，虽然当前的语言模型可能在一定程度上拥有抽象的任务解决技能，但他们通常也依赖狭窄的、不可转移的任务解决
方式

这些结果激发了对语言模型性能的更仔细的解释，从而为区分这些行为的不同方面做出努力

<img src="https://img.saraba1st.com/forum/202307/07/022846cyz1r4yrr6yrsl16.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-020849.jpg</strong> (165.57 KB, 下载次数: 0)

下载附件

2023-7-7 02:28 上传

GPT-4在各种任务(蓝色)和对应的反事实情况下(橙色)的默认版本上的性能，使用了零样本思维链提示，可以看到GPT-4与默认任务实例相比，在反事实变体任务上始终表现不佳

<img src="https://img.saraba1st.com/forum/202307/07/022906t0qiilqfpcrjlwkf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-022343.jpg</strong> (414.19 KB, 下载次数: 0)

下载附件

2023-7-7 02:29 上传

<img src="https://img.saraba1st.com/forum/202307/07/022906qmqexzuzl0cinvn0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-022308__01.jpg</strong> (307.29 KB, 下载次数: 0)

下载附件

2023-7-7 02:29 上传

对于每项任务，评估了GPT-4(gpt-4-0314,OpenAI,2023)、GPT-3.5(gpt-3.5-turbo-0301)、Claude(claude-v1.3,Anthropic,2023)和PaLM-2(text-bison-001,Anil et al.,2023)

由于这些是闭源模型，没有任何有关其大小、架构和保留细节的信息，其中最大的PaLM模型并不能公开访问，只能测试第二大的版本

对于每项任务，都通过在提示中添加短语“让我们一步一步思考”来尝试鼓励和不鼓励模型逐步推理，在反事实任务变体上的结果，无论是否有0-shot CoT，对于大多数情况， LM都表现出了高于随机的反事实性能，表明拥有一定程度的目标能力，但是总体来说看到了一个一致的模式，即LM在反事实任务变体上的表现要差得多

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 657#       发表于 2023-7-7 03:19

 本帖最后由 Machinery 于 2023-7-7 03:20 编辑 

KokoMind

LLM可以理解社交互动吗？

项目主页:https://chats-lab.github.io/KokoMind/

github项目仓库:https://github.com/CHATS-lab/KokoMind

引入了Koko Mind，这是一个具有多方社交互动的数据集，用于评估LLM的社交理解能力

想象一下：假如您正在参加一个充满活力的鸡尾酒会，充满了热闹的谈话声和玻璃杯的叮当声，而您是一个悠闲的观察者，舒适地躲在角落里

即使如此，您仍然可以轻松地弄清楚不同人之间的社会关系，了解正在发生的事情，甚至通过阅读人们的言语和非言语线索来提供社交建议

如果一个大型语言模型(LLM)能够复制这种水平的社交能力，那么我们可以说它具有一定的社交能力，那么不同的LLM表现能力如何？

<img src="https://img.saraba1st.com/forum/202307/07/031702tbot3ulojsb33tut.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-031652.jpg</strong> (93.25 KB, 下载次数: 0)

下载附件

2023-7-7 03:17 上传

GPT-4在通常情况下位居榜首，在很多情况下紧随其后的是Claude，以text-davinci-003作为参考，评估了AlpacaEval的不同模型，并进行了消融研究，其中包括从上下文中删除了括号中的非语言线索(例如，紧张地喝咖啡等)，以下是一些有趣的要点

1.在基于LLM的自动评估者中，与Claude相比，GPT-4在识别获胜模型方面表现出更大的确定性和信心

2.当上下文没有非语言线索，并且交互要么完全由GPT-4生成，要么基于电影时，Claude的表现优于GPT-4(两位自动评估者都同意)，而如果上下文包含非语言线索，GPT-4总是比Claude更好，一种可能的解释是GP​​T-4是一种多模态模型，因此正如预期的那样，它可以更好地理解额外的非语言信息，当提供非语言线索时，基于LLM的评估者们都认为表现最好的模型比成绩较差的模型确实的具有更实质性的优势

3.人们可能会想，如果社交互动是由GPT-4生成的，是否意味着GPT-4已经可以回答这些问题了？实际上问题的类型(问题的形式和性质)更加影响答案，而问题来自于谁(问题的来源)的影响相对较小

4.看起来基于LLM的评估者发现在与心理理论无关的任务中更容易确定更好的模型，尤其是从ToMi数据集生成的样本中，这可能是因为即使是LLM评估者也可能很难在心理理论背景下辨别正确答案

5.克劳德在多数情况下都可以给出不错的社交建议

<img src="https://img.saraba1st.com/forum/202307/07/031808b05z4pssbspi26w5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-025848.jpg</strong> (167.76 KB, 下载次数: 0)

下载附件

2023-7-7 03:18 上传

数据集示例:

<img src="https://img.saraba1st.com/forum/202307/07/031726w2z2o22lxcsoc238.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-025910.jpg</strong> (204.82 KB, 下载次数: 0)

下载附件

2023-7-7 03:17 上传

数据集分布:

<img src="https://img.saraba1st.com/forum/202307/07/031748bg29m8k0v0i22okp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-025932.jpg</strong> (125.5 KB, 下载次数: 0)

下载附件

2023-7-7 03:17 上传

虽然本项目在很多方面都令人兴奋，但也有一定的局限性，Koko Mind数据集的规模相对较小，这可能会限制结论的广泛适用性和全面性

其次， Koko Mind中的所有交互都是由GPT-4生成的，需要人工验证，这使得数据集难以扩展，虽然Koko Mind在数据集中提供了经过人工验证的答案，但并没有在评估时使用这些答案作为参考，并且由于这些答案是由 GPT-4 生成的，因此它们可能会偏向 GPT-4

未来的研究可以集中在如何评估模型上具有经过人工验证的机器生成的参考答案，评估中的模型型号都是2023年6月1日之前的版本，新发布的模型型号可能会有更好的性能

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 658#       发表于 2023-7-7 04:27

Flacuna

通过FLAN微调释放Vicuna解决问题的能力

github项目仓库:https://github.com/declare-lab/flacuna

hugface模型权重下载:https://huggingface.co/declare-lab/flacuna-13b-v1.0

数据集下载地址:https://huggingface.co/datasets/declare-lab/flan-mini

最近，instruct eval(github项目地址:https://github.com/declare-lab/instruct-eval)的发布为利用编码器-解码器或仅解码器架构的大型语言模型(LLM)的性能提供了评估基准

有趣的是，尽管是四年前推出的构架，基于T5的LLM(例如Flan-T5)在需要通用问题解决技能的任务上继续优于最新的基于解码器的LLM(例如LLaMa和Vicuna)

这种性能差异可归因于三个关键因素：预训练数据、主干架构、指令数据集

在这份技术报告中，主要关注的是利用基于LLAMA微调的大型语言模型Vicuna来调查第三个因素的影响，该模型在ChatGPT对话上进行了微调

为了实现这一目标，使用了名为Flan-mini的定制指令数据集集合对Vicuna进行了微调，该集合包括称为Flan的大规模指令数据集的子集，以及其他源自ChatGPT/GPT-4的各种代码相关数据集和会话数据集，这些数据包含大量解决问题任务需求的能力

实验结果表明，Flacuna解决问题的能力是通过在FLAN数据集上微调Vicuna来获得的，从而导致Flacuna在instruct eval中对于众多基准数据集的测试的显著改进

Flan-mini数据集分布:

<img src="https://img.saraba1st.com/forum/202307/07/042209qzd7l7uwuxpovduh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-041713.jpg</strong> (92.13 KB, 下载次数: 0)

下载附件

2023-7-7 04:22 上传

基准测试成绩:

<img src="https://img.saraba1st.com/forum/202307/07/042320beh0c14da0zoaycf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-041828.jpg</strong> (101.57 KB, 下载次数: 0)

下载附件

2023-7-7 04:23 上传

<img src="https://img.saraba1st.com/forum/202307/07/042727q68q62lm27aq66gr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-042709.jpg</strong> (291.06 KB, 下载次数: 0)

下载附件

2023-7-7 04:27 上传

下表展示了Flacuna在IMPACT数据集上的写作能力，该数据集是InstructEval评估套件的组成部分，生成的响应由ChatGPT进行评估，并按1到5的等级对它们的相关性和连贯性进行评分:

<img src="https://img.saraba1st.com/forum/202307/07/042442m3703t4jdjjzq324.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-041908.jpg</strong> (70.55 KB, 下载次数: 0)

下载附件

2023-7-7 04:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 659#       发表于 2023-7-7 04:48

 本帖最后由 Machinery 于 2023-7-7 04:50 编辑 

DragonDiffusion

在扩散模型上启用拖动式操作

项目主页:https://mc-e.github.io/project/DragonDiffusion/

github项目仓库:https://github.com/MC-E/DragonDiffusion

<img src="https://img.saraba1st.com/forum/202307/07/045037tbywoykwlwccyjos.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-044148.jpg</strong> (252.5 KB, 下载次数: 0)

下载附件

2023-7-7 04:50 上传

尽管现有的大规模文本到图像(T2I)模型能够根据详细的文本描述生成高质量的图像，但它们通常缺乏精确编辑生成的或真实图像的能力

在本文中，提出了一种新颖的图像编辑方法DragonDiffusion，可以在扩散模型上进行拖动式操作控制编辑(类似GragGan)

具体来说，基于扩散模型中的中间特征的强对应性构建了分类器指导(classifier guidance)，分类器指导可以通过特征对应loss将编辑信号转换为梯度，以修改扩散模型的中间表征，基于此指导策略，还构建了多尺度指导来考虑语义和几何对齐，而且，添加了跨分支的自注意力以保持原始图像和编辑结果之间的一致性

本方法通过有效的设计，实现了对生成图像或真实图像的各种编辑模式，例如对象移动、对象大小调整、对象外观替换和内容拖动等操作

值得注意的是，所有编辑和内容保存信号都来自图像本身，模型不需要微调或附加的模块

<img src="https://img.saraba1st.com/forum/202307/07/044816mkqdam4844m1nok4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-044226.jpg</strong> (95.23 KB, 下载次数: 0)

下载附件

2023-7-7 04:48 上传

设计示意图，提出的方法由两个分支组成，即引导分支和生成分支，模型基于Stable Diffusion构建，无需微调或训练

<img src="https://img.saraba1st.com/forum/202307/07/044832u13qvl8g7fwwf7ep.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-044349.jpg</strong> (98.14 KB, 下载次数: 0)

下载附件

2023-7-7 04:48 上传

使用不同层的特征作为指导重建原始图像的图示，在实验中，将zT设置为随机高斯噪声，将mgen 、 mgud设置为零矩阵，并将mshare设置为一矩阵

<img src="https://img.saraba1st.com/forum/202307/07/044840s8i5i1gil2lw8hhl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-044520.jpg</strong> (259.67 KB, 下载次数: 0)

下载附件

2023-7-7 04:48 上传

对比loss和修复loss在对象移动任务中所扮演的操作的可视化，设计的对比loss可以消除多对象现象，而修复loss可以在缺失区域生成更自然的内容

操作实例:

<img src="https://img.saraba1st.com/forum/202307/07/044845g33g4v3dz4t843ed.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-044727.jpg</strong> (416.54 KB, 下载次数: 0)

下载附件

2023-7-7 04:48 上传

<img src="https://img.saraba1st.com/forum/202307/07/044845os3aj5ng5ggk3ky4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-044736.jpg</strong> (223.62 KB, 下载次数: 0)

下载附件

2023-7-7 04:48 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 660#       发表于 2023-7-7 07:37

 本帖最后由 Machinery 于 2023-7-7 07:40 编辑 

DiT-3D

探索用于3D形状生成的平面扩散Transformer

项目主页:https://dit-3d.github.io/

github项目地址:https://github.com/DiT-3D/DiT-3D

<img src="https://img.saraba1st.com/forum/202307/07/073629h4vynt8b4vm00i18.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-065235.jpg</strong> (46.51 KB, 下载次数: 0)

下载附件

2023-7-7 07:36 上传

<img src="https://img.saraba1st.com/forum/202307/07/073629a6sq9bqn8qtyx40x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-065245.jpg</strong> (41.75 KB, 下载次数: 0)

下载附件

2023-7-7 07:36 上传

 本文提出了DiT-3D，一种新颖的普通扩散Transformer构架，可以进行高保真和多样化的3D形状生成，直接对体素化的点云进行去噪处理

最近的扩散Transformer(例如DiT)已经证明了它们在生成高质量2D图像方面的强大功效，然而Transformer架构在3D形状生成方面是否表现同样出色仍有待确定，因为之前的3D扩散方法大多采用U-Net

为了弥补这一差距，提出了一种用于3D形状生成的新型扩散Transformer，即DiT-3D，它可以使用普通Transformer直接对体素化点云进行去噪处理

与现有的U-Net方法相比，DiT-3D在模型大小方面更具可扩展性，还可以产生更高质量的生成

具体来说，DiT-3D采用了DiT的设计理念，但通过合并3D位置和区块嵌入(patch embeddings)来对其进行修改，以自适应地聚合来自体素化点云的输入

为了降低3D形状生成中自注意力的计算成本，将3D窗口注意力合并到Transformer Block中，因为体素的附加维度增加的3D Token长度可能导致高计算量，最后，使用线性层和去像素化层来预测去噪点云

此外，本文架构也支持从2D到3D的高效微调，其中在ImageNet上预训练的DiT-2D检查点权重可以显著改进ShapeNet上的DiT-3D性能

ShapeNet数据集上的实验结果表明，所提出的DiT-3D在高保真和多样化的3D点云生成方面实现了SOTA性能

<img src="https://img.saraba1st.com/forum/202307/07/073642fabqmkazhibazb50.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-065431.jpg</strong> (130.29 KB, 下载次数: 0)

下载附件

2023-7-7 07:36 上传

DiT-3D将体素化点云作为输入，使用区块化算子(patchification operator)来生成Token级别的区块嵌入(patch embeddings)，其中3D位置嵌入被添加到一起，然后基于3D窗口注意力的多个Transformer Block从所有输入Token中提取点化体素表征，最后对线性层的未区块化的体素张量输出进行去体素化，以预测点云空间中的噪声

评估结果:

<img src="https://img.saraba1st.com/forum/202307/07/073724ezc4hh7pl2l2h47p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-070457.jpg</strong> (390.53 KB, 下载次数: 0)

下载附件

2023-7-7 07:37 上传

<img src="https://img.saraba1st.com/forum/202307/07/073724folzjnmlglpz3jvg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-070433.jpg</strong> (184.52 KB, 下载次数: 0)

下载附件

2023-7-7 07:37 上传

去噪生成实例:

<img src="https://img.saraba1st.com/forum/202307/07/073733xeacrfrrravzc1qs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-070523.jpg</strong> (187.1 KB, 下载次数: 0)

下载附件

2023-7-7 07:37 上传

<img src="https://img.saraba1st.com/forum/202307/07/073733l9w9299wm69o62ws.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230707-070530.jpg</strong> (153.59 KB, 下载次数: 0)

下载附件

2023-7-7 07:37 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 661#       发表于 2023-7-8 03:27

 本帖最后由 Machinery 于 2023-7-8 03:29 编辑 

CodeGen2.5

小巧而强大的编程模型

博客项目说明:https://blog.salesforceairesearch.com/codegen25/

相关论文:https://arxiv.org/abs/2305.02309

模型权重下载:https://github.com/salesforce/CodeGen

github项目代码库:https://github.com/salesforce/CodeGen

CodeGen2.5的模型参数仅7B，作为对比的其他编程模型，比如CodeGen1-16B，CodeGen2-16B，StarCoder-15B等，虽然参数量小了一半，但实际性能却更加优秀

具有稳健的填充采样(robust infill sampling)能力，这意味着模型可以“读取”当前光标位置左右方向的大小文本增强生成效果，为个人计算机配置情况下的服务和本地部署优化了快速注意力(Flash attention )的快速采样(fast sampling)效率，模型授权许可使用Apache 2.0

<img src="https://img.saraba1st.com/forum/202307/08/032815k2i08u2bj108i8di.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-031402.jpg</strong> (171.52 KB, 下载次数: 0)

下载附件

2023-7-8 03:28 上传

n=200时的HumanEval通过率，仅包含7B参数的CodeGen2.5的性能优于早期模型的两倍以上，多语言代码模型(Multi-lingual Code Models)使用多种编程语言进行训练，单语言代码模型(Mono-lingual Code Models)则仅在Python上进行微调

<img src="https://img.saraba1st.com/forum/202307/08/032821jl0tlqjuiqee999g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-031839.jpg</strong> (36.98 KB, 下载次数: 0)

下载附件

2023-7-8 03:28 上传

n=40时的HumanEval单行填充通过率，该基准测试衡量了模型“填充中间”一段代码(“中间”已被遮蔽)的能力，为了产品化，CodeGen2.5引入了专门用于截断的哨兵令牌(specialized sentinel token)，并具有非常高的填充性能

<img src="https://img.saraba1st.com/forum/202307/08/032900kfzykpikpi2w6qza.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-032851.jpg</strong> (39.13 KB, 下载次数: 0)

下载附件

2023-7-8 03:29 上传

在指令调整模型上，n=200时的HumanEval通过率，指令调优模型意味着在特定的指令数据集上进行微调，以提高根据英文指令生成代码的能力

<img src="https://img.saraba1st.com/forum/202307/08/032844slvp7lr7wwrrl3t7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-032832.jpg</strong> (38.39 KB, 下载次数: 0)

下载附件

2023-7-8 03:28 上传

使用NVIDIA Triton支持的快速注意力，各种推理框架的采样延迟(以毫秒为单位)，上下文长度为2000个令牌，批量大小设置为2，不同数量的Token代表代码助手产品的实际设置，CodeGen2.5具有更低的延迟，可以有效改善用户体验

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 662#       发表于 2023-7-8 03:52

mPLUG-DocOwl

用于文档理解的模块化的多模态大语言模型

github项目代码库:https://github.com/X-PLUG/mPLUG-DocOwl

<img src="https://img.saraba1st.com/forum/202307/08/034942vzjvd4alapzgz8al.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-034710.jpg</strong> (268.99 KB, 下载次数: 0)

下载附件

2023-7-8 03:49 上传

文档理解是指从各种类型的数字文档，例如网页中自动提取、分析和理解信息，现有的多模型(Multi-model)大型语言模型(MLLM)，包括mPLUG-Owl，已经在浅显的无OCR(Optical character recognition)文本识别中展示了有前途的零样本能力，这表明它们拥有在无OCR文档理解方面的潜力

然而，如果没有对应的相关领域内训练，这些模型往往会忽略细粒度的OCR特征，例如复杂的表格或大文本块，但实际上这些内容对于无OCR的文档理解至关重要

在本文中，提出了基于mPLUG-Owl的mPLUG-DocOwl，用于无OCR文档理解，具体来说，首先构建一个包含广泛的视觉文本理解任务的指令调整数据集，然后通过统一的指令调整策略在纯语言、通用视觉和语言、以及文档指令调整数据集上联合训练模型，增强了无OCR的文档理解能力，还构建了一个无OCR的文档指令理解评估数据集LLMDoc，以更好地对比模型在指令遵循和文档理解方面的能力

实验结果表明，mPLUG-DocOwl模型优于现有的多模态模型，展示了其强大的文档理解能力，无需特定的微调，mPLUG-DocOwl就可以很好地使用在各种下游任务上

相关训练数据集与使用例:

<img src="https://img.saraba1st.com/forum/202307/08/035008sgkd002ky4fqpqzi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-034758.jpg</strong> (85.66 KB, 下载次数: 0)

下载附件

2023-7-8 03:50 上传

<img src="https://img.saraba1st.com/forum/202307/08/035003rm3imq61kigxizox.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-034729.jpg</strong> (318.48 KB, 下载次数: 0)

下载附件

2023-7-8 03:50 上传

基准对比评估成绩:

<img src="https://img.saraba1st.com/forum/202307/08/035038fya1w91x1co7ya8y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-034823.jpg</strong> (103.54 KB, 下载次数: 0)

下载附件

2023-7-8 03:50 上传

mPLUG-DocOwl与minigpt-4模型在LLMdoc上的效果对比

<img src="https://img.saraba1st.com/forum/202307/08/035122n9e9z15glu29ueey.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-034842.jpg</strong> (284.36 KB, 下载次数: 0)

下载附件

2023-7-8 03:51 上传

<img src="https://img.saraba1st.com/forum/202307/08/035122zky8did166k863m5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-034900.jpg</strong> (293.98 KB, 下载次数: 0)

下载附件

2023-7-8 03:51 上传

mPLUG-DocOwl的失败案例:

<img src="https://img.saraba1st.com/forum/202307/08/035201luu9c838t3tuur4c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-034915.jpg</strong> (186.3 KB, 下载次数: 0)

下载附件

2023-7-8 03:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 663#       发表于 2023-7-8 04:25

LONGLLAMA

聚焦(Focused)的Transformer，上下文拓展的比对(Contrastive)训练

相关论文:https://arxiv.org/abs/2307.03170

注:We release the checkpoints and the source code of LONGLLAMA , see also our colab.

大型语言模型具有以上下文方式合并新信息的卓越能力，然而由于有效上下文长度的限制，这种方法的全部潜力常常受到限制

这个问题的其中一种解决方案是赋予注意力层访问外部记忆的权限，外部记忆由键值对(key-value pairs)组成，然而随着文档数量的增加，相关键对比不相关键的比例会下降，导致模型更加关注不相关的键

因此这里揭示了一个重大挑战，称之为分心问题(distraction issue)，其中与不同语义值相关的键可能会重叠，从而使它们难以区分

为了解决这个问题，引入了Focused Transformer(FoT)，这是一种采用受比对学习启发的训练过程的技术， 这种新颖的方法增强了(键与值)空间的结构，从而能够扩展上下文长度

本文方法允许对预先存在的大型模型进行微调，以延长其有效上下文，对3B和7B的OpenLLaMA检查点权重的微调证明了这一点

由此产生的模型，命名为LongLLaMA，在需要长上下文的任务中表现出了进步，LongLLaMA模型可以熟练地管理256k上下文长度内容以进行密钥检索(passkey retrieval)

<img src="https://img.saraba1st.com/forum/202307/08/042338dfsysb445dppfs2x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-040814.jpg</strong> (42.13 KB, 下载次数: 0)

下载附件

2023-7-8 04:23 上传

与原始OpenLLaMA模型相比，LONGLLAMA-3B在密钥检索方面的准确性，本文方法超出了训练时的长度，在100k的上下文长度下实现了94.5%的准确率，在256k个Token下实现了73%的准确率，而基线模型则无法处理比其训练长度(2k)更长的上下文

<img src="https://img.saraba1st.com/forum/202307/08/042331p306m2nvww81wj3d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-041031.jpg</strong> (140.9 KB, 下载次数: 0)

下载附件

2023-7-8 04:23 上传

聚焦Transformer概述，在推理过程中，记忆注意力层(绿色)通过kNN查找使用键值对的外部记忆，这有效地扩展了其上下文长度，该层使用crossbatch进行训练，简而言之，来自当前上下文Ccurr的Tokens以可微分的方式(Att+∇)参与同一文档的先前上下文Cprev，也参与其他文档的d−1个上下文，后者作为“反面”例子，旨在更好地塑造(键与值)空间

<img src="https://img.saraba1st.com/forum/202307/08/042346t2gra9aaof96ag9p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-041601.jpg</strong> (39.21 KB, 下载次数: 0)

下载附件

2023-7-8 04:23 上传

分心问题，使用不同参数d值训练的FOT与标准Transformer基线进行比较，在评估过程中，两个模型都会看到先前的局部上下文以及所选层中其他文档的一些上下文(如在跨批次训练过程中)，对于文档δ，测量p上注意力质量的分布
比例图x:模型可以看到的文档中的上下文数量
比例图y:当前文档的先前局部上下文的平均注意力质量

相关评估成绩:

<img src="https://img.saraba1st.com/forum/202307/08/042406nm2aipzcrrcxene7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-042136.jpg</strong> (105.96 KB, 下载次数: 0)

下载附件

2023-7-8 04:24 上传

<img src="https://img.saraba1st.com/forum/202307/08/042406ov44bwmd4dddnd7k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-042146.jpg</strong> (92.81 KB, 下载次数: 0)

下载附件

2023-7-8 04:24 上传

<img src="https://img.saraba1st.com/forum/202307/08/042406n0lpaopxp03zoll7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-042211.jpg</strong> (125.63 KB, 下载次数: 0)

下载附件

2023-7-8 04:24 上传

<img src="https://img.saraba1st.com/forum/202307/08/042406rbig3cbqqx33bicv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-042221.jpg</strong> (125.54 KB, 下载次数: 0)

下载附件

2023-7-8 04:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 664#       发表于 2023-7-8 05:31

reframe thoughts

训练模型来生成、识别和重构无益的想法

github项目地址:https://github.com/facebookresearch/ParlAI/tree/main/projects/reframe_thoughts

当前有许多关于良好的心理健康(well-being)的认知方法，例如识别和重构无益的想法，在过去几十年中获得了相当多的实证支持，但缺少以自助的形式进行真正广泛的使用

其中的一个障碍是缺乏足够具体和多样化的专业实践资料，这项工作检查是否可以利用当前的语言模型来生成几乎无限数量的实践资料，说明与特定给定上下文相匹配的标准的无益思维模式，并生成合适的积极重构建议

本文提出了PATTERNRE FRAME，这是一个包含约10000个想法示例的新颖数据集，其中包含以给定角色为条件的无用思维模式，并伴有约27000个积极的重构想法建议

通过使用该数据集来训练或评估当前模型，表明现有模型已经可以成为强大的工具，可以帮助生成大量定制的实践资料和假设，而无需或只需要很少的额外模型训练

<img src="https://img.saraba1st.com/forum/202307/08/053059h22346y32t2t3dlg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-052349.jpg</strong> (447.58 KB, 下载次数: 0)

下载附件

2023-7-8 05:30 上传

来自PATTERNRE FRAME数据集的无益想法及其重构版本的示例，思维模式的定义源自wiki

<img src="https://img.saraba1st.com/forum/202307/08/053130nzj3ul4cy4ks9ys9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-052447.jpg</strong> (131.09 KB, 下载次数: 0)

下载附件

2023-7-8 05:31 上传

数据集中不同模式的无用想法分布的混淆矩阵，行代表用于收集第一个任务中无用想法的模式，列代表注释者在第二个任务中选择的模式，正如预期的那样，一些相关模式，例如低估积极因素(DP)和心理过滤(MF)，表现出很强的跨标签关联

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 665#       发表于 2023-7-8 05:44

 本帖最后由 Machinery 于 2023-7-8 05:47 编辑 

MMPreTrain

MMPreTrain是一个基于PyTorch的开源预训练工具箱

github项目地址:https://github.com/open-mmlab/mmpretrain

中文说明页:https://github.com/open-mmlab/mmpretrain/blob/main/README_zh-CN.md

支持BLIP-2、LLaVA、MiniGPT4等多模态算法和COCOCaption、VQA、VizWiz、ScienceQA等数据集，也支持先进的主干应用，如EVA02、DINOv2、ViTSAM、InternImage、SparK等

<img src="https://img.saraba1st.com/forum/202307/08/054522nhewvmm414r441m6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-054507.jpg</strong> (77.8 KB, 下载次数: 0)

下载附件

2023-7-8 05:45 上传

<img src="https://img.saraba1st.com/forum/202307/08/054522x9qdttzd6rppe5t6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-054454.jpg</strong> (108.83 KB, 下载次数: 0)

下载附件

2023-7-8 05:45 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 666#       发表于 2023-7-8 05:57

LLongMA

LLongMA是一系列OpenLLaMA微调模型，使用线性位置插值缩放在8k上下文长度上进行训练(from:https://twitter.com/EnricoShippole/status/1677346578720256000?s=19)

LLongMA-7B权重下载:https://huggingface.co/conceptofmind/LLongMA-7b

LLongMA-3B权重下载:https://huggingface.co/conceptofmind/LLongMA-3b

<img src="https://img.saraba1st.com/forum/202307/08/055724esikrzzskicoa1ac.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-055445__01.jpg</strong> (179.58 KB, 下载次数: 0)

下载附件

2023-7-8 05:57 上传

<img src="https://img.saraba1st.com/forum/202307/08/055724gwmmh0nyhy02hj91.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-055458__01.jpg</strong> (183.16 KB, 下载次数: 0)

下载附件

2023-7-8 05:57 上传

<img src="https://img.saraba1st.com/forum/202307/08/055724esbtyteu4i4ia4sm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230708-055513.jpg</strong> (243.87 KB, 下载次数: 0)

下载附件

2023-7-8 05:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 667#       发表于 2023-7-9 03:08

InternLM-7B

InternLM的7B开源版本

hugface项目仓库:https://github.com/InternLM/InternLM

简介如下图所示，或者查看中文说明(地址:https://github.com/InternLM/InternLM/blob/main/README-zh-Hans.md)

internlm/intern-7b权限:https://huggingface.co/internlm/internlm-7b

intern-chat-7b chat版本权重:https://huggingface.co/internlm/internlm-chat-7b

internlm/intern-chat-7b 8k上下文版本权重:https://huggingface.co/internlm/internlm-chat-7b-8k

<img src="https://img.saraba1st.com/forum/202307/09/030820tyjjakyqrwytt955.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230709-030353.jpg</strong> (302.46 KB, 下载次数: 0)

下载附件

2023-7-9 03:08 上传

<img src="https://img.saraba1st.com/forum/202307/09/030820ptye11y1mc1c11c0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230709-030413.jpg</strong> (300.56 KB, 下载次数: 0)

下载附件

2023-7-9 03:08 上传

<img src="https://img.saraba1st.com/forum/202307/09/030820fo6ezexxeyxz7q11.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230709-030421.jpg</strong> (232.89 KB, 下载次数: 0)

下载附件

2023-7-9 03:08 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 668#       发表于 2023-7-9 04:19

 本帖最后由 Machinery 于 2023-7-9 04:21 编辑 

T-MARS

通过规避学习文本特征来改进视觉表征

项目主页:https://tmars-clip.github.io/

github项目代码仓库:https://github.com/locuslab/t-mars

来自网络的大型多模态数据集为学习通用视觉表征的一系列新方法提供了动力，推动了计算机视觉的最新技术发展，并彻底改变了零样本和少样本识别

其中面临的一个关键抉择是如何管理这些越来越大的数据集，例如LAION-5B数据集的创建者选择仅保留CLIP相似度分数超过指定阈值的图像标题对

<img src="https://img.saraba1st.com/forum/202307/09/041838jedhmaa22ee0ydqe.jpg" referrerpolicy="no-referrer">

<strong>20230709_041710.jpg</strong> (651.59 KB, 下载次数: 0)

下载附件

2023-7-9 04:18 上传

在本文中提出了一种新颖的SOTA数据过滤方法，其动机是研究组观察到近40%的LAION图像包含与标题显著重叠的文本，直观地说，这些数据可能是浪费的，因为这会激励模型进行OCR识别而不是学习视觉特征，然而，随意的删除所有的此类数据也不太行，因为会丢弃包含视觉特征的图像(除了重叠的文本)

T-MARS(文本遮蔽和重新评分/Text Masking and Re-Scoring)仅过滤掉那些，文本在其余视觉特征中占主导地位的图文对，首先遮蔽掉图像中的文本，然后过滤掉那些具有较低CLIP相似度得分的文本对的掩码图像

实际实验中，T-MARS在DataComp(数据过滤基准)的“中等规模”上优于排名靠前的方法，在ImageNet上优于排名第一的方法6.5%，在VTAB上优于排名第一的方法4.7%。 此外，通过对从2M到64M的各种数据池大小的系统评估表明，随着数据和计算呈指数级拓展，T-MARS的准确性增益会呈线性增加

<img src="https://img.saraba1st.com/forum/202307/09/041930q27833inh4q2x5ih.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230709-041026.jpg</strong> (146.55 KB, 下载次数: 0)

下载附件

2023-7-9 04:19 上传

<img src="https://img.saraba1st.com/forum/202307/09/041930ai9kjl5j4z5r57dc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230709-041231.jpg</strong> (225.11 KB, 下载次数: 0)

下载附件

2023-7-9 04:19 上传

<img src="https://img.saraba1st.com/forum/202307/09/041930ytz1i7818b0iiztm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230709-041246.jpg</strong> (179.21 KB, 下载次数: 0)

下载附件

2023-7-9 04:19 上传

<img src="https://img.saraba1st.com/forum/202307/09/041930agbzxtwctg66h3wc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230709-041431.jpg</strong> (484.78 KB, 下载次数: 0)

下载附件

2023-7-9 04:19 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 669#       发表于 2023-7-10 01:13

OpenLLaMA 7Bv2/Otter-MPT7B Image

OpenLLaMA 7Bv2模型，该模型在Falcon的精细化Web 数据集、 starcoder数据集、wikipedia、arxiv以及RedPajama的books和stackexchange上进行训练，v2版本修复了前代存在的分词与代码生成方面的问题，3Bv2版本模型也即将推出

<img src="https://img.saraba1st.com/forum/202307/10/011336v3ooc8e548pm3fcf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-011314.jpg</strong> (110.69 KB, 下载次数: 0)

下载附件

2023-7-10 01:13 上传

github项目仓库:https://github.com/openlm-research/open_llama

hugface权重下载:https://huggingface.co/openlm-research/open_llama_7b_v2

Otter-MPT7B Image模型通过合并OpenFlamingv2权重来更新模型，并对其进行专门调整以使其提升长答案和短答案的生成能力

github项目仓库:https://github.com/Luodian/Otter

hugface权重下载:https://huggingface.co/luodian/OTTER-Image-MPT7B

新版Demo演示:https://otter.cliangyu.com/

<img src="https://img.saraba1st.com/forum/202307/10/011347g998lulz8oeuaudb.jpg" referrerpolicy="no-referrer">

<strong>20230710_011250.jpg</strong> (146.36 KB, 下载次数: 0)

下载附件

2023-7-10 01:13 上传

<img src="https://img.saraba1st.com/forum/202307/10/011347dxy336747greyyx6.jpg" referrerpolicy="no-referrer">

<strong>20230710_011252.jpg</strong> (143.34 KB, 下载次数: 0)

下载附件

2023-7-10 01:13 上传

<img src="https://img.saraba1st.com/forum/202307/10/011347vzyuili14hyfei0i.jpg" referrerpolicy="no-referrer">

<strong>20230710_011254.jpg</strong> (190.69 KB, 下载次数: 0)

下载附件

2023-7-10 01:13 上传

<img src="https://img.saraba1st.com/forum/202307/10/011347c4o88ixciybobihn.jpg" referrerpolicy="no-referrer">

<strong>20230710_011257.jpg</strong> (143.82 KB, 下载次数: 0)

下载附件

2023-7-10 01:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 670#       发表于 2023-7-10 02:16

 本帖最后由 Machinery 于 2023-7-10 02:19 编辑 

knowno

可以寻求帮助的机器人，大型语言模型规划者的不确定性调整

项目主页:https://robot-help.github.io/

github项目仓库(待整理):https://github.com/google-research/google-research/tree/master/knowno

大型语言模型(LLM)展现出广泛的有前景的能力，比如从逐步规划到常识推理，这可为机器人提供实用性先验，但仍然容易产生自信的幻觉预测

在这项工作中提出了KnowNo，这是一个用于衡量和调整基于LLM的规划者的不确定性的框架，以便他们知道何时不知道并在需要时寻求帮助

<img src="https://img.saraba1st.com/forum/202307/10/021537ogrbgn8xa6txttbz.jpg" referrerpolicy="no-referrer">

<strong>20230710_014525.jpg</strong> (212.93 KB, 下载次数: 0)

下载附件

2023-7-10 02:15 上传

KnowNo建立在共形预测(conformal prediction)理论的基础上，为任务完成提供统计学保证，同时最大限度地减少复杂的多步骤规划设置中的人工帮助

其中涉及具有不同模糊模式的任务(例如从空间不确定性到数字不确定性，从人类偏好到威诺格拉德(Winograd Schema)模式)的各种模拟和真实机器人设置的实验表明，KnowNo的表现优于现代基线(这些基线可能集成使用了广泛的提示调整)，在提高效率和自主性方面，同时提供正式的统计学保证

KnowNo可以与开箱即用的LLM一起使用，无需进行模型微调，并提出了一种有前途的轻量级方法来建模不确定性，该方法可以随着基础模型不断增长的功能进行补充和扩展

<img src="https://img.saraba1st.com/forum/202307/10/021605zn6fgin4txsciciy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-021213.jpg</strong> (208.82 KB, 下载次数: 0)

下载附件

2023-7-10 02:16 上传

在机器人能够获得基于LLM的规划器和人类帮助的设置中需要实现以下两个目标：校准置信度(机器人应该寻求足够的帮助以确保用户指定的任务成功达到统计保证水平)，最小化场外帮助(机器人应该通过缩小任务中可能存在的模糊性来最小化其寻求的帮助总量)，这些充分性和最小性条件统称为不确定性对齐

<img src="https://img.saraba1st.com/forum/202307/10/021618szpp3qwlqnnwwzlm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-021220.jpg</strong> (110.49 KB, 下载次数: 0)

下载附件

2023-7-10 02:16 上传

KnowNo以共形预测为基础，正式量化LLM不确定性并实现不确定性对齐，首先构建一个校准数据集，涵盖机器人遇到的各种场景，对于每种情况，KnowNo都会提示LLM生成合理的选项，然后要求其选择一个，类似于多项选择题回答(MCQA/Multiple Choice Question Answering)

<img src="https://img.saraba1st.com/forum/202307/10/021629zz0hh48f1j0nqfnj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-021239.jpg</strong> (95.94 KB, 下载次数: 0)

下载附件

2023-7-10 02:16 上传

KnowNo获得LLM预测A、B、C、D、E这五个选项的似然度，然后利用校准数据的真实选项的似然度进行校准，同时设定似然阈值，在给定新场景的测试时，预测集需要生成可能性高于阈值的选项，共形预测在其中提供统计保证，即真实选项以用户指定级别的概率包含在预测集中，共形预测同时具有在理论上保证生成最小的平均预测集结果的优点

<img src="https://img.saraba1st.com/forum/202307/10/021641wkv37bdtlxlzv8la.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-021246.jpg</strong> (147.32 KB, 下载次数: 0)

下载附件

2023-7-10 02:16 上传

KnowNo采用一种简单的人工干预设置，如果某个场景的预测集不是单一的(即包含多个最终选项)，KnowNo会认为机器人不确定正确的行动并触发人类的帮助，在人类如实提供帮助的情况下，KnowNo将覆盖保障从共形预测转变为任务完成保障，由于共形预测提供的预测集较小，因此人类需要提供帮助微乎其微，这两个属性实现了校准置信度和最小化帮助的目标，KnowNo 还可以应用于多步骤规划设置(每个步骤可能有多个可接受的选项)，并在实际工作中衍生出新的其他扩展方法

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 671#       发表于 2023-7-10 03:06

 本帖最后由 Machinery 于 2023-7-10 03:07 编辑 

Whisper-AT

抗噪声的自动语音识别模型也是强大的通用音频事件标记模型

github项目仓库:https://github.com/yuangongnd/whisper-at

演示Demo:https://c916b4a5ed75eac733.gradio.live/

也可用于本地notebook的colab文件:https://github.com/YuanGongND/whisper-at/blob/main/sample/whisper_at_demo.ipynb

527种分类的AudioSet音频事件类型标签表:https://github.com/YuanGongND/whisper-at/blob/main/audioset_label.csv

在本文中，重点关注了openai的Whisper模型的背景声音识别，whisper是一种最新的自动语音识别模型，使用在不同条件下记录的约68万小时的标注的语音语料数据集进行训练而获得的

首先展示了一个有趣的发现，虽然whisper对于现实世界的背景声音(例如背景音乐等)非常稳健，但它的音频表征实际上不是噪声不变的，而是与非语音高度相关，这表明Whisper理论上可以识别以噪声类型为条件的语音

有了这一发现，通过冻结Whisper的主干并在其之上训练轻量级的音频标记模型可以构建统一的音频标记和语音识别模型Whisper-AT

Whisper-AT的额外计算花费成本不到1%，除了语音文本之外，还可以在单​​次前向传递中识别音频事件并生成相关标记

模型构架:

<img src="https://img.saraba1st.com/forum/202307/10/030547if232kk3lj3pjk2t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-025958.jpg</strong> (116.77 KB, 下载次数: 0)

下载附件

2023-7-10 03:05 上传

Whisper-AT是一种联合的音频标记和语音识别模型，它继承了OpenAI Whisper强大的语音识别能力，其ASR性能与原始Whisper完全相同，API接口和使用方式也与原版OpenAI Whisper相同，因此用户可以从原版Whisper无缝切换到Whisper-AT，在内部，Whisper-AT冻结所有原始Whisper参数，并在Whisper编码器表征之上训练时间和分层Transformer，以执行音频标记任务

<img src="https://img.saraba1st.com/forum/202307/10/030615s1ez3i99imf3zt7j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-030036.jpg</strong> (194.6 KB, 下载次数: 0)

下载附件

2023-7-10 03:06 上传

令人惊讶的是，ASR模型的噪声鲁棒性与其中间表征中编码的一般背景声音(ASR 噪声)信息量呈正相关，在上图的上半部分中，当语音(Librispeech)受到ESC-50中越来越多的背景声音的污染时，Whisper的鲁棒性明显增强(单词错误率增加较小)

在上图的下半部分中，展示了Whisper的中间表征在相同的ESC-50数据上获得了最佳的线性探测声音分类精度，这表明Whisper编码了大多数背景声音信息，与其他模型不同的是，Whisper甚至在最深层也对背景声音信息进行编码，其中PR=自监督预训练； FT=PR和微调模型

相关评估结果:

<img src="https://img.saraba1st.com/forum/202307/10/030643a0717jfj0j00ez6l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-030459.jpg</strong> (202.31 KB, 下载次数: 0)

下载附件

2023-7-10 03:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 672#       发表于 2023-7-10 05:41

CMMLU

测量中文的大规模多任务语言理解

github项目仓库:https://github.com/haonan-li/CMMLU

随着大型语言模型(LLM)的能力不断进步，评估其性能变得越来越重要和具有挑战性，本文旨在通过引入CMMLU来弥补这一差距

CMMLU是一个涵盖自然科学、社会科学、工程和人文学科等多个学科的综合性中文基准

通过对18款面向多语言和中文的高级LLM进行了全面评估，评估他们在不同学科和环境中的表现，结果显示，大多数现有的LLM很难达到50%的平均准确率，即使提供了上下文示例和思维链提示，而随机基线则为25%

这凸显了LLM的巨大改进空间，此外，还通过大量实验来确定了影响模型性能的因素，并提出增强LLM的方向，CMMLU填补了评估中文环境下大型语言模型的知识和推理能力的部分空白

<img src="https://img.saraba1st.com/forum/202307/10/053745a0px3n3gyfz3x1o6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-053730.jpg</strong> (725.31 KB, 下载次数: 0)

下载附件

2023-7-10 05:37 上传

<img src="https://img.saraba1st.com/forum/202307/10/053745v8tc22l722d24bgt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-053707.jpg</strong> (329.53 KB, 下载次数: 0)

下载附件

2023-7-10 05:37 上传

数据集相关数据分布与论文原始榜单数据:

<img src="https://img.saraba1st.com/forum/202307/10/054005j6jwtbdtbb4p4paj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-053822.jpg</strong> (66.9 KB, 下载次数: 0)

下载附件

2023-7-10 05:40 上传

<img src="https://img.saraba1st.com/forum/202307/10/054005nnzh0vxmxdnhz5n0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-053855.jpg</strong> (245.48 KB, 下载次数: 0)

下载附件

2023-7-10 05:40 上传

<img src="https://img.saraba1st.com/forum/202307/10/054005w3q87q7c8cgw0ssw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-053927.jpg</strong> (179.75 KB, 下载次数: 0)

下载附件

2023-7-10 05:40 上传

<img src="https://img.saraba1st.com/forum/202307/10/054054ts22ahysys272hsf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230710-054043.jpg</strong> (210.43 KB, 下载次数: 0)

下载附件

2023-7-10 05:40 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 673#       发表于 2023-7-11 03:31

teaching arithmetic

教授小型Transformer模型计算

github项目代码库:https://github.com/lee-ny/teaching_arithmetic

(注:该代码库基于NanoGPT项目代码实现，研究组对代码库进行了一些修改以支持进行实验)

当对大量文本数据进行训练时，像GPT-4这样的大型语言模型在通用任务(例如基本算术)上表现出新兴的能力，即使这些任务实际上并没有使用无监督的下一个Token预测目标进行明确的编码

本文研究了通过随机初始化训练的小型Transformer使用下一个标记预测目标的建模方式有效地进行算术运算学习，例如加法、乘法和平方根等基本函数的准确运算

首先实验证明传统的训练数据对于算数学习并不是最有效的，简单的训练样本的格式更改可以非常显著的提高准确率，随着训练数据量的增加，模型的性能会出现急剧的阶段性提升，在某些情况下，这种阶段性的性能提升可以通过深度学习模型与低秩矩阵完成问题之间的关联来解释

然后，在之前的工作的基础上，对包括中间步骤结果的思想链(COT)数据进行训练，即使完全没有预训练，这种方法也能同时显着提高准确性、样本复杂性和收敛速度

同时还研究了训练过程中算数数据和文本数据之间的相互作用，并检查了小样本提示、预训练和模型规模的效果，此外，我们还讨论了长度泛化挑战

本文工作的结果强调了高质量、有指导性的数据的重要性，这些数据同时考虑了下一个单词预测目标建模的特定特征，可以让模型可以快速获得算数能力

<img src="https://img.saraba1st.com/forum/202307/11/033055md7y1odt8hhxox8t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-032451.jpg</strong> (73.6 KB, 下载次数: 0)

下载附件

2023-7-11 03:30 上传

<img src="https://img.saraba1st.com/forum/202307/11/033055cgxcd097bxnh6x7n.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-032732.jpg</strong> (177.94 KB, 下载次数: 0)

下载附件

2023-7-11 03:30 上传

本文研究的四种数据格式化方法：
1.普通:标准加法格式化
2.反转:反转输出
3.简化暂存器:按数字记录总和和进位
4.详细暂存器：提供详细的加法中间步骤

<img src="https://img.saraba1st.com/forum/202307/11/033103c8k43ffbyyzzeo99.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-032805.jpg</strong> (269.86 KB, 下载次数: 0)

下载附件

2023-7-11 03:31 上传

通过使用这些不同的格式化方法转换的数据从头开始训练小型Transformer模型以进行加法，结果如右图所示，展示了数据格式化在性能和样本效率方面的关键作用

简单的Plain方法永远不会达到100%的准确度，并且随着我们提高数据格式的详细程度，其余学习加法的方法的样本复杂性会稳步降低

<img src="https://img.saraba1st.com/forum/202307/11/033114at9yso0yha1ad2d1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-032900.jpg</strong> (30.47 KB, 下载次数: 0)

下载附件

2023-7-11 03:31 上传

减法任务的各种数据格式化方法(普通、反向和两个版本的详细暂存器(DS))之间的性能比较，实验是在NanoGPT模型上进行的，该模型在10000个示例的数据集上进行训练，版本2包含操作数比较，与版本1相比，其性能显着降低，这一观察结果强调了中间步骤的构建对模型性能的重大影响

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 674#       发表于 2023-7-11 04:01

GPT4RoI

在感兴趣的区域(Region-of-Interest)上对大型语言模型进行指令调整

github项目代码库:https://github.com/jshilong/GPT4RoI

在图像-文本对上调整大语言模型(LLM)的指令已经实现了前所未有的视觉-语言多模态能力，然而这些的视觉语言对齐仅建立在图像级别上，缺乏图像区域级对齐限制了这些方法在细粒度多模态理解方面的进步

<img src="https://img.saraba1st.com/forum/202307/11/035814xdk9qd0oppw1j797.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-035043.jpg</strong> (233.72 KB, 下载次数: 0)

下载附件

2023-7-11 03:58 上传

在本文中提出了对感兴趣区域进行指令调整的方法，其中的关键设计是将边界框重新表述为空间指令的格式，将空间指令和语言嵌入提取的视觉特征的交错序列输入到LLM，并以指令调整格式对转换后的区域文本数据进行训练

本文提出的区域级视觉语言模型(称为GPT4RoI)带来了超越图像级理解的全新对话和交互体验，其中包括:
1.可控性：用户可以通过语言和空间指令与模型进行交互，以灵活调整问题的细节程度
2.容量：GPT4RoI模型不仅支持单独的区域空间指令，还支持多区域，这解锁了更多区域级多模态能力，例如详细的区域标题和复杂的区域推理
 3.组合：任何现成的物体检测器都可以作为空间指令提供者，以便从GPT4RoI模型中挖掘信息丰富的物体属性，如颜色、形状、材质、动作、与其他物体的关系等

<img src="https://img.saraba1st.com/forum/202307/11/035827fpjgywjhgdwh6n8h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-035212.jpg</strong> (105.57 KB, 下载次数: 0)

下载附件

2023-7-11 03:58 上传

GPT4RoI与其他多模态模型的对比

GPT4RoI是一种基于区域文本对的大语言模型(LLM)指令调整的视觉语言模型，它能够处理包含交错的语言序列和空间信息&lt;region&gt;的用户指令，实现了细粒度的多模态理解任务，例如生成区域标题或推理

<img src="https://img.saraba1st.com/forum/202307/11/035939kabzpp7tz07bbbjd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-035227.jpg</strong> (50.8 KB, 下载次数: 0)

下载附件

2023-7-11 03:59 上传

图像-文本对的视觉指令调整和区域-文本对的空间指令调整的比较，每个对象的边界框和文本描述在区域文本数据集中提供，在训练过程中，空间指令来自注释，而在推理过程中，空间指令由用户输入给出

<img src="https://img.saraba1st.com/forum/202307/11/035959hpccoq5lh22sd88g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-035349.jpg</strong> (100.42 KB, 下载次数: 0)

下载附件

2023-7-11 03:59 上传

GPT4RoI是一种端到端视觉语言模型，用于处理包含空间信息的指令，例如&lt;region&gt;，在Token化和转换为嵌入期间，指令中&lt;region&gt;的嵌入被多级图像特征的RoIAlign结果替换，随后所有嵌入都可以发送到大型语言模型(LLM)进行进一步处理，类似于纯文本指令，其中还可以利用整个图像的特征来捕获全局信息，并为了简洁起见在图中省略了这部分

训练数据信息与具体实例:

<img src="https://img.saraba1st.com/forum/202307/11/040020mdqil0y5z7yixgiy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-035641.jpg</strong> (246.01 KB, 下载次数: 0)

下载附件

2023-7-11 04:00 上传

<img src="https://img.saraba1st.com/forum/202307/11/040020kxrxy443xiiqbx4y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-035658.jpg</strong> (147.62 KB, 下载次数: 0)

下载附件

2023-7-11 04:00 上传

<img src="https://img.saraba1st.com/forum/202307/11/040020u4jj7myvjjkvm4yh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-035720.jpg</strong> (360.31 KB, 下载次数: 0)

下载附件

2023-7-11 04:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 675#       发表于 2023-7-11 04:28

 本帖最后由 Machinery 于 2023-7-11 04:31 编辑 

MeetingQA

会议记录信息的提取后问答

项目主页:https://archiki.github.io/meetingqa.html

注:

<img src="https://img.saraba1st.com/forum/202307/11/042846uar4u5rgzkkrc3wr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-042823.jpg</strong> (31.6 KB, 下载次数: 0)

下载附件

2023-7-11 04:28 上传

<img src="https://img.saraba1st.com/forum/202307/11/042641jnlk2pfpfsulldqs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-041043.jpg</strong> (133.82 KB, 下载次数: 0)

下载附件

2023-7-11 04:26 上传

随着在线会议平台和强大的自动语音识别系统的普遍使用，会议记录已成为自然语言任务的一个新的有趣领域，最近有关会议记录的工作仅限于总结和提取行动项目，然而，会议讨论也有一个有用的问答(QA)组件，这对于理解讨论或会议内容至关重要，并且可用于在长记录的基础上构建交互式界面进行处理

因此，在这项工作中，引入了MeetingQA，这是一个提取QA数据集，其中包含会议参与者提出的问题和相应的响应

问题可以是开放式的寻求积极的讨论，而答案可以是多跨度的并分布在多个发言者中，研究组对几个强大的基线(包括长上下文语言模型和最近的指令调整模型)进行的全面实证研究表明，模型普遍在这项任务上表现不佳，并且严重落后于人类表现，本文实际提出了一种有用的、 社区需要改进的挑战性新任务

<img src="https://img.saraba1st.com/forum/202307/11/042650rftu1r44a1tay9f2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-041059.jpg</strong> (135.96 KB, 下载次数: 0)

下载附件

2023-7-11 04:26 上传

对来自AMI(增强多方交互)语料库的公开会议进行了标注，手动转录的会议时间约为100小时，为此招募了标注者来标记记录中的哪些句子回答了每个问题以及元数据，发现标注者之间的一致性很高，Krippendorff的α为0.73，以每次会议61美元的价格获得了166次会议的标注

问题类型：即使以“是/否”方式提出的问题也是寻求信息并引发详细答复，约50%的问题是寻求意见，约20%的问题是修辞性的

答案类型：30%的问题无法回答，40%的答案是多跨度(非连续句子)，48%涉及多个发言者，近70%的多发言者答案包含参与者之间某种程度的分歧

 长度分布：文字记录、问题和相应答案的平均长度分别为5.9K、12和35个单词

人类表现：在测试集中的250个问题上，F1=84.6

<img src="https://img.saraba1st.com/forum/202307/11/043033ocuuumm0d2uk27u1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-042948.jpg</strong> (174.58 KB, 下载次数: 0)

下载附件

2023-7-11 04:30 上传

比较了经过微调的单跨度模型的性能和人类在不同答案类型上的性能(最佳数字以粗体显示)，Intermediate Train Data表示使用的中间训练数据，缺少中间训练数据则表示直接进行微调，†由于参考字符串为空，因此无法回答的问题的所有分数都相同

<img src="https://img.saraba1st.com/forum/202307/11/043020bwgl261wuwzvw96q.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-042957.jpg</strong> (111.02 KB, 下载次数: 0)

下载附件

2023-7-11 04:30 上传

比较了不同评估指标和答案类型的微调的多跨度模型的性能

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 676#       发表于 2023-7-11 05:26

 本帖最后由 Machinery 于 2023-7-11 05:32 编辑 

规范库

社会情景的规范知识库

github项目仓库:https://github.com/SALT-NLP/normbank

<img src="https://img.saraba1st.com/forum/202307/11/052339cy2q2ztb42vy9p4v.jpg" referrerpolicy="no-referrer">

<strong>20230711_045952.jpg</strong> (256.53 KB, 下载次数: 0)

下载附件

2023-7-11 05:23 上传

NormBank，这是一个包含155k情景规范的知识库，该资源旨在为交互式、辅助和协作人工智能系统提供灵活的规范推理，与之前的常识性资源不同，NormBank的每个推论建立在多元社会文化框架内，其中包括环境(例如餐厅)、代理人的角色(服务员、顾客)、他们的属性(年龄、性别)以及其他身体、社会、和文化限制(例如，温度或运营国家/地区)

<img src="https://img.saraba1st.com/forum/202307/11/052409c4jgqffnmu4xfmq3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-050000.jpg</strong> (135.31 KB, 下载次数: 0)

下载附件

2023-7-11 05:24 上传

总的来说，NormBank包含引入并迭代完善的分类的63k个独特约束，然后以不同的组合应用约束来构建社会规范

在这些操纵下，规范是非单调的，人们可以通过稍微更新其框架来取消原来的推理，同时发现有证据表明神经模型可以帮助可靠地扩展NormBank的范围和覆盖范围，通过一系列转移实验进一步证明了该资源的实用性

<img src="https://img.saraba1st.com/forum/202307/11/052417j26vc6kxikji0pez.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-051757.jpg</strong> (73.6 KB, 下载次数: 0)

下载附件

2023-7-11 05:24 上传

NORMBANK有什么特别之处？ 规范以情境约束为基础——环境和个人属性，以及角色和其他行为，在此示例中，对于咖啡馆中的顾客来说，在其原型环境中喝咖啡是一项受到鼓励的活动，但对于在同一家咖啡馆中工作的咖啡师来说，或者对于儿童年龄的学生在教室中这样做来说，这是违反规范的 ，这些代表了NORMBANK中的一些非单调规范推理

用于约束NORMBANK的SCENE演剧化框架示例，餐厅设置由环境中的出席人数(不拥挤)和一天中的时间(夜间)指定，两种代理角色，客户和服务者； 后者由年龄段(成人)和性别(男性)属性指定，前者有饮酒、约会等行为

注：图形仅供参考，NORMBANK是一个文本数据集，不包含任何图像

<img src="https://img.saraba1st.com/forum/202307/11/052504kn8xitunfu8f8z1o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-052450.jpg</strong> (102.36 KB, 下载次数: 0)

下载附件

2023-7-11 05:25 上传

约束生成结果，左图为自动评估，实验表明BART比其他生成模型更具有优势，中间图的生成的约束属于SCENE分类法67.2-100%[Tax.  Constr.]并在30-64%的时间内使用预填充约束[Pre-pop.Constr.]，时间取决于解码策略，右图为人类评估，显示出令人鼓舞的结果，经过NORMBANK训练的BART可以生成合理的、正确的、规范的和相关的约束，用于自动扩展NORMBANK，此处，突出显示了最佳微调模型结果，以粗体显示

数据集分布与构建:

<img src="https://img.saraba1st.com/forum/202307/11/052608f3rt0024i4s2yz3y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-050410.jpg</strong> (123.11 KB, 下载次数: 0)

下载附件

2023-7-11 05:26 上传

<img src="https://img.saraba1st.com/forum/202307/11/052608r7gpyyyfgypqkgny.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230711-050422.jpg</strong> (117.72 KB, 下载次数: 0)

下载附件

2023-7-11 05:26 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 677#       发表于 2023-7-12 03:51

AnimateDiff

在无需进行特定微调的情况下为您的个性化文本到图像扩散模型生成图片制作连贯的动画

github项目地址:https://animatediff.github.io/

随着文本到图像模型(例如Stable Diffusion)和相应的个性化技术(例如DreamBooth和LoRA)的进步，大众得以在可承受的成本范围内将他们的想象力转化为高质量的图像，在这之后，对生成的图像进行动画化的技术需求很大，需要进一步将生成的静态图像与动态运动相结合

在本报告中，提出了一个实用的框架，可以一劳永逸地对大多数现有的个性化文本到图像模型进行动画化处理，从而节省模型特定调整的工作量

<img src="https://img.saraba1st.com/forum/202307/12/034947bfschb6slfyz569e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-034418.jpg</strong> (213.74 KB, 下载次数: 0)

下载附件

2023-7-12 03:49 上传

AnimateDiff的核心是将全新的初始化运动建模模块插入到冻结的文本到图像模型中，并在视频剪辑片段上对其进行训练，以提取合理的运动先验

只要经过一次预先的训练，这个简单地注入的运动建模模块，可以让所有源自相同基础文本到图像模型的个性化微调版本都可以轻松地变成文本驱动的动画生成模型，从而产生多样化和个性化的动画图像，而无需再次训练

对跨动漫图片和现实照片的几个具有代表性的个性化文本到图像模型进行了评估，证明了提出的框架可以帮助这些模型生成时间平滑的动画剪辑片段，同时保留其输出的风格和多样性

项目框架:

<img src="https://img.saraba1st.com/forum/202307/12/034901bwrqwp55dpp4wk44.jpg" referrerpolicy="no-referrer">

<strong>framework.jpg</strong> (458.88 KB, 下载次数: 0)

下载附件

2023-7-12 03:49 上传

AnimateDiff可以轻松作为插件插入由用户训练的个性化T2I模型中，例如直接从CivitAI或Huggingface等平台下载的图像模型，并生成具有适当运动效果的动画剪辑片段

效果演示(项目主页可查看示例的动态视频):

<img src="https://img.saraba1st.com/forum/202307/12/034919epo0hhqltyum2hr3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-034127.jpg</strong> (322.15 KB, 下载次数: 0)

下载附件

2023-7-12 03:49 上传

对比效果:

<img src="https://img.saraba1st.com/forum/202307/12/035044b4ik6dkyaw262idc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-034735.jpg</strong> (315.24 KB, 下载次数: 0)

下载附件

2023-7-12 03:50 上传

消融实验:

<img src="https://img.saraba1st.com/forum/202307/12/035057czrbl25vrwhw5xn3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-034806.jpg</strong> (264.72 KB, 下载次数: 0)

下载附件

2023-7-12 03:50 上传

失败案例:

<img src="https://img.saraba1st.com/forum/202307/12/035106g21zyinjpfuvdd2y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-034819.jpg</strong> (154.93 KB, 下载次数: 0)

下载附件

2023-7-12 03:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 678#       发表于 2023-7-12 04:32

VampNet

通过遮蔽(Mask)声学Token模型进行音乐生成

github项目地址库:https://github.com/hugofloresgarcia/vampnet

音频试听样本:https://tinyurl.com/bdfj7rdx

预训练模型:https://zenodo.org/record/8136545

本文引入了VampNet，一种用于音乐合成、压缩、重制和变化的遮蔽声学Token建模方法

模型在训练期间使用可变遮蔽计划，使之能够在模型推理过程中应用各种遮蔽方法(又称为遮蔽提示)从模型中采样连贯的音乐

VampNet构架是非自回归的，利用双向Transformer来处理前向传递中的所有Token，只需36次采样，VampNet就可以生成连贯的高保真音乐波形

通过以各种遮蔽的方式提示VampNet，可以将其应用于音乐压缩、修复、重制、延续和变化循环(vamping)等任务

在适当的提示下，VampNet能够保持音乐的风格、流派、乐器和其他高级语义方面，这种灵活的提示能力使VampNet能够成为强大的音乐辅助创作工具

<img src="https://img.saraba1st.com/forum/202307/12/043146na9kuu9ok46cckau.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-041350.jpg</strong> (119.88 KB, 下载次数: 0)

下载附件

2023-7-12 04:31 上传

VampNet概览图，首先使用音频标记器将音频转换为一系列离散Token，之后Token被遮蔽，然后传递到遮蔽生成模型，该模型通过两个级别的高效迭代并行解码采样生成过程来预测被遮蔽的Token的值，之后将结果解码回音频

<img src="https://img.saraba1st.com/forum/202307/12/043139nbtg0qvub6bmi6h6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-042151.jpg</strong> (141.14 KB, 下载次数: 0)

下载附件

2023-7-12 04:31 上传

训练、采样和提示VampNet

训练：使用遮蔽声学Token模型来训练VampNet，首先随机遮蔽一组输入声学Token的一部分，并学习使用可变遮蔽时间表来预测遮蔽的Token集合，粗粒度模型只被训练遮蔽粗略的Token，而从粗到精的训练则只会遮蔽细粒度的Token

采样：使用并行迭代解码从VampNet中采样新的声学标记序列，在每次迭代中采样最置信的预测Token的子集

提示：VampNet可以通过多种方式提示来生成音乐，例如，它可以进行定期提示设置，其中输入序列中的每个第P个时间步长都被取消遮蔽，或者以节拍驱动的方式提示，其中歌曲中节拍标记周围的时间步长被取消遮蔽

<img src="https://img.saraba1st.com/forum/202307/12/043155dca49q9grn90frfg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-042515.jpg</strong> (57.41 KB, 下载次数: 0)

下载附件

2023-7-12 04:31 上传

使用不同数量的采样步骤获取的VampNet样本的梅尔重建误差(上方)和Fréchet音频距离(FAD，下方)，使用P=16的周期性遮蔽提示获取，样本是通过解压缩令牌生成的，以极低的比特率(50 bps)有效地生成了输入音乐的变化

<img src="https://img.saraba1st.com/forum/202307/12/043203hcm9h3mhvhxsz53x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-042525.jpg</strong> (68.74 KB, 下载次数: 0)

下载附件

2023-7-12 04:32 上传

图 4. 使用不同类型的提示获取的VampNet 10s样本的多尺度梅尔频谱图误差(上方)和Fréchet音频距离(FAD，下方)

<img src="https://img.saraba1st.com/forum/202307/12/043225t04nz94ns091xwkn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-043217.jpg</strong> (91.94 KB, 下载次数: 0)

下载附件

2023-7-12 04:32 上传

不同比特率下VampNet样本的梅尔频谱图误差(上方)和Fréchet音频距离(FAD，下方)，根据噪声比例r用随机Token替换输入序列中的Token来提供基线对比

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 679#       发表于 2023-7-12 04:52

PCFG

大型语言模型可以作为通用的模式生成机器

项目主页:https://general-pattern-machines.github.io/

项目代码与colab:coming soon

预训练的大语言模型(LLM)能够以自回归的方式完成复杂的标记序列生成，从概率上下文无关语法(PCFG/probabilistic context-free grammars)程序生成任意标记序列，到抽象推理语料库(ARC/Abstract Reasoning Corpus)中发现的更丰富的空间模式

令人惊讶的是，即使使用从词汇表中随机采样的标记来表达序列，也可以部分保留模式完成能力，这些结果表明，无需任何额外的培训，LLM就可以在上下文学习的驱动下充当通用序列建模者

在这项工作中，研究了如何将这些零样本能力应用于机器人技术中的问题，从推断代表随时间变化的状态的数字序列来完成简单的运动，到从最小到最大的奖励条件轨迹的提示，这些轨迹可以发现并表征闭环策略(例如CartPole的稳定控制器)

虽然由于延迟、上下文大小限制和计算成本，目前很难在实际系统中部署，但使用LLM驱动低级机器控制的方法可能会令人兴奋地了解单词之间的模式如何转换为具体的操作

<img src="https://img.saraba1st.com/forum/202307/12/045155wph37qs9se3s9qsc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-044218.jpg</strong> (107.17 KB, 下载次数: 0)

下载附件

2023-7-12 04:51 上传

开箱即用的LLM可以完成(图中以突出显示)以任意Token表达复杂的ARC模式

开箱即用的预训练LLM可以作为通用模式机的基本版本，识别和完成数字或任意符号的标记序列，表征机器人技术和顺序决策中的抽象问题

<img src="https://img.saraba1st.com/forum/202307/12/045202c3crecr3efporecr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-044146.jpg</strong> (169.79 KB, 下载次数: 0)

下载附件

2023-7-12 04:52 上传

实验表明，在某种程度上，LLM可以在上下文中学习
1.序列变换(例如，对符号的空间重新排列进行推理，用于动态建模和下采样图像的下一状态预测)
2.完成简单功能(例如推断运动感觉演示)
3.元模式以改进返回条件策略(例如发现振荡行为以稳定CartPole)

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 680#       发表于 2023-7-12 05:27

 本帖最后由 Machinery 于 2023-7-12 05:29 编辑 

Semantic-SAM

以任何粒度(Granularity)分割和识别任何事物

github项目代码库:https://github.com/UX-Decoder/Semantic-SAM

Demo演示:http://semantic-sam.xyzou.net:6080/

Demo演示2:http://semantic-sam.xyzou.net:6081/

Semantic-SAM是一种通用图像分割模型，可以以任何所需的粒度分割和识别任何内容

本文的模型提供了两个关键优势：语义意识和粒度丰富，为了实现语义感知，研究组跨越三个不同的粒度整合多个数据集，并引入对象和部分的解耦分类，这使Semantic-SAM能够捕获丰富的语义信息

对于多粒度能力，在训练期间提出了一种多选择学习方案，使每次点击都能生成与多个真实掩码相对应的多个级别的掩码，值得注意的是，这项工作代表了在SA-1B、通用和部分分割数据集上联合训练模型的首次尝试

实验结果和可视化表明模型成功实现了语义感知和粒度丰富，此外将SA-1B训练与其他分割任务(例如全景和部分分割)相结合，可以进一步提高性能

<img src="https://img.saraba1st.com/forum/202307/12/052506n129jpp22mmae85m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-050758__01.jpg</strong> (791.85 KB, 下载次数: 0)

下载附件

2023-7-12 05:25 上传

Semantic-SAM能够处理各种分割任务，包括开放集和交互式分割，进一步进行实例分割、语义分割、全景分割和部分分割，同时能够输出不同粒度的多级语义，最左边图像上的红点是点击，将模型与修复模型连接起来以执行多级修复，提示分别是“蜘蛛侠”和“宝马汽车”，请注意，只需单击一次即可分别生成b和c中的结果

<img src="https://img.saraba1st.com/forum/202307/12/052516zo2kcddzmg4sh4n2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-051452.jpg</strong> (111.24 KB, 下载次数: 0)

下载附件

2023-7-12 05:25 上传

Semantic-SAM是一个通用分割框架，可以采用多种类型的分割数据，包括通用分割数据、部分分割数据和与类无关的分割数据，视觉编码器用于提取图像特征，掩码解码器可以进行通用分割和具有各种类型提示的提示分割

对于点和框，通过锚定框将它们输入到掩码解码器。 由于点输入的粒度存在模糊性，因此模型将每个点复制6次，并赋予它们不同级别的嵌入，点提示的输出掩码与多个不同粒度的真实掩码相匹配

<img src="https://img.saraba1st.com/forum/202307/12/052553mlk7sttvz7oqkklt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-051721.jpg</strong> (73.49 KB, 下载次数: 0)

下载附件

2023-7-12 05:25 上传

解耦目标整体与部分

在Semantic-SAM中构建训练目标的loss，四个loss分别是部分分类、对象分类、框loss和掩码loss，最后一列表示匹配中的真实掩码的数量

<img src="https://img.saraba1st.com/forum/202307/12/052641gbz4z0bbvmy02ay3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-052058.jpg</strong> (48.68 KB, 下载次数: 0)

下载附件

2023-7-12 05:26 上传

交互式学习策略比较
A.一对一：专注于对象级别的传统交互式分割模型，即 SEEM
B.多对一：单粒度的多选学习，即SAM
C.多对多：本文模型，强制模型预测一次点击的所有可能的粒度，以实现更可控的细分，因此输出粒度更加丰富，可以生成不同的输出掩码

演示实例:

<img src="https://img.saraba1st.com/forum/202307/12/052659sezseu3iercwbxgv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-052319.jpg</strong> (428.32 KB, 下载次数: 0)

下载附件

2023-7-12 05:26 上传

<img src="https://img.saraba1st.com/forum/202307/12/052659tggkd1a1h1vpqasz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-052334.jpg</strong> (193.54 KB, 下载次数: 0)

下载附件

2023-7-12 05:26 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 681#       发表于 2023-7-12 05:39

 本帖最后由 Machinery 于 2023-7-12 05:40 编辑 

SVIT

扩大视觉指令调整数据集

github项目仓库:https://github.com/BAAI-DCAI/Visual-Instruction-Tuning

由于基础模型的出现，大语言和视觉模型被集成起来，获得了视觉字幕、对话、问答等多模态能力，尽管现有的多模态模型在视觉理解和推理方面表现出了令人印象深刻的性能，但它们的局限性很大程度上由于缺乏高质量的指令调优数据，尚未得到充分探索

为了突破多模态能力的极限，构建了320万样本的视觉指令调整数据集来提高视觉指令调整能力(SVIT/Sale up Visual Instruction Tuning)，其中包括160万个对话问答(QA)对和160万个复杂推理QA对以及106K详细图像描述

除了数量之外，所提出的数据集还具有高质量和丰富的多样性的特点，这是通过使用丰富的图像手动提示GPT-4生成的，通过实证验证，在SVIT上训练多模态模型可以显着提高视觉感知、推理和规划方面的多模态性能

与其他数据集的对比:

<img src="https://img.saraba1st.com/forum/202307/12/053706q355phbr3j3dlj5r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-053533.jpg</strong> (60.02 KB, 下载次数: 0)

下载附件

2023-7-12 05:37 上传

样本实例(请注意图片并未提供给gpt4):

<img src="https://img.saraba1st.com/forum/202307/12/053718ujrwz74wnwcn1wdr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-053545.jpg</strong> (411.03 KB, 下载次数: 0)

下载附件

2023-7-12 05:37 上传

数据集分布:

<img src="https://img.saraba1st.com/forum/202307/12/053748amree1yspupmuyqj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-053612.jpg</strong> (158.24 KB, 下载次数: 0)

下载附件

2023-7-12 05:37 上传

存在问题的生成样本:

<img src="https://img.saraba1st.com/forum/202307/12/053842aqv8hsv8bberb2q8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-053620.jpg</strong> (184.06 KB, 下载次数: 0)

下载附件

2023-7-12 05:38 上传

与其他模型的对比:

<img src="https://img.saraba1st.com/forum/202307/12/053912grenjev1mrmvnvjv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-053631.jpg</strong> (392.67 KB, 下载次数: 0)

下载附件

2023-7-12 05:39 上传

<img src="https://img.saraba1st.com/forum/202307/12/053912ke83dntz7dmez8md.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230712-053642.jpg</strong> (324.78 KB, 下载次数: 0)

下载附件

2023-7-12 05:39 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 682#       发表于 2023-7-13 04:30

CSD-Edit

协作分数蒸馏(Collaborative Score Distillation)实现一致连贯的视觉合成图片

项目主页:https://subin-kim-cv.github.io/CSD/

github项目地址:https://github.com/subin-kim-cv/CSD

大规模文本到图像扩散模型的生成先验可以在不同的视觉模式上实现广泛的生成和编辑效果

然而，当使用这些先验适应复杂的视觉模式(通常表示为多个图像或者视频时)之间的一致性时，任务非常具有挑战性

在本文中，采用了一种新颖的方法来应对这一挑战，即协作分数蒸馏(CSD/Collaborative Score Distillation)，CSD基于Stein变分梯度下降(SVGD/Stein Variational Gradient Descent)

具体来说，通过将多个样本视为SVGD更新中的“粒子”，并结合它们的评分函数来同步提取一组图像的生成先验，因此，CSD有助于跨2D图像的信息无缝集成，从而在多个样本之间实现一致的视觉合成

展示了CSD在各种任务中的有效性，包括全景图像、视频和3D场景的可视化编辑，实验结果证明了CSD作为增强样本间一致性的通用方法的能力，从而扩大了文本到图像扩散模型的适用性

<img src="https://img.saraba1st.com/forum/202307/13/042834avouak7u8wdwllog.jpg" referrerpolicy="no-referrer">

<strong>476ba16b-9327-47f5-af5d-c3523f7e6181.jpg</strong> (233.86 KB, 下载次数: 0)

下载附件

2023-7-13 04:28 上传

<img src="https://img.saraba1st.com/forum/202307/13/042834ghhve7rjyexe2xs0.jpg" referrerpolicy="no-referrer">

<strong>a2a4c58a-be65-4683-88f9-ad0960d82ea7.jpg</strong> (171.35 KB, 下载次数: 0)

下载附件

2023-7-13 04:28 上传

(左方)Instruct-Pix2Pix应用于512x512的缩小图像时，会产生低质量的结果，并在编辑后丢失许多细节
 (中间)本文方法CSD-Edit，可在区块之间提供一致的图像编辑，从而实现给定指令的最佳保真度
 (右方)Instruct-Pix2Pix在裁剪后的区块上使用时，会导致补丁之间的图像编辑不一致

<img src="https://img.saraba1st.com/forum/202307/13/042845ss0gfhg1bgxhguh0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-042615.jpg</strong> (376.73 KB, 下载次数: 0)

下载附件

2023-7-13 04:28 上传

4K分辨率图像编辑生成

<img src="https://img.saraba1st.com/forum/202307/13/042930l3135pt232tyk27y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-042713.jpg</strong> (209.73 KB, 下载次数: 0)

下载附件

2023-7-13 04:29 上传

<img src="https://img.saraba1st.com/forum/202307/13/042936fv1s444mgdy4mw1s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-042726.jpg</strong> (93 KB, 下载次数: 0)

下载附件

2023-7-13 04:29 上传

全景图像生成与目标编辑修改

<img src="https://img.saraba1st.com/forum/202307/13/043009av323m8zj2n0nxdu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-042739.jpg</strong> (250.41 KB, 下载次数: 0)

下载附件

2023-7-13 04:30 上传

3D场景合成与基线效果对比

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 683#       发表于 2023-7-13 04:46

Emu

多模态生成预训练

github项目地址:https://github.com/baaivision/Emu

演示Demo:http://218.91.113.230:9002/

Emu是一种基于Tranformer的多模态基础模型，可以在多模态上下文中无缝生成图像和文本，可以通过一个模型适用于所有自回归训练过程，不加区别地接受任何单模态或多模态数据输入(例如交错的图像、文本和视频)

首先，视觉信号被编码为嵌入，并与文本Token一起形成交错的输入序列，然后，以对下一个文本Token进行分类或对多模态序列中的下一个视觉嵌入进行回归的统一目标，对Emu进行端到端训练

这种多功能的多模态使得能够大规模探索不同的预训练数据源，例如具有交错帧和文本的视频、具有交错图像和文本的网页，以及网络规模的图像文本对和视频文本对等

Emu可以作为图像到文本和文本到图像任务的通用多模态接口，并支持上下文图像和文本生成，在广泛的零样本/少样本任务中，包括图像字幕、视觉问答、视频问答和文本到图像生成，与最先进的大型多模态模型相比，Emu表现出了卓越的性能，通过指令调整实现的多模式助手等扩展功能也具有令人印象深刻的性能

<img src="https://img.saraba1st.com/forum/202307/13/044432mvb1d11tzhb728b5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-044228.jpg</strong> (70.42 KB, 下载次数: 0)

下载附件

2023-7-13 04:44 上传

Emu以自回归方式统一不同模态的建模，视觉信号首先被编码为嵌入，并与文本标记一起形成交错序列，训练目标是对下一个文本标记进行分类或对下一个视觉嵌入进行回归，在推理中，回归的视觉嵌入通过微调的潜在扩散模型被解码为真实的图像

<img src="https://img.saraba1st.com/forum/202307/13/044438nbbpqgjuly4fhlnu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-044116.jpg</strong> (102.69 KB, 下载次数: 0)

下载附件

2023-7-13 04:44 上传

交错的视频文本数据，故事板缩略图和字幕的组合创建了按时间戳排序的自然交错的视频和文本序列

<img src="https://img.saraba1st.com/forum/202307/13/044606xs20t3n03dnwlk20.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-044242.jpg</strong> (133.82 KB, 下载次数: 0)

下载附件

2023-7-13 04:46 上传

<img src="https://img.saraba1st.com/forum/202307/13/044606neskeuekhe6h46de.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-044307.jpg</strong> (350.63 KB, 下载次数: 0)

下载附件

2023-7-13 04:46 上传

<img src="https://img.saraba1st.com/forum/202307/13/044606rc41eca8j4hk8jjv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-044316.jpg</strong> (195.69 KB, 下载次数: 0)

下载附件

2023-7-13 04:46 上传

<img src="https://img.saraba1st.com/forum/202307/13/044606tdd33av8lccl3533.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-044326.jpg</strong> (324.42 KB, 下载次数: 0)

下载附件

2023-7-13 04:46 上传

<img src="https://img.saraba1st.com/forum/202307/13/044606flgsa6l6gp3di60l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-044339.jpg</strong> (437.46 KB, 下载次数: 0)

下载附件

2023-7-13 04:46 上传

评估与实际演示

<img src="https://img.saraba1st.com/forum/202307/13/044547kdfoh44st64f8h68.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-043800.jpg</strong> (691.22 KB, 下载次数: 0)

下载附件

2023-7-13 04:45 上传

<img src="https://img.saraba1st.com/forum/202307/13/044547cjlycwjymwcfi0jm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-043823.jpg</strong> (134.86 KB, 下载次数: 0)

下载附件

2023-7-13 04:45 上传

github提供模型权重下载

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 684#       发表于 2023-7-13 05:32

Solo-Performance-Prompting

github项目代码库:https://github.com/MikeWangWZHL/Solo-Performance-Prompting

在大型语言模型中释放认知协同(Cognitive Synergy)，通过多人自协作(Multi-Persona Self-Collaboration)来解决任务的代理(Agent)

人类智力的蓬勃发展依赖于认知协同，不同认知过程之间的协作和信息整合比孤立的个体认知过程能够产生更好的结果

尽管大型语言模型(LLM)作为一般任务解决代理已表现出良好的性能，但它们仍然难以处理需要密集领域知识和复杂推理的任务

<img src="https://img.saraba1st.com/forum/202307/13/052915agnb2hbnb5rbp2s5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052418.jpg</strong> (86.93 KB, 下载次数: 0)

下载附件

2023-7-13 05:29 上传

在本文中提出了SPP(Solo Performance Prompting)，通过与多个角色进行多轮自协作，将单个LLM转变为认知协同者

认知协同者是指与多个思维协作，结合各自的优势和知识，以提高解决问题的能力和复杂任务中的整体表现的智能代理，通过根据任务输入动态识别和模拟不同的角色，SPP释放了LLM认知协同的潜力

<img src="https://img.saraba1st.com/forum/202307/13/052950qf3lza3o33vosf3f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052619.jpg</strong> (353.07 KB, 下载次数: 0)

下载附件

2023-7-13 05:29 上传

与使用单个或固定数量的角色相比，在LLM中分配多个细粒度的角色可以带来更好的解决问题的能力

本文在三个具有挑战性的任务上评估了SPP：冷知识创意写作(Trivia Creative Writing)、机密代号游戏协作(Codenames Collaborative)和逻辑网格谜题(Logic Grid Puzzle)，涵盖知识密集型和推理密集型类型

与之前的作品如Chain-of-Thought等单纯增强LLM推理能力的作品不同，SPP能够有效引出内部知识获取能力，减少幻觉，保持强大的推理能力

使用SPP进行操作的任务示例，LLM根据任务输入自动识别参与者，此示例表明标准提示可能会导致事实错误，而SPP中的专家角色有助于准确获取知识，有助于得出连贯且信息丰富的最终答案

与其他提示方法的对比:

<img src="https://img.saraba1st.com/forum/202307/13/053041nhzx678fa64afrhk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052633.jpg</strong> (423.16 KB, 下载次数: 0)

下载附件

2023-7-13 05:30 上传

Trivia Creative Writing任务与成绩:

<img src="https://img.saraba1st.com/forum/202307/13/053106yf8rx44jll6tt8ff.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052700.jpg</strong> (291.75 KB, 下载次数: 0)

下载附件

2023-7-13 05:31 上传

<img src="https://img.saraba1st.com/forum/202307/13/053106rsp63696zddds9dr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052728.jpg</strong> (300.48 KB, 下载次数: 0)

下载附件

2023-7-13 05:31 上传

Codenames Collaborative任务与成绩:

<img src="https://img.saraba1st.com/forum/202307/13/053135tlzgnx4ktkdhht5t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052746.jpg</strong> (183.45 KB, 下载次数: 0)

下载附件

2023-7-13 05:31 上传

<img src="https://img.saraba1st.com/forum/202307/13/053135wudujfvz944wf04g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052800.jpg</strong> (442.66 KB, 下载次数: 0)

下载附件

2023-7-13 05:31 上传

Logic Grid Puzzle任务与成绩:

<img src="https://img.saraba1st.com/forum/202307/13/053151ve57kwe27wzaad6a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052811.jpg</strong> (169.84 KB, 下载次数: 0)

下载附件

2023-7-13 05:31 上传

<img src="https://img.saraba1st.com/forum/202307/13/053151jn724bmudiqdnf2y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-052819.jpg</strong> (58.6 KB, 下载次数: 0)

下载附件

2023-7-13 05:31 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 685#       发表于 2023-7-13 06:22

Objaverse-XL

一个包含超过1000万个3D对象的开放数据集

论文链接:https://objaverse.allenai.org/objaverse-xl-paper.pdf

注(数据集尚未公开):

<img src="https://img.saraba1st.com/forum/202307/13/062150r233ddv3fazv9f3d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-061931.jpg</strong> (62.29 KB, 下载次数: 0)

下载附件

2023-7-13 06:21 上传

<img src="https://img.saraba1st.com/forum/202307/13/062007d7rady9acc9rysy7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-060657.jpg</strong> (235.56 KB, 下载次数: 0)

下载附件

2023-7-13 06:20 上传

自然语言处理和2D视觉模型主要通过扩大训练数据规模在许多任务上取得了能力提升，然而，3D视觉任务并没有取得同样的进展，部分原因是获取高质量3D数据的难度

<img src="https://img.saraba1st.com/forum/202307/13/062017thwsppzw7nln14ln.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-061037.jpg</strong> (184.29 KB, 下载次数: 0)

下载附件

2023-7-13 06:20 上传

在本文中展示了Objaverse-XL，这是一个包含超过1000万个3D对象的数据集，数据集包含来自不同来源的，进行过重复数据删除的3D对象，包括了手动设计的对象、地标和日常物品的摄影测量扫描以及历史和古董文物的专业扫描

Objaverse-XL代表了3D数据集领域最大的规模和多样性，为3D视觉带来了重大的新可能性

实验证明了Objaverse-XL提供的规模所带来的改进，通过训练Zero123进行新视图合成，利用超过1亿张多视图渲染图像，实现了强大的零样本泛化能力

借助Objaverse-XL，训练了Zero123-XL(3D基础模型)，观察到令人难以置信的3D泛化能力，Objaverse 1.0于去年12月发布，这是朝着正确方向迈出的一步，但对于800K个3D对象来说仍然相对较小

<img src="https://img.saraba1st.com/forum/202307/13/062042kce50e8qyqyo1zoy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-061100.jpg</strong> (226.71 KB, 下载次数: 0)

下载附件

2023-7-13 06:20 上传

 Objaverse-XL的尺寸要大一个数量级，而且更加多样化，与原始Zero123模型相比，Zero123-XL在零样本泛化能力方面有了显著提高，甚至能够对草图、卡通和人物进行新视图合成

借助基本的Zero123-XL基础模型，可以使用DreamFusion执行图像到3D生成，让模型引导NeRF生成新视图

除此之外，还展示了Zero123-XL和PixelNeRF的强劲缩放性能提升效果

样本元数据与Zero123-XL生成效果:

<img src="https://img.saraba1st.com/forum/202307/13/062106pb4ouwo777wmc7be.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-061146.jpg</strong> (320.96 KB, 下载次数: 0)

下载附件

2023-7-13 06:21 上传

<img src="https://img.saraba1st.com/forum/202307/13/062106w3frff2s48zrzsyq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-061213.jpg</strong> (208.74 KB, 下载次数: 0)

下载附件

2023-7-13 06:21 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 686#       发表于 2023-7-13 06:31

EasyEdit

EasyEdit是一个用于编辑大型语言模型内部知识的多功能工具，支持各种编辑方法(例如SERAC、IKE、MEND、KN、ROME、MEMIT等)，现已在github上提供，同时将不断更新新版本

github项目地址:https://github.com/zjunlp/EasyEdit

<img src="https://img.saraba1st.com/forum/202307/13/063042wrqwzw9zto9ott9o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-062845.jpg</strong> (254.36 KB, 下载次数: 0)

下载附件

2023-7-13 06:30 上传

<img src="https://img.saraba1st.com/forum/202307/13/063042j308g1drzip7g37i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-062925.jpg</strong> (247.46 KB, 下载次数: 0)

下载附件

2023-7-13 06:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 687#       发表于 2023-7-13 07:03

 本帖最后由 Machinery 于 2023-7-13 07:06 编辑 

world-to-words

世界到单词：通过视觉语言模型中的快速映射来获取基础开放词汇

相关论文:arxiv.org/abs/2306.08685

github项目仓库:https://github.com/sled-group/world-to-words

数据集:https://huggingface.co/datasets/sled-umich/GOVA-flickr

OctoBERT模型权重:https://huggingface.co/sled-umich/OctoBERT

<img src="https://img.saraba1st.com/forum/202307/13/070121r2ryn2ftzd4crjn0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230713-070105.jpg</strong> (57.73 KB, 下载次数: 0)

下载附件

2023-7-13 07:01 上传

将语言与其在物理世界中的所指对象联系起来的能力(称为基准/grounding)对于学习和理解单词的基础含义至关重要

虽然人类能够在新词学习中表现出快速映射，但目前尚不清楚现代视觉语言模型是否能够真正以其基准意义来表征语言，以及基准如何进一步引导新词学习

为此引入基准开放词汇习得(GOVA/Grounded Open Vocabulary Acquisition)来检查开放世界语言学习中的基准和引导

作为初步尝试，构建了面向对象的BERT模型(OctoBERT)，这是一种新的视觉基础语言模型，通过对图像文本对进行预训练，突出基准作为目标

通过大量的实验和分析，证明OctoBERT是一个更加连贯、快速的基准单词学习器，预训练期间获得的基础能力有助于模型更快、更稳健地学习未见过的单词

<img src="https://img.saraba1st.com/forum/202307/13/070148vxgi0fjev6vx0exg.jpg" referrerpolicy="no-referrer">

<strong>20230713_064942.jpg</strong> (99.8 KB, 下载次数: 0)

下载附件

2023-7-13 07:01 上传

从认知角度来看，人类展示了快速映射基准的能力(用最少的信息引导新单词)，例如，即使“焚化炉”这个词对于语言学习者来说是陌生的，他们仍然可以轻松地找到传达其含义的物体

从实际角度来看，昂贵的基准标注很难覆盖视觉语言预训练期间的词汇空间，我们需要使VLM能够在原始图像-文本对的少样本中学习基础新单词，而无需任何显式的单词-对象映射

<img src="https://img.saraba1st.com/forum/202307/13/070211vh5555kjpz6rkalo.jpg" referrerpolicy="no-referrer">

<strong>20230713_065242.jpg</strong> (136.72 KB, 下载次数: 0)

下载附件

2023-7-13 07:02 上传

因此引入了基准开放词汇习得问题，其中模型首先在预训练期间获得基准能力，然后在没有基准监督的情况下转移这种能力，在少样本中学习未见的单词

<img src="https://img.saraba1st.com/forum/202307/13/070222c46agfdcmc184da6.jpg" referrerpolicy="no-referrer">

<strong>20230713_065256.jpg</strong> (134.78 KB, 下载次数: 0)

下载附件

2023-7-13 07:02 上传

OctoBERT是一种基于视觉的语言模型，构架如上，可以联合学习定位视觉感知中的实体并根据对象表征执行语言建模

<img src="https://img.saraba1st.com/forum/202307/13/070239sj8fw1f9j29jk9e9.jpg" referrerpolicy="no-referrer">

<strong>20230713_065558.jpg</strong> (110.98 KB, 下载次数: 0)

下载附件

2023-7-13 07:02 上传

实验结果表明，基准预训练可以从头开始高效、有效的单词习得，以及少样本新单词习得，不会出现明显的遗忘，可以像人类一样轻松映射自己不熟悉的概念的能力

经过认知调查并发现模型仍然类似LLM一样严重依赖浅层的统计数据，努力获取视觉上不太显著的概念，与人类的熟悉度和具体直觉不一致等情况

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 688#       发表于 2023-7-14 03:30

 本帖最后由 Machinery 于 2023-7-14 03:32 编辑 

PolyLM

开源多语言大型语言模型，PolyLM是一个通晓多语言的LLM模型，涵盖中文、英文、西班牙语、法语、德语、俄语、葡萄牙语、意大利语、阿拉伯语、日语、韩语、泰语、越南语和印尼语等18个语言，该模型可以应用于对话问答、文本生成、机器翻译和情感分析等领域，能够自动生成高质量的多语言文本，从而为跨语言、文化的交流提供便利

modelscope项目权重下载:https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation/summary

大型语言模型(LLM)表现出卓越的理解、推理和生成以下自然语言指令的能力，然而大部分LLM的发展主要集中在英语等高资源语言上，从而限制了其在其他语言中的适用性和研究

因此本文推出了PolyLM，这是一种在6400亿个Token上训练的多语言LLM，有两种模型大小:1.7B和13B

为了增强其多语言能力，将双语数据集成到了训练数据中，并且使用了课程学习策略，在预训练时将非英语数据的比例从第一阶段的30%增加到最后阶段的60%

此外，还提出了一种多语言自指令方法，自动生成了132.7K不同的多语言指令用于模型微调

为了评估模型的性能，收集了几个现有的多语言任务，包括多语言理解、问答、生成和翻译等任务

大量实验表明，PolyLM在多语言任务上超越了LLaMA和BLOOM等其他开源模型，同时在英语方面保持了同等的性能

modelscope项目页面预览:

<img src="https://img.saraba1st.com/forum/202307/14/032859yvfuuj8fglvv6gux.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230714-032331.jpg</strong> (163.01 KB, 下载次数: 0)

下载附件

2023-7-14 03:28 上传

训练数据与分词架构比对:

<img src="https://img.saraba1st.com/forum/202307/14/032926zpp633ukkuj23263.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230714-032352.jpg</strong> (145.39 KB, 下载次数: 0)

下载附件

2023-7-14 03:29 上传

<img src="https://img.saraba1st.com/forum/202307/14/032926mc7b8v7kg5cmuvlb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230714-032412.jpg</strong> (201.36 KB, 下载次数: 0)

下载附件

2023-7-14 03:29 上传

相关评估成绩:

<img src="https://img.saraba1st.com/forum/202307/14/033022qt1c7c2y9rehctwt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230714-032736.jpg</strong> (176 KB, 下载次数: 0)

下载附件

2023-7-14 03:30 上传

<img src="https://img.saraba1st.com/forum/202307/14/033022otbbmpphlzkp69zo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230714-032749.jpg</strong> (134.39 KB, 下载次数: 0)

下载附件

2023-7-14 03:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 689#       发表于 2023-7-14 07:49

Kandinsky2.2

简介如下图所示

github项目地址:https://github.com/ai-forever/Kandinsky-2

<img src="https://img.saraba1st.com/forum/202307/14/074925mlatrlp5p00p5tv0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230714-045550.jpg</strong> (37.1 KB, 下载次数: 0)

下载附件

2023-7-14 07:49 上传

<img src="https://img.saraba1st.com/forum/202307/14/074925zzctdscgykxcw3q4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230714-045419.jpg</strong> (461.14 KB, 下载次数: 0)

下载附件

2023-7-14 07:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 690#       发表于 2023-7-15 04:16

 本帖最后由 Machinery 于 2023-7-15 04:18 编辑 

Animate-A-Story

通过检索增强视频生成来讲述故事

相关论文:https://arxiv.org/abs/2307.06940

项目主页:https://videocrafter.github.io/Animate-A-Story/

github项目地址:https://github.com/VideoCrafter/Animate-A-Story

<img src="https://img.saraba1st.com/forum/202307/15/041241km50bpxx1kz3bhvv.jpg" referrerpolicy="no-referrer">

<strong>acaec4e7-51e0-43dd-8d25-0899d180f160.jpg</strong> (219.63 KB, 下载次数: 0)

下载附件

2023-7-15 04:12 上传

生成用于讲述视觉故事的视频可能是一个乏味且复杂的过程，通常需要真人拍摄或图形动画渲染

为了绕过这些挑战，其中的一个关键想法是利用大量现有视频剪辑片段，并通过定制其外观来合成连贯的讲故事视频，本文通过开发一个由两个功能模块组成的框架来实现这一目标

1.运动结构检索，它为提供的候选视频，查询文本描述的所需场景或相关的场景人物运动上下文

2.结构引导的文本到视频合成，它在运动结构和文本提示的指导下生成情节对齐的视频

对于第一个模块，本文框架利用了现成的视频检索系统并提取视频深度作为运动结构，对于第二个模块，提出了一种可控视频生成模型，该模型提供对结构和角色的灵活控制，视频是按照结构指导和外观指导合成的

为了确保片段之间的视觉一致性，还提出了一种有效的概念个性化方法，该方法允许通过文本提示指定所需的角色身份，实验展示了本文方法相对于各种现有基线的显著优势

此外，对合成故事视频的用户研究证明了Animate-A-Story框架的有效性，并表明了类似实际应用的广阔潜力

<img src="https://img.saraba1st.com/forum/202307/15/041313p1cl6qp14ooddqd3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-035828.jpg</strong> (160.07 KB, 下载次数: 0)

下载附件

2023-7-15 04:13 上传

检索增强视频合成框架的流程图，给定文本故事脚本，首先提取关键情节并将其描述调整为文本查询和提示，每个情节通过两个模块转换为生成的视频剪辑片段：视频检索系统和结构引导的文本到视频生成模型

<img src="https://img.saraba1st.com/forum/202307/15/041323o3fg3slggl3fyic6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-040848.jpg</strong> (82.14 KB, 下载次数: 0)

下载附件

2023-7-15 04:13 上传

可调整结构引导的文本到视频模型的概述，通过使用源视频的深度信息来指导视频合成过程，该模型由两个分支组成：一个通用的文本到视频合成分支，它是潜在空间中的视频扩散模型，以及一个用于编码和施加结构控制的侧分支，控制机制是按元素添加特征，值得注意的是，深度控制是可调整的，这个属性对于进一步的角色重新渲染至关重要

<img src="https://img.saraba1st.com/forum/202307/15/041329p3jz2tz4vwjw21v8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-040900.jpg</strong> (82.55 KB, 下载次数: 0)

下载附件

2023-7-15 04:13 上传

不同个性化方法的概念图，为了克服生成角色的不一致问题，研究了现有的个性化方法，并提出了一种重新渲染目标角色外观的新方法，将CLIP文本编码器和降噪器U-Net的所有参数保持冻结，并学习与时间步长相关的Token嵌入来表示目标角色的语义特征，此外，还在注意力模块中的q、k、v投影层中插入一个新分支，调整了预训练的权重以更好的表征角色

<img src="https://img.saraba1st.com/forum/202307/15/041340r66llk1ooiwy11xi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-040911.jpg</strong> (57.39 KB, 下载次数: 0)

下载附件

2023-7-15 04:13 上传

调整参数𝜏的效果，小的𝜏值可以放松深度控制，使形状向角色形状渲染，同时从深度上保持粗略的布局和动作控制，该技术可以生成泰迪熊的视频，而不需要检索泰迪熊的运动视频，而泰迪熊的运动视频很难收集，因为缺乏泰迪熊的各种运动(例如做瑜伽)的真实视频

<img src="https://img.saraba1st.com/forum/202307/15/041349un44emonk6oloqta.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-040931.jpg</strong> (445.71 KB, 下载次数: 0)

下载附件

2023-7-15 04:13 上传

流程中核心组件的消融实验结果，包括结构引导、角色重新渲染和TimeInv

<img src="https://img.saraba1st.com/forum/202307/15/041554nhccxbpbwezx9c3l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-041002.jpg</strong> (219.92 KB, 下载次数: 0)

下载附件

2023-7-15 04:15 上传

与之前的个性化方法的定量比较，展示了使用四种不同方法的两个角色的生成结果，对于每种方法，都会显示一个视频剪辑片段并使用相同的随机种子，每个视频剪辑片段显示两帧，帧采样步长为8

<img src="https://img.saraba1st.com/forum/202307/15/041603lxo80m8nysd4dx66.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-041111.jpg</strong> (136.62 KB, 下载次数: 0)

下载附件

2023-7-15 04:16 上传

所提出的时间步长变量Textual Inversion(TimeInv)使用预训练的稳定扩散对图像个性化的有效性，同一列中的结果是在相同的训练步骤和随机种子下进行比较的，这表明本方法可以作为图像和视频生成任务个性化的通用方法

评估结果:

<img src="https://img.saraba1st.com/forum/202307/15/041613uzppaw4nwglupwhn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-041104.jpg</strong> (94.06 KB, 下载次数: 0)

下载附件

2023-7-15 04:16 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 691#       发表于 2023-7-15 04:45

 本帖最后由 Machinery 于 2023-7-15 04:49 编辑 

factor

对于生成式语言模型的真实性进行评估的基准

github项目地址(coming soon):https://github.com/AI21Labs/factor

当在指定领域内部署语言模型(LM)之前，衡量其在该领域中生成事实上不正确信息的倾向非常重要，现有的事实生成评估方法侧重于从LM本身采样的事实，因此无法控制评估的事实集，并且可能无法充分代表罕见和不可能的事实情况

本文提出了FACTOR，一种通过语料库转换进行事实评估的方法，这是一种用于评估LM事实性的可扩展方法

FACTOR自动将感兴趣的事实语料库转换为基准，评估LM从语料库中生成与真实事实类似但不正确的陈述的倾向

通过使用factor框架创建的两个基准，Wiki-FACTOR和News-FACTOR并进行评测

证明了模型的基准分数会随着模型大小的增加而增加，当LM通过检索增强进行生成时也会提高，同时基准分数与困惑度相关，但这两个指标在模型排名上并不总是一致，当困惑度和基准分数不一致时，后者可以更好地反映了开放式生成中的事实性(由人类标注者测量评估)

<img src="https://img.saraba1st.com/forum/202307/15/044207qr9415wwwggrm62i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-043022.jpg</strong> (89.31 KB, 下载次数: 0)

下载附件

2023-7-15 04:42 上传

FACTOR评估任务中的每个示例都包含一个前缀和四个补全，其中只有一个实际上是正确的(本示例中的补全答案A)，非事实补全B、C、D(红色)是根据不同的事实错误类型生成的，评估的模型分别为每个补全分配似然分数，如果它将最高的可能性分配给所有答案中那个事实正确的完成，则可以认为它的“想法”是“正确的”

<img src="https://img.saraba1st.com/forum/202307/15/044213hi97e4x0fit9ei0p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-043908.jpg</strong> (41.05 KB, 下载次数: 0)

下载附件

2023-7-15 04:42 上传

来自GPT-Neo模型系列(蓝色圆圈，1.3B-20B)和OPT模型系列(红色三角形，1.3B-66B)的Wiki-FACTOR分数与相对应的wiki上的困惑度，这两个度量相关，但在排名上可能不一致，例如，OPT-66B LM比GPT-J-6B LM具有更高的困惑度，但Wiki-FACTOR准确性更好，在后边注释了两种模型生成的文本，并表明更好的Wiki-FACTOR可以预测更真实的文本生成

<img src="https://img.saraba1st.com/forum/202307/15/044220ylej4fdjwmzje5l0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-044033.jpg</strong> (188.57 KB, 下载次数: 0)

下载附件

2023-7-15 04:42 上传

错误类型示例，上方的原始文本由前缀和粗体完成句组成，每个示例都会对不同类型的原始完成引入不同的红色扰动

<img src="https://img.saraba1st.com/forum/202307/15/044402eib6uv6fb4rrmzh0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-044122.jpg</strong> (83.12 KB, 下载次数: 0)

下载附件

2023-7-15 04:44 上传

对于 GPT-2(蓝色圆圈)、GPT-Neo(红色三角形)和OPT(黄色正方形)系列的模型，每个模型大小的Wiki-FACTOR和News-FACTOR的准确度

<img src="https://img.saraba1st.com/forum/202307/15/044409b118lsiiwwtffl6f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-044138.jpg</strong> (97.89 KB, 下载次数: 0)

下载附件

2023-7-15 04:44 上传

GPT-Neo和OPT模型的Wiki-FACTOR事实准确性与其IC-RALM变体相比，IC-RALM方法可以实现所有模型的改进效果

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 692#       发表于 2023-7-15 05:05

 本帖最后由 Machinery 于 2023-7-15 05:08 编辑 

T2I-CompBench

开放世界组合式文本到图像生成的综合评估基准

github项目地址:https://github.com/Karine-Huang/T2I-CompBench

尽管最近的文本到图像模型具有令人惊叹的生成高质量图像的能力，但当前的方法通常难以有效地将具有不同属性和关系的对象组合成复杂且连贯的场景

本文提出了T2I-CompBench，一个开放世界组合式文本到图像生成的综合评估基准，包含6000个组合式文本提示，由3个大类别(属性绑定、对象关系和复杂组合)，6个子类别(颜色绑定、 形状绑定、纹理绑定、空间关系、非空间关系和复杂组合)组成

进一步提出了几个专门设计用于评估组合式文本到图像生成的评估指标，引入了一种新方法，即通过奖励驱动样本选择(GORS)进行生成模型微调，以提高预训练文本到图像模型的合成能力

进行了大量的实验和评估，在T2I-CompBench上对以前的各种组合式生成方法进行基准测试，并验证了评估指标和GORS方法的有效性

<img src="https://img.saraba1st.com/forum/202307/15/050259a7wg1hsrfpd7fpib.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-045913.jpg</strong> (179.11 KB, 下载次数: 0)

下载附件

2023-7-15 05:02 上传

T2I-CompBench的评估样本统计情况

<img src="https://img.saraba1st.com/forum/202307/15/050313g3uh136d6aaoa2d9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-045947.jpg</strong> (148.14 KB, 下载次数: 0)

下载附件

2023-7-15 05:03 上传

评估指标:解耦的BLIP-VQA测试

<img src="https://img.saraba1st.com/forum/202307/15/050502m6t4rz2moaat4kms.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-050411.jpg</strong> (200.17 KB, 下载次数: 0)

下载附件

2023-7-15 05:05 上传

评估指标:minigpt4

<img src="https://img.saraba1st.com/forum/202307/15/050525ziqlmzlbb0i744dl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-050427.jpg</strong> (138.62 KB, 下载次数: 0)

下载附件

2023-7-15 05:05 上传

评估指标:UniDet

<img src="https://img.saraba1st.com/forum/202307/15/050542buc0oyavdjz7u665.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-050438.jpg</strong> (111.73 KB, 下载次数: 0)

下载附件

2023-7-15 05:05 上传

三合一方法评估

<img src="https://img.saraba1st.com/forum/202307/15/050340gz4q4b8mhzc4clzh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-050130.jpg</strong> (83.13 KB, 下载次数: 0)

下载附件

2023-7-15 05:03 上传

BLIP-VQA用于属性绑定评估，UniDet用于空间关系评估，MiniGPT4-CoT作为统一指标

<img src="https://img.saraba1st.com/forum/202307/15/050345xmxkh71z7bmwrxjw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-050220.jpg</strong> (206.75 KB, 下载次数: 0)

下载附件

2023-7-15 05:03 上传

用于组合式文本到图像生成的GORS方法、评估结果、实例对比

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 693#       发表于 2023-7-15 05:49

InternVid/ViCLIP

用于多模态理解和生成的大规模视频文本数据集与ViCLIP

相关论文:https://arxiv.org/abs/2307.06942

github项目仓库:https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid

本文介绍了InternVid，一个以视频为中心的大规模多模态数据集，能够通过它学习强大且可迁移的视频文本表征，以实现多模态理解和生成

InternVid数据集包含超过700万个视频，视频总持续时间近76万小时，包含2.34亿个视频剪辑片段，并附有总共4.1B个单词的详细描述

同时引入了一种可扩展的数据构造方法，通过大型语言模型(LLM)，自主构建高质量的视频文本数据集，从而展示其在大规模学习视频语言表征方面的功效

具体来说，利用了多尺度方法来生成与视频相关的描述，此外，还介绍了ViCLIP，一种基于ViT-L的视频文本表征学习模型，通过对比学习在InternVid上学习，该模型展示了领先的零镜头动作识别和有竞争力的视频检索性能

除了识别和检索等基本视频理解任务之外，InternVid数据集和ViCLIP模型还有广泛的应用，它们非常有利于生成交错的视频文本数据，可以用以学习以视频为中心的对话系统，推进视频到文本和文本到视频生成研究

这些资源为对多模态视频理解和生成感兴趣的研究人员和从业者提供了丰富的拓展工具

<img src="https://img.saraba1st.com/forum/202307/15/054613rcpl7kckilq2ccpr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-053427.jpg</strong> (180.92 KB, 下载次数: 0)

下载附件

2023-7-15 05:46 上传

样本示例(每个视频剪辑片段展示三帧)、相应生成的字幕以及InternVid中的ASR转录，在标题中，用蓝色突出显示名词，用绿色突出显示动词，使用LLM将非英语翻译成英语

<img src="https://img.saraba1st.com/forum/202307/15/054623bttt3lbolbtv21lt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-053651.jpg</strong> (181.3 KB, 下载次数: 0)

下载附件

2023-7-15 05:46 上传

InternVid与其他类似数据集的对比，InternVid通常使用720p分辨率，而其他数据集多数为360p到512p

<img src="https://img.saraba1st.com/forum/202307/15/054630fhbd2gomzgjjvh82.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-053822.jpg</strong> (124.06 KB, 下载次数: 0)

下载附件

2023-7-15 05:46 上传

提出的多尺度视频字幕生成工作流程，粗、细粒度的标题分别用绿色和深绿色标记

<img src="https://img.saraba1st.com/forum/202307/15/054639twci4ls4eo8i4qni.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-054142.jpg</strong> (130.29 KB, 下载次数: 0)

下载附件

2023-7-15 05:46 上传

InternVid 中三种格式的交错视频文本数据生成与ViCLIP的框架

<img src="https://img.saraba1st.com/forum/202307/15/054648uwioljmjj2nemddc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-054353.jpg</strong> (205.41 KB, 下载次数: 0)

下载附件

2023-7-15 05:46 上传

InternVid的交错视频文本数据格式，每个剪辑片段的标题和ASR转录内容分别以黑色和灰色显示，可以通过放弃ASR转录来实现交错的视频文本数据格式，为了获得数据格式，将多个视频与交错的视频文本数据连接了起来

<img src="https://img.saraba1st.com/forum/202307/15/054800fq15slog60dlgsgz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-054407.jpg</strong> (168.18 KB, 下载次数: 0)

下载附件

2023-7-15 05:48 上传

<img src="https://img.saraba1st.com/forum/202307/15/054800ze9laa2amecajns2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-054425.jpg</strong> (384.71 KB, 下载次数: 0)

下载附件

2023-7-15 05:48 上传

零样本行动认知评估基准结果与零样本文本到视频生成的SOTA结果

<img src="https://img.saraba1st.com/forum/202307/15/054854qdcuxx1didb8wrzx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-054444.jpg</strong> (258.38 KB, 下载次数: 0)

下载附件

2023-7-15 05:48 上传

<img src="https://img.saraba1st.com/forum/202307/15/054854sh397ooynf3pxfme.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-054500.jpg</strong> (379.58 KB, 下载次数: 0)

下载附件

2023-7-15 05:48 上传

<img src="https://img.saraba1st.com/forum/202307/15/054854svs4onb101zb0ghn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-054514.jpg</strong> (312.34 KB, 下载次数: 0)

下载附件

2023-7-15 05:48 上传

多种任务的具体使用实例

<img src="https://img.saraba1st.com/forum/202307/15/054909ogvb00vgltunc2el.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-054534.jpg</strong> (495.25 KB, 下载次数: 0)

下载附件

2023-7-15 05:49 上传

ASR转录的词云统计结果

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 694#       发表于 2023-7-15 08:55

 本帖最后由 Machinery 于 2023-7-15 09:02 编辑 

CM3Leon

缩放自回归多模态模型：预训练和指令调整

相关博客:https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning/

META新作品CM3Leon(发音为“Chameleon”)，这是一种检索增强、基于Token、仅解码器的多模态语言模型，能够生成和填充文本和图像

CM3Leon使用CM3多模态架构，展示了扩展和调整更多样化的指令类型数据的极端优势，它是第一个使用改编自纯文本语言模型的配方进行训练的多模态模型，训练包括大规模检索增强预训练阶段和多任务监督微调(SFT)两个阶段

<img src="https://img.saraba1st.com/forum/202307/15/085825kc8yg3w5wu3r9jtu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-085721.jpg</strong> (113.96 KB, 下载次数: 0)

下载附件

2023-7-15 08:58 上传

它也是一个通用模型，可以进行文本到图像和图像到文本的生成，使之能够引入产生高质量输出的独立对比解码方法，同时测试了最优CFG(Classifier-Free Guidance/无分类指导)数值FID质量

<img src="https://img.saraba1st.com/forum/202307/15/085135nta4z1445ew07a1e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-083940.jpg</strong> (231.13 KB, 下载次数: 0)

下载附件

2023-7-15 08:51 上传

CM3Leon零样本生成展示(无检索增强)，CM3Leon可以生成复杂的组合对象、长尾分布对象(比如用石头雕刻的Khachkar–Armenian十字架)以及历史疑难生成问题，例如手部图像和理解生成对应文本

<img src="https://img.saraba1st.com/forum/202307/15/085812xykoyy6oakgcy9o6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-085732.jpg</strong> (125.9 KB, 下载次数: 0)

下载附件

2023-7-15 08:58 上传

大量实验表明，该方法对于多模态模型非常有效，CM3Leon在文本到图像生成方面实现了SOTA性能，训练计算量比同类方法少5倍(零样本MS-COCO FID为4.88)，SFT之后，CM3Leon还可以在语言引导的图像编辑到图像控制的生成和分割等任务中展现出前所未有的可控性的水平

文本到图像生成领域中图像数据源的伦理影响一直是备受争议的话题，在本研究中，仅使用了Shutterstock数据集的许可图像，因此可以避免与图像所有权和归属相关的问题，而无需牺牲性能

<img src="https://img.saraba1st.com/forum/202307/15/090143fxnjb1ub0bmjkxn0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-090123.jpg</strong> (545.42 KB, 下载次数: 0)

下载附件

2023-7-15 09:01 上传

监督微调(SFT)对于训练ChatGPT等大型语言模型(LLM)至关重要，尽管如此，它在多模态环境中的应用在很大程度上仍未得到探索，SFT可以训练模型更好的理解指令或提示，从而增强其在新任务甚至零样本任务中的性能，使用了以上数据集进行了监督微调

<img src="https://img.saraba1st.com/forum/202307/15/085408qm8qev2p1tt8tvv8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-084530.jpg</strong> (185.64 KB, 下载次数: 0)

下载附件

2023-7-15 08:54 上传

本文基于实践发现指令调整显著增强了跨各种任务的多模态模型性能，例如图像标题生成、视觉问答、基于文本的编辑和条件图像生成等，研究组针对各种混合图像和文本任务对CM3Leon进行了微调，将每个任务构造为一系列交错的文本和图像示例

<img src="https://img.saraba1st.com/forum/202307/15/085418zdomokdism4osdbz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-084845.jpg</strong> (126.83 KB, 下载次数: 0)

下载附件

2023-7-15 08:54 上传

空间基准图像生成允许用户将空间信息集成到图像生成的文本提示中，每个对象都由离散标记表示，通过使用MS-COCO、Openimage和Object365等对象检测数据集编译了300万个训练样本

如何编写任务使用户能够请求模型根据文本提示创建标志或徽标，使用OCR检测器从Shutterstock数据集中查找了合适的示例，最终生成了200000个示例以进行学习

<img src="https://img.saraba1st.com/forum/202307/15/085418vh9hdtt4p143tsp3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-085018.jpg</strong> (107.13 KB, 下载次数: 0)

下载附件

2023-7-15 08:54 上传

示例显示了SFT-CM3Leon-7B模型的可以生成各种长形式的文本完成任务

<img src="https://img.saraba1st.com/forum/202307/15/085509lntt5dhgzvvvr0v4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230715-085036.jpg</strong> (124.4 KB, 下载次数: 0)

下载附件

2023-7-15 08:55 上传

非常不错的长尾生成能力…

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 695#       发表于 2023-7-16 04:44

CLIPMasterPrints

使用潜在变量进化来欺骗对比语言图像预训练模型(CLIP)

相关论文:https://arxiv.org/abs/2307.03798

github项目仓库:https://github.com/matfrei/CLIPMasterPrints

利用大规模视觉和文本数据的模型，例如对比语言图像预训练模型(CLIP)，变得越来越重要

<img src="https://img.saraba1st.com/forum/202307/16/044424c311212n5xldqes1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230716-043844.jpg</strong> (227.55 KB, 下载次数: 0)

下载附件

2023-7-16 04:44 上传

在这项工作中表明，尽管这些模型具有多样能力，但它们很容易受到欺瞒图像的影响，欺瞒图像能够在大量不同的提示下最大化CLIP模型的置信度得分，同时人类无法识别

<img src="https://img.saraba1st.com/forum/202307/16/044433sfgw7vptgpvntmmd.jpg" referrerpolicy="no-referrer">

<strong>20230716_043905.jpg</strong> (270.61 KB, 下载次数: 0)

下载附件

2023-7-16 04:44 上传

本文演示了如何通过进化策略或随机梯度下降搜索生成模型的潜在空间来挖掘欺瞒图像，研究了挖掘的欺瞒图像的属性，发现在少量图像标题上训练的图像可能会泛化为大量语义相关的标题

<img src="https://img.saraba1st.com/forum/202307/16/044437vuu4uuiccu3cecjj.jpg" referrerpolicy="no-referrer">

<strong>20230716_044059.jpg</strong> (129.15 KB, 下载次数: 0)

下载附件

2023-7-16 04:44 上传

同时评估了两种可能的缓解策略，发现欺瞒示例的脆弱性与对比预训练多模态网络中的模态差距密切相关

因此，从易受攻击的角度来看，需要减少CLIP和相关多模态方法中的模态差距

挖掘的欺瞒示例在CLIP分数方面优于所有的实际艺术品，因此可以欺瞒所有显示目标标题的模型

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 696#       发表于 2023-7-16 04:56

 本帖最后由 Machinery 于 2023-7-16 04:59 编辑 

DNA-Rendering

一个大规模、高保真的演员多视图渲染动作数据集

项目主页:https://dna-rendering.github.io/

高多样性:数据收集涉及500人，有529套不同的服装，269种不同类型的日常动作，以及153种不同类型的特殊表演，包括一些动作的相关交互对象

 高保真:构建了专业的多视图系统来捕获数据，该系统包含60个同步摄像机，最大分辨率为4096×3000，速度为15fps

丰富的标注:提供现成的标注，包括2D/3D人体关键点、前景掩码与SMPL-X模型

<img src="https://img.saraba1st.com/forum/202307/16/045644an5l93qsf9ek01tl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230716-045500.jpg</strong> (156 KB, 下载次数: 0)

下载附件

2023-7-16 04:56 上传

<img src="https://img.saraba1st.com/forum/202307/16/045644e83nk823l3v63f3f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230716-045523.jpg</strong> (176.28 KB, 下载次数: 0)

下载附件

2023-7-16 04:56 上传

<img src="https://img.saraba1st.com/forum/202307/16/045644o8xm911cxu8px83u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230716-045550.jpg</strong> (583.28 KB, 下载次数: 0)

下载附件

2023-7-16 04:56 上传

对应的多种不同方法的数据集基准测试结果:

<img src="https://img.saraba1st.com/forum/202307/16/045811dsak03v13wv4t6l4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230716-045735.jpg</strong> (262.67 KB, 下载次数: 0)

下载附件

2023-7-16 04:58 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 697#       发表于 2023-7-16 05:12

mmpose

MMPose是一款基于PyTorch的姿态分析的开源工具箱，详细介绍请参照下图与项目readme

github项目地址:https://github.com/open-mmlab/mmpose

<img src="https://img.saraba1st.com/forum/202307/16/051240ai9t9edy9tgedkrg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230716-051021__01.jpg</strong> (708.4 KB, 下载次数: 0)

下载附件

2023-7-16 05:12 上传

<img src="https://img.saraba1st.com/forum/202307/16/051240hckeoz8seqlaqhgh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230716-051021__02.jpg</strong> (158.11 KB, 下载次数: 0)

下载附件

2023-7-16 05:12 上传

<img src="https://img.saraba1st.com/forum/202307/16/051317gtcc3tuo1gbf3f3t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230716-051306.jpg</strong> (462.32 KB, 下载次数: 0)

下载附件

2023-7-16 05:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 698#       发表于 2023-7-17 05:00

 本帖最后由 Machinery 于 2023-7-17 05:06 编辑 

DiffDreamer

使用条件扩散模型实现一致的无监督单视图场景外推

项目主页:https://primecai.github.io/diffdreamer

github项目仓库:https://github.com/primecai/DiffDreamer

<img src="https://img.saraba1st.com/forum/202307/17/045609p4840ec987c6ccp7.jpg" referrerpolicy="no-referrer">

<strong>99635956-5ccc-49f0-929a-d70a5bd90516.jpg</strong> (83.41 KB, 下载次数: 0)

下载附件

2023-7-17 04:56 上传

场景外推(Scene extrapolation)，通过给定图像来生成逐渐场景外推的新视图的想法是一项有前景但具有挑战性的任务

对于每个预测帧，必须联合解决重绘和3D细化的问题，这是不适定(ill posed)的并且包含高度的模糊性，此外，远距离场景的训练数据很难获得，通常缺乏足够的视图来推断准确的摄影机姿态

本文引入了DiffDreamer，一个无监督框架，能够合成长期的摄影机轨迹的新视图，同时仅需要互联网收集的自然场景图像进行训练

利用引导去噪步骤的随机性，训练扩散模型来细化投影的RGBD图像，但在多个过去和未来的帧上调整去噪步骤以进行推理

实验证明，图像条件扩散模型可以有效地执行远程场景外推，同时保持一致性，明显优于之前基于GAN的方法，DiffDreamer是一种强大且高效的场景外推解决方案，尽管流程中监督有限，但仍能产生令人印象深刻的结果

<img src="https://img.saraba1st.com/forum/202307/17/045631zhxc2xcctkk2c94t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230717-045620.jpg</strong> (68.76 KB, 下载次数: 0)

下载附件

2023-7-17 04:56 上传

框架工作流程的概述图，通过训练图像条件扩散模型，以在给定损坏的图像和缺失区域掩码的情况下执行图像到图像的细化和重绘

在推理时，对三种条件进行随机条件确定：对预先的帧(黑色箭头)进行简单的前向扭曲，通过扭曲未来帧(蓝色箭头)进行锚定条件，以及通过扭曲虚拟帧(红色箭头)进行先行条件确认

通过重复这个渲染-优化-重复工作流程获取给定图像的推理序列，在当前图像的过去和未来帧上施加约束，可以使图像序列生成更连贯平滑

<img src="https://img.saraba1st.com/forum/202307/17/045654s15t7k9m939tziju.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230717-045504.jpg</strong> (627.41 KB, 下载次数: 0)

下载附件

2023-7-17 04:56 上传

失败案例:

<img src="https://img.saraba1st.com/forum/202307/17/050046bgzu5z6og5f59355.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230717-050014.jpg</strong> (140.39 KB, 下载次数: 0)

下载附件

2023-7-17 05:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 699#       发表于 2023-7-17 05:39

openchat

OpenChat是一系列监督微调(SFT)的开源语言模型，旨在验证数据质量对于模型的重要性

github项目主页:https://github.com/imoneoi/openchat

openchat_v2_w权重下载:https://huggingface.co/openchat/openchat_v2_w

openchat_v2权重下载:https://huggingface.co/openchat/openchat_v2

<img src="https://img.saraba1st.com/forum/202307/17/053846bzq90w4rbe9bqc52.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230717-051540.jpg</strong> (444.66 KB, 下载次数: 0)

下载附件

2023-7-17 05:38 上传

相关评估成绩:

<img src="https://img.saraba1st.com/forum/202307/17/053846gwqw42wmt25pmstq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230717-051624.jpg</strong> (117.36 KB, 下载次数: 0)

下载附件

2023-7-17 05:38 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 700#       发表于 2023-7-18 02:31

Copy Is All You Need

复制粘贴式语言建模方法

github项目仓库:https://github.com/gmftbyGMFTBY/Copyisallyouneed

多数的文本生成模型通过从固定词表中按顺序选择单词来组成输出，在本文中，将文本生成过程表述为从现有文本集合中逐步复制文本段的过程(例如单词或短语)

通过计算有意义的文本片段的上下文表示，并使用高效的向量搜索工具包对它们进行索引，文本生成的任务被分解为一系列复制和粘贴操作：在每个时间步骤，从文本集合中寻找合适的文本范围，而不是从独立的词表中进行选择

根据自动和人工评估，标准语言建模基准(WikiText-103)上的实验表明，本文方法实现了更好的生成质量，此外，由于解码步骤的减少，其推理效率可与Token级自回归模型相媲美

同时还表明，本方法可以通过简单地切换到特定于对应领域的文本集合而无需额外的训练来实现有效的对应域自适应

最后观察到，本文方法通过简单地扩展到更大的文本集合而获得了额外的性能提升，同样无需进一步训练

<img src="https://img.saraba1st.com/forum/202307/18/023005unmmwxluwj5hj00a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-021821.jpg</strong> (120.92 KB, 下载次数: 0)

下载附件

2023-7-18 02:30 上传

COG流程概述图，给定前缀文本(The Dune film wasreleased)，COG从文档中检索3个短语(不同颜色)，并从固定词汇中生成3个标记(Before、that和逗号,)组成整个生成答案

将COG方法与以下三个基线进行了比较:
1.Transformer(目前神经语言模型事实上最好的模型)，具体来说，在实验中对预训练的GPT2模型进行了微调
2.kNN-LM，一种检索增强生成模型，通过使用k最近邻(kNN)模型线性插值其下一个标记分布来扩展预训练的神经语言模型
3.RETRO，另一种检索增强生成模型，结合了冻结BERT检索器、可微编码器和分块交叉注意机制来预测下一个标记，由于没有可以访问的预先训练的RETRO模型，因此在WikiText-103数据集上从头训练了RETRO

<img src="https://img.saraba1st.com/forum/202307/18/023019wbrikw99wh88m7p3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-022506.jpg</strong> (98.65 KB, 下载次数: 0)

下载附件

2023-7-18 02:30 上传

WikiText-103测试集的自动评估，对于每个具有核采样的模型，运行10次并记录平均MAUVE和多样性分数

<img src="https://img.saraba1st.com/forum/202307/18/023024k6pw8vvwxbcb01bg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-022621.jpg</strong> (94.08 KB, 下载次数: 0)

下载附件

2023-7-18 02:30 上传

COG方法在WikiText-103测试集上生成的示例，虚线方块表示内容(红色)是从Token词​​表复制的，实心方块表示内容(以蓝色突出显示)是从其他文档复制的

人工评估结果与拓展实验:

<img src="https://img.saraba1st.com/forum/202307/18/023040tcjtwldaa09jft4t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-022820.jpg</strong> (58.78 KB, 下载次数: 0)

下载附件

2023-7-18 02:30 上传

<img src="https://img.saraba1st.com/forum/202307/18/023040o5em0vl155v1ks95.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-022842.jpg</strong> (111.36 KB, 下载次数: 0)

下载附件

2023-7-18 02:30 上传

<img src="https://img.saraba1st.com/forum/202307/18/023040yqzmhnvvgivvznq3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-022922.jpg</strong> (172.99 KB, 下载次数: 0)

下载附件

2023-7-18 02:30 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 701#       发表于 2023-7-18 03:54

SHIP

使用合成提示改进CLIP的零样本泛化能力

github项目地址:https://github.com/mrflogs/SHIP

随着人们对CLIP等预训练视觉语言模型的兴趣日益浓厚，最近的研究重点是使这些模型适应下游任务，尽管取得了有希望的结果，但大多数现有方法需要所有类别的标记数据，由于长尾定律和齐夫定律(Zipf's law)，这可能并不适用于现实世界的应用， 例如，某些类别可能完全缺乏标记数据，比如部分新兴出现的概念

为了解决这个问题，提出了一种名为SHIP的即插即用生成方法来改进现有的微调方法，具体来说，使用变分自编码器(variational autoencoders/VAE)引入了一个生成器，通过将合成的提示和对应的类名输入到CLIP的文本编码器来重建视觉特征

通过这种方式，可以轻松获得仅标签类的合成特征，此后混合标注和合成的特征，使用现成的方法对CLIP进行微调，从基础到新的泛化、跨数据集迁移学习和广义零样本学习的大量实验证明了本方法的优越性

<img src="https://img.saraba1st.com/forum/202307/18/035152gkuxlx7xm6mjjyun.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-034215.jpg</strong> (102.17 KB, 下载次数: 0)

下载附件

2023-7-18 03:51 上传

所提出的模型架构基于VAE框架构建，包括VAE编码器和生成器，在训练阶段，使用CLIP视觉编码器提取图像特征，VAE编码器将其编码为潜在代码z，随后将其限制在先验分布中

接下来，生成器利用编码信息重建输入特征，值得注意的是，此处引入了一种新颖的基于CLIP的生成器，它包含两个子网络：轻量级MLP和冻结的CLIP文本编码器

MLP将潜在代码z转换为局部偏差，随后将其添加到全局可学习提示向量中以构造最终提示，然后将提示与类名一起输入到冻结文本编码器中以获得重建的特征

在生成阶段，从先验分布中采样潜在代码，然后将其与新的类名一起使用以合成相应的特征，使用现成的方法配合基础类名和合成的新类来微调CLIP

<img src="https://img.saraba1st.com/forum/202307/18/035212q7q8ffwyu77ertqf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-034545.jpg</strong> (319.19 KB, 下载次数: 0)

下载附件

2023-7-18 03:52 上传

基础到新类的概括效果，提出的模型在少样本训练集(基础)上进行训练，然后在基础类和新类上进行评估

+SHIP 表示将本文的方法添加到以前的现成方法中，Tip-Adapter的结果未包含在表中，因为它无法对新类进行测试

基础类和新类的平均准确度分别由术语“基础”和“新”表示，而它们的合成平均值表示为H，最佳结果以粗体显示

<img src="https://img.saraba1st.com/forum/202307/18/035326f430bbe3q7uxp0q5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-035304.jpg</strong> (78.95 KB, 下载次数: 0)

下载附件

2023-7-18 03:53 上传

跨数据集迁移学习，这些方法在源数据集(ImageNet)上进行训练，然后在目标数据集上进行评估，表中为目标数据集的平均准确性，为了量化本文方法的性能增益，计算使用了本文方法(CoOp + SHIP)和基线方法(CoOp)获得的结果之间的差异

<img src="https://img.saraba1st.com/forum/202307/18/035409jecalh0z0i0hatuo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-034846.jpg</strong> (137.88 KB, 下载次数: 0)

下载附件

2023-7-18 03:54 上传

消融实验结果

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 702#       发表于 2023-7-18 04:21

do not mask randomly

不要随机遮蔽(MASK)，通过只遮蔽对应领域内关键字进行有效的领域自适应预训练

github项目仓库(待整理):https://github.com/shahriargolchin/do-not-mask-randomly

提出了一种介于通用预训练和微调之间的新范式，任务无关的对应领域内预训练的方法

通过在训练时有选择的遮蔽对应领域内的关键字，即提供目标领域的紧凑表示的单词，使用KeyBERT识别此类关键字，使用六种不同的设置来评估这种范式

结果表明，使用领域内预训练策略进行微调的PLM模型始终优于使用随机遮蔽的领域内预训练方法，同样胜过了遵循常见的预训练然后进行微调的PLM

<img src="https://img.saraba1st.com/forum/202307/18/041919orff97o5ff4r97ol.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-041633.jpg</strong> (335.6 KB, 下载次数: 0)

下载附件

2023-7-18 04:19 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 703#       发表于 2023-7-18 04:33

 本帖最后由 Machinery 于 2023-7-18 04:38 编辑 

MegaWika

数以百万计的报告及其来源，涵盖50种不同语言

相关论文:https://arxiv.org/abs/2307.07049

数据集下载:https://huggingface.co/datasets/hltcoe/megawika

为了促进新的协作式人工智能辅助报告生成模型的开发，引入了MegaWika，由50种不同语言的1300万篇wiki文章及其7100万条参考源材料组成

使用了无数程序处理该数据集，超越了最初的wiki引文提取和自动抓取网络内容，包括翻译了用于跨语言应用程序的非英语文章以及提供用于自动语义分析的FrameNet解析

MegaWika是最大的句子级报告生成资源，也是唯一的多语言报告生成数据集，通过语义分层样本手动分析该资源的质量，最后，为自动报告生成的关键步骤提供基线结果和训练模型：跨语言问答和引文检索

每种语言的Wiki转存均在2022年3月25日至2022年10月20日期间下载，大多数Wiki转存是在2022年4月下载的，这应被视为MegaWika的有效知识截止点

<img src="https://img.saraba1st.com/forum/202307/18/043657m4l1jwmwzhl412wj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-043642.jpg</strong> (330.8 KB, 下载次数: 0)

下载附件

2023-7-18 04:36 上传

<img src="https://img.saraba1st.com/forum/202307/18/043356ra3232a2cv3x8mtc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-043118.jpg</strong> (156.89 KB, 下载次数: 0)

下载附件

2023-7-18 04:33 上传

<img src="https://img.saraba1st.com/forum/202307/18/043356drxzc9hcpohrh5qi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-043129.jpg</strong> (93.82 KB, 下载次数: 0)

下载附件

2023-7-18 04:33 上传

<img src="https://img.saraba1st.com/forum/202307/18/043356giglj8xxvh0kxvjp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230718-043159.jpg</strong> (213.27 KB, 下载次数: 0)

下载附件

2023-7-18 04:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 704#       发表于 2023-7-19 03:23

SKED

基于草图引导的文本3D编辑

项目主页:https://sked-paper.github.io/

代码:coming soon

文本到图像的扩散模型逐渐被引入计算机图形学中，使得最近可以在开放域中开发文本到3D工作流程的方法成为可能

然而，出于交互式编辑的目的，通过简单的文本界面对内容进行定位操作编辑可能会很困难，将用户引导的草图与文本到图像流程相结合，为用户提供更直观的控制是必要的

尽管如此，由于SOTA文本到3D方法依赖于通过任意渲染视图的梯度来优化神经辐射场(NeRF)，因此对草图的调节并不简单

在本文中，提出了SKED，一种用于编辑由NeRF表示的3D形状的技术，SKED仅利用来自不同视图的两个引导草图来改变现有的NERF，编辑区域通过预先训练的扩散模型尊重提示的语义

为了确保生成的输出符合所提供的草图，提出了新的损失函数来生成所需的编辑，同时保留了基础NERF实例中的密度和辐射度，通过几个定性和定量实验证明了提出的方法的有效性

<img src="https://img.saraba1st.com/forum/202307/19/032246am76cu17g3fmyqcq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-031346.jpg</strong> (102.82 KB, 下载次数: 0)

下载附件

2023-7-19 03:22 上传

<img src="https://img.saraba1st.com/forum/202307/19/032257wzwhghutnguewys2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-031358.jpg</strong> (41.68 KB, 下载次数: 0)

下载附件

2023-7-19 03:22 上传

本文方法的草图引导，基于文本的3D编辑方法示例，以预训练的NERF作为输入，多视图草图确定编辑的粗略区域和文本提示，并生成可确定区域的、有意义的编辑

<img src="https://img.saraba1st.com/forum/202307/19/032252t354pmjjj4p3jmmj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-031411.jpg</strong> (86.62 KB, 下载次数: 0)

下载附件

2023-7-19 03:22 上传

SKED概览图，从至少两个视图渲染基本NeRF模型Fo，并在它们上绘制草图(Ci)，编辑算法的输入是将这些草图预处理为蒙版(Mi)和文本提示

在与DreamFusion类似的每次迭代中，渲染随机视图并应用分数蒸馏损失(Score Distillation Loss)以在语义上与文本提示对齐，并计算Lpres，通过将Fe的密度和颜色输出限制为与远离草图区域的Fo相似，来保留基本NERF的密度和辐射度

最后，使用草图视图的对象遮罩渲染来定义Lsil，这种损失可以确保对象的蒙版占据草图区域

相关评估结果:

<img src="https://img.saraba1st.com/forum/202307/19/032315kin44ezf2ar4ef3e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-032117.jpg</strong> (278.69 KB, 下载次数: 0)

下载附件

2023-7-19 03:23 上传

<img src="https://img.saraba1st.com/forum/202307/19/032315vq6ao1du3hrzorhu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-032131.jpg</strong> (285.62 KB, 下载次数: 0)

下载附件

2023-7-19 03:23 上传

消融实验效果:

<img src="https://img.saraba1st.com/forum/202307/19/032331ms6hydb5ydn8zgnm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-032145.jpg</strong> (129.35 KB, 下载次数: 0)

下载附件

2023-7-19 03:23 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 705#       发表于 2023-7-19 03:48

Retentive Network

Transformer大型语言模型的的后继者

相关论文:https://arxiv.org/abs/2307.08621

智源相关论文文章:https://hub.baai.ac.cn/view/27935

微软相关系列构架项目代码库:https://github.com/microsoft/unilm

在这项工作中，提出了Retentive Network(RetNet)作为大型语言模型的基础架构，同时实现训练并行性、低成本推理和良好的性能

研究组从理论上得出了递归和注意力之间的联系，然后，提出了序列建模的Retentive机制，它支持三种计算范式，即并行、递归和分块递归

具体来说，并行表示允许训练并行性，循环递归表示可实现低成本O(1)推理，从而在不牺牲性能的情况下提高解码吞吐量、降低延迟和提高GPU显存利用效率，分块递归表示有助于进行具有线性复杂度的高效长序列建模，其中每个块在递归总结块的同时并行编码

语言建模的实验结果表明，RetNet取得了良好的扩展效果、并行训练、低成本部署和高效推理，有趣的特性可以使RetNet成为大型语言模型Transformer的有力继承者

<img src="https://img.saraba1st.com/forum/202307/19/034842deh77ynwgbmfd1k1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-034700.jpg</strong> (76.54 KB, 下载次数: 0)

下载附件

2023-7-19 03:48 上传

<img src="https://img.saraba1st.com/forum/202307/19/034842xgljgclmgjioivkv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-034818.jpg</strong> (269.99 KB, 下载次数: 0)

下载附件

2023-7-19 03:48 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 706#       发表于 2023-7-19 04:02

AlpaGasus

用更少的数据训练更好的Alpaca

项目主页:https://lichang-chen.github.io/AlpaGasus/

大型语言模型(LLM)通过对监督指令/响应数据进行指令微调(IFT)来获得指令跟随能力，然而，广泛使用的IFT数据集(例如Alpaca的52k数据)包含许多低质量实例，这些实例具有不正确或不相关的响应，这对IFT具有误导性和有害性

在本文中，提出了一种简单有效的数据选择策略，该策略使用强大的LLM(例如ChatGPT)自动识别和删除低质量数据

为此，引入了AlpaGasus，它通过仅对从52k Alpaca数据中过滤出来的9k高质量数据进行微调，根据GPT-4在多个测试集上的评估，AlpaGasus显着优于原始Alpaca，并且其13B参数变体在测试任务上与其教师LLM(即Text-Davinci-003)的性能匹配超过90%

它还提供了5.7倍地更快的训练速度，将7B参数版本的Alpaca的训练时间从80分钟(相对于Alpaca)减少到14分钟

总体而言，AlpaGasus 展示了一种新颖的以数据为中心的IFT范例，该范例可普遍应用于指令调整数据，从而实现更快的训练和更好的指令跟随模型

<img src="https://img.saraba1st.com/forum/202307/19/040213p4ehhe7hujhayau3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-040031.jpg</strong> (203.17 KB, 下载次数: 0)

下载附件

2023-7-19 04:02 上传

<img src="https://img.saraba1st.com/forum/202307/19/040213aj2827f2rm0eerfa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-040115.jpg</strong> (68.59 KB, 下载次数: 0)

下载附件

2023-7-19 04:02 上传

<img src="https://img.saraba1st.com/forum/202307/19/040213q0tsiooq1qtedriz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-040128.jpg</strong> (110.2 KB, 下载次数: 0)

下载附件

2023-7-19 04:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 707#       发表于 2023-7-19 04:36

BuboGPT

在多模态大型语言模型中实现视觉基准(Visual Grounding)

项目主页:https://bubo-gpt.github.io/

github项目主页:https://github.com/magic-research/bubogpt

模型权重下载:https://huggingface.co/magicr/BuboGPT-ckpt/resolve/main/bubogpt_7b.pth

相关数据集:https://huggingface.co/datasets/magicr/BuboGPT/

LLM在通过语言与人类互动方面表现出了卓越的能力，特别是使用了指令跟随数据后，LLM的最新进展，例如MiniGPT-4、LLaVA 和X-LLM等，通过结合图像、视频和语音等多模态输入，进一步扩大了它们的能力

尽管它们能够有效地生成对给定模态信号的精确和详细的语言理解，但这些LLM放弃了对输入的特定部分进行基准(ground)对齐的能力，因此只能构建粗粒度的映射

然而，文本与其他模态之间明确且信息丰富的对应关系不仅会改善用户体验，而且有助于扩展多模态LLM的应用场景

本文提出BuboGPT，一种具有视觉基础的多模态LLM，可以在视觉、音频和语言之间进行跨模态交互，提供对视觉对象和其他给定模态的细粒度理解，因此，当BuboGPT为某个对象生成响应或描述时，它能够指出该对象在图像中的具体位置

本文贡献有两个方面：
1.基于SAM的现成视觉基础模块，可提取句子中的实体并在图像中找到相应的掩码
2.两阶段训练方案和指令数据集，以赋予文本-图像-音频联合理解，实际实验表明，BuboGPT在与人类交互过程中实现了令人印象深刻的多模态理解和视觉基准能力，当提供任意模态组合(对齐或未对齐)时，它始终表现良好

<img src="https://img.saraba1st.com/forum/202307/19/043643euv6d2t3orei43o7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-043150.jpg</strong> (86.29 KB, 下载次数: 0)

下载附件

2023-7-19 04:36 上传

如图所示，通过对文本、视觉和音频进行联合多模态理解和聊天，这是通过学习以及与预先训练的Vicuna对齐的共享表征空间来实现的，还构建了一个现成的视觉基准流程来探索不同视觉对象和模态之间的细粒度关系

训练过程与使用实例:

<img src="https://img.saraba1st.com/forum/202307/19/043631fu7siedxohs23ed2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-043328.jpg</strong> (189.1 KB, 下载次数: 0)

下载附件

2023-7-19 04:36 上传

<img src="https://img.saraba1st.com/forum/202307/19/043631vcxn99z80s3nra2f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-043503__01.jpg</strong> (908.85 KB, 下载次数: 0)

下载附件

2023-7-19 04:36 上传

<img src="https://img.saraba1st.com/forum/202307/19/043631mg3kdkl8mq5rgp13.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-043503__02__01.jpg</strong> (464.8 KB, 下载次数: 0)

下载附件

2023-7-19 04:36 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 708#       发表于 2023-7-19 04:50

TOAST

通过注意力引导进行迁移学习

github项目代码仓库:https://github.com/bfshi/TOAST

迁移学习涉及使预先训练的模型适应新的下游任务，然而在实际情况中观察到当前的迁移学习方法通​​常无法关注与任务相关的特征

<img src="https://img.saraba1st.com/forum/202307/19/044915yxmx4zbj9lslbqss.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-044538.jpg</strong> (113.57 KB, 下载次数: 0)

下载附件

2023-7-19 04:49 上传

在这项工作中，探索了重新聚焦模型注意力以进行迁移学习，引入自上而下的注意力引导(TOAST/Top-Down Attention Steering)，这是一种新颖的迁移学习算法，它可以保持预训练的主干网络冻结的同时，在输出中选择与任务相关的特征，并将这些特征反馈给模型以将模型注意力引导到与任务有关的特征中

通过重新集中注意力，TOAST在许多迁移学习基准上取得了SOTA结果，同时只具有少量的可调参数，与完全微调、LoRA、提示微调相比，TOAST显著提高了一系列细粒度视觉分类数据集的性能(例如FGVC数据集上的结果81.1%-&gt;86.2%)

TOAST在指令跟随语言生成方面也优于完全微调的Alpaca和Vicuna模型

<img src="https://img.saraba1st.com/forum/202307/19/044922rpwd5iixxiohtgju.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-044557.jpg</strong> (86.36 KB, 下载次数: 0)

下载附件

2023-7-19 04:49 上传

TOAST冻结预先训练的主干并调整自上而下的注意力模块以重新集中模型的注意力

<img src="https://img.saraba1st.com/forum/202307/19/044940k2l0lbhrsblxmrsx.jpg" referrerpolicy="no-referrer">

<strong>20230719_044931.jpg</strong> (155.46 KB, 下载次数: 0)

下载附件

2023-7-19 04:49 上传

此外，还可以将其应用到LLaMA等语言模型中，实验发现用TOAST调优的LLaMA可以实现比Alpaca和Vicuna更高的性能

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 709#       发表于 2023-7-19 05:07

Humans in 4D

用Transformer重建和追踪人体

项目主页:https://shubham-goel.github.io/4dhumans/

github项目代码仓库:https://github.com/shubham-goel/4D-Humans

Demo演示:https://huggingface.co/spaces/brjathu/HMR2.0

colab:https://colab.research.google.com/drive/1Ex4gE5v1bPR3evfhtG7sDHxQGsWwNwby

提出了一种方法，只要给定任何真实自然场景的视频，就可以联合重建3D底层人体，并随着时间的推移跟踪这些人体

通过使用一个基于人体网格重建的完全“改造”的网络版本，HMR 2.0推进了最先进的技术，并展示了分析过去难以从单个图像重建的不寻常姿态的能力

为了分析视频，通过使用HMR 2.0的3D重建作为以3D运行的跟踪系统的输入，能够与多人交互并通过遮挡事件保持身份

4DHumans在通过单眼视频跟踪人物方面取得了SOTA结果，此外还展示了HMR 2.0在动作识别下游任务上的有效性，与之前基于姿态的动作识别方法相比取得了显著改进

在演示中，在左侧显示输入视频，在右侧显示重建的人类，没有任何时间平滑，颜色指示随时间变化的轨迹身份，本方法在能见度差、极端截断和极端遮挡的情况下，可靠地处理常见和不常见的人体姿态

<img src="https://img.saraba1st.com/forum/202307/19/050645r2b2w28wdwba8sv2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-050358.jpg</strong> (130.3 KB, 下载次数: 0)

下载附件

2023-7-19 05:06 上传

<img src="https://img.saraba1st.com/forum/202307/19/050645x50e4mmmecc0wcx3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-050408.jpg</strong> (275.38 KB, 下载次数: 0)

下载附件

2023-7-19 05:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 710#       发表于 2023-7-19 05:46

 本帖最后由 Machinery 于 2023-7-19 05:50 编辑 

SEED

在大型语言模型中种下视觉的种子

github项目仓库:https://github.com/AILab-CVC/SEED

本文提出SEED，这是一种精心设计的图像标记器，它为大型语言模型(LLM)提供了查看和绘图的能力

图像标记器的研究在此前陷入了僵局，因为采用量化视觉标记的框架由于多模态理解(与 BLIP-2 等相比)或生成(与稳定扩散等相比)方面的性能和收敛性不佳而失去了重要性

尽管存在局限性，研究组仍然对其统一视觉和文本表示的自然能力充满信心，利用了LLM的原始训练配方促进可扩展的多模态训练

在本研究中确定了SEED架构和训练的两个关键原则，可有效简化后续与LLM的衔接:
1.图像标记应该独立于2D物理块的位置，而是以1D因果依赖性生成，实际表现中符合LLM从左到右自回归预测机制一致的内在相互依赖性
2.图像标记应捕获与单词语义抽象程度一致的高级语义，并在标记器训练阶段针对区分性和重建进行优化， 因此，现成的LLM能够通过高效的LoRA调整结合SEED来执行图像到文本和文本到图像的生成

全面的多模态预训练和指令调整可能会产生更好的结果，留待未来研究，此文版本的SEED使用了64 个V100 GPU和500万公开可用的图像文本对构建，在5.7天内完成了训练

初步研究强调了离散视觉标记在多功能多模态LLM中的巨大潜力以及适当的图像标记器在更广泛的研究中的重要性

<img src="https://img.saraba1st.com/forum/202307/19/054450zgty958dcqzyuqi5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-053538.jpg</strong> (81.25 KB, 下载次数: 0)

下载附件

2023-7-19 05:44 上传

所提出的SEED是一个离散图像标记器，可以生成具有一维因果依赖性和高级语义的量化视觉编码，SEED视觉标记使LLM能够通过交错图像文本数据的多模态自回归来执行视觉理解和生成

<img src="https://img.saraba1st.com/forum/202307/19/054457bq3n7td6mdl3lsdj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-053734.jpg</strong> (114.3 KB, 下载次数: 0)

下载附件

2023-7-19 05:44 上传

SEED分词器概览图，可以生成具有因果依赖性和高级语义的离散视觉编码

<img src="https://img.saraba1st.com/forum/202307/19/054609hougqgokrrgdbfgo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-053923.jpg</strong> (55.33 KB, 下载次数: 0)

下载附件

2023-7-19 05:46 上传

零样本图像文本检索的评估，因果编码是量化的因果嵌入

<img src="https://img.saraba1st.com/forum/202307/19/054615qg0gaa4llcp2za0o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-054005.jpg</strong> (561.14 KB, 下载次数: 0)

下载附件

2023-7-19 05:46 上传

SEED标记器的重建图像(原始图像 → SEED 标记化 → 因果视觉编码 → SEED 去标记化 → 重建图像)，在语义上与原始输入图像一致

<img src="https://img.saraba1st.com/forum/202307/19/055020hu0e4b0idsby30ze.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-054956.jpg</strong> (70.16 KB, 下载次数: 0)

下载附件

2023-7-19 05:50 上传

使用高效LoRA调整对SEED-OPT2.7B进行多模态自回归训练的概述，仅使用64 个 V100 GPU和500万个图像字幕对，在44小时内完成了训练

对比评估与生成实例:

<img src="https://img.saraba1st.com/forum/202307/19/054641zbbzedjlql8j2be8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-054308.jpg</strong> (378.07 KB, 下载次数: 0)

下载附件

2023-7-19 05:46 上传

<img src="https://img.saraba1st.com/forum/202307/19/054641eitdd5iiuyujd5oj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230719-054324.jpg</strong> (522.74 KB, 下载次数: 0)

下载附件

2023-7-19 05:46 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 711#       发表于 2023-7-20 02:43

Llama 2

Llama 2是一组经过预训练和微调的大型语言模型(LLM)，参数规模从70亿到700亿不等，经过微调的LLM(型号Llama 2-Chat)在对话任务上进行了优化，在测试中Llama 2在大多数基准上都优于开源聊天模型，并且根据对有用性和安全性的人工评估，可能是闭源模型的合适替代品

项目主页:https://ai.meta.com/llama/

权重下载地址:https://huggingface.co/meta-llama

<img src="https://img.saraba1st.com/forum/202307/20/024154clrhu2rndi6u5ohq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-020025.jpg</strong> (126.61 KB, 下载次数: 0)

下载附件

2023-7-20 02:41 上传

与其他开源和闭源模型相比，Llama 2-Chat的有用性人类评估结果，人类评估者在约4k数量的提示(包括单轮对话和多轮对话)上比较了不同模型生成结果

<img src="https://img.saraba1st.com/forum/202307/20/024200vvxi7fc00zxlol0t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-020040.jpg</strong> (73.14 KB, 下载次数: 0)

下载附件

2023-7-20 02:42 上传

根据GPT-4评估，使用了商业许可的基线和Llama 2-Chat之间的有用性和安全性的胜率结果，为了补充人类评估的不足，使用了一个更强大的模型，不受内部指导的影响，绿色区域表示GPT-4评估中LLaMA2模型更好，为了消除平局，使用了胜利/(胜利+失败)计算，其中随机交换了模型呈现给GPT-4的响应结果顺序以减轻自动评估的偏差

<img src="https://img.saraba1st.com/forum/202307/20/024206y5e9jtyvpt28mohp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-020542.jpg</strong> (53.08 KB, 下载次数: 0)

下载附件

2023-7-20 02:42 上传

Llama 2-Chat与其他开源或闭源模型相比的安全人类评估结果，人类评估者通过约2000 个对抗性提示(包括单轮对话和多轮对话)来判断模型代是否存在安全违规行为

向公众发布以下模型用于研究和商业用途:
1.Llama 2，Llama 1的更新版本，基于新的公开可用数据组合进行训练，将预训练语料库的大小增加了40%，将模型的上下文长度加倍，采用了分组查询注意力(grouped-query attention)，发布了7B、13B 和70B参数的Llama 2变体，实际中还训练了34B变体，但只在本文中对此进行了报告，并未发布

 2.Llama 2-Chat，Llama 2的微调版本，针对对话数据集进行了优化，同时也发布了7B、13B 和 70B参数的该模型的对应变体

<img src="https://img.saraba1st.com/forum/202307/20/024215lxtwe7n55qhewp0e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-021644.jpg</strong> (55.29 KB, 下载次数: 0)

下载附件

2023-7-20 02:42 上传

Llama 2-Chat 的训练过程：首先使用公开的在线资源对Llama 2进行预训练，接下来通过应用监督微调创建了Llama 2-Chat的初始版本，随后使用人类反馈强化学习(RLHF/Reinforcement Learning with Human Feedback)方法逐步细化对话，特别是通过拒绝采样(rejection sampling)和近端策略优化(PPO/Proximal Policy Optimization)来迭代完善模型，在整个RLHF阶段，逐步的累积迭代的奖励模型数据的同时增强模型，确保奖励模型的分布正确

训练过程模型总共接触了近两万亿Token的语料库，在预训练中采用了Llama 1中的大部分预训练设置和模型架构，这意味着使用了标准Transformer架构，RMSNorm应用预归一化(pre-normalization)，使用了SwiGLU激活函数和旋转位置嵌入，与Llama 1的主要架构差异包括增加的上下文长度和分组查询注意力(GQA/grouped-query attention)，同时在之后通过消融实验详细说明了这些差异，以证明它们的重要性

<img src="https://img.saraba1st.com/forum/202307/20/024226ktvwo16z11ukuoz1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-022053.jpg</strong> (61.67 KB, 下载次数: 0)

下载附件

2023-7-20 02:42 上传

Llama 2系列型号，所有模型均使用4M Token的全局批量大小进行训练，更大的模型(34B和70B)使用了分组查询注意力来提高推理可扩展性

<img src="https://img.saraba1st.com/forum/202307/20/024234lz7jpj1ljk1lph1h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-022607.jpg</strong> (75.27 KB, 下载次数: 0)

下载附件

2023-7-20 02:42 上传

Llama 2模型的训练Loss，比较了Llama 2系列模型的训练Loss，观察到在两万亿Token上进行预训练后，模型仍然没有显示出任何饱和的迹象

分词器使用了与Llama 1相同的分词器，采用了字节对编码(BPE/bytepair encoding)算法，并使用了SentencePiece的实现，与Llama 1一样，将所有数字拆分为单独的数字，并使用字节来分解未知的UTF-8 字符，总词汇量为32k个Token

<img src="https://img.saraba1st.com/forum/202307/20/024255ea5wjee4saeuweea.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-022840.jpg</strong> (128.64 KB, 下载次数: 0)

下载附件

2023-7-20 02:42 上传

与开源基础模型相比，分组的学术基准的总体表现

<img src="https://img.saraba1st.com/forum/202307/20/024301jx99qza42fxqj2pa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-023501.jpg</strong> (70.35 KB, 下载次数: 0)

下载附件

2023-7-20 02:43 上传

与闭源模型在学术基准上的比较结果

<img src="https://img.saraba1st.com/forum/202307/20/024306rf86u0e5uc9h5pdu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-023636.jpg</strong> (79.89 KB, 下载次数: 0)

下载附件

2023-7-20 02:43 上传

预训练数据中的语言分布，百分比&gt;=0.005%，大多数数据都是英文的，这意味着Llama 2在英语用例中表现最佳，大的未知类别部分是由编程代码数据组成的

<img src="https://img.saraba1st.com/forum/202307/20/024311mnaeirqov0revvr0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-023947.jpg</strong> (52.09 KB, 下载次数: 0)

下载附件

2023-7-20 02:43 上传

预训练LLM在自动安全基准上的评估，对于TruthfulQA，展示了既真实又信息丰富的生成的百分比(越高越好)，对于ToxiGen，给出了有毒生成的百分比(越小越好)

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 712#       发表于 2023-7-20 02:50

Amazon Berkeley Objects (ABO) Dataset

经CC BY 4.0许可的Amazon产品数据集，包含元数据、目录图像与3D模型

项目主页:https://amazon-berkeley-objects.s3.amazonaws.com/index.html

通过指定产品名称、产品类型关键字并选择仅显示具有360°视图图像或3D模型的产品，ABO中共计147702个产品

<img src="https://img.saraba1st.com/forum/202307/20/025049socwmhjln8rjf5lw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-024830.jpg</strong> (219.46 KB, 下载次数: 0)

下载附件

2023-7-20 02:50 上传

<img src="https://img.saraba1st.com/forum/202307/20/025049acmagr75mgmb9crf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-024918.jpg</strong> (164.79 KB, 下载次数: 0)

下载附件

2023-7-20 02:50 上传

<img src="https://img.saraba1st.com/forum/202307/20/025049loozto2migpeeemi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-024957.jpg</strong> (149.93 KB, 下载次数: 0)

下载附件

2023-7-20 02:50 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 713#       发表于 2023-7-20 03:14

Text2Tex

通过扩散模型进行文本驱动的3D纹理合成

项目主页:https://daveredrum.github.io/Text2Tex/

相关论文:https://daveredrum.github.io/Text2Tex/static/Text2Tex.pdf

github项目代码库:https://github.com/daveredrum/Text2Tex

Text2Tex根据给定的文本提示生成3D网格模型的高质量纹理，通过将修复结合到预先训练的深度感知图像扩散模型中，可以从多个视点逐步合成高分辨率的部分纹理

为了避免伪影，还提出了一种自动视图序列生成方案来确定更新部分纹理的下一个最佳视图，大量实验表明，Text2Tex明显优于现有的文本驱动方法和基于GAN的方法

<img src="https://img.saraba1st.com/forum/202307/20/031251mzhn67zfh6d14zx7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-025406.jpg</strong> (106.61 KB, 下载次数: 0)

下载附件

2023-7-20 03:12 上传

在Text2Tex中，通过“生成然后优化”方案逐步的生成纹理

在渐进式纹理生成中，首先从初始预设的视点渲染对象，再根据输入的提示，通过深度到图像扩散模型生成新的外观，并将生成的图像投影回部分纹理，然后重复这个过程，直到最后一个预设视点输出初始纹理网格，在随后的纹理细化中，从一系列自动选择的视点更新初始纹理，以细化拉伸和模糊的伪像

<img src="https://img.saraba1st.com/forum/202307/20/031352l6qplhqphihyvjpy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-030504.jpg</strong> (71.67 KB, 下载次数: 0)

下载附件

2023-7-20 03:13 上传

将当前视图动态划分生成掩码，以指导深度感知修复模型，对于“新”区域，从高斯白噪声中对新对象外观进行去噪，对于“更新”区域，通过对部分噪声图像片段进行去噪来细化先前的纹理，将纹理冻结在该视图的“保留”区域中

<img src="https://img.saraba1st.com/forum/202307/20/031401ka0uhpl6a4gh33au.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-030635.jpg</strong> (102.67 KB, 下载次数: 0)

下载附件

2023-7-20 03:14 上传

在细化阶段，通过在每一步选择具有最大归一化面积的“更新”区域的视点来自动确定视点的顺序，以中等扩散去噪强度更新“更新”区域中的2D视图，然后在每个细化步骤结束时，更新的对象外观被反投影到纹理空间

<img src="https://img.saraba1st.com/forum/202307/20/031406wkrzpfnpklzp4fph.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-030853.jpg</strong> (187.62 KB, 下载次数: 0)

下载附件

2023-7-20 03:14 上传

Objaverse上的定性比较，将本文方法的纹理网格与CLIPMesh、Text2Mesh、Latent-Paint以及Objaverse的纹理进行比较，与基线相比，本文方法针对输入的几何形状生成了更加一致和详细的3D纹理结果，彩色图像效果最佳

相关评估与实例效果:

<img src="https://img.saraba1st.com/forum/202307/20/031429y4u1howup6o1wou5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-030954.jpg</strong> (247.82 KB, 下载次数: 0)

下载附件

2023-7-20 03:14 上传

<img src="https://img.saraba1st.com/forum/202307/20/031429wdiv4dovh9uvz5k0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-031020.jpg</strong> (334.68 KB, 下载次数: 0)

下载附件

2023-7-20 03:14 上传

<img src="https://img.saraba1st.com/forum/202307/20/031429h4xkoa1ao1o3oj36.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-025727.jpg</strong> (146.12 KB, 下载次数: 0)

下载附件

2023-7-20 03:14 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 714#       发表于 2023-7-20 04:10

 本帖最后由 Machinery 于 2023-7-20 04:11 编辑 

COLLIE

约束文本生成任务的系统化构建

项目主页:https://collie-benchmark.github.io/

github项目地址库:https://github.com/princeton-nlp/Collie

约束下的文本生成引起了人们对自然语言处理的兴趣日益浓厚，特别是随着大型语言模型的能力迅速提高，然而，现有的约束生成基准通常侧重于固定约束类型(例如生成包含某些单词的句子等)，这已被证明对于GPT-4等SOTA语言模型来说很容易

本文提出了COLLIE，一个基于语法的框架，允许指定具有不同生成级别(单词、句子、段落、总结)和语言建模挑战(例如语言理解、逻辑推理、计数、语义规划)的丰富的组合约束

还开发了在给定约束结构和原始文本语料库的情况下自动提取任务实例的工具，使用COLLIE编译了COLLIE-v1 数据集，其中包含1132个实例，13个约束结构

对五种最先进的指令调整语言模型进行系统实验，并分析它们的性能以揭示缺陷，COLLIE被设计为可扩展且轻量级的，有益于社区开发更先进的约束生成系统与研究

<img src="https://img.saraba1st.com/forum/202307/20/040927mmwckb6crz2mqonk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230720-040325.jpg</strong> (87.09 KB, 下载次数: 0)

下载附件

2023-7-20 04:09 上传

COLLIE 框架用于轻松约束、规范结构、示例提取、指令渲染和模型评估，整个流程的步骤描述如上图:
规范：用户指定约束结构，没有特定的目标值(表示为*)
提取：约束结构用于从包含目标值的文本语料库中提取示例
渲染：约束结构和目标值被渲染成自然语言指令提示
评估：根据约束和提取的示例评估模型的生成结果

在此示例中，模型(gpt-3.5-turbo)违反了约束，超出了字数限制，并将“mankind”一词留在末尾而不是指定的位置

<img src="https://img.saraba1st.com/forum/202307/20/040946jnfnlgn0xjrhzrrr.jpg" referrerpolicy="no-referrer">

<strong>20230720_040355.jpg</strong> (81.38 KB, 下载次数: 0)

下载附件

2023-7-20 04:09 上传

COLLIE是一种牧羊犬，可以帮助引导美洲驼和羊驼等家养动物，本方法的核心思想是：一个简单的语法来指定多样化的组合文本约束，只需要跨文本级别（单词/句子/段落/总结）的“计数”和“位置”

<img src="https://img.saraba1st.com/forum/202307/20/040951wjwoba0k05ezopbk.jpg" referrerpolicy="no-referrer">

<strong>20230720_040511.jpg</strong> (40.07 KB, 下载次数: 0)

下载附件

2023-7-20 04:09 上传

虽然在COLLIE-v1数据集上GPT-4远远强于其他LLM，但在语言理解、逻辑推理、计数方面的挑战，其表现值50.9%也远非完美

<img src="https://img.saraba1st.com/forum/202307/20/040957e84ahrlufy6uafsp.jpg" referrerpolicy="no-referrer">

<strong>20230720_040723.jpg</strong> (102.1 KB, 下载次数: 0)

下载附件

2023-7-20 04:09 上传

同时还发布了COLLIE框架，您可以在其中轻松定义、提取、渲染、评估新的约束，这些约束对于LLM来说可能更有趣、更有创意、更具挑战性，而无需人工数据收集

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 715#       发表于 2023-7-21 03:03

FABRIC

通过迭代反馈对扩散模型进行个性化定制

github项目地址库:https://github.com/sd-fabric/fabric

在现今这个视觉内容生成日益由机器学习所驱动的时代，将人类反馈集成到生成模型中为用户增强体验，为模型增加输出质量是不可或缺的

本研究探讨了将迭代人类反馈纳入基于扩散的文本到图像模型的生成过程的策略并提出了FABRIC(Feedback via Attention-Based Reference Image Conditioning)，这是一种适用于各种流行扩散模型的免训练方法，它利用最广泛使用的扩散模型架构中存在的自注意力层来调节一组反馈图像上的扩散过程

为了确保对本方法进行严格评估，引入了全面的评估方法，提供了一种稳健机制来量化集成人类反馈的生成模型的性能

通过详尽的实验与评估分析，生成结果在多轮迭代反馈中得到了改善，隐式地优化了任意用户偏好，潜在应用方面可以扩展到个性化内容创建和定制等领域

<img src="https://img.saraba1st.com/forum/202307/21/030142ygd3u8dgfsxxfw3y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-024632.jpg</strong> (75.91 KB, 下载次数: 0)

下载附件

2023-7-21 03:01 上传

FABRIC的图示，本方法不仅可以根据文本提示生成图像，还可以根据多轮生成过程中表达的用户偏好来生成图像

<img src="https://img.saraba1st.com/forum/202307/21/030227n9p9u9yypupotuxy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-024832.jpg</strong> (96.53 KB, 下载次数: 0)

下载附件

2023-7-21 03:02 上传

FABRIC通过基于注意力的调节机制结合用户反馈来改进模型生成的结果

<img src="https://img.saraba1st.com/forum/202307/21/030231vhn8mhnxxh3k311w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-025623.jpg</strong> (96.89 KB, 下载次数: 0)

下载附件

2023-7-21 03:02 上传

在研究中对两个版本的FABRIC进行了评估

第一个版本称为 FABRIC，建立在微调的Stable Diffusion1.5检查点(dreamlike-photoreal-2.0)之上

第二个版本称为FABRIC+HPS LoRA，通过将其应用在人类偏好评分(HPS/Human Preference Score)的Stable Diffusion1.5的LoRA之上，进一步增强了FABRIC方法，之所以选择将FABRIC+HPS LoRA版本纳入评估中，是因为它已被证明能够生成与人类偏好紧密匹配的图像

<img src="https://img.saraba1st.com/forum/202307/21/030237ta0g0azmq066kqkk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-025739.jpg</strong> (200.91 KB, 下载次数: 0)

下载附件

2023-7-21 03:02 上传

基于目标图像的反馈选择的结果，正反馈可以提高目标与基线的相似度，并且使用正反馈和负反馈可以进一步提高目标相似，与此同时，任何类型的反馈都会**降低生成图像的多样性

<img src="https://img.saraba1st.com/forum/202307/21/030243usszl29ell9niski.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-025921.jpg</strong> (83.23 KB, 下载次数: 0)

下载附件

2023-7-21 03:02 上传

将提示进行dropout似乎是用CLIP相似性换取生成分布更多多样性的有效方法

<img src="https://img.saraba1st.com/forum/202307/21/030248nljlccqflqlklfal.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-025950.jpg</strong> (144.37 KB, 下载次数: 0)

下载附件

2023-7-21 03:02 上传

FABRIC：参考图像在一定步长之前都是噪声，在去噪过程中将提取的键和值注入到U-Net的自注意力中

使用实例:

<img src="https://img.saraba1st.com/forum/202307/21/030258xopsistjmplzepbr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-025939.jpg</strong> (509.69 KB, 下载次数: 0)

下载附件

2023-7-21 03:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 716#       发表于 2023-7-21 03:14

DialogStudio

为对话式人工智能打造最丰富、最多样化的统一数据集集合

github项目主页:https://github.com/salesforce/DialogStudio

尽管对话人工智能取得了进步，但语言模型在处理不同的对话任务时遇到了挑战，而且现有的对话数据集通常缺乏多样性和全面性

为了解决这些问题，本文引入了DialogStudio：最大、最多样化的对话数据集集合，以一致的格式统一，同时保留其原始信息

集合包含来自开放域对话、面向任务的对话、自然语言理解、对话推荐、对话总结和基于知识的准确对话的相关数据，使其成为对话研究和模型训练的极其丰富和多样化的资源

为了进一步增强DialogStudio的实用性，确定了每个数据集的许可证，并为选定的对话设计领域感知提示，以促进指令感知微调

此外，还使用数据集集合开发了新的对话式AI模型，并且在零样本和少样本学习场景中的实验证明了DialogStudio的优越性

为了提高透明度并支持数据集和基于任务的研究以及语言模型预训练，DialogStudio将公开发布

数据集分布与模型数据说明:

<img src="https://img.saraba1st.com/forum/202307/21/031239caruyylyrir7tt7b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-031007.jpg</strong> (419.56 KB, 下载次数: 0)

下载附件

2023-7-21 03:12 上传

<img src="https://img.saraba1st.com/forum/202307/21/031239d8hzh9m10emze081.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-031048.jpg</strong> (167.22 KB, 下载次数: 0)

下载附件

2023-7-21 03:12 上传

DialogStudio样本:

<img src="https://img.saraba1st.com/forum/202307/21/031338t9nb8pc9bqk4zkq8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-031135.jpg</strong> (196.51 KB, 下载次数: 0)

下载附件

2023-7-21 03:13 上传

对应微调模型的评估结果:

<img src="https://img.saraba1st.com/forum/202307/21/031400e9m23qw1mqnq9ctb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-031155.jpg</strong> (125.17 KB, 下载次数: 0)

下载附件

2023-7-21 03:14 上传

<img src="https://img.saraba1st.com/forum/202307/21/031400v514m4xccx5mkc1j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-031210.jpg</strong> (284.42 KB, 下载次数: 0)

下载附件

2023-7-21 03:14 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 717#       发表于 2023-7-21 03:40

Android in the Wild

用于Android设备控制的大规模数据集

github项目代码库:https://github.com/google-research/google-research/tree/master/android_in_the_wild

随着人们对自动设备控制系统越来越感兴趣，这些系统可以解释人类自然语言指令并通过直接控制其用户界面在数字设备上执行它们

本文提出了一种用于自动设备控制研究的数据集，即Android in the Wild(AITW)，它比当前数据集大几个数量级，该数据集包含设备交互的人类演示，屏幕和操作以及相应的自然语言指令

包含715k个事件场景、30k条独特指令、四个版本的Android(v10-13)、具有不同屏幕分辨率的八种设备类型(Pixel 2 XL到Pixel 6)上的操作，包含需要对语言和视觉上下文进行语义理解的多步骤任务

该数据集提出了一个新的挑战：必须从用户界面的视觉外观推断出可用的操作，而且动作空间不是由简单的基于UI元素的动作组成，更包含精确的手势组成(例如，水平滚动以操作小部件)

组织数据集对设备控制系统进行了鲁棒性分析，即系统在新任务描述、新应用程序或新平台版本存在的情况下的表现如何，开发了两个自动代理者并报告了对应整个数据集的性能表现

<img src="https://img.saraba1st.com/forum/202307/21/033909e0zkk1u3kg90i797.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-033554.jpg</strong> (193.22 KB, 下载次数: 0)

下载附件

2023-7-21 03:39 上传

创建AITW的数据收集工作流程，评估者会收到随机选择的指令，之后评估者通过以自然的方式与设备交互来执行任务，除了打字以及主页和后退按钮交互之外，还捕获了精确的手势(用指向手指移动位置的箭头绘制滑动)，对高级事件的事后重新标记可用于生成单步任务

<img src="https://img.saraba1st.com/forum/202307/21/033919iwhqunbaqq7jbjwq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-033605.jpg</strong> (82.45 KB, 下载次数: 0)

下载附件

2023-7-21 03:39 上传

与其他类似数据集的对比

<img src="https://img.saraba1st.com/forum/202307/21/033937ly4ytas6w89qgtsu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-033629.jpg</strong> (89.22 KB, 下载次数: 0)

下载附件

2023-7-21 03:39 上传

<img src="https://img.saraba1st.com/forum/202307/21/033937pi61n60sw1lnp2wr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-033648.jpg</strong> (157.31 KB, 下载次数: 0)

下载附件

2023-7-21 03:39 上传

对于数据集样本的详细统计分析情况

评估结果:

<img src="https://img.saraba1st.com/forum/202307/21/033958ntanatbcbjcycjxq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-033727.jpg</strong> (107.6 KB, 下载次数: 0)

下载附件

2023-7-21 03:39 上传

github说明页:

<img src="https://img.saraba1st.com/forum/202307/21/034011gv8bbggip2hw48hg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-033225.jpg</strong> (254.88 KB, 下载次数: 0)

下载附件

2023-7-21 03:40 上传

<img src="https://img.saraba1st.com/forum/202307/21/034011h11ruo5ono2j1okn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-033258.jpg</strong> (392.94 KB, 下载次数: 0)

下载附件

2023-7-21 03:40 上传

<img src="https://img.saraba1st.com/forum/202307/21/034011uuxph2nxrinoyrrr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230721-033358.jpg</strong> (440.81 KB, 下载次数: 0)

下载附件

2023-7-21 03:40 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 718#       发表于 2023-7-22 05:48

 本帖最后由 Machinery 于 2023-7-22 05:52 编辑 

TokenFlow

一致的扩散特征实现一致的视频编辑

项目主页:https://diffusion-tokenflow.github.io/

github项目仓库:https://github.com/omerbt/TokenFlow

更多补充视频样本和编辑结果:https://diffusion-tokenflow.github.io/sm/supp.html

<img src="https://img.saraba1st.com/forum/202307/22/054437wffh0r0grw9hhzo0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-052110.jpg</strong> (158.27 KB, 下载次数: 0)

下载附件

2023-7-22 05:44 上传

生成式AI的应用最近已扩展到视频领域，然而，当前SOTA视频生成模型在视觉质量和生成内容的控制方面仍然落后于图像模型

本文提出了一个框架，名为TokenFlow，利用文本到图像扩散模型来完成文本驱动的视频编辑任务，具体来说，给定源视频和目标文本提示，TokenFlow会生成遵循目标文本提示的高质量视频，同时保留输入视频的空间布局和运动连续性，实现更连贯的效果

具体实现方法基于一个关键的洞察性观察结果，即可以通过在扩散特征空间中强制一致性对齐来获得编辑视频结果的一致性，显式传播基于帧间对应关系的扩散特征，这些在模型中可以轻松获取，因此本文的方法框架不需要任何训练或微调，并且可以与任何现成的文本到图像编辑方法结合使用

观察到视频的时间一致性水平与其特征表征的时间一致性密切相关，通常的自然视频的特征具有共享的、时间一致的表征，当按帧编辑视频时，这种一致性就会被打破，本方法设法确保了与原始视频特征相同水平的特征一致性

通过在编辑过程时强制跨帧画面的内部扩散特征的一致性，可以实现时间一致的编辑，使用原始视频特征之间的对应关系，跨帧传播一小组经过编辑的特征来实现这一点

<img src="https://img.saraba1st.com/forum/202307/22/054531dhoyihk5dy58hrbh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-052816.jpg</strong> (143.06 KB, 下载次数: 0)

下载附件

2023-7-22 05:45 上传

给定输入视频I，反转每一帧，提取其Token(即自注意力模块的输出特征)，并使用最近邻搜索(nearest-neighbor search)提取视频帧间特征的对应关系

在每个去噪步骤中:
1.从噪声视频J_t中采样关键帧，并使用扩展的注意力块联合编辑它们，生成编辑Token集T_base
2.根据原始视频特征预先计算的对应关系，在视频中传播编辑后的Token

为了对J_t进行去噪，将每个帧馈送到网络，并用第二步获得的Token替换生成的Token

<img src="https://img.saraba1st.com/forum/202307/22/054550psyn6i66shsex9md.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-053114.jpg</strong> (164.97 KB, 下载次数: 0)

下载附件

2023-7-22 05:45 上传

随时间进行扩散特征，左方：给定输入视频(第一行)，对每个帧应用DDIM反演，从ϵθ中最高分辨率的解码层提取特征

之后对从所有帧中提取的特征(即自注意力模块的输出Token)应用PCA，并可视化前三个组件(第二行)，进一步可视化RGB和特征(第三行)的x-t切片(在原始帧上以红色标记)

特征表征在时间上是一致的，相应的区域在视频中使用相似的特征进行编码，中间为通过在每个帧上应用图像编辑方法获得的编辑视频的帧和特征可视化，RGB不一致的模式在特征空间中也很明显(例如狗的身体上)

本方法强制编辑后的视频与原始视频拥有一样的特征一致性，这转化为了RGB空间中连贯且高质量的编辑结果

<img src="https://img.saraba1st.com/forum/202307/22/054601xc1mm13q360mwdze.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-054231.jpg</strong> (194.46 KB, 下载次数: 0)

下载附件

2023-7-22 05:46 上传

细粒度特征对应，从源视频帧中提取的特征(即来自自注意力模块的输出标记)用于重建附近的帧， 这是通过以下方式完成的：
1.在所有层和所有生成时间步骤中，用源视频中最接近的特征交换目标中的每个特征，以及使用最近邻特征在RGB空间中简单进行扭曲，在从最高分辨率解码层提取的源视频特征和目标视频特征之间进行计算，最终目标被忠实地重建，展示了高水平的空间粒度和特征之间的共享内容

视频编辑实例效果:

<img src="https://img.saraba1st.com/forum/202307/22/054618mgu5w58uqeiogx4f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-052835.jpg</strong> (434.97 KB, 下载次数: 0)

下载附件

2023-7-22 05:46 上传

<img src="https://img.saraba1st.com/forum/202307/22/054618rlknunuzl2l91kzy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-053031.jpg</strong> (192.47 KB, 下载次数: 0)

下载附件

2023-7-22 05:46 上传

<img src="https://img.saraba1st.com/forum/202307/22/054740qv0v9cx5ea42zy9y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-054315.jpg</strong> (377.04 KB, 下载次数: 0)

下载附件

2023-7-22 05:47 上传

与其他视频方法的对比效果:

<img src="https://img.saraba1st.com/forum/202307/22/054750gks7skh4hy3he26v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-054325.jpg</strong> (311.52 KB, 下载次数: 0)

下载附件

2023-7-22 05:47 上传

生成质量评估:

<img src="https://img.saraba1st.com/forum/202307/22/054815xe63f1164318keto.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-054344.jpg</strong> (600.07 KB, 下载次数: 0)

下载附件

2023-7-22 05:48 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 719#       发表于 2023-7-22 06:10

 本帖最后由 Machinery 于 2023-7-22 06:12 编辑 

Meta-Transformer

多模态学习的统一框架

项目主页:https://kxgong.github.io/meta_transformer/

github项目代码库:https://github.com/invictus717/MetaTransformer

多模态学习旨在构建能够处理和关联来自多种模态的信息的模型，尽管该领域已经发展多年，但由于各种模态(例如自然语言、2D图像、3D点云、音频、视频、时间序列、表格化数据)之间固有的差别，设计一个统一的神经网络来处理这些模态仍然具有挑战性

在这项工作中，我们提出了一个名为Meta-Transformer的框架，它利用冻结的编码器来执行多模态感知，而无需任何配对的多模态训练数据

在Meta-Transformer中，来自各种模态的原始输入数据被映射到共享Token空间中，允许具有冻结参数的后续编码器提取输入数据的高级语义特征

Meta-Transformer由三个主要组件组成:统一数据分词器、模态共享编码器和下游任务的特定任务头(task-specific heads)，它是第一个使用不配对的数据对跨12种不同模态执行统一学习的框架

不同基准的实验表明，Meta-Transformer可以处理广泛的任务，包括基础感知(文本、图像、点云、音频、视频)、实际应用(X射线、红外、高光谱和IMU)以及数据挖掘(图形、表格和时间序列) 

Meta-Transformer预示着使用Transformer开发统一多模态智能的广阔前景

<img src="https://img.saraba1st.com/forum/202307/22/060953f8u657b95zu83c90.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-060045.jpg</strong> (164.74 KB, 下载次数: 0)

下载附件

2023-7-22 06:09 上传

统一多模态学习，Meta-Transformer利用相同的主干模型来编码自然语言、图像、点云、音频、视频、红外、高光谱、X 射线、时间序列、表格、惯性测量单元(IMU)和图形数据

<img src="https://img.saraba1st.com/forum/202307/22/061014dcc7tbcvcq77dqf7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-060336.jpg</strong> (93.69 KB, 下载次数: 0)

下载附件

2023-7-22 06:10 上传

与其他多模态方法的对比

<img src="https://img.saraba1st.com/forum/202307/22/061031pk1hhhd1xvwv9vxt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-060353.jpg</strong> (120.43 KB, 下载次数: 0)

下载附件

2023-7-22 06:10 上传

Meta-Transformer由数据到序列Token化、统一特征编码和下游任务学习组成，该框架用文本、图像、点云和音频进行说明

<img src="https://img.saraba1st.com/forum/202307/22/061039laasf7gpfxdb777g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-060510.jpg</strong> (119.5 KB, 下载次数: 0)

下载附件

2023-7-22 06:10 上传

数据到序列Token化的图示。 在A中提出了包含分组、卷积和变换过程的元方案，在B-E表示应用元方案进行构建块处理文本、图像、点云和音频

相关评估设置与评估结果:

<img src="https://img.saraba1st.com/forum/202307/22/061252nc9u003cq839mrqn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-061235.jpg</strong> (106.09 KB, 下载次数: 0)

下载附件

2023-7-22 06:12 上传

<img src="https://img.saraba1st.com/forum/202307/22/061055nnnb33q7ejjzenns.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-060821.jpg</strong> (390.38 KB, 下载次数: 0)

下载附件

2023-7-22 06:10 上传

<img src="https://img.saraba1st.com/forum/202307/22/061055c3zggiihjidnz60d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-060832.jpg</strong> (369.56 KB, 下载次数: 0)

下载附件

2023-7-22 06:10 上传

<img src="https://img.saraba1st.com/forum/202307/22/061055mx5zcv28scrr3m8r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-060846.jpg</strong> (194.58 KB, 下载次数: 0)

下载附件

2023-7-22 06:10 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 720#       发表于 2023-7-22 06:31

FLASK

基于对齐技能集(Skill Sets)的细粒度语言模型评估

相关论文:https://arxiv.org/abs/2307.10928

可交互的演示Demo:https://kaistai.github.io/FLASK/

github项目代码库:https://github.com/kaistAI/FLASK

大型语言模型(LLM)的评估具有挑战性，因为符合人类价值观需要多种技能的组合，并且所需的技能集因指令而异

最近的研究以两种方式评估LLM的表现，比如对几个独立基准进行自动评估，或者基于人类或机器的评估，对响应给出总体评分

然而，这两种设置都是粗粒度的评估，没有考虑需要实例使用时技能组合的用户指令的性质，这限制了对LLM真实能力的理解

在本文中引入了FLASK，基于对齐技能集的细粒度语言模型评估(Fine-grained Language Model Evaluation based on Alignment SKill Sets)，一种具有细粒度的评估方法，可用于基于模型和基于人类的评估，将粗粒度评分，分解为实例使用时的需求技能集的级别

具体来说，定义了LLM遵循开放式用户指令所需的12项细粒度技能，并通过为每个实例分配一组技能来构建评估集

此外，通过标注每个实例的目标领域和难度级别，FLASK提供了一个整体视图，根据技能、领域和难度对模型性能进行全面分析，使用FLASK比较了多个开源和专有的LLM，并观察到基于模型的评估和基于人类的评估之间高度相关的发现

FLASK可以使开发人员能够通过分析LLM精通特定技能的因素来更准确地衡量模型性能以及如何改进模型性能，对于从业者来说，FLASK可以通过各种LLM之间的综合比较，针对特定情况推荐合适的模型

1.观察到，即使对于最先进的开源模型，当前开源LLM的逻辑思维和背景知识能力也明显低于专有LLM约25%和10%左右

2.观察到，不同的技能需要不同的模型大小才能有效地获得它们，例如，虽然简洁性和洞察力等技能的获取在一定规模后会达到饱和，但对于较大的模型，逻辑正确性等技能的获取会更有效

3.即使是SOTA专有LLM也在FLASK-HARD集(FLASK评估集的子集)上挣扎，其中仅选择具有挑战性的实例，与整个集的性能相比，某些技能的性能表现下降高达50%

<img src="https://img.saraba1st.com/forum/202307/22/063127pck18wc111c68yc6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-062529.jpg</strong> (347.21 KB, 下载次数: 0)

下载附件

2023-7-22 06:31 上传

<img src="https://img.saraba1st.com/forum/202307/22/063127cu264neqdaavxsbn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-062939.jpg</strong> (218.38 KB, 下载次数: 0)

下载附件

2023-7-22 06:31 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 721#       发表于 2023-7-22 06:46

 本帖最后由 Machinery 于 2023-7-22 06:53 编辑 

SciBench

评估大学水平的大型语言模型科学解决问题的能力

相关论文:https://arxiv.org/abs/2307.10635

github项目地址:https://github.com/mandyyyyii/scibench

最近的大型语言模型(LLM)的在许多数学基准上展现了显著的进步，然而，这些基准大多只针对初中和高中科目，仅包含多项选择题，并且仅限于有限的初等算术运算范围

为了解决这些问题，本文引入了一个扩展的基准测试套件SciBench，旨在系统地检查解决复杂科学问题所需的推理能力

SciBench 包含两个精心策划的数据集：一个开放集，具有来自数学、化学和物理教科书的一系列大学水平的科学问题，另一个封闭集，具有来自计算机科学和数学本科水平考试的问题

基于这两个数据集，对两个具有代表性的LLM采用各种激励策略进行了深入的基准研究。 结果显示，目前的LLM成绩不佳，总分仅为35.80%

此外，通过详细的用户研究，将LLM所犯的错误分为十种解决问题的能力，分析表明，没有任何一种激励策略能够显著优于其他策略，并且一些显示出某些解决问题技能的改进的策略会导致其他技能的下降

<img src="https://img.saraba1st.com/forum/202307/22/064624j5y6n6oo8t62o6aa.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-063928.jpg</strong> (162.75 KB, 下载次数: 0)

下载附件

2023-7-22 06:46 上传

物理化学的一个示例问题，以及在两种提示策略下生成的解决方案，具有思想链CoT提示的GPT-4计算错误，而提示Python作为外部工具的GPT-4会误解数学方程，错误以红色突出显示，更正以紫色显示

<img src="https://img.saraba1st.com/forum/202307/22/064907wb8v4lu188z8leuw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-064846.jpg</strong> (84.81 KB, 下载次数: 0)

下载附件

2023-7-22 06:49 上传

封闭考试数据集的统计数据，报告每次考试中的问题实例数量以及考试中包含详细解决方案的问题比例，进一步报告了不同格式的问题比例，包括自由回答、多项选择和判断题，作为参考，括号中的数字表示分配给问题的评分

<img src="https://img.saraba1st.com/forum/202307/22/064628mpr3ngns1sgqry1l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-064328.jpg</strong> (307.33 KB, 下载次数: 0)

下载附件

2023-7-22 06:46 上传

其他相关评估结果

<img src="https://img.saraba1st.com/forum/202307/22/064618ekfawwkaoncwomk1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-064530.jpg</strong> (39.55 KB, 下载次数: 0)

下载附件

2023-7-22 06:46 上传

评估流程，包括在人工标注者的帮助下分析LLM和参考(正确)解决方案，以确定错误原因，然后将这些原因总结为LLM可能面临挑战的十项基本的科学问题解决技能

随后，LLM验证者会自动将每个错误回答的问题归因于缺乏特定技能，由此产生的错误概况可以通过某些提示策略来解释改进的技能以及直接比较各种策略

数据集分布:

<img src="https://img.saraba1st.com/forum/202307/22/064646au52rbg5l7nal0z5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-064058.jpg</strong> (132.93 KB, 下载次数: 0)

下载附件

2023-7-22 06:46 上传

<img src="https://img.saraba1st.com/forum/202307/22/065110lwc7mc22d42855q8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-065047.jpg</strong> (138.01 KB, 下载次数: 0)

下载附件

2023-7-22 06:51 上传

六种设置下GPT-3.5在文本数据集上的错误概况，以及揭示了其十种基本解决问题能力的缺陷分布图

与其他类似数据集的相关比较:

<img src="https://img.saraba1st.com/forum/202307/22/065249a54eba22cbv3gvng.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230722-065234.jpg</strong> (220.85 KB, 下载次数: 0)

下载附件

2023-7-22 06:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 722#       发表于 2023-7-23 06:04

FreeWilly

指令微调模型

项目主页:https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models

FreeWilly1增量模型权重:https://huggingface.co/stabilityai/FreeWilly1-Delta-SafeTensor

FreeWilly2模型权重:https://huggingface.co/stabilityai/FreeWilly2

Stability AI&amp;CarperAI联合出品，FreeWilly1及其后继产品FreeWilly2，这是两个功能强大的开源微调LLM，两种模型在不同的基准测试中都表现出了卓越的推理能力

FreeWilly1模型基座为LLaMA 65B基础模型，使用标准Alpaca格式的监督微调(SFT)通过新的综合生成的数据集进行微调

FreeWilly2模型基座为LLaMA 2 70B，在某些任务上达到了与GPT-3.5相媲美的性能

两种模型都在并在非商业许可下发布以促进开放研究

<img src="https://img.saraba1st.com/forum/202307/23/060410n5hkiqfzqhibkbbv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230723-055849.jpg</strong> (434.21 KB, 下载次数: 0)

下载附件

2023-7-23 06:04 上传

FreeWilly系列模型的训练受到来自Microsoft的论文“Orca：从 GPT-4 的复杂解释轨迹中进行渐进式学习”中首创的方法的启发，虽然生成数据的过程相似，但数据源有所不同

<img src="https://img.saraba1st.com/forum/202307/23/060419dx9ssky51596ap99.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230723-060005.jpg</strong> (331.42 KB, 下载次数: 0)

下载附件

2023-7-23 06:04 上传

为了评估这些模型，使用了EleutherAI的lm-eval-harness，并添加了AGIEval，两种FreeWilly模型在许多领域都表现出色，包括复杂的推理、理解语言的微妙之处以及回答与专业领域相关的复杂问题，例如解决法律和数学问题等

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 723#       发表于 2023-7-24 02:13

L-Eval

为长上下文语言模型建立标准化评估

github项目地址:https://github.com/OpenLMLab/LEval

最近，人们对有效地处理单轮长输入(例如总结论文)或者具有更广泛历史记录的对话聊天，扩展指令跟随模型的上下文长度越来越感兴趣

虽然GPT-4和Claude等专有模型在处理数以万计的上下文Token方面表现出了相当大的进步，但开源模型依然仍处于实验的早期阶段

目前尚不清楚，与基于检索的方法或仅在分块上下文上训练的模型相比，开发这些长上下文模型是否可以为实际下游任务带来实质性收益

为了应对这一挑战，本文研究组建议对长上下文语言模型进行标准化评估，因此构建了L-Eval，一个包含411个长文档和2000多个查询-响应对，由作者手动标注和检查，涵盖法律、金融、学校讲座、冗长对话、新闻、长篇小说和会议等领域的数据集

L-Eval还采用了多样化的评估方法和指导风格，能够更可靠地评估长上下文语言模型(LCLM/Long Context Language Models)，研究结果表明，虽然开源模型通常落后于商业模型，但它们仍然表现出令人印象深刻的性能

LLaMA2在仅4k上下文长度的开放式任务上取得了最佳结果(与Turbo-16k相比，获胜率45%)，而ChatGLM2在具有8k输入标记的封闭式任务上取得了最佳结果

<img src="https://img.saraba1st.com/forum/202307/24/021214jwu1tx5turux99ur.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-020452.jpg</strong> (172.81 KB, 下载次数: 0)

下载附件

2023-7-24 02:12 上传

该表显示了L-Eval数据集的统计数据，data-name表示数据集的名称，Instruction-style表示数据集中的任务类型或指令风格，#samples表示样本数量，#insts表示为每个样本提供的指令数量，doc-len表示文档输入的平均长度，inst-len表示为每个样本提供的指令的平均长度，Ans-len对应于数据集中每个样本的预期输出的平均长度

<img src="https://img.saraba1st.com/forum/202307/24/021222osmghebm2gqwmmmg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-020507.jpg</strong> (189.37 KB, 下载次数: 0)

下载附件

2023-7-24 02:12 上传

对于当前LCLM的5个子任务的考试评估，Ret表示是否对基本模型使用基于检索的算法，Tokens表示模型的输入token的数量，为了评估进一步微调的有效性，使用不同的输入长度

<img src="https://img.saraba1st.com/forum/202307/24/021229h2nxgmaspg6yiey0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-020934.jpg</strong> (109.03 KB, 下载次数: 0)

下载附件

2023-7-24 02:12 上传

L-Eval开放式生成任务的不同模型与Turbo-16k-0613的比较的总体结果，Turbo3.5是一个有偏见的评估器，更喜欢长答案，因此，如果您有GPT-4 API，研究组建议最好不要使用Turbo3.5评估器评估生成结果

<img src="https://img.saraba1st.com/forum/202307/24/021234czvpv6w5fpwfe050.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-021016.jpg</strong> (204.78 KB, 下载次数: 0)

下载附件

2023-7-24 02:12 上传

各种模型在不同数据集上的F1分数

<img src="https://img.saraba1st.com/forum/202307/24/021238lb4eu8gtzezenntb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-021111.jpg</strong> (221.43 KB, 下载次数: 0)

下载附件

2023-7-24 02:12 上传

各种模型在基于查询的摘要和生成任务上的性能

<img src="https://img.saraba1st.com/forum/202307/24/021243o5rk8bbri6f5o43b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-021121.jpg</strong> (231.89 KB, 下载次数: 0)

下载附件

2023-7-24 02:12 上传

各种模型在长文档摘要任务上的性能

<img src="https://img.saraba1st.com/forum/202307/24/021249o4s5cctz0t0svvcr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-020328.jpg</strong> (316.3 KB, 下载次数: 0)

下载附件

2023-7-24 02:12 上传

L-Eval的人工标注流程界面

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 724#       发表于 2023-7-24 08:09

OBJECT/3DIT

语言引导3D感知(Language-guided 3D-aware)图像编辑

项目主页:https://prior.allenai.org/projects/object-edit

现有的图像编辑工具虽然功能强大，但通常会忽略投影图像潜在的3D几何，因此，使用这些工具进行编辑可能会脱离图像形成过程中基础的几何形状和照明条件

在这项工作中提出了语言引导3D感知编辑的新要求，要求图像中的对象应该遵循潜在的3D场景上下文进行语言指令编辑

<img src="https://img.saraba1st.com/forum/202307/24/080409vxuze2o99at5h983.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-075356.jpg</strong> (103.5 KB, 下载次数: 0)

下载附件

2023-7-24 08:04 上传

为了促进实现这一目标的进展，本文发布了OBJECT：一个数据集，包含根据程序生成的3D场景创建的40万个编辑示例，每个示例由输入图像、语言编辑指令和编辑后的图像组成

同时介绍了3DIT：用于四个编辑任务的单任务和多任务模型，模型显示出令人印象深刻的能力，可以理解整个场景的3D组成，并考虑周围的物体、表面、照明条件、阴影和物理上合理的物体配置

令人惊讶的是，仅对来自OBJECT的合成场景进行训练，3DIT的编辑功能可以推广到现实世界的图像编辑

<img src="https://img.saraba1st.com/forum/202307/24/080314mepj7ofotp5l3dxe.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-075126.jpg</strong> (168.51 KB, 下载次数: 0)

下载附件

2023-7-24 08:03 上传

除了提供精确的旋转角度或图像上的点位置等数字信息之外，还允许用户使用自然语言描述来指定要编辑的对象，直观的语言界面与编辑的精确几何控制的结合产生了一个易于使用但具有高度表现力和可控性的编辑系统

<img src="https://img.saraba1st.com/forum/202307/24/080720fb25c1c9yycyiyz1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-075158__01.jpg</strong> (390.69 KB, 下载次数: 0)

下载附件

2023-7-24 08:07 上传

<img src="https://img.saraba1st.com/forum/202307/24/080720wbqpvpjpnyvr0orq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-075229.jpg</strong> (220.51 KB, 下载次数: 0)

下载附件

2023-7-24 08:07 上传

<img src="https://img.saraba1st.com/forum/202307/24/080720olj9yk8vcbx6mcca.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-075247.jpg</strong> (657.03 KB, 下载次数: 0)

下载附件

2023-7-24 08:07 上传

<img src="https://img.saraba1st.com/forum/202307/24/080845uhh4bh51i2ifm4mt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-080813.jpg</strong> (268.02 KB, 下载次数: 0)

下载附件

2023-7-24 08:08 上传

从3DIT生成的示例以及OBJECT基准测试中四个任务中每一个任务的基线结果

<img src="https://img.saraba1st.com/forum/202307/24/080858lsxhuihs11sxuorc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-075437.jpg</strong> (177.81 KB, 下载次数: 0)

下载附件

2023-7-24 08:08 上传

使用生成的样本进行定量评估，对于每种方法，每个测试图像生成四个样本，根据PSNR指标选择最佳图像来代表每个样本，并对这些值在样本之间进行平均，为了确保指标集中在变换后的对象上，而不是大部分保持不变的背景上，指标是使用变换后的周围区域对象蒙版来计算的

<img src="https://img.saraba1st.com/forum/202307/24/080907m9hsb6erjl3e93hc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-075933.jpg</strong> (138 KB, 下载次数: 0)

下载附件

2023-7-24 08:09 上传

3DIT处理3D感知图像编辑的各种挑战的能力，例如： 透视尺寸变化、综合生成新视点图片、生成遮挡区域、在渲染对象及其阴影时考虑场景照明

<img src="https://img.saraba1st.com/forum/202307/24/080913kz103pz1znz2uo3p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-080115.jpg</strong> (61.18 KB, 下载次数: 0)

下载附件

2023-7-24 08:09 上传

人工评估的结果，评估者对根据几何精度和3D照明一致性评估的3DIT的偏好，由于基线方法维持几何质量和照明一致性的能力有限，因此很少受到青睐

<img src="https://img.saraba1st.com/forum/202307/24/080919xd9h2oq4mv77t4mx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-080219.jpg</strong> (228.29 KB, 下载次数: 0)

下载附件

2023-7-24 08:09 上传

3DIT能够推广到现实世界，同时仅在合成数据集上进行训练，在图示中为四个编辑任务中的每一个显示不同的提示

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 725#       发表于 2023-7-24 08:29

 本帖最后由 Machinery 于 2023-7-24 08:31 编辑 

CNOS

基于CAD的新型对象分割模型，强大的基线对比效果

github项目地址库：https://github.com/nv-nguyen/cnos

本文提出了一种简单的三阶段方法，使用CAD模型来分割RGB图像中的未见对象

利用最新强大的基础模型DINOv2和Segment Anything，构造描述符并生成提案，包括给定输入的RGB图像的二元掩码(binary mask)

通过将提案与CAD模型创建的参考描述符进行匹配，实现了精确的对象ID分配以及模态掩码

通过实验证明，本文方法在基于CAD的新型对象分割方面取得了SOTA结果，使用相同的BOP评估规则，在BOP挑战的七个核心数据集上超过了现有方法19.8%

<img src="https://img.saraba1st.com/forum/202307/24/082848rcw7wy4xbhmy0kk4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-081942.jpg</strong> (452.68 KB, 下载次数: 0)

下载附件

2023-7-24 08:28 上传

CNOS是一种简单的三阶段方法，用于基于CAD的新型对象分割，它基于Segmenting Anything与DINOv2，可用于任何对象而无需重新训练，CNOS优于在目标对象上训练的有监督MaskRCNN模型(在 CosyPose中)，CNOS已被用作2023年BOP challenge 2023中任务5和任务6的基线

方法概述，给定运行时的一组CAD模型和RGB图像，首先渲染一组模板并提取它们的视觉描述符，然后使用提案网络来分割2D图像中所有可能的对象，并使用相同的描述符网络提取它们的视觉描述符，最后使用余弦相似度来匹配这两组描述符，并应用视图聚合和argmax为每个提案分配一个对象ID

<img src="https://img.saraba1st.com/forum/202307/24/082857hryqeeefxy3e7457.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-082727.jpg</strong> (127.36 KB, 下载次数: 0)

下载附件

2023-7-24 08:28 上传

使用 Pyrender渲染的Linemod数据集中“benchwise”对象模板的可视化，展示了从正二十面体定义的视点渲染的42个模板，以实现有效的模板匹配

相关对比评估结果

<img src="https://img.saraba1st.com/forum/202307/24/082907gwty05ax0s497lk2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-082803.jpg</strong> (526 KB, 下载次数: 0)

下载附件

2023-7-24 08:29 上传

一些实例：

<img src="https://img.saraba1st.com/forum/202307/24/082941vd7foox67x4zhvdo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-082146.jpg</strong> (402.72 KB, 下载次数: 0)

下载附件

2023-7-24 08:29 上传

<img src="https://img.saraba1st.com/forum/202307/24/082941w8ceu7coe5cqso8s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230724-082200.jpg</strong> (171.5 KB, 下载次数: 0)

下载附件

2023-7-24 08:29 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 726#       发表于 2023-7-25 01:22

Subject-Diffusion

开放域个性化文本到图像生成，无需测试时微调

项目主页:https://oppo-mente-lab.github.io/subject_diffusion/

github项目地址:https://github.com/OPPO-Mente-Lab/Subject-Diffusion

<img src="https://img.saraba1st.com/forum/202307/25/012113wezf490elezrfekc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-005836.jpg</strong> (263.52 KB, 下载次数: 0)

下载附件

2023-7-25 01:21 上传

使用扩散模型生成个性化图像的最新进展非常迅速，然而开放域和非微调个性化图像生成领域的发展进展相当缓慢

在本文中提出了主题扩散(Subject-Diffusion)，一种新颖的开放域个性化图像生成模型，除了不需要在测试时微调之外，同时支持单张参考图像即可生成任何特定领域的单个或多主题的个性化图像

首先构建了一个自动数据标记工具，使用LAION-Aesthetics数据集构建了一个由76M个图像及其相应的主题检测边界框、分割掩码和文本描述组成的大规模数据集

其次设计了一个新的统一框架，使用粗糙定位和细粒度参考图像控制来结合文本和图像语义，以最大限度地提高主题保真度和泛化能力

此外还采用注意力控制机制来支持多主题主体生成，广泛的定性和定量结果表明，Subject-Diffusion在单个、多个和人类定制图像生成方面优于其他SOTA框架

<img src="https://img.saraba1st.com/forum/202307/25/012136raautjjrqm6xvi1z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-012125.jpg</strong> (153.44 KB, 下载次数: 0)

下载附件

2023-7-25 01:21 上传

生成训练数据的过程涉及以下步骤：首先利用BLIP-2为输入图像提供标题，然后使用spaCy从标题句子(caption sentence)的整段里提取每个单词作为标签，提取的标签用作Grounding DINO的输入，以获得每个物体的检测框，然后用作SAM获取相应物体分割掩码的提示，最终所有模态组合成结构化数据以生成多模态数据集

<img src="https://img.saraba1st.com/forum/202307/25/012207zic5ny24d6cj54t6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-012012.jpg</strong> (198.72 KB, 下载次数: 0)

下载附件

2023-7-25 01:22 上传

相关评估结果与超参数利用

<img src="https://img.saraba1st.com/forum/202307/25/012207wxvh98ex5ev88boe.jpg" referrerpolicy="no-referrer">

<strong>739f3f3b-e0cd-44a6-a5a0-e249fd136640.jpg</strong> (937.1 KB, 下载次数: 0)

下载附件

2023-7-25 01:22 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 727#       发表于 2023-7-25 07:18

OxfordTVG-HIC

从图像中制作幽默的字幕说明

项目主页:https://torrvision.com/tvghic/

数据集下载地址:https://drive.google.com/drive/folders/1BDuUcMeaWrFD8TwgHLhFPkuAwmoHaVNQ

github项目主页:https://github.com/runjiali-rl/Oxford_HIC

本文介绍了OxfordTVG-HIC(幽默图像字幕/Humorous Image Captions)，这是一个用于幽默文本生成和理解的大型数据集

幽默是一种抽象的、主观的、依赖于情境的认知结构，涉及多种认知因素，使其生成和解释成为一项具有挑战性的任务

因此，幽默的生成和理解可以作为评估深度学习方法处理抽象和主观信息能力的新任务，由于数据稀缺，与幽默相关的生成任务(例如字幕说明)仍未得到充分探索，为了弥补这一差距，在本文中引入了OxfordTVG-HIC，一个提供了大约290万个带有幽默分数标注的图像文本对数据集，以训练通用的幽默字幕模型

与现有的字幕数据集相反，OxfordTVG-HIC具有广泛的情感和语义多样性，导致脱离上下文的示例特别有利于产生幽默，此外，OxfordTVG-HIC的构造不涉及任何攻击性内容，同时还展示了如何利用OxfordTVG-HIC来评估生成文本的幽默感

通过对训练模型的可解释性分析，确定了对引发幽默预测(和生成)有影响的视觉和语言线索，定性地观察到，这些观察结果与认知心理学中幽默的良性冲突理论(benign violation theory of humour)相一致

<img src="https://img.saraba1st.com/forum/202307/25/071457btlav8japai60izm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-070544.jpg</strong> (137.46 KB, 下载次数: 0)

下载附件

2023-7-25 07:14 上传

来自 OxfordTVG-HIC和COCO的图像文本样本，在OxfordTVG-HIC中，猫图像的说明文字并不描述猫的身体特征，而是描述可能引发猫面部表情的情况，这些情况会产生幽默的表达效果，因为它们并不具有冒犯性，也没有违反观众的日常生活期望(良性冲突理论)，另一方面，COCO中类似猫图像的标题明确描述了图像中的事实

<img src="https://img.saraba1st.com/forum/202307/25/071512twweee0gweavarws.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-070732.jpg</strong> (136.06 KB, 下载次数: 0)

下载附件

2023-7-25 07:15 上传

与其他数据集的对比

<img src="https://img.saraba1st.com/forum/202307/25/071519n32impqmam8ddl0v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-070816.jpg</strong> (57 KB, 下载次数: 0)

下载附件

2023-7-25 07:15 上传

OxfordTVG-HIC 在每张图像的字幕数量的均值和方差方面比其他图像字幕数据集大得多

<img src="https://img.saraba1st.com/forum/202307/25/071608k8x9m8sx98ossw98.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-071005.jpg</strong> (192.33 KB, 下载次数: 0)

下载附件

2023-7-25 07:16 上传

数据集的情感与语法模式分析

<img src="https://img.saraba1st.com/forum/202307/25/071641l6z4hsz4s3fso01h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-071023.jpg</strong> (124.96 KB, 下载次数: 0)

下载附件

2023-7-25 07:16 上传

通过不同损失所训练的模型在未见图像上生成的说明文字，位置条件损失生成的字幕呈现为蓝色，从中可以发现位置条件损失有效解决了交叉熵多样性有限的问题

<img src="https://img.saraba1st.com/forum/202307/25/071656uqdcqe3pcr3z39c1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-071306.jpg</strong> (249.15 KB, 下载次数: 0)

下载附件

2023-7-25 07:16 上传

评估其他数据集的幽默分数等

<img src="https://img.saraba1st.com/forum/202307/25/071734oblbahhbi6yoan6s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-071342.jpg</strong> (294.02 KB, 下载次数: 0)

下载附件

2023-7-25 07:17 上传

注意力热力图与可视化梯度分析

生成实例:

<img src="https://img.saraba1st.com/forum/202307/25/071750ryyc3cq2qcnftc1m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-071404.jpg</strong> (575.67 KB, 下载次数: 0)

下载附件

2023-7-25 07:17 上传

以及失败案例:

<img src="https://img.saraba1st.com/forum/202307/25/071808ktm44von8rnkmumf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230725-071419.jpg</strong> (588.97 KB, 下载次数: 0)

下载附件

2023-7-25 07:18 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

﹍﹍﹍

评分

 参与人数 1战斗力 +1

|昵称|战斗力|理由|
|----|---|---|
| 苦悩の梨| + 1||

查看全部评分


*****

####  3233  
##### 728#       发表于 2023-7-25 11:36

这帖子终于又能看了，贴主辛苦


*****

####  Machinery  
##### 729#       发表于 2023-7-26 01:25

3D-LLM

将3D世界注入大型语言模型

项目主页:https://vis-www.cs.umass.edu/3dllm/

github项目仓库:https://github.com/UMass-Foundation-Model/3D-LLM

大型语言模型 (LLM) 和视觉语言模型 (VLM) 已被证明在多项任务上表现出色，例如常识推理等

<img src="https://img.saraba1st.com/forum/202307/26/012143tvmjh120202cc6j0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-011237.jpg</strong> (53.02 KB, 下载次数: 0)

下载附件

2023-7-26 01:21 上传

尽管这些模型非常强大，但它们并不以3D物理世界为基础，而3D物理世界中涉及更丰富的概念，例如空间关系、可供性(affordances)、物理、布局等，在这项工作中，通过将3D世界注入大型语言模型中，引入了全新的3D-LLM系列

<img src="https://img.saraba1st.com/forum/202307/26/012211p8n433dneqrdjb2d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-011412.jpg</strong> (188.32 KB, 下载次数: 0)

下载附件

2023-7-26 01:22 上传

<img src="https://img.saraba1st.com/forum/202307/26/012228m6amszxvzmamlrmm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-011428.jpg</strong> (181.39 KB, 下载次数: 0)

下载附件

2023-7-26 01:22 上传

<img src="https://img.saraba1st.com/forum/202307/26/012228y10ykn344n73ccyl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-011443.jpg</strong> (354.52 KB, 下载次数: 0)

下载附件

2023-7-26 01:22 上传

具体来说，3D-LLM可以将3D点云及其特征作为输入，并执行各种3D相关任务，包括字幕、密集字幕、3D问答、任务分解、3D基准(grounding)、3D辅助对话、导航等，使用设计的三种类型的提示机制，收集超过30万个涵盖这些任务的3D语言数据(3D-language data)

为了有效地训练3D-LLM，首先利用3D特征提取器从渲染的多视图图像中获取3D特征，然后使用2D VLM模型(Visual language model)作为骨干来训练3D-LLM，通过引入3D定位机制，3D-LLM可以更好地捕获3D空间信息

ScanQA上的实验表明，本文方法的模型大幅优于SOTA基线(例如BLEU-1分数超过前SOTA分数9%)，此外，对保留的3D字幕、任务组合和3D辅助对话数据集进行的实验表明，本文方法的模型优于2D VLM，定性示例还表明模型可以执行超出现有LLM和VLM范围的更多任务

<img src="https://img.saraba1st.com/forum/202307/26/012300zh6udvhbdbgghz1d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-011623.jpg</strong> (329.88 KB, 下载次数: 0)

下载附件

2023-7-26 01:23 上传

3D语言数据(3D-language data)生成流程

<img src="https://img.saraba1st.com/forum/202307/26/012359q73q18f13a1k37x8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-012344.jpg</strong> (115.39 KB, 下载次数: 0)

下载附件

2023-7-26 01:23 上传

3D-LLM框架概览图，前两列显示了3D特征提取器，首先从3D场景中渲染一些多视图图像，提取2D密集特征，然后使用三种方法从这些多视图图像构建3D特征，随后3D特征和输入语言提示被输入到3D-LLM以生成响应，还使用了一种3D定位机制，以更好地捕获3D空间信息

相关评估对比结果:

<img src="https://img.saraba1st.com/forum/202307/26/012418vr5rer2hr9rqr2yg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-011856.jpg</strong> (274.58 KB, 下载次数: 0)

下载附件

2023-7-26 01:24 上传

<img src="https://img.saraba1st.com/forum/202307/26/012418nth1hdha0aasjaum.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-011905.jpg</strong> (236.66 KB, 下载次数: 0)

下载附件

2023-7-26 01:24 上传

<img src="https://img.saraba1st.com/forum/202307/26/012418l51666bu7n86uon3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-012006.jpg</strong> (111.55 KB, 下载次数: 0)

下载附件

2023-7-26 01:24 上传

对象导航的可视化与使用实例:

<img src="https://img.saraba1st.com/forum/202307/26/012458k1yasjksnneknzy4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-011939.jpg</strong> (348.93 KB, 下载次数: 0)

下载附件

2023-7-26 01:24 上传

<img src="https://img.saraba1st.com/forum/202307/26/012458mryww4vgc48hkcwq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-012028.jpg</strong> (191.74 KB, 下载次数: 0)

下载附件

2023-7-26 01:24 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 730#       发表于 2023-7-26 01:53

 本帖最后由 Machinery 于 2023-7-26 01:57 编辑 

Encyclopedic VQA

细粒度类别的详细属性的视觉提问

github项目地址:https://github.com/google-research/google-research/tree/master/encyclopedic_vqa

<img src="https://img.saraba1st.com/forum/202307/26/014738zujy6llo0yu00uo8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014343.jpg</strong> (163.08 KB, 下载次数: 0)

下载附件

2023-7-26 01:47 上传

Encyclopedic-VQA是一个大规模视觉问答(VQA)数据集，包含实例使用有关的细粒度类别的详细属性的视觉问题样本

<img src="https://img.saraba1st.com/forum/202307/26/014751pw6n4w3fo4b8nn3l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014406.jpg</strong> (433.93 KB, 下载次数: 0)

下载附件

2023-7-26 01:47 上传

它包含221k个独特的问题+答案对，每个问题+答案与(最多5个)图像相匹配，从而产生总共1M个VQA样本

此外，本数据集还附带来自wiki的受控知识库，标记了支持每个答案的证据，实验中发现Encyclopedic VQA数据集对大型视觉+语言模型提出了严峻的挑战，因为这些模型在数据集上表现均为不佳：PaLI是OK-VQA的SOTA模型，但它在本文数据集上仅达到了13.0%的准确率

此外，通过实验表明，使用从知识库检索相关信息的机制来增强大型模型，可以在回答百科全书式问题方面取得进展，具有完美检索的预言实验(oracle experiment)在Encyclopedic VQA数据集的单跳部分上实现了87.0%的准确率，而自动检索增强原型的准确率为48.8%

数据收集流程模板并自动化生成单跳问题:

<img src="https://img.saraba1st.com/forum/202307/26/014842fjuvv0z6wuwi3lgo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014415.jpg</strong> (205.67 KB, 下载次数: 0)

下载附件

2023-7-26 01:48 上传

使用桥接的实体生成双跳问题的图示:

<img src="https://img.saraba1st.com/forum/202307/26/015010aqjjyq606jvicn6q.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014450.jpg</strong> (111.9 KB, 下载次数: 0)

下载附件

2023-7-26 01:50 上传

数据集分析、问题分布、以及OK-VAQ的SOTA模型PALI的成绩:

<img src="https://img.saraba1st.com/forum/202307/26/015150xchc79bpi8b9ce11.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014500.jpg</strong> (117.86 KB, 下载次数: 0)

下载附件

2023-7-26 01:51 上传

<img src="https://img.saraba1st.com/forum/202307/26/015150zenhrnqff5ha4u5z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014541.jpg</strong> (103.3 KB, 下载次数: 0)

下载附件

2023-7-26 01:51 上传

单跳问题测试准确率，以及一些测试实例:

<img src="https://img.saraba1st.com/forum/202307/26/015227ovygtgzsgllatlog.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014635.jpg</strong> (93.37 KB, 下载次数: 0)

下载附件

2023-7-26 01:52 上传

<img src="https://img.saraba1st.com/forum/202307/26/015255tspsxy9k6yeelgsp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014650.jpg</strong> (351.6 KB, 下载次数: 0)

下载附件

2023-7-26 01:52 上传

Encyclopedic VQA全面样本情况分析表:

<img src="https://img.saraba1st.com/forum/202307/26/015335tki3uxb85lxxd7bu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-014706.jpg</strong> (128.87 KB, 下载次数: 0)

下载附件

2023-7-26 01:53 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 731#       发表于 2023-7-26 09:06

interpolation

使用扩散模型在给定图像对之间进行平滑的插值生成

项目主页:https://clintonjwang.github.io/interpolation

github项目地址:https://clintonjwang.github.io/interpolation

<img src="https://img.saraba1st.com/forum/202307/26/090626wmrmm155mp0m0rrc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-084654.jpg</strong> (131.83 KB, 下载次数: 0)

下载附件

2023-7-26 09:06 上传

图像生成和编辑的一个鲜为人知的前沿领域是在给定的两个输入图像之间进行平滑插值(interpolating between two input images)生成的任务，这是当前部署的所有图像生成工作流程中都缺少的功能

这样的特性可以扩展此类模型的创造性应用，并提出了一种使用潜在扩散模型进行零样本插值图像生成的方法

<img src="https://img.saraba1st.com/forum/202307/26/090604kqjg1jabna4llns5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-084722.jpg</strong> (244.63 KB, 下载次数: 0)

下载附件

2023-7-26 09:06 上传

以一系列降低噪声水平的序列在潜在空间中应用插值，然后根据从文本反转(textual inversion)和可选的主体姿势(subject poses)衍生的插值文本嵌入(interpolated text embeddings)进行去噪，为了获得更高的图像一致性或指定其他标准，可以生成多个候选图像并使用CLIP来选择最高质量的图像

在不同的主体姿势、图像风格和图像内容中获得了令人信服的平滑插值，并表明诸如FID等标准定量指标不足以衡量图像插值的生成质量

<img src="https://img.saraba1st.com/forum/202307/26/090211jpp8nhhnhnbcr111.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-084928.jpg</strong> (177.33 KB, 下载次数: 0)

下载附件

2023-7-26 09:02 上传

姿势调节可以减轻相邻帧之间姿势突然变化的发生，即使预测姿势不正确也是如此

<img src="https://img.saraba1st.com/forum/202307/26/090216vussluincjfklzk9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-085156.jpg</strong> (73.55 KB, 下载次数: 0)

下载附件

2023-7-26 09:02 上传

当输入图像被风格化时，OpenPose无法生成高置信度的姿势，因此首先使用潜在扩散模型执行图像到图像的转换，在应用OpenPose之前将输入图像转换为照片风格，即使转换后的图像质量较低，通常依然可以成功

<img src="https://img.saraba1st.com/forum/202307/26/090222q4vnqrurvqwvulbw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-085602.jpg</strong> (389.06 KB, 下载次数: 0)

下载附件

2023-7-26 09:02 上传

不同插值方案的比较，从输入图像导出的潜在图像添加噪声，并对插值的潜在图像进行去噪以生成输出帧，与其他类似于alpha混合(resemble alpha blending)的方法相比，这种方法达成了从人到山之间更令人信服的语义转换效果

<img src="https://img.saraba1st.com/forum/202307/26/090229gfkkkkp3zjpkbfbv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-085942.jpg</strong> (188.56 KB, 下载次数: 0)

下载附件

2023-7-26 09:02 上传

失败案例，本文方法在弥合风格、语义或布局方面的巨大差距面前依然有所局限

使用实例:

<img src="https://img.saraba1st.com/forum/202307/26/090245inng87w1wz91i113.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-085849.jpg</strong> (323.98 KB, 下载次数: 0)

下载附件

2023-7-26 09:02 上传

<img src="https://img.saraba1st.com/forum/202307/26/090245y00wuuw6tgu2s8jw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-085900.jpg</strong> (475.59 KB, 下载次数: 0)

下载附件

2023-7-26 09:02 上传

<img src="https://img.saraba1st.com/forum/202307/26/090245v2i62ifxxfj6kxxe.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-085918.jpg</strong> (407.5 KB, 下载次数: 0)

下载附件

2023-7-26 09:02 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 732#       发表于 2023-7-26 09:55

RippleEdits

评估语言模型中知识编辑的连锁效应

github项目仓库:https://github.com/edenbiran/RippleEdits/

现代语言模型以庞大的大小捕获了大量事实知识，然而随着时间的推移，一些事实可能会被错误地归纳或变得过时，从而导致事实上不正确的生成结果，这导致了各种知识编辑方法的发展，这些方法允许更新模型已经编码在参数内的事实

对这些方法的评估主要集中于测试单个事实是否已成功注入，以及对其他受测试的类似预测是否改变，这种评估实际上是有限的，因为注入一个事实后，会以模型需要更新其他事实的形式引入“连锁效应(Ripple Effects)”

为了解决这个问题，本文提出了一套新颖的评估标准，考虑了编辑对相关事实的影响，然后使用这些标准构建了ripple，一个基于5K事实编辑的判断基准，可以捕获各种类型的连锁效应

在ripple上评估了各种知识编辑方法，表明当前方法未能在模型知识中引入一致变化，此外，还发现了一个简单的上下文编辑基线在基准测试中获得了最好的分数，这表明了当前模型知识编辑的一个有前途的研究方向

<img src="https://img.saraba1st.com/forum/202307/26/095319x8ql834w8623q3ls.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-092403.jpg</strong> (128.27 KB, 下载次数: 0)

下载附件

2023-7-26 09:53 上传

与现有知识编辑基准相比，RippleEdits的评估范围图示，对于给定的事实编辑，需要考虑编辑对于模型知识的“连锁效应”

<img src="https://img.saraba1st.com/forum/202307/26/095331kjyapppfvpafgn8w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-093004.jpg</strong> (116 KB, 下载次数: 0)

下载附件

2023-7-26 09:53 上传

6个评估标准的测试示例，编辑自身模拟为向实体prince添加节点(sibling)，并在每个条件的顶部显示一个粗体箭头，在节点关系上显示一个编辑符号

对于每个测试，输入主题显示为蓝色，目标对象显示为绿色，其他节点显示为橙色，边的颜色源自其目标节点

对于Logical Generalization，需要插入知识图谱的附加事实，在关系旁边显示有编辑符号

对于Compositions I和Composition II，模型需要跳过编辑才能到达目标

在Subject Aliasing中，验证编辑是否也会传播到输入的释义

在Forgetfulness中，验证在必要关系中共享输入主题和关系的其他目标，会不会被遗忘

在Relation Specificity中，验证主题的其他关系有没有被修改

<img src="https://img.saraba1st.com/forum/202307/26/095358sidd6d66x8d6iddl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-094249.jpg</strong> (56.76 KB, 下载次数: 0)

下载附件

2023-7-26 09:53 上传

为以下修改编辑生成RippleEdits测试的图示:(Bill Gates, Spouse, Melinda Gates) → (Bill Gates, Spouse, Ricciarda Cybo Malaspina)

首先从知识图谱中采样原始事实，对于修改编辑，通过选择与原始对象共享相同类型的对象来创建反事实修改，主要步骤为通过使用知识图谱和采样应保留或修改的新三元组来生成评估测试，进行后期编辑，最后利用预定义的模板将知识图谱三元组翻译为自然语言短语

相关评估结果:

<img src="https://img.saraba1st.com/forum/202307/26/095532g7at7jsjtjjjnop7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-094436.jpg</strong> (275.08 KB, 下载次数: 0)

下载附件

2023-7-26 09:55 上传

<img src="https://img.saraba1st.com/forum/202307/26/095532kzoohpmhvoogbcov.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-095437.jpg</strong> (239.58 KB, 下载次数: 0)

下载附件

2023-7-26 09:55 上传

<img src="https://img.saraba1st.com/forum/202307/26/095532fzg9e9nrggpnxj8r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-095511.jpg</strong> (104.06 KB, 下载次数: 0)

下载附件

2023-7-26 09:55 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 733#       发表于 2023-7-26 10:35

Decomposition Faithfulness

问题分解提高了模型生成推理的可信度

相关论文:https://arxiv.org/abs/2307.11768

github项目仓库:https://github.com/anthropics/DecompositionFaithfulnessPaper

随着大型语言模型(LLM)执行更困难的任务，验证其行为的正确性和安全性变得更加困难，帮助解决这个问题的一种方法是促使LLM将他们的推理具体化，例如，让他们在回答问题时生成逐步推理(思想链/CoT)

推理可以使我们检查模型用于执行任务的过程，然而这种方法依赖于模型，需要实际推理中的陈述推理忠实依赖于实际情况，但现实则并非总是如此

为了提高CoT推理的准确性，可以让模型通过将问题分解为子问题来生成推理，基于分解的方法在问答任务上实现了强大的性能，有时接近CoT，同时提高了模型在最近提出的几个指标上陈述的推理的可信度

通过强制模型在不同的上下文中回答更简单的子问题，远远提高了模型生成推理相对于CoT的忠实度，同时仍然实现了CoT的一些性能提升

结果表明，提高模型生成推理的可信度是可能的，持续的改进将使我们能够通过推理验证LLM行为的正确性和安全性

<img src="https://img.saraba1st.com/forum/202307/26/103308s2kl2le2k7nruyn9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-102300.jpg</strong> (91.66 KB, 下载次数: 0)

下载附件

2023-7-26 10:33 上传

思维链、思维链分解和事实分解的问答表现和可信度得分的帕累托边界(Pareto frontier)，每种方法都使用了带有高质量演示的少样本提示，基于分解的方法在研究的任务中取得了良好的性能，同时生成推理步骤，这些步骤更忠实地解释了为什么语言模型会这样回答

<img src="https://img.saraba1st.com/forum/202307/26/103316kcu6eijej8uajyyx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-102725.jpg</strong> (184.92 KB, 下载次数: 0)

下载附件

2023-7-26 10:33 上传

本文研究的每种方法的高级概览图(省略了一些格式)，用于提示模型在回答问题之前生成推理，还为每种方法使用说明和少样本提示

思维链由模型在预测最终答案之前在一次采样调用中生成的逐步推理组成，思维链分解则包括在一次采样调用中生成一系列更简单的子问题及其各自的答案，形式上类似于思维链，然后再预测最终答案

事实分解还会生成子问题和答案，但会在新的上下文中回答每个子问题，分解降低了模型使用原始问题中的虚假信息回答子问题的可能性，使模型的推理更忠实可信

<img src="https://img.saraba1st.com/forum/202307/26/103345fi68tx85miymfeta.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-102851.jpg</strong> (68.71 KB, 下载次数: 0)

下载附件

2023-7-26 10:33 上传

推理生成方法的性能和可信度，思维链实现了最佳的问答准确性，而事实分解实现了最佳的推理可信度，所有指标都是四项问答任务的平均值

<img src="https://img.saraba1st.com/forum/202307/26/103355aaw2zylyl0ct2wjy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-102918.jpg</strong> (248.76 KB, 下载次数: 0)

下载附件

2023-7-26 10:33 上传

prompt格式和中间过程展示

<img src="https://img.saraba1st.com/forum/202307/26/103400dxlos5j2mrax2mmk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-103135.jpg</strong> (64.17 KB, 下载次数: 0)

下载附件

2023-7-26 10:34 上传

在四项任务的评估中使用每种提示策略的模型的基线问答准确性，事实分解优于零样本和少样本基线，并且思想链和思想链分解实现了最强的性能，推理生成方法在HotpotQA和StrategyQA上的表现优于零样本/少样本，这两个任务最适合逐步推理或问题分解

一些相关推理性能的评估指标结果:

<img src="https://img.saraba1st.com/forum/202307/26/103509q5k4fw8811crug98.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230726-103148.jpg</strong> (201.48 KB, 下载次数: 0)

下载附件

2023-7-26 10:35 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  lvcha  
##### 734#       发表于 2023-7-26 15:25

打扰，借楼请教一下。

我门外汉，只稍微懂一点深度学习。

我公司有一套java 开发框架，里面提供一大堆service restful/java api什么的。

想利用使用类似chatgpt的模型帮我生成基于我们这堆api的java代码，

模型需要本地的。

这个该怎么开始啊？


*****

####  Machinery  
##### 735#       发表于 2023-7-26 15:42

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61795401&amp;ptid=2126390" target="_blank">lvcha 发表于 2023-7-26 15:25</a>
打扰，借楼请教一下。

我门外汉，只稍微懂一点深度学习。</blockquote>
有几个选择可以解决这个问题，比如考虑用别人调好的本地代码模型或者api模型，之后提示里提供给模型一些描述样本，用few shot生成或者配上类似toolformer之类的约束生成插件，也可以自己准备标注好的样本或者数据配合很多开源微调框架直接调模型<img src="https://static.saraba1st.com/image/smiley/face2017/011.png" referrerpolicy="no-referrer">

总体来说效果不会很理想，要理想一定需要花大功夫<img src="https://static.saraba1st.com/image/smiley/face2017/068.png" referrerpolicy="no-referrer">

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  fufusako  
##### 736#       发表于 2023-7-26 16:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61790745&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-7-26 09:06</a>

interpolation

使用扩散模型在给定图像对之间进行平滑的插值生成</blockquote>
虽然是用了AI识别再生成，但这效果好像还不如用补帧软件啊

要是能给出一个流畅的在两个动作间切换的样例就好了，不过我估计做不到，给的成功例子都什么鬼


*****

####  Machinery  
##### 737#       发表于 2023-7-27 00:56

LoraHub

通过动态的LoRA组合实现高效的跨任务泛化能力

github项目地址(待整理):https://github.com/sail-sg/lorahub

LoRA模块可通过以下网址访问获取:https://www.huggingface.co/models?search=lorahub

低秩适应(LoRA/Low-rank adaptations)通常用于在下游任务中针对性微调大型语言模型(LLM)，本文研究了LoRA跨任务泛化的可组合性，并提出了LoraHub，一种策略式框架，旨在有目的性的集成经过不同给定任务训练的LoRA模块，框架目标是在模型未见任务上实现高适应力的能力表现

只需举几个新任务的例子，LoraHub就可以实现多个LoRA模块的流畅组合，从而消除了对人类专业知识的需求，最关键的是，这种组合既不需要额外的模型参数也不需要梯度

从Big-Bench Hard (BBH)基准得出的实证结果表明，LoraHub可以有效地模仿少样本场景中的上下文学习能力，排除了在每个推理输入中使用上下文示例的必要性

本文研究的其中一个重大贡献是LoRA社区，用户可以在其中分享他们训练的LoRA模块，从而促进使用这些模块应用于新任务

预计该资源将促进通用智能的提升以及对于LLM发展做出重大帮助

<img src="https://img.saraba1st.com/forum/202307/27/005608ed4c98yodzckada7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-004526.jpg</strong> (58.03 KB, 下载次数: 0)

下载附件

2023-7-27 00:56 上传

零样本学习、少样本上下文学习和少样本LoraHub学习(本文方法)的图示，请注意，Compose过程是按任务而不是按示例执行的，LoraHub实现了与零样本学习类似的推理吞吐量，但在BIG-Bench Hard (BBH)基准上实现了接近少样本上下文学习的性能

<img src="https://img.saraba1st.com/forum/202307/27/005613qbdijd5d0fbx5bdb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-004710.jpg</strong> (83 KB, 下载次数: 0)

下载附件

2023-7-27 00:56 上传

LoraHub包含两阶段，COMPOSE阶段和ADAPT阶段：
1.在COMPOSE阶段，现有的LoRA模块被集成到一个统一的模块中，使用一组权重(表示为w)作为系数

2.在ADAPT阶段，合并的LoRA模块根据来自未见任务的几个示例进行评估，随后，应用无梯度算法来细化w，执行K次迭代后，会生成一个高度适应性的LoRA模块，该模块可以与LLM合并来执行预期的任务

<img src="https://img.saraba1st.com/forum/202307/27/005633w94ts12uxwsm4ch4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-004922.jpg</strong> (283.63 KB, 下载次数: 0)

下载附件

2023-7-27 00:56 上传

零样本学习 (Zero)、少样本上下文学习(ICL)和少样本LoraHub学习之间的性能比较分析，在比较成绩中，为每个任务使用5个示例作为ICL和LoraHub的少样本演示，LoraHub的平均(avg)性能是通过使用不同随机种子的5次不同运行计算得出的，而最佳(best)性能则报告为这些运行中获得的最大值

<img src="https://img.saraba1st.com/forum/202307/27/005641cjgmzjyu9y1j1noz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-005343.jpg</strong> (77.83 KB, 下载次数: 0)

下载附件

2023-7-27 00:56 上传

这三幅图显示了传统微调 (FFT)、LoRA微调(LoRA)和LoraHub学习(Ours)在不同数量的任务演示示例中的性能比较，x轴表示示例数量，y轴表示任务性能，使用精确匹配的指标进行量化，如结果所示，当未见过的任务的可用示例少于20个时，LoraHub可能会优于lora调整

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  thallium  
##### 738#       发表于 2023-7-27 01:00

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61790745&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-7-26 09:06</a>
interpolation

使用扩散模型在给定图像对之间进行平滑的插值生成</blockquote>
什么梗图制造机

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 739#       发表于 2023-7-27 01:44

 本帖最后由 Machinery 于 2023-7-27 01:46 编辑 

ARB

大型语言模型的高级推理基准测试集

项目主页:https://arb.duckai.org/

github项目代码库:https://github.com/TheDuckAI/arb

测试问题样例(不包含测试集):https://arb.duckai.org/home

大型语言模型(LLM)在各种定量推理和知识基准测试中表现出了卓越的性能，然而随着LLM获得的分数越来越高，尽管尚未达到这些领域的人类专家级表现，但其中许多基准正在失去实用性

本文介绍ARB(Advanced Reasoning Benchmark)，一种由多个领域的高级推理问题组成的新基准，ARB提出了比之前的基准更具挑战性的测试问题，其中包含数学、物理、生物、化学和法律方面

作为ARB的子集，引入了一组具有挑战性的数学和物理问题，这些问题需要高级符号象征推理和深度的领域知识理解，在ARB上评估了最新的模型，例如GPT-4和Claude等，证明当前模型在很多要求严格的任务上的得分远低于50%

为了提高自动和辅助评估能力，引入了基于评分标准的评估方法，允许GPT-4对其自己的中间推理步骤进行评分，还对ARB的困难符号推理子集进行了人工评估，发现标注者与GPT-4的评估分数之间有希望达成一致性

按领域划分的基准测试数据集中的问题类型

<img src="https://img.saraba1st.com/forum/202307/27/014335jck6ttkdkdcxr144.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-013643.jpg</strong> (118.3 KB, 下载次数: 0)

下载附件

2023-7-27 01:43 上传

ARB基准自动评分组件的模型准确率

<img src="https://img.saraba1st.com/forum/202307/27/014600kh1jbgjhhhjpw1z3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-014543.jpg</strong> (72.73 KB, 下载次数: 0)

下载附件

2023-7-27 01:46 上传

<img src="https://img.saraba1st.com/forum/202307/27/014354trprzs4a5rzarryp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-014033.jpg</strong> (65.73 KB, 下载次数: 0)

下载附件

2023-7-27 01:43 上传

GPT-4在ARB中的数学和物理问题的错误

<img src="https://img.saraba1st.com/forum/202307/27/014400dijlxcxiktjmi4bk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-014046.jpg</strong> (87.58 KB, 下载次数: 0)

下载附件

2023-7-27 01:44 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 740#       发表于 2023-7-27 23:14

 本帖最后由 Machinery 于 2023-7-27 23:15 编辑 

WavJourney

使用大型语言模型进行合成音频创作

项目主页:https://audio-agi.github.io/WavJourney_demopage/

github项目代码库:https://github.com/Audio-AGI/WavJourney

大型语言模型(LLM)通过集成不同的专家模型来解决错综复杂的语言和视觉任务方面显示出了巨大的潜力，尽管它们在推进人工智能生成内容(AIGC)领域具有重要意义，但它们在智能生成音频内容方面的潜力仍未得到开发

在这项工作中，解决了在文本指令的指导下创建含有语音、音乐和音效的故事情节的音频内容的问题，WavJourney，一个利用LLM连接各种音频模型进而生成音频内容的系统

给定听觉场景的文本描述，WavJourney首先提示LLM生成用于音频讲故事的结构化脚本，音频脚本基于空间关系组织不同的音频元素，作为音频的概念化表现，音频脚本为人类创作者提供了交互式和可解释的基本原理，之后，音频脚本被输入脚本编译器，将其转换为计算机程序，程序的每一行都调用特定于任务的音频生成模型或计算操作函数(例如接续、混合等操作)，然后执行程序以获得可解释的音频生成解决方案

展示了WavJourney在各种现实世界场景中的实用性，包括科幻小说、教育和广播剧等，WavJourney的可解释性和交互性设计促进了多轮对话中的人机协同创作，增强了音频制作中的创意控制和适应性

<img src="https://img.saraba1st.com/forum/202307/27/231415e7ax7xmuacwuaz3d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-231205.jpg</strong> (248.2 KB, 下载次数: 0)

下载附件

2023-7-27 23:14 上传

WavJourney的概览图，LLM首先被提示成为一名音频剧本作家，而作为音频的概念表现，音频脚本为用户提供了交互式且可解释的界面，然后使用脚本编译器来编译音频脚本并作为计算机程序执行，执行过程也可以由一组专家音频生成模型提供支持，图中示例说明了一个科幻小说场景的创作

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 741#       发表于 2023-7-27 23:31

WebArena

用于构建自治自主代理浏览的现实Web环境

项目主页:https://webarena.dev/

<img src="https://img.saraba1st.com/forum/202307/27/232830op5rnkh6f4r9ijkn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232015.jpg</strong> (100.75 KB, 下载次数: 0)

下载附件

2023-7-27 23:28 上传

随着生成式人工智能的进步，自主代理通过自然语言命令管理日常任务的令人兴奋的潜力已经显现，然而，当前的代理主要是在简化的合成环境中创建和测试的，这远远限制了现实世界的场景表现

在本文中，构建了一个高度真实且可重复的代理命令和控制环境，具体来说，本文专注于在网站上执行任务的自主代理，并创建一个具有四个常见领域的功能齐全的网站的环境：电子商务、社交论坛讨论、协作软件开发和内容管理

环境充满了工具(例如地图)和外部知识库(例如用户手册等)，以鼓励模型像人类一样解决任务，以WebArena环境为基础，构建了一组基准任务，重点在于评估任务完成的功能正确性

基准测试中的任务是多样化的、长期的，旨在模拟人类在互联网上日常执行的任务，本文设计并实现了多个自主代理，集成了行动前推理等最新技术

结果表明，解决复杂任务具有挑战性：其中最好的基于GPT-4的代理仅实现了10.59%的端到端任务成功率，这些结果凸显了进一步开发强大代理的必要性，当前最先进的LM在这些现实任务中的性能还远未达到完美，并且WebArena可以用来衡量这种进展

WebArena是一个独立的、可本地托管的Web环境，用于构建自主代理，WebArena创建了四个流行类别的网站，其功能和数据模仿了现实世界的同等内容

为了模拟人类解决问题的能力，WebArena还嵌入了工具和知识资源作为独立网站，WebArena引入了将高级现实自然语言命令解释为具体的基于网络的交互的基准，同时提供带注释的程序，旨在以编程方式验证每个任务的功能正确性

<img src="https://img.saraba1st.com/forum/202307/27/232849tnj03naxjdv3vn9j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232139.jpg</strong> (130.94 KB, 下载次数: 0)

下载附件

2023-7-27 23:28 上传

可以在WebArena中执行的全面的高级任务，完成此类任务需要复杂的长期规划和推理能力，为了实现所述的目标，代理需要通过搜索wiki来找出匹兹堡有哪些艺术博物馆，之后应该在地图上识别每个博物馆的位置，并根据收集的信息优化行程，最后，代理还需要使用计划的路线更新相应存储库中的README文件

<img src="https://img.saraba1st.com/forum/202307/27/232854hv7sssaerus9assn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232307.jpg</strong> (99.24 KB, 下载次数: 0)

下载附件

2023-7-27 23:28 上传

将观察设计为网页的URL和内容，并提供将内容表示为屏幕截图、HTML DOM树和可访问树的选项，中图和右图的内容都被修剪以节省演示空间

<img src="https://img.saraba1st.com/forum/202307/27/232905t6v9o11s961m7s12.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232500.jpg</strong> (103.28 KB, 下载次数: 0)

下载附件

2023-7-27 23:29 上传

在WebArena中可以进行的操作

<img src="https://img.saraba1st.com/forum/202307/27/232956zskz9ezakkzwxplk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232533.jpg</strong> (142.11 KB, 下载次数: 0)

下载附件

2023-7-27 23:29 上传

WebArena内的资源与网站分类

两种评估方法与自主代理行动的评估成绩:

<img src="https://img.saraba1st.com/forum/202307/27/233011ycr64xmavibcxlpm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232619.jpg</strong> (233.98 KB, 下载次数: 0)

下载附件

2023-7-27 23:30 上传

<img src="https://img.saraba1st.com/forum/202307/27/233011oiivmvix6fzxflfw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232639.jpg</strong> (71.45 KB, 下载次数: 0)

下载附件

2023-7-27 23:30 上传

成功与失败的任务分布:

<img src="https://img.saraba1st.com/forum/202307/27/233058zukakhpkkjddla6u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232715.jpg</strong> (140.72 KB, 下载次数: 0)

下载附件

2023-7-27 23:30 上传

与其他类似基准的对比:

<img src="https://img.saraba1st.com/forum/202307/27/233112oocc911omb0wzwk9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-232728.jpg</strong> (140.72 KB, 下载次数: 0)

下载附件

2023-7-27 23:31 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 742#       发表于 2023-7-27 23:51

 本帖最后由 Machinery 于 2023-7-27 23:52 编辑 

HQTrack

高质量追踪任何事物

github项目地址:https://github.com/jiawen-zhu/HQTrack

视觉对象跟踪是计算机视觉中的一项基本的计算机视频任务，最近感知算法的能力显著增强，使得单/多目标和基于框/掩码的跟踪得以统一，其中SAM模型(Segment Anything Model)备受关注

在本文中提出了HQTrack，一个用于高质量跟踪视频中任何内容的框架，HQTrack主要由视频多对象分割器(VMOS/video multi-object segmenter)和掩码细化器(MR/mask refiner)组成

给定视频初始帧中要跟踪的对象，VMOS会将对象掩码传播到当前帧，此阶段的掩码结果尚且不够准确，因为VMOS是在几个较为接近的视频对象分割(VOS)数据集上进行训练的，其泛化到复杂场景和角落场景的能力有限

为了进一步提高跟踪掩码的质量，采用了预训练的MR模型来细化跟踪结果，作为对范式有效性的证明，在不使用任何测试时数据增强和模型集成等技巧的情况下，HQTrack在视觉对象跟踪和分割(VOTS2023/Visual Object Tracking and Segmentation)挑战中达成了第二名

HQTrack框架概览图:

<img src="https://img.saraba1st.com/forum/202307/27/235008seebn6dbeeeg88e0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-234729.jpg</strong> (199.68 KB, 下载次数: 0)

下载附件

2023-7-27 23:50 上传

消融实验测试:

<img src="https://img.saraba1st.com/forum/202307/27/235037z0bb888jmm9m88mr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-234757.jpg</strong> (173.82 KB, 下载次数: 0)

下载附件

2023-7-27 23:50 上传

评估结果:

<img src="https://img.saraba1st.com/forum/202307/27/235105aj0lk1gf0bqpbpfr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-234824__01.jpg</strong> (455.78 KB, 下载次数: 0)

下载附件

2023-7-27 23:51 上传

<img src="https://img.saraba1st.com/forum/202307/27/235105qnmzuzmgngrtjumn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-234910.jpg</strong> (50.94 KB, 下载次数: 0)

下载附件

2023-7-27 23:51 上传

github页面:

<img src="https://img.saraba1st.com/forum/202307/27/235126ve2h5qclgykccyse.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230727-234646.jpg</strong> (372.18 KB, 下载次数: 0)

下载附件

2023-7-27 23:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 743#       发表于 2023-7-28 01:08

FacTool

生成式人工智能中的事实检测，多任务和多领域场景的增强工具框架

github项目仓库:https://github.com/GAIR-NLP/factool

生成式预训练模型的出现促进了高质量文本的合成，但也给识别生成文本中的事实错误带来了挑战

特别是，现在更广泛的任务在由生成模型处理时面临着越来越大的包含事实错误的风险，同时生成的文本往往很长，并且缺乏对各个事实的明确定义的粒度，以及缺乏明确证据的事实核查过程

<img src="https://img.saraba1st.com/forum/202307/28/010657w17x7pp9xw1x0tsq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-005756.jpg</strong> (131.6 KB, 下载次数: 0)

下载附件

2023-7-28 01:06 上传

考虑到上述挑战，提出了FacTool，这是一种与任务和领域无关的框架，用于检测大型语言模型(例如ChatGPT等)生成的文本的事实错误，对四种不同任务(基于知识的QA、代码生成、数学推理和科学文献分析)的实验都表明了该方法的有效性

<img src="https://img.saraba1st.com/forum/202307/28/010704gtszs4g0w4md0adw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-005951.jpg</strong> (108.74 KB, 下载次数: 0)

下载附件

2023-7-28 01:07 上传

已发布的事实性检测方法在生成的响应和基于收集的证据进行验证的声明方面的比较， “scenario”代表相应方法已被证明是合理的任务和领域，其中“sci”代表科学

<img src="https://img.saraba1st.com/forum/202307/28/010732od2vqhvzvcvicqr0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-010041.jpg</strong> (82 KB, 下载次数: 0)

下载附件

2023-7-28 01:07 上传

不同任务的事实错误定义

<img src="https://img.saraba1st.com/forum/202307/28/010740ofc1psf8f0z08y81.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-010333.jpg</strong> (283.07 KB, 下载次数: 0)

下载附件

2023-7-28 01:07 上传

提出的四个领域的事实检测框架：基于知识的QA、代码生成、数学问题解决和科学文献分析写作

<img src="https://img.saraba1st.com/forum/202307/28/010745znnrx6z6rvt7k4ot.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-010404.jpg</strong> (66.16 KB, 下载次数: 0)

下载附件

2023-7-28 01:07 上传

用于检测代码生成中的事实错误的单元测试库生成

相关数据与评估结果:

<img src="https://img.saraba1st.com/forum/202307/28/010801gc24w4jl8m9jftf9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-010516.jpg</strong> (152.86 KB, 下载次数: 0)

下载附件

2023-7-28 01:08 上传

<img src="https://img.saraba1st.com/forum/202307/28/010801pm9rbzyburwjtjhi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-010548.jpg</strong> (317.87 KB, 下载次数: 0)

下载附件

2023-7-28 01:08 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 744#       发表于 2023-7-28 01:17

StableSwarmUI

Emad推荐的Stability官方ALPHA版本Webui竞品(而且更适合SDXL)

github项目主页:https://github.com/Stability-AI/StableSwarmUI

<img src="https://img.saraba1st.com/forum/202307/28/011711a3vzvpv9htfh3hf3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-011702.jpg</strong> (298.8 KB, 下载次数: 0)

下载附件

2023-7-28 01:17 上传

<img src="https://img.saraba1st.com/forum/202307/28/011625c25eg5765vjjvojz.jpg" referrerpolicy="no-referrer">

<strong>20230728_011533.jpg</strong> (222.41 KB, 下载次数: 0)

下载附件

2023-7-28 01:16 上传

<img src="https://img.saraba1st.com/forum/202307/28/011625f6cbzoz72wbmhc7w.jpg" referrerpolicy="no-referrer">

<strong>20230728_011536.jpg</strong> (196.65 KB, 下载次数: 0)

下载附件

2023-7-28 01:16 上传

<img src="https://img.saraba1st.com/forum/202307/28/011807lziqccqkqki5lwbr.jpg" referrerpolicy="no-referrer">

<strong>20230728_011754.jpg</strong> (134.6 KB, 下载次数: 0)

下载附件

2023-7-28 01:18 上传

<img src="https://img.saraba1st.com/forum/202307/28/011625r2z3gaivifboy9yi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230728-011522.jpg</strong> (266.41 KB, 下载次数: 0)

下载附件

2023-7-28 01:16 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 745#       发表于 2023-7-29 02:53

PointOdyssey

用于长程点跟踪的大规模综合数据集

项目主页:https://pointodyssey.com/

gdrive数据集下载:https://drive.google.com/drive/folders/1AyoI_9fw-G1X1MYEmOBuTFnCjX6PjoH6?usp=drive_link

hugface数据集下载:https://huggingface.co/datasets/aharley/pointodyssey/tree/main

PointOdyssey 是一个大规模合成数据集，用于训练和评估长程细粒度追踪算法，研究组的目标着重于通过具有自然运动(naturalistic motion)的长视频来推进SOTA技术，以匹配收集的人体动作数据3D扫描，在具有随机3D素材的室外场景以及精心构建的室内场景中重新利用人类和动物动作的捕捉数据实现的

通过随机多样化角色外观、运动轮廓、材质、照明、3D素材和天气效果来创造组合的多样性

数据集目前包含105个视频，平均长度为2000帧，对应的标注内容比之前的工作多了几个数量级，现有模型可以在PointOdyssey数据集中从头开始训练，表现优于已开源的其他变体

最后对PIPs点跟踪方法进行了修改，拓宽了其时间感受野，从而提高了其在PointOdyssey以及两个现实世界基准上的性能

与其他数据集的对比:

数据生成收集工作流程，利用真实人体捕捉数据的同时保证多样性:

PIPs点跟踪方法概览图：
上方:对于任何查询点pt，首先计算相似度代价体积(similarity cost volume)Cpt，通过利用局部背景下的信息特征并结合全局指导来保持一致和稳健的跟踪

下方：使用1D Resnet迭代更新位置轨迹

相关评估结果:

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 746#       发表于 2023-7-29 03:18

Take-A-Photo(TAP)

点云模型的3D到2D生成预训练

项目主页:https://tap.ivg-research.xyz

github项目地址:https://github.com/wangzy22/TAP

随着MAE引领的掩码图像建模的势不可挡的趋势，生成式预训练在提升2D视觉基础模型的性能方面表现出了巨大的潜力，然而在3D视觉中，对基于Transformer的主干的过度依赖以及点云的无序自然性限制了生成式预训练的进一步发展

在本文中提出了一种新颖的3D到2D生成预训练方法，适用于任何点云模型，通过交叉注意力机制从不同的指示姿态生成视图图像作为预训练方案，生成视图图像比点云对应物具有更精确的监督，从而可以帮助3D主干更好地理解点云的几何结构和立体关系

实验结果证明了3D到2D生成预训练相对于之前的预训练方法的优越性，本方法还可以有效指导了架构的性能提升方法，在对ScanObjectNN分类和ShapeNetPart分割任务进行微调时实现了SOTA性能

<img src="https://img.saraba1st.com/forum/202307/29/031621b864z7wg5lekeseh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230729-030754.jpg</strong> (39.01 KB, 下载次数: 0)

下载附件

2023-7-29 03:16 上传

3D到2D生成预训练的原理流程，照片方法模块将姿态条件显式编码为来自主干模型的3D特征，2D生成器将姿态条件特征解码为不同的视图图像

<img src="https://img.saraba1st.com/forum/202307/29/031615yrvwdm0o666gkov7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230729-030723.jpg</strong> (56.8 KB, 下载次数: 0)

下载附件

2023-7-29 03:16 上传

TAP预训练方法的工作流程，首先设计了一个查询生成器来编码姿态条件并附加交叉注意力层，根据姿态条件将3D点云特征F3d转换为2D视图图像特征F2d，2D生成器预测的与姿势相关的视图图像通过MSE损失由基准真实(ground truth)视图图像进行监督训练

可视化的生成预训练样本效果图，第一行为TAP生成结果，第二行为基准真实图像:

<img src="https://img.saraba1st.com/forum/202307/29/031820rcbsl4v41prcv54w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230729-031455.jpg</strong> (90.6 KB, 下载次数: 0)

下载附件

2023-7-29 03:18 上传

相关评估结果:

<img src="https://img.saraba1st.com/forum/202307/29/031655kr38zc2h9vsb2q8v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230729-031422.jpg</strong> (114.93 KB, 下载次数: 0)

下载附件

2023-7-29 03:16 上传

<img src="https://img.saraba1st.com/forum/202307/29/031655vzbhtkhnz1sg97zs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230729-031439.jpg</strong> (215.1 KB, 下载次数: 0)

下载附件

2023-7-29 03:16 上传

<img src="https://img.saraba1st.com/forum/202307/29/031912uu7i79lo108pu7b8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230729-031518.jpg</strong> (142.51 KB, 下载次数: 0)

下载附件

2023-7-29 03:19 上传

消融实验:

<img src="https://img.saraba1st.com/forum/202307/29/031846la4c3aancpnlzpay.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230729-031534.jpg</strong> (210.32 KB, 下载次数: 0)

下载附件

2023-7-29 03:18 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 747#       发表于 2023-7-30 03:00

peS2o

4000万篇规模的论文已清洗数据集，简介如下

github项目页:https://github.com/allenai/peS2o

hugface数据集地址:https://huggingface.co/datasets/allenai/peS2o

<img src="https://img.saraba1st.com/forum/202307/30/030024f12ctenutaq1xc79.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230730-025854.jpg</strong> (111.43 KB, 下载次数: 0)

下载附件

2023-7-30 03:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 748#       发表于 2023-7-30 03:06

TransGPT · 致远

TransGPT，国内首款开源交通大模型

github项目地址:https://github.com/DUOMO/TransGPT

模型权重下载:https://huggingface.co/DUOMO-Lab/TransGPT-v0

Demo1:https://bc18ebd4c22f6cfb17.gradio.live/

Demo2:https://d7a24e72c1e7f12d9e.gradio.live/

主要致力于在真实交通行业中发挥实际价值，能够实现交通情况预测、智能咨询助手、公共交通服务、交通规划设计、交通安全教育、协助管理、交通事故报告和分析、自动驾驶辅助系统等功能

<img src="https://img.saraba1st.com/forum/202307/30/030644wpi00pkgsr6k6si4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230730-030444__01.jpg</strong> (758.11 KB, 下载次数: 0)

下载附件

2023-7-30 03:06 上传

<img src="https://img.saraba1st.com/forum/202307/30/030644fzebfaecxiyprp8p.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230730-030444__02.jpg</strong> (493.41 KB, 下载次数: 0)

下载附件

2023-7-30 03:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)

*****

####  Machinery  
##### 749#       发表于 2023-7-30 03:12

 Llama2-Code-Interpreter

基于llama2的Code Interpreter(openai)复刻实现

github项目地址:https://github.com/SeungyounShin/Llama2-Code-Interpreter

<img src="https://img.saraba1st.com/forum/202307/30/031202df8ro338pfyl696h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230730-031136.jpg</strong> (216.12 KB, 下载次数: 0)

下载附件

2023-7-30 03:12 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 750#       发表于 2023-7-31 23:07

Med-Flamingo

多模态医疗小样本学习者

github项目地址:https://github.com/snap-stanford/med-flamingo

医学是一个多方面的领域，需要各种跨模态的综合信息处理，医学视觉语言生成模型(MVLMs/Medical generative vision-language models)朝这个方向迈出了第一步，并带来了许多令人兴奋的临床应用

然而，现有的模型通常必须在相当大的下游数据集上进行微调，这造成了很大的限制，因为在许多医疗应用中数据稀缺，这样就需要一种能够从少数示例中实时学习的模型

提出了Med-Flamingo，一种适用于医学领域的多模态少样本学习者，基于OpenFlamingo-9B，通过继续对来自出版物和教科书的配对的交错医学图像文本数据集进行预训练，Med-Flamingo解锁了少样本医学视觉问答生成(VQA/few-shot generative medical visual question answering)能力，在多个数据集上对其进行了评估，其中包括一个新颖的具有挑战性的视觉USMLE风格问题的开放VQA数据集

此外，对医学VQA生成进行了首次人类评估，医生在交互式应用程序中审查问题，在这个过程中模型进行匿名生成答案

Med-Flamingo在临床医生的评分中将生成医学VQA的性能提高了20%，并且首先实现了多模态医学少样本问题适应，例如基本原理生成等

<img src="https://img.saraba1st.com/forum/202307/31/230544c9sksfv1zffgskfz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-225649.jpg</strong> (88.08 KB, 下载次数: 0)

下载附件

2023-7-31 23:05 上传

Med-Flamingo通过文本和视觉信息生成开放式响应来回答复杂的多模态医学问题的示例

<img src="https://img.saraba1st.com/forum/202307/31/230550kjn0azqat1spy82s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-225754.jpg</strong> (160.51 KB, 下载次数: 0)

下载附件

2023-7-31 23:05 上传

Med-Flamingo模型概览图与学习的三个步骤:
首先使用来自一般医学领域的配对交错图像文本数据(源自出版物和教科书)来预训练Med-Flamingo模型，在OpenFlamingo检查点初始化模型，继续对医学图像文本数据进行预训练

其次，执行少样本视觉问答生成(VQA)，为此利用了两个现有的医学VQA数据集和一个新的Visual USMLE风格数据集

最后与临床医生进行了一项人类评分研究，在给定图像、问题和正确答案的背景下对各种生成进行评分，人类评估是通过专用应用程序进行的，并产生临床评估分数，作为主要评估指标

<img src="https://img.saraba1st.com/forum/202307/31/230557tr824hq0h7j7rk2h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-230128.jpg</strong> (81.01 KB, 下载次数: 0)

下载附件

2023-7-31 23:05 上传

MTB数据集的医学教材类别分布图，使用Claude-1模型将每个书名分类为49个手动创建的类别之一或“其他”类别

<img src="https://img.saraba1st.com/forum/202307/31/230606cyamaw8cdn1ocnmf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-230246.jpg</strong> (106.95 KB, 下载次数: 0)

下载附件

2023-7-31 23:06 上传

通过示例说明多模态医疗少样本提示，这里的少样本提示允许用户自定义响应格式，例如，为所提供的答案提供理由等，此外，多模态少样本提示也可以成为提供诸如从医学文献中检索到的相关上下文的能力

<img src="https://img.saraba1st.com/forum/202307/31/230612fby0zo7d6zyztngz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-230228.jpg</strong> (92.67 KB, 下载次数: 0)

下载附件

2023-7-31 23:06 上传

 人类评分评估界面

相关基准指标评估结果:

<img src="https://img.saraba1st.com/forum/202307/31/230643zds0au2f2iigiz5g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-230427.jpg</strong> (216.82 KB, 下载次数: 0)

下载附件

2023-7-31 23:06 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 751#       发表于 2023-7-31 23:32

 本帖最后由 Machinery 于 2023-7-31 23:33 编辑 

Seal-3D

NERF(神经辐射场/Neural Radiance Fields)的交互式像素级编辑

项目主页:https://windingwind.github.io/seal-3d/

github项目仓库:https://github.com/windingwind/seal-3d/

随着隐式神经表征或神经辐射场(NeRF)的流行，迫切需要能够与隐式3D模型交互的编辑方法，以完成处理后重建场景和3D内容创建等任务

虽然之前的作品从不同角度探索了NeRF编辑方法，但它们在编辑灵活性、质量和速度方面受到限制，无法提供直接的编辑响应和即时预览效果，这其中最关键的挑战是构思一种本地可编辑的神经表征，它可以直接反映编辑指令并立即更新效果

为了弥合这之间的差距，提出了一种新的隐式表征交互式编辑方法和系统，称之为Seal-3D，它允许用户以像素级和自由的方式编辑NeRF模型，并具有广泛的类NeRF主干，可以立即编辑效果并预览

为了实现这些效果，提案的代理函数将编辑指令映射到NeRF模型的原始空间，以及具有局部预训练和全局微调的师生训练策略来解决这些挑战，NeRF编辑系统可以进行各种编辑类型，可以实现1秒级的交互速度来实现引人注目的编辑效果

<img src="https://img.saraba1st.com/forum/202307/31/233104x36sbr5nc46cgz8e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-232328.jpg</strong> (101.95 KB, 下载次数: 0)

下载附件

2023-7-31 23:31 上传

第一个交互式像素级NeRF编辑工具，设计了一种交互式用户编辑方法和系统，Seal-3D，左图为通过新颖的预训练策略实现即时(小于1s)预览，通过短时间(1~2分钟)的微调，可以进一步获得高质量的编辑结果，右图为编辑工具的编辑结果与左上的原始表面上，其丰富的阴影细节在视图上一致

<img src="https://img.saraba1st.com/forum/202307/31/233112cpg063gtf1hh05hz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-232634.jpg</strong> (95.21 KB, 下载次数: 0)

下载附件

2023-7-31 23:31 上传

编辑框架图示，左方为用户编辑后目标空间的3D点和视图方向映射到原始空间，以从教师模型fTθ获取指导ct和σt，用于学生训练，右图为学生训练由两个阶段组成，快速预训练，通过局部损失更新网络的部分参数来提供即时预览，以及全局损失的微调

重建评估与对比结果:

<img src="https://img.saraba1st.com/forum/202307/31/233142q0lmpxmx4gh7ichv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-232939.jpg</strong> (197.57 KB, 下载次数: 0)

下载附件

2023-7-31 23:31 上传

<img src="https://img.saraba1st.com/forum/202307/31/233142crycrfcjre1fiy7d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-232950.jpg</strong> (198.35 KB, 下载次数: 0)

下载附件

2023-7-31 23:31 上传

<img src="https://img.saraba1st.com/forum/202307/31/233142dnzm1x41aqyqcbpb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-233005.jpg</strong> (192.54 KB, 下载次数: 0)

下载附件

2023-7-31 23:31 上传

全局微调效果与消融实验结果:

<img src="https://img.saraba1st.com/forum/202307/31/233156vcfxjly4f5395cjs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-233019.jpg</strong> (220.65 KB, 下载次数: 0)

下载附件

2023-7-31 23:31 上传

项目主页的样本预览:

<img src="https://img.saraba1st.com/forum/202307/31/233213y5fvogdrrgipp5dd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-232612.jpg</strong> (123.11 KB, 下载次数: 0)

下载附件

2023-7-31 23:32 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 752#       发表于 2023-8-1 00:01

UnifiedInstructionTuning

探索指令微调的格式一致性

项目主页:https://github.com/thunlp/UnifiedInstructionTuning

指令微调已成为增强大型语言模型，使其遵循人类指令的有效方法，实验结果表明，增加训练数据中指令的多样性和数量可以持续增强泛化性能，这有助于最近收集各种指令并将现有指令调整数据集集成到更大的集合中，然而，不同的用户有其独特的指令表达方式，不同数据集的指令风格和格式往往存在差异，即格式不一致

在这项工作中，研究了格式不一致如何影响指令调整的性能，并提出了一个名为“统一指令微调”(UIT/Unified Instruction Tuning)的框架，它调用了OpenAI API在不同指令微调数据集之间自动进行格式转换

实验证明UIT成功地提高了未见指令的泛化性能，这凸显了格式一致性对于指令调优的重要性，为了使UIT框架更加实用，进一步提出了一种新颖的基于困惑度的去噪方法来降低自动格式转换的噪声

还训练了一个更小的离线模型，该模型实现了与 OpenAI API相当的格式转换能力，以降低实践成本

<img src="https://img.saraba1st.com/forum/202307/31/235932e2axaktf4k0acxn2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-234907.jpg</strong> (138.24 KB, 下载次数: 0)

下载附件

2023-7-31 23:59 上传

提议的格式转换框架，适用于两种设置情况：测试时传输和训练​​时传输，s1，····，sN表示原始指令格式的训练数据，t1，···，tN表示所有转换的目标格式的训练数据

<img src="https://img.saraba1st.com/forum/202307/31/235944etf7vfv7tfs8u8mu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-235104.jpg</strong> (85.5 KB, 下载次数: 0)

下载附件

2023-7-31 23:59 上传

具有代表性的不同格式指令微调数据集

<img src="https://img.saraba1st.com/forum/202307/31/235949flxcr48rcg8z1gvg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-235152.jpg</strong> (177.28 KB, 下载次数: 0)

下载附件

2023-7-31 23:59 上传

使用UIT转换指令格式，现有的指令格式在不同的数据集上表现出差异，可以分为三种不同的异构格式：任务级别、实例级别和关键字级别，UIT利用种子并行数据(seed parallel data)自动进行不同格式之间的格式转换

<img src="https://img.saraba1st.com/forum/202307/31/235955ivdv9bobbyuqnlsy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-235347.jpg</strong> (151.44 KB, 下载次数: 0)

下载附件

2023-7-31 23:59 上传

使用GPT3.5进行格式转换的示例，其中使用3个并行示例提示模型生成第4个示例的目标指令

<img src="https://img.saraba1st.com/forum/202308/01/000004srzqian2dr2b90zv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-235456.jpg</strong> (288.3 KB, 下载次数: 0)

下载附件

2023-8-1 00:00 上传

测试时格式转换与训练时格式转换的评估结果

<img src="https://img.saraba1st.com/forum/202308/01/000009baz428bewqdf3q7u.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-235609.jpg</strong> (30.52 KB, 下载次数: 0)

下载附件

2023-8-1 00:00 上传

不同样本数量的测试和训练时去噪策略的性能

相关评估结果:

<img src="https://img.saraba1st.com/forum/202308/01/000032dn50az8inlc5cfnl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-235719.jpg</strong> (304.12 KB, 下载次数: 0)

下载附件

2023-8-1 00:00 上传

实例演示:

<img src="https://img.saraba1st.com/forum/202308/01/000051a5cn5ncoq3cjppqq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-235753.jpg</strong> (418.29 KB, 下载次数: 0)

下载附件

2023-8-1 00:00 上传

<img src="https://img.saraba1st.com/forum/202308/01/000051wrregz2c2444er3l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230731-235802.jpg</strong> (548.66 KB, 下载次数: 0)

下载附件

2023-8-1 00:00 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 753#       发表于 2023-8-2 01:52

ToolLLM

使大型语言模型熟练掌握16000多个真实世界的API使用

github项目地址:https://github.com/OpenBMB/ToolBench.

尽管开源大型语言模型(LLM)及其变体(例如LLaMA和Vicuna)最近取得了很大进步，但它们在执行更高级别的任务方面仍然受到很大限制，例如遵循人类指令使用外部工具(API)，这是因为当前的指令调优主要集中在基本语言任务而不是工具使用领域，这与私有的SOTA LLM(例如ChatGPT等)形成鲜明对比，后者展示了出色的工具使用能力，但不幸的是闭源的

为了促进开源LLM的工具使用能力，引入了ToolLLM，这是一个数据构建、模型训练和评估的通用工具使用框架

首先介绍的是ToolBench，一个供工具使用的指令调整数据集，通过使用ChatGPT自动化创建的，具体来说，从RapidAPI Hub收集了16464个真实世界的RESTful API，涵盖49个类别，然后通过提示ChatGPT生成涉及这些API的各种人工指令，涵盖单工具和多工具场景，最后使用ChatGPT为每条指令搜索有效的解决方案路径，API调用链(chain of API calls)

为了使搜索过程更加高效，开发了一种新颖的基于深度优先搜索的决策树(DFSDT/depth-first search-based decision tree)，使LLM能够评估多个推理轨迹并扩展搜索空间，通过实验证明了DFSDT能够显著增强LLM的规划和推理能力

为了有效评估工具使用情况，开发了一个自动评估器：ToolEval，在ToolBench上微调LLaMA并获得ToolLLaMA，ToolEval表明ToolLLaMA表现出执行复杂指令和泛化到未见过的API的卓越能力，并且表现出了与ChatGPT相当的性能。

为了使工作流程更加实用，还设计了一个神经API检索器来为每条指令推荐合适的API，从而无需手动选择API

<img src="https://img.saraba1st.com/forum/202308/02/015127aorpn2knmn47qfsr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-014327.jpg</strong> (74.14 KB, 下载次数: 0)

下载附件

2023-8-2 01:51 上传

构建ToolBench的三个阶段以及如何训练API检索器和ToolLLaMA，在指令推理过程中，API检索器向ToolLLaMA推荐相关API，ToolLLaMA执行多轮API调用以得出最终答案，整个推理过程由ToolEval评估

<img src="https://img.saraba1st.com/forum/202308/02/015137kgd4ngcdsexx9rnp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-014515.jpg</strong> (39.06 KB, 下载次数: 0)

下载附件

2023-8-2 01:51 上传

工具使用评估中不同方法的通过率和胜率(越高越好)，对于胜率，将每种方法与ChatGPT-ReACT进行比较，DFSDT是在ReACT基础上改进的推理策略，ToolLLaMA超越了Text-Davinci-003，几乎与ChatGPT性能相当

<img src="https://img.saraba1st.com/forum/202308/02/015142fmp3pe4qmia3semq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-014645.jpg</strong> (81.6 KB, 下载次数: 0)

下载附件

2023-8-2 01:51 上传

ToolBench与用于工具学习的其他著名指令调整数据集的比较

<img src="https://img.saraba1st.com/forum/202308/02/015147vclxrht2q51cowrh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-014726.jpg</strong> (157.03 KB, 下载次数: 0)

下载附件

2023-8-2 01:51 上传

左图为RapidAPI的层次结构，右图为指令生成过程

<img src="https://img.saraba1st.com/forum/202308/02/015152ikf9ggefuqiegmk2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-014825.jpg</strong> (155.41 KB, 下载次数: 0)

下载附件

2023-8-2 01:51 上传

左图为模型推理过程中DFSDT与传统CoT或ReACT策略的比较，右图展示了使用ChatGPT的解决方案路径标注过程的一部分

相关评估结果:

<img src="https://img.saraba1st.com/forum/202308/02/015207ww25bc7tnbqkxwb2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-015002.jpg</strong> (174.57 KB, 下载次数: 0)

下载附件

2023-8-2 01:52 上传

<img src="https://img.saraba1st.com/forum/202308/02/015207bxfezv1zlzu89p1h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-015014.jpg</strong> (120.78 KB, 下载次数: 0)

下载附件

2023-8-2 01:52 上传

<img src="https://img.saraba1st.com/forum/202308/02/015207hrjrll0ljrken8jz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-015028.jpg</strong> (135.89 KB, 下载次数: 0)

下载附件

2023-8-2 01:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 754#       发表于 2023-8-2 02:10

LP-MusicCaps

基于大型语言模型的伪标签音乐字幕说明

github项目地址:https://github.com/seungheondoh/lp-music-caps

自动音乐字幕说明可以为给定的音乐曲目生成自然语言描述，在增强对大量音乐数据的理解与关系方面具有巨大的潜力，尽管这非常重要，但由于现有音乐语言数据集的收集过程成本高昂且耗时且规模有限，研究人员面临着挑战

为了解决这个数据稀缺问题，可以使用大型语言模型(LLM)从大规模标签数据集中人工生成描述句子，在这个过程中研究组生成了大约220万个字幕说明以及50万个音频片段，将其称为基于大型语言模型的伪标签音乐字幕说明数据集，简称LP-MusicCaps

使用自然语言处理和人类评估领域使用的各种定量评估指标对大规模伪标签音乐字幕说明数据集进行了系统评估，此外还使用数据集训练了基于Transformer的音乐字幕说明模型，并在零样本和迁移学习设置下对其进行了评估，结果表明，提出的方法优于监督的基线模型效果

<img src="https://img.saraba1st.com/forum/202308/02/021032srxwdwoouwmzgb8r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-020334.jpg</strong> (142.19 KB, 下载次数: 0)

下载附件

2023-8-2 02:10 上传

通过向大型语言模型提供指令和手动标注标签来生成伪字幕的过程

<img src="https://img.saraba1st.com/forum/202308/02/021037w303spzw22iys6zz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-020443.jpg</strong> (207.96 KB, 下载次数: 0)

下载附件

2023-8-2 02:10 上传

数据集样本示例

<img src="https://img.saraba1st.com/forum/202308/02/021042ijjsb80eclasr421.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-020507.jpg</strong> (257.38 KB, 下载次数: 0)

下载附件

2023-8-2 02:10 上传

提案的方法与其他伪标签生成方法的指标对比结果以及AB测试的正负平评估结果

<img src="https://img.saraba1st.com/forum/202308/02/021046gc8fcp2hho3fip2j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-020554.jpg</strong> (163.04 KB, 下载次数: 0)

下载附件

2023-8-2 02:10 上传

与其他数据集的对比以及提案的Transformer跨模态模型架构

相关评估结果:

<img src="https://img.saraba1st.com/forum/202308/02/021051el6uvxl5go25zaul.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-020632.jpg</strong> (134.85 KB, 下载次数: 0)

下载附件

2023-8-2 02:10 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 755#       发表于 2023-8-2 02:33

MovieChat

密集Token到稀疏记忆以实现长视频理解

项目主页:https://rese1f.github.io/MovieChat

github项目地址:https://github.com/rese1f/MovieChat

最近，通过集成视频基础模型和大型语言模型，可以构建视频理解系统克服特定的预定义视觉任务(specific pre-defined vision tasks)的限制，然而现有的系统只能处理帧数很少的视频，对于长视频，计算复杂性、显存成本和长态时间连续理解是依然需要克服的挑战

受Atkinson-Shiffrin记忆模型的启发，开发了一种记忆机制，包括快速更新的短期记忆和紧凑的持续长期记忆，通过在Transformer中使用Token作为记忆的载体，MovieChat 在长视频理解方面实现了SOTA性能

<img src="https://img.saraba1st.com/forum/202308/02/023203cr4zkyz493keyk3d.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-022309__01.jpg</strong> (169.05 KB, 下载次数: 0)

下载附件

2023-8-2 02:32 上传

Y轴为千兆字节(GB)下的GPU显存成本与X轴的帧数对比，在224×224的分辨率下测试所有方法的仅视觉推理，无需帧采样，虽然之前的方法只能支持大约100帧的推理，但MovieChat可以在24GB显存显卡上处理超过10K帧的视频，在每帧GPU显存成本平均增加(每帧需要21.3KB到200MB)方面，MovieChat比其他方法有近10000倍的优势

<img src="https://img.saraba1st.com/forum/202308/02/023207ziobnicfco5bocx2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-022637.jpg</strong> (147.71 KB, 下载次数: 0)

下载附件

2023-8-2 02:32 上传

MovieChat框架，通过使用滑动窗口方法(sliding window approach)来提取视频特征并以Token形式表征它们，然后将其按顺序逐帧输入到短期记忆中，短期记忆有固定的长度，当达到设定的限制时，最早的令牌将被弹出并合并到长期记忆中

总共设计了两种推理模式：全局模式，专门利用长期记忆；断点模式，额外将当前短期记忆作为视频表征的一部分，断点模式允许在特定时刻理解视频，最后经过投影层后，视频表征被输入到大型语言模型中用以与用户交互

使用样例:

<img src="https://img.saraba1st.com/forum/202308/02/023230kfgedg17iqefix72.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-023038.jpg</strong> (455.63 KB, 下载次数: 0)

下载附件

2023-8-2 02:32 上传

…

相关评估结果:

<img src="https://img.saraba1st.com/forum/202308/02/023250iuirxbjgzoboorqf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-023058.jpg</strong> (113 KB, 下载次数: 0)

下载附件

2023-8-2 02:32 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 756#       发表于 2023-8-2 11:36

 本帖最后由 Machinery 于 2023-8-2 11:37 编辑 

UnIVAL

图像、视频、音频与语言等任务的统合模型

github项目地址:https://github.com/mshukor/UnIVAL

大型语言模型(LLM)使得对通用智能代理的追求不再是一个幻想，构建此类通用模型的一个关键障碍是任务和模式的多样性和异质性(heterogeneity)

一种有希望的解决方案是统合，允许在一个统一的框架内支持无数的任务和模态，虽然在海量数据集上训练的大型模型(例如 Flamingo等)可以支持两种以上的模态，但当前的中小型统一模型仍然仅限于2种模态，通常是图像文本或视频文本

是否有可能有效地构建一个可以支持所有模态的统一模型？本文提出了UnIVAL，不依赖于繁多的数据集大小或数十亿参数的模型

不到0.25B参数的UnIVAL模型超越了两种模态，将文本、图像、视频和音频统一到一个模型中，模型本身基于任务平衡和多模态课程学习在许多任务上进行了有效的预训练

UnIVAL在跨图像和视频文本任务上对比现有SOTA方法具有竞争性能，可以从图像和视频文本模态中学习特征表征，尽管没有对音频进行预训练，但模型在音频文本任务上进行微调时依然实现了有竞争性的性能结果

得益于统合模型，提出了一项通过对不同多模态任务训练的模型进行权重插值来进行多模态模型合并的新研究，展示了它们对于分布外泛化的好处，最后还通过展示任务之间的协同作用来激励统合效果

<img src="https://img.saraba1st.com/forum/202308/02/113335rdsh71cetx977egs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-111825__01.jpg</strong> (260.15 KB, 下载次数: 0)

下载附件

2023-8-2 11:33 上传

UnIVAL模型构架，序列到序列模型统一了架构、任务、输入/输出格式和训练目标(下一个标记预测)，UnIVAL针对图像和视频文本任务进行了预训练，并且可以进行微调以处理预训练期间未使用的新模态(音频文本)和任务(文本到图像生成)

<img src="https://img.saraba1st.com/forum/202308/02/113433egqjyozggofzhogj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113416.jpg</strong> (90.48 KB, 下载次数: 0)

下载附件

2023-8-2 11:34 上传

不同基础模型的比较，UnIVAL方法是在相对较小的数据集上进行预训练的，可以处理图像/视频/音频文本模态

<img src="https://img.saraba1st.com/forum/202308/02/113441zceuref989z8sq9e.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-112304.jpg</strong> (63.88 KB, 下载次数: 0)

下载附件

2023-8-2 11:34 上传

多模态课程学习，在不同阶段对UnIVAL进行不同的预训练:
第一次预训练是在文本语料上进行经典语言模型的训练，第二阶段则是在图像和文本数据上训练模型以获得图像语言模型，第三阶段在视频文本数据上对模型进行额外训练，以获得视频图像语言模型，为了获得多模态统合模型，应该对模型进行多种模态的训练，按照此过程，UnIVAL最终可用于解决图像/视频/音频文本任务

多模态课程学习与单阶段的效果对比，附加为多模态平衡数据集的效果对比，以及不同任务之间的协同作用效果:

<img src="https://img.saraba1st.com/forum/202308/02/113546t5po7wxz27tp2pbs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-112717.jpg</strong> (55.18 KB, 下载次数: 0)

下载附件

2023-8-2 11:35 上传

<img src="https://img.saraba1st.com/forum/202308/02/113546wpivpriy6th2ocvo.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-112727.jpg</strong> (78.46 KB, 下载次数: 0)

下载附件

2023-8-2 11:35 上传

<img src="https://img.saraba1st.com/forum/202308/02/113546supkf2xynxpwibpf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-112930.jpg</strong> (104.73 KB, 下载次数: 0)

下载附件

2023-8-2 11:35 上传

评估结果:

<img src="https://img.saraba1st.com/forum/202308/02/113610t5hdtdtybktqbozs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113058.jpg</strong> (95.32 KB, 下载次数: 0)

下载附件

2023-8-2 11:36 上传

<img src="https://img.saraba1st.com/forum/202308/02/113610vkizfnzn78l87izn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113111.jpg</strong> (189.17 KB, 下载次数: 0)

下载附件

2023-8-2 11:36 上传

<img src="https://img.saraba1st.com/forum/202308/02/113610z8gtl6jg9nxicjk8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113120.jpg</strong> (85.95 KB, 下载次数: 0)

下载附件

2023-8-2 11:36 上传

<img src="https://img.saraba1st.com/forum/202308/02/113610oobzuvnlw1qtneej.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113127.jpg</strong> (114.51 KB, 下载次数: 0)

下载附件

2023-8-2 11:36 上传

<img src="https://img.saraba1st.com/forum/202308/02/113610d1p1adw7ddxt41dp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113140.jpg</strong> (151.44 KB, 下载次数: 0)

下载附件

2023-8-2 11:36 上传

<img src="https://img.saraba1st.com/forum/202308/02/113610eaw3rziz88ereg63.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113149.jpg</strong> (81.23 KB, 下载次数: 0)

下载附件

2023-8-2 11:36 上传

<img src="https://img.saraba1st.com/forum/202308/02/113611clbdzq6n061qlb5h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113158.jpg</strong> (104.96 KB, 下载次数: 0)

下载附件

2023-8-2 11:36 上传

<img src="https://img.saraba1st.com/forum/202308/02/113611z01001zjpt6t9po2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-113211.jpg</strong> (102.81 KB, 下载次数: 0)

下载附件

2023-8-2 11:36 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 757#       发表于 2023-8-2 12:25

AntGPT

大型语言模型可以帮助预测视频的长期动作吗？

项目地址:https://brown-palm.github.io/AntGPT

代码:coming soon

通过了解行动者当前的动作(例如打鸡蛋等)，是否可以预测之后通常会发生什么，能否更好的预测行动者的未来动作(例如混合鸡蛋)？

<img src="https://img.saraba1st.com/forum/202308/02/122426rdu3g7ht3nnhl0uk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-122115.jpg</strong> (358.85 KB, 下载次数: 0)

下载附件

2023-8-2 12:24 上传

如果我们还知道行动者的长期目标(例如做蛋炒饭)怎么办？长期动作预期(LTA/long-term action anticipation)任务旨在通过排列动词与名词序列形式的视频观察来预测行动者的未来行为，这对于人机交互至关重要

<img src="https://img.saraba1st.com/forum/202308/02/122413f0ei0riiaeqhh1ez.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-122102.jpg</strong> (177.34 KB, 下载次数: 0)

下载附件

2023-8-2 12:24 上传

本文从两个角度制定了LTA任务：自下而上的方法，通过对时间动态建模来自回归预测下一步动作；以及自上而下的方法，推断参与者的目标并规划完成目标所需的过程

其中假设大型语言模型(LLM)已经在程序文本数据(例如食谱、操作方法)上进行了预训练，有潜力可以从这两个方面帮助LTA，它可以帮助提供有关下一步可能采取的行动的先验知识，并分别在给定过程的观察部分的情况下推断出目标

为了利用LLM，提出了一个两阶段框架AntGPT，它首先识别观察到的视频中已经执行的动作，然后要求LLM通过条件生成来预测未来的动作，或者通过CoT提示来推断目标并规划整个过程

<img src="https://img.saraba1st.com/forum/202308/02/122437lcnncb07bhewilml.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-122156.jpg</strong> (283.58 KB, 下载次数: 0)

下载附件

2023-8-2 12:24 上传

Ego4D LTA v1和v2基准、EPIC-Kitchens-55以及EGTEA GAZE+的实验结果证明了本文提出的方法的有效性，AntGPT在所有上述基准上都实现了SOTA性能，并且可以成功推断目标，从而通过定性分析执行目标条件的“反事实”预测

<img src="https://img.saraba1st.com/forum/202308/02/122446gyymrystyuz11ztz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-120206__01.jpg</strong> (410.09 KB, 下载次数: 0)

下载附件

2023-8-2 12:24 上传

AntGPT是一个视觉语言框架，旨在探索如何将大型语言模型的新兴能力纳入视频长期动作预期(LTA)中，LTA任务本质上是对人类行为进行视频观察，以预测行为者未来的行为

为了表示LLM的视频信息，使用动作识别模型将视频观察表示为离散动作标签，它们将视觉信息和语言联系起来，使LLM能够执行下游推理任务

首先查询LLM以推断观察到的行为背后的目标，然后将目标信息合并到仅视觉工作流程中，看看这种目标条件预测是否有帮助，还使用了LLM直接对人类活动的时间动态进行建模，看看LLM是否具有有关行动先验的内置知识，最后则是使用流行的提示策略测试LLM在少样本设置中执行预测任务的能力

AntGPT展现了以下新功能：
1.通过几次观察来预测目标：观察到LLM非常有能力预测参与者的目标，即使观察到的人类行为不完美，在上文中展示了一些成功的例子，其中给出了正确的行动和目标，然后在查询中，输入观察到的动作序列，并让LLM输出目标
2.用目标信息增强视觉框架：为了证明输出目标是否对LTA任务有帮助，将目标信息编码到文本特征中，并将其合并到视觉框架中，以执行“目标条件”的未来预测并观察到SOTA级改进
3.建模动作时间动态：探讨了LLM是否可以直接充当推理支柱来建模时间动作动态，为此对域内动作序列上的LLM进行了直接微调，并观察到 ​LLM可以比从头开始训练的Transformer带来额外的更多改进
4.在少样本设置中预测未来行动：进一步研究了LLM如何在少样本设置中执行LTA任务，当仅在上下文中演示几个例子时，LLM仍然可以预测未来的行动序列，此外还尝试了流行的提示策略

<img src="https://img.saraba1st.com/forum/202308/02/122500york4lasay6k6se6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-120226__01.jpg</strong> (856.71 KB, 下载次数: 0)

下载附件

2023-8-2 12:25 上传

1.对于 LTA 任务，视频和LLM之间的良好接口是什么？：尝试了各种预处理技术，发现将视频片段表示为离散动作标签与LLM交互的性能非常好，允许LLM从视频观察中执行下游推理
2.LLM可以推断目标吗？它们对自上而下的LTA有帮助吗？：结论是，LLM可以推断目标，并且它们对于以目标为条件的自上而下的LTA特别有帮助，正如实验所证明的，目标条件预测框架始终比仅视觉框架表现得更好
3.LLM是否掌握了有助于自下而上LTA的有关时间动态的先验知识？：发现经过微调的LLM比从头开始训练的类似Transformer模型具有更好的推理能力，即使输出结构不完善且后处理粗糙，LLM的表现仍然优于Transformer同行们
4.了解目标会影响LLM在少样本设置中预测的行动吗？：观察到所有基于LLM的方法在少样本设置下的性能都比Transformer好得多，尤其是在名词预测方面，这表明利用LLM中编码的先验知识进行LTA任务的有效性

<img src="https://img.saraba1st.com/forum/202308/02/122525i631mh3vl6arl04v.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-120231__01.jpg</strong> (604.01 KB, 下载次数: 0)

下载附件

2023-8-2 12:25 上传

文中所做的一项有趣的定性实验，虽然得出了结论，推断的目标是有用的，并且会在情境学习期间影响LLM，但还是想看看如果给出一个替代目标而不是真正推断的目标，LLM的输出会受到什么影响？ 

观察到LLM确实根据目标做出反应，例如，将推断目标“修复机器”切换为“检查机器”时，LLM会预测一些与“检查机器”专门相关的操作，如“读取仪表”、“记录数据”等

相关评估结果:

<img src="https://img.saraba1st.com/forum/202308/02/122536iengu6gu0dfkghu9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-122244.jpg</strong> (106.48 KB, 下载次数: 0)

下载附件

2023-8-2 12:25 上传

<img src="https://img.saraba1st.com/forum/202308/02/122536skgfl7exyxecvuvx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-122253.jpg</strong> (168.67 KB, 下载次数: 0)

下载附件

2023-8-2 12:25 上传

<img src="https://img.saraba1st.com/forum/202308/02/122536nxmemxf9nwbwxznx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-122300.jpg</strong> (72.8 KB, 下载次数: 0)

下载附件

2023-8-2 12:25 上传

<img src="https://img.saraba1st.com/forum/202308/02/122536uonns9kcbz0bbd0n.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-122311.jpg</strong> (234.37 KB, 下载次数: 0)

下载附件

2023-8-2 12:25 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 758#       发表于 2023-8-2 12:41

SEED-Bench

利用生成理解对多模态LLM进行基准测试

github项目主页:https://github.com/AILab-CVC/SEED-Bench

基于强大的大语言模型(LLM)，最近的生成式多模态大型语言模型(MLLM)作为一个关键研究领域而受到瞩目，表现出卓越的理解和生成能力

在本文中引入了名为SEED-Bench的基准，对MLLM的生成理解进行了评估，作为全面评估生成模型的第一步，SEED-Bench包含19K个多项选择题，具有准确的人工标注(比现有基准大6倍)，覆盖了12个评估维度，包括对图像和视频模态的理解

开发了一个先进的工作流程，用于生成针对特定评估维度的多项选择题，集成自动过滤和手动验证流程，具有源自人工标注的真实选项的多项选择问题可以对模型性能进行客观有效的评估，从而消除了评估过程中人工或GPT干预的需要

进一步评估了18个模型在所有12个维度上的性能，涵盖空间和时间理解。 通过评估结果揭示了现有MLLM的局限性，本文的目标是推动SEED-Bench为未来的研究提供见解，将推出并持续维护排行榜，为社区提供评估和调查模型能力的平台

<img src="https://img.saraba1st.com/forum/202308/02/124003xsh28hfvzccyn6hc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-123646.jpg</strong> (144.3 KB, 下载次数: 0)

下载附件

2023-8-2 12:40 上传

左图为SEED-Bench中12个评估维度的概述，包括空间和时间理解，其中条形中的数字表示每个维度中人工标注的多项选择题的数量，右图为总体排行榜，显示了18个模型在12个评估维度上的平均准确率

数据集样本以及与其他类似数据集的对比:

<img src="https://img.saraba1st.com/forum/202308/02/124035choib3sspp3qsoph.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-123830.jpg</strong> (393.75 KB, 下载次数: 0)

下载附件

2023-8-2 12:40 上传

12个不同的评估维度:

<img src="https://img.saraba1st.com/forum/202308/02/124049j2x8zndiocz29uo6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-123847.jpg</strong> (279.29 KB, 下载次数: 0)

下载附件

2023-8-2 12:40 上传

数据集收集流程:

<img src="https://img.saraba1st.com/forum/202308/02/124103aig8wkkmhhjgkxgg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-123859.jpg</strong> (194.27 KB, 下载次数: 0)

下载附件

2023-8-2 12:41 上传

相关评估结果:

<img src="https://img.saraba1st.com/forum/202308/02/124142v4alsl826fhuc84k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-123912.jpg</strong> (230.81 KB, 下载次数: 0)

下载附件

2023-8-2 12:41 上传

<img src="https://img.saraba1st.com/forum/202308/02/124143x4dgs131313pu3cn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-123925.jpg</strong> (501.86 KB, 下载次数: 0)

下载附件

2023-8-2 12:41 上传

<img src="https://img.saraba1st.com/forum/202308/02/124143fcupnbb4oc796o3r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230802-123935.jpg</strong> (123.24 KB, 下载次数: 0)

下载附件

2023-8-2 12:41 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 759#       发表于 2023-8-3 03:35

LISA

通过大语言模型进行推理分割(Reasoning Segmentation)

github项目地址:https://github.com/dvlab-research/LISA

尽管感知系统(perception systems)近年来取得了明显进步，但它们在执行视觉识别任务之前依然依赖于明确的人类指令来识别目标对象或类别，此类系统缺乏主动推理和理解用户隐含意图的能力

在本文中确立并提出了一个新的分割任务，推理分割(reasoning segmentation)，该任务的目标为输入复杂与隐含的查询文本(given a complex and implicit query text)后输出需求的分段掩码(segmentation mask)，此外还建立了一个由一千多个图像指令对组成的基准，结合了复杂的推理和世界知识以进行评估

最后提出了LISA(大型语言指令分割助手/large Language Instructed Segmentation Assistant)，它继承了多模态大型语言模型(LLM)的语言生成能力，同时还具备生成分割掩码的能力

使用&lt;SEG&gt; Token扩展了原始词汇表，并提出嵌入作为掩码范式来解锁分割功能，值得注意的是，LISA可以处理涉及以下情况的案例:1.复杂推理；2.世界知识；3.解释性答案；4.多轮对话

此外，当专门在无推理数据集上进行训练时，它表现出了强大的零样本能力，当仅使用239个推理分割图像指令对微调模型的情况下，可以进一步提高性能 

实验表明，本基准与方法不仅解锁了新的推理分割功能，而且在复杂推理分割和标准引用分割任务中也被证明是有效的

<img src="https://img.saraba1st.com/forum/202308/03/033336t163hh16nkghyyg1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-032236__01.jpg</strong> (619.9 KB, 下载次数: 0)

下载附件

2023-8-3 03:33 上传

为当前的多模态LLM解锁了新的细分功能，由此产生的模型(LISA)能够处理涉及以下情况的任务：复杂推理、世界知识、解释性答案、多轮对话

<img src="https://img.saraba1st.com/forum/202308/03/033341lj94i4wg0g9gzvm1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-032435.jpg</strong> (121.76 KB, 下载次数: 0)

下载附件

2023-8-3 03:33 上传

带标注的图像指令对的示例，左图为简短查询，右图为长查询

<img src="https://img.saraba1st.com/forum/202308/03/033356dzk4aers2z0yk1ab.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-032604.jpg</strong> (100.39 KB, 下载次数: 0)

下载附件

2023-8-3 03:33 上传

LISA的概览图，给定输入图像和文本查询，多模态LLM生成文本输出，输出结果中最后的&lt;SEG&gt;令牌的最后一层嵌入，可以通过解码器解码为分段掩码还原成分割掩码，视觉主干可以灵活选择(例如SAM或者Mask2Former等)

<img src="https://img.saraba1st.com/forum/202308/03/033444rrscgmskenqyk0ec.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-032836.jpg</strong> (191.75 KB, 下载次数: 0)

下载附件

2023-8-3 03:34 上传

不同类型数据的训练数据格式化说明，包括语义分割数据、引用分割数据和视觉问答(VQA)数据

<img src="https://img.saraba1st.com/forum/202308/03/033453dexwf8rxr2rwea66.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-033008.jpg</strong> (164.75 KB, 下载次数: 0)

下载附件

2023-8-3 03:34 上传

LISA和之前的有关工作之间的推理分割结果

<img src="https://img.saraba1st.com/forum/202308/03/033458bkptpta20tkunija.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-033046.jpg</strong> (320.33 KB, 下载次数: 0)

下载附件

2023-8-3 03:34 上传

引用分割评估结果与消融实验

<img src="https://img.saraba1st.com/forum/202308/03/033503z86pq6bqhnnqb8lm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-033121.jpg</strong> (255.47 KB, 下载次数: 0)

下载附件

2023-8-3 03:35 上传

方法样本结果可视化

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 760#       发表于 2023-8-3 17:49

Qwen-7B

通义千问-7B(Qwen-7B)是阿里云研发的通义千问大模型系列的70亿参数规模的开源模型，目前有预训练与chat两个版本

github项目地址:https://github.com/QwenLM/Qwen-7B

Qwen-7B权重下载(hugface仓库):https://huggingface.co/Qwen/Qwen-7B

Qwen-7B-Chat权重下载(hugface仓库):https://huggingface.co/Qwen/Qwen-7B-Chat

Demo演示:https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary

Qwen-7B是基于Transformer的大型语言模型，在超大规模的预训练数据上进行训练得到，预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等

<img src="https://img.saraba1st.com/forum/202308/03/174916ommkmnefey9e4ifm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-174524.jpg</strong> (337.85 KB, 下载次数: 0)

下载附件

2023-8-3 17:49 上传

<img src="https://img.saraba1st.com/forum/202308/03/174916aijye501d622jgen.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-174755.jpg</strong> (406.72 KB, 下载次数: 0)

下载附件

2023-8-3 17:49 上传

<img src="https://img.saraba1st.com/forum/202308/03/174916bxznj4o4i364x4yy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230803-174810.jpg</strong> (316.91 KB, 下载次数: 0)

下载附件

2023-8-3 17:49 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 761#       发表于 2023-8-4 03:34

 本帖最后由 Machinery 于 2023-8-4 03:37 编辑 

AudioCraft

meta开源简单易用的音频生成AI框架

博客说明页:https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/

github项目地址:https://github.com/facebookresearch/audiocraft

<img src="https://img.saraba1st.com/forum/202308/04/033238lmegzjgjeenjgj46.jpg" referrerpolicy="no-referrer">

<strong>364044687_188110547422666_6558067645303623389_n.jpg</strong> (852.26 KB, 下载次数: 0)

下载附件

2023-8-4 03:32 上传

想象一下，一位专业音乐家能够探索创作新的作品，而无需在乐器上演奏任意单个音符，或者是独立游戏开发者在预算有限的情况可以使用逼真的音效和环境噪音创作虚拟世界，又或者是，小企业主轻松地将配乐添加到他们最新的Instagram帖子中，这就是充满希望的AudioCraft，一个简单有效的框架，在对原始音频信号(raw audio signals)而不是MIDI或钢琴纸卷(piano roll)进行训练后，可以根据基于文本的用户输入生成高质量、逼真的音频和音乐

AudioCraft包含三个模型：MusicGen、AudioGen和EnCodec，其中，MusicGen使用Meta拥有且专门进行特别授权的音乐进行训练，根据基于文本的用户输入生成音乐，而AudioGen使用公共音效进行训练，同样根据基于文本的用户输入提示生成音频

同时将发布EnCodec解码器的改进版本，它可以用更少的伪影(fewer artifacts)生成更高质量的音乐，预训练的AudioGen模型可以生成环境声音和效果音，例如狗叫声、汽车喇叭声或木地板上的脚步声等

生成任何类型的高保真音频都需要对多尺度的复杂信号和模式进行建模，音乐可以说是最具挑战性的音频类型，因为它由局部与长程模式组成，从一组音符到具有多种乐器的全局音乐结构等，利用人工智能生成连贯自然的音乐通常来说是通过使用MIDI或钢琴纸卷等符号表征来解决的

然而，这些方法无法完全掌握音乐中的细微表达差别和风格元素，最近的相关进展通过利用自监督音频表征学习(self-supervised audio representation learning)和许多分层或级联模型来生成音乐，将原始音频输入复杂的系统，以便捕获信号中的长程结构，同时生成高质量的音频，但在这个领域还可以做更多的事情

AudioCraft系列模型能够产生具有长期一致性的高质量音频，并且可以自然交互，与该领域之前的工作相比，通过使用AudioCraft，简化了音频生成模型的整体设计的同时，为人们提供了使用Meta在过去几年中开发的现有模型的完整方法，同时也使他们能够突破极限并开发自己的模型

AudioCraft适用于音乐和声音的生成和压缩，所有这些都可以在同一个地方使用，因为它很容易构建和重用，所以想要构建更好的声音生成器、压缩算法或音乐生成器的人可以在同一个代码库中完成这一切，并在其他人所做的基础上进行构建

虽然为了简化模型付出了大量的工作，但团队同样致力于确保 AudioCraft能够支持最先进的技术(SOTA/state of the art)，人们可以轻松扩展模型并使其适应他们的研究用例，一旦允许人们访问模型并根据他们的需求进行调整，就有几乎无限的可能性

一种简单的音频生成方法，从原始音频信号生成音频具有挑战性，因为需要对极长的序列进行建模，以44.1 kHz采样(常见标准)的典型几分钟音乐曲目由数百万个时间步组成，相比之下Llama和Llama 2等基于文本的生成模型所输入的文本被处理为子词(sub-words)，每个样本仅代表几千个时间步

<img src="https://img.saraba1st.com/forum/202308/04/033250nsdhq9zcn0zq0pkd.png" referrerpolicy="no-referrer">

<strong>362278500_245853288291883_2304974600919081225_n.png</strong> (117.7 KB, 下载次数: 0)

下载附件

2023-8-4 03:32 上传

为了应对这一挑战，使用EnCodec神经音频编解码器(EnCodec neural audio codec)从原始信号中学习离散音频Token(discrete audio tokens)，这为模型提供了音乐样本的新固定“词汇”，然后可以在这些离散的音频Tokens上训练自回归语言模型，再使用EnCodec解码器将Token转换回音频空间时生成新的Token以及新的声音和音乐

从波形中学习音频Token，EnCodec是一种有损的神经编码解码器，经过专门训练，可以压缩任何类型的音频并以高保真度重建原始信号，它由一个带有残差向量量化瓶颈(residual vector quantization bottleneck)的自动编码器组成，该瓶颈可生成多个具有固定词汇的并行音频Token流，不同的流捕获不同级别的音频波形信息，使之能够从所有Token流中重建高保真度的音频

训练音频语言模型，使用单个自回归语言模型对来自EnCodec的音频标记进行递归建模(recursively model)，介绍了一种简单的方法来利用Token并行流的内部结构，通过单一模型和优雅的Token交错模式，可以有效地对音频序列进行建模，同时捕获音频中的长期依赖性，并创作高品质的音乐

<img src="https://img.saraba1st.com/forum/202308/04/033308b15p6jom9i6u09p3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-032938.jpg</strong> (203.15 KB, 下载次数: 0)

下载附件

2023-8-4 03:33 上传

<img src="https://img.saraba1st.com/forum/202308/04/033357mj3kfjjjffccu76h.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-033109__01__01.jpg</strong> (525.99 KB, 下载次数: 0)

下载附件

2023-8-4 03:33 上传

<img src="https://img.saraba1st.com/forum/202308/04/033357x3sxqdzb1rkx3u30.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-033109__01__02.jpg</strong> (665.52 KB, 下载次数: 0)

下载附件

2023-8-4 03:33 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 762#       发表于 2023-8-4 14:10

 本帖最后由 Machinery 于 2023-8-4 14:11 编辑 

The All-Seeing Project

迈向开放世界的全景视觉识别与理解

github项目仓库:https://github.com/OpenGVLab/All-Seeing

hugface的Demo演示:https://huggingface.co/spaces/OpenGVLab/all-seeing

All-Seeing(AS)项目，用于识别和理解开放世界中的一切的大规模数据和模型，使用在循环中融入人类反馈和高效模型的可扩展数据引擎，研究组创建了一个新数据集(AS-1B)，其中包含超过10亿个局部图像区域，并用语义标签、问答对和详细标题文本进行了标注

它广泛涵盖了现实世界中350万个常见和罕见的概念，并拥有1322亿个描述概念及其属性的Token，利用这个新的数据集，开发了全视模型(ASM/All-Seeing model)，一个用于全景视觉识别和理解的统一框架，该模型使用开放式语言提示和位置进行训练，这使其能够以出色的零样本性能推广到各种视觉和语言任务，包括区域文本检索、区域识别、字幕和问答等

<img src="https://img.saraba1st.com/forum/202308/04/140102lh1kqrrk8nqglq91.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-134016.jpg</strong> (164.96 KB, 下载次数: 0)

下载附件

2023-8-4 14:01 上传

 All-Seeing项目与其他流行的大型基础模型项目的比较，本文的主要目的在于深入解决LLM对于理解视觉输入以及VLLM在有效利用区域感知信息方面的局限性

<img src="https://img.saraba1st.com/forum/202308/04/140159nntcizkcnctoc1cx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-134552.jpg</strong> (219.84 KB, 下载次数: 0)

下载附件

2023-8-4 14:01 上传

AS-1B数据集中的语义概念和标注，AS-1B数据集中的语义标签涵盖了广泛的概念，从常见的对象到具有属性的稀有且细粒度的类别，除了简短的语义标签之外，还提供详细的标注，包括视觉问答对和图片的局部标题说明

<img src="https://img.saraba1st.com/forum/202308/04/140208jc4pklktkcu1pkat.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-134611.jpg</strong> (166.91 KB, 下载次数: 0)

下载附件

2023-8-4 14:02 上传

与其他流行的视觉或视觉语言数据集的对比，“#”表示某物的编号，可以看到AS-1B数据集比之前的局部级数据集具有更大的数据规模和多样性

<img src="https://img.saraba1st.com/forum/202308/04/140214r55095dss1e5eoez.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-134824.jpg</strong> (119.07 KB, 下载次数: 0)

下载附件

2023-8-4 14:02 上传

AS-1B数据集的数据收集流程，数据由自动标注流程(A、B、C、D)和人工验证阶段(E)组成，结合强大的对象检测器、LLM和VLLM来生成不同区域的开放世界局部位置和标注，自动标注由人类专家进行采样和验证，自动标注与人工验证结果一起使用来训练局部感知对齐和生成模型，然后将其用于自动标注流程中以提高数据质量

<img src="https://img.saraba1st.com/forum/202308/04/140222tjzjeeebr52qrpll.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135109.jpg</strong> (105.52 KB, 下载次数: 0)

下载附件

2023-8-4 14:02 上传

全视模型的架构和任务建模，结合了局部感知图像分词器来执行局部文本对齐任务，图像级和局部级特征被编码为视觉标记V，并与用户的文本输入一起输入到基于LLM的解码器中

全视模型采用特定的提示设计，允许LLM解码器使用共享参数的统一架构来处理生成任务和判别任务，通过添加软提示Token(例如Pg和Pd)来指示所需的任务，并使用“⟨align⟩”标记在LLM的输出中执行图像文本对齐，“⟨bos⟩”则表示句子的开始标记

<img src="https://img.saraba1st.com/forum/202308/04/140235guurkiurtwhtk4ln.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135558.jpg</strong> (210.97 KB, 下载次数: 0)

下载附件

2023-8-4 14:02 上传

上图为数据集的局部标注情况以及自动标注语义的来源模型，下图不同来源语义标签的准确性

<img src="https://img.saraba1st.com/forum/202308/04/140504vt7fgvxhv5b7xvxb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135811.jpg</strong> (138.16 KB, 下载次数: 0)

下载附件

2023-8-4 14:05 上传

不同来源的模型生成的定位的局部区域，字幕说明以及问答等情况的具体情况分布

<img src="https://img.saraba1st.com/forum/202308/04/140535uj6x58d6tu66hmxn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135837.jpg</strong> (559.92 KB, 下载次数: 0)

下载附件

2023-8-4 14:05 上传

语义标签的具体样本

<img src="https://img.saraba1st.com/forum/202308/04/140617u48dbvs44044v191.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135846.jpg</strong> (508.98 KB, 下载次数: 0)

下载附件

2023-8-4 14:06 上传

样本的详细局部定位说明与问答等示例

<img src="https://img.saraba1st.com/forum/202308/04/140703l4hg4cvc49fcgg7z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135857.jpg</strong> (237.77 KB, 下载次数: 0)

下载附件

2023-8-4 14:07 上传

<img src="https://img.saraba1st.com/forum/202308/04/140703zqj35myjofljmqf3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135906.jpg</strong> (199.64 KB, 下载次数: 0)

下载附件

2023-8-4 14:07 上传

对于全视模型的评估结果

<img src="https://img.saraba1st.com/forum/202308/04/140752p7f4lru7uar88u7m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135926.jpg</strong> (192.12 KB, 下载次数: 0)

下载附件

2023-8-4 14:07 上传

数据迭代过程的可视化，迭代过程提高了标签的准确性，其中可视化了三种类型的模型:
标签1：产生原始CLIP的标签
标签2：由R-CLIP或ASM产生的标签，以标签1作为输入数据进行训练
标签3：由R-CLIP或ASM产生的标签，并根据人工验证数据进一步调整

<img src="https://img.saraba1st.com/forum/202308/04/141034liubldehrbl6rljd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-135935.jpg</strong> (104.6 KB, 下载次数: 0)

下载附件

2023-8-4 14:10 上传

字幕任务的人工评估结果，其中要求标注者选择包含有关图像/区域的最多信息的标题，同时不会产生任何事实错误

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 763#       发表于 2023-8-4 15:03

VisAlign

用于测量人工智能和人类在视觉感知方面的一致性程度的数据集

相关论文:https://arxiv.org/abs/2308.01525

github项目地址:https://github.com/jiyounglee-0523/VisAlign

AI一致性(AI alignment)模型是指朝着人类预期目标、偏好或道德原则行事的模型，鉴于大多数大规模的深度学习模型都是黑匣子，无法手动控制，分析模型与人类之间的相似性可以成为确保人工智能代理安全的稳妥措施

在本文中，重点关注了模型与人类的视觉感知对齐，进一步称为人工智能-人类视觉对齐，具体来说则是构造了一个新的数据集，用于测量图像分类方面的人工智能与人类视觉对齐的程度，这是机器感知的一项基本任务

为了评估人工智能与人类视觉对齐，数据集应包含现实世界中可能出现的各种场景的样本，并具有黄金人类感知标签(gold human perception labels)

数据集由三组样本组成，即Must-Act(必须分类)、Must-Abstain(必须放弃)和Uncertain(不确定)，根据图像中视觉信息的数量和清晰度，进一步分为八类，所有样品均带有金色人类感知标签，甚至连不确定(严重模糊)的样本标签也是通过众包标注者获得的

数据集的有效性得到了抽样理论、调查设计相关统计理论以及相关领域专家的验证，使用数据集，分析了五种流行的视觉感知模型和七种放弃方法的视觉对齐和可靠性

<img src="https://img.saraba1st.com/forum/202308/04/150051ok5zq2k55pjz2k8q.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-144600__01.jpg</strong> (478.69 KB, 下载次数: 0)

下载附件

2023-8-4 15:00 上传

VisAlign概览图，其中示例图像是参考Zebra(斑马)类给出的

第1类：逼真的斑马图像/第2类：斑马过马路/ 第3类：类别1图像中添加了轻微的噪点/第4类：卡车的图片/第5类：大象的头部和两个四肢，以及斑马的剩余身体/第6类：驴/第7类：一件衣服上画有斑马/第8类：两张图片，一张是经过裁剪的，另一张是毛玻璃模糊的，分别都是斑马

<img src="https://img.saraba1st.com/forum/202308/04/150114px6tz6gic8p7cz0t.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-145139.jpg</strong> (59.36 KB, 下载次数: 0)

下载附件

2023-8-4 15:01 上传

INVisAlign和其他相关数据集在定义的要求上的比较，△表示仅涵盖了数据集场景的子集

<img src="https://img.saraba1st.com/forum/202308/04/150230xyv21dmxd2y1vb41.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-145234__01.jpg</strong> (632.13 KB, 下载次数: 0)

下载附件

2023-8-4 15:02 上传

开放测试集上五个种子的基于距离(distance-based)的视觉对齐和可靠性分数的平均值和标准偏差，粗体表示各类别中表现最好的，下划线表示次之，其中Deep Ensemble没有标准差，因为它使用所有五个模型的输出

<img src="https://img.saraba1st.com/forum/202308/04/150238aworwyaxxx3tsx3r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-145322__01.jpg</strong> (602.69 KB, 下载次数: 0)

下载附件

2023-8-4 15:02 上传

ImageNet预训练模型基于距离的视觉对齐和5个种子的可靠性评分的平均偏差和标准偏差

<img src="https://img.saraba1st.com/forum/202308/04/150244uhfzhh2ebrvbhhb2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230804-145354.jpg</strong> (566.65 KB, 下载次数: 0)

下载附件

2023-8-4 15:02 上传

5个种子的自监督的基于距离的视觉对齐和可靠性评分的平均偏差和标准偏差

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  flyinsea  
##### 764#       发表于 2023-8-4 15:10

claude2有人有使用web转换的api接入的经验吗


*****

####  Machinery  
##### 765#       发表于 2023-8-4 15:20

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61905628&amp;ptid=2126390" target="_blank">flyinsea 发表于 2023-8-4 15:10</a>
claude2有人有使用web转换的api接入的经验吗</blockquote>
SillyTavern和slaude

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  flyinsea  
##### 766#       发表于 2023-8-4 17:40

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61905753&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-8-4 15:20</a>

SillyTavern和slaude

—— 来自 S1Fun</blockquote>
谢了，后面那个大概slack不支持claude2了，前面那个我看看


*****

####  Machinery  
##### 767#       发表于 2023-8-5 02:15

 本帖最后由 Machinery 于 2023-8-5 02:18 编辑 

dynalang

学习用语言模拟世界

项目主页:https://dynalang.github.io/

github项目地址库:https://github.com/jlin816/dynalang

Dynalang，一个利用多种类型的语言来解决任务，通过多模态世界模型使用语言来预测未来的架构

为了与人类互动并在世界中行动，智能代理(agents)需要了解人们使用的语言范围并将其与视觉世界联系起来

虽然当前的智能代理可以通过任务奖励学习执行简单的语言指令，但本文目标是构建利用多种类型的语言来传达常识、描述世界状态、提供交互式反馈等能力的智能代理

其中的关键想法是，语言可以帮助智能代理预测未来(language helps agents predict the future)，将会观察到什么，世界将如何运转，以及哪些情况可以得到奖励，这种观察下的视角统一了语言理解与未来预测任务，以将其作为一个强大的自监督学习目标

Dynalang，一种学习多模态世界模型的智能代理，该模型可以预测未来的文本和图像表示，并学习从模型想象的场景(Rollout)中采取行动，与仅使用语言来预测动作的传统智能代理不同，Dynalang通过使用过去的语言来预测未来的语言、视频和奖励，从而获得丰富的语言理解，除了可以在环境中的在线交互中学习之外，Dynalang还可以在文本、视频或两者的数据集上进行预训练，而无需操作或奖励

通用使用网格世界中的语言提示(language hints in grid worlds)到导航逼真扫描的模拟房屋环境，Dynalang可以利用不同类型的语言来提升任务表现，包括环境描述、游戏规则和说明等

<img src="https://img.saraba1st.com/forum/202308/05/021236dhhvyaxi9z6vu9h3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-014232.jpg</strong> (92.63 KB, 下载次数: 0)

下载附件

2023-8-5 02:12 上传

Dynalang使用语言来理解世界，这符合世界建模范式，架构以DreamerV3为基础，这是一个基于模型的RL(强化学习)智能代理，Dynalang从智能代理在环境中行动时收集的经验数据中不断学习

<img src="https://img.saraba1st.com/forum/202308/05/021250omhzzbfv1fkspf18.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-014250.jpg</strong> (85.25 KB, 下载次数: 0)

下载附件

2023-8-5 02:12 上传

左图为世界模型将每个时间步的文本和图像压缩为潜在表征，从而根据表征，模型被训练来重建原始观察结果，预测奖励，并预测下一个时间步的表征，直观地说，世界模型根据它在文本中读取的内容来学习它应该期望在世界上看到什么(what it should expect to see in the world)

右图则为Dynalang通过在压缩的世界模型表征之上训练策略网络来选择操作，它接受世界模型中想象的场景(rollouts)的训练，并学习为了最大化预测的奖励应该进行的行动

与之前一次消耗一个句子或一个段落的多模态模型不同，Dynalang被设计为将视频和文本建模为一个统一的序列，在同一时间内一次只消耗一个图像帧和一个文本标记，直观上来说这类似于人类在现实世界中接收输入的方式，接收单一的多模态流，听语言需要时间，将所有内容建模为一个序列，可以像语言模型一样在文本数据上对模型进行预训练，并提高RL性能

<img src="https://img.saraba1st.com/forum/202308/05/021302q5xlpt265eyl5l2y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-015724.jpg</strong> (119.98 KB, 下载次数: 0)

下载附件

2023-8-5 02:13 上传

HomeGrid中的语言提示(Language Hints)，引入了HomeGrid以在一个适当的环境中评估智能代理，除了任务指令之外，智能代理还会接收一些语言提示，HomeGrid中的提示模拟了代理可能从人类那里学习或阅读文本的知识，提供有帮助但不是解决任务所必需的信息：

1.未来观察(Future Observations)描述了代理将来可能观察到的内容，例如“盘子在厨房里”
2.更正(Corrections)根据代理正在执行的操作提供交互式反馈，例如“转身”
3.动态(Dynamics)描述了环境的动态，例如“Pedal打开了垃圾箱”

<img src="https://img.saraba1st.com/forum/202308/05/021347ihdwtly74nnh7azs.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-020103.jpg</strong> (55.94 KB, 下载次数: 0)

下载附件

2023-8-5 02:13 上传

尽管智能代理并没有接受过一段文本对应的观察结果的明确监督，但Dynalang仍然可以通过预测的未来目标学习将所有类型的基准(ground)语言与环境联系起来，Dynalang的性能优于以语言为条件的IMPALA和R2D2，它们难以使用不同类型的语言，并且在超出指令的语言场景时往往表现更差

<img src="https://img.saraba1st.com/forum/202308/05/021357k3i8abf9w6j3caii.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-020256.jpg</strong> (159.07 KB, 下载次数: 0)

下载附件

2023-8-5 02:13 上传

Messenger中的游戏手册，通过对Messenger游戏环境进行评估，以测试智能代理如何从更长、更复杂的文本中学习，这些文本需要对文本和视觉观察进行多跳推理，智能代理必须对描述每个情节动态的文本手册进行推理，并将其与对环境中实体的观察相结合，以确定从哪些实体获取消息以及避免哪些实体，Dynalang的性能表现优于IMPALA和R2D2，以及特定于任务构架的EMMA基线，该基线使用专门的架构对文本和观察进行推理，特别是在最困难的第3阶段中

<img src="https://img.saraba1st.com/forum/202308/05/021515i71b8grtxj1bh7x8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-020606.jpg</strong> (123.96 KB, 下载次数: 0)

下载附件

2023-8-5 02:15 上传

<img src="https://img.saraba1st.com/forum/202308/05/021515jrzr6nq4czn5nn44.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-020551.jpg</strong> (54.63 KB, 下载次数: 0)

下载附件

2023-8-5 02:15 上传

在居住地(Habitat)中遵循指令，Dynalang能够处理逼真的视觉观察并执行居住地中的指令，智能代理必须遵循自然语言指令，在逼真的房屋扫描环境中导航到目标位置，在Dynalang中，指令跟随可以通过将其视为预测的未来奖励而统一在同一预测框架中使用

<img src="https://img.saraba1st.com/forum/202308/05/021521juiipbsvk6w61kiz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-020901.jpg</strong> (79.79 KB, 下载次数: 0)

下载附件

2023-8-5 02:15 上传

LangRoom中的基准语言生成，就像语言可以影响智能代理将看到的内容的预测一样，智能代理观察到的内容也会影响它所期望听到的语言(例如关于它所看到的内容的真实陈述)，通过在LangRoom的动作空间中输出语言，Dynalang可以生成基于环境的语言来执行具身(embodied)问答

文本预训练，由于使用语言进行世界建模与学习使用世界模型进行操作是解耦的，因此可以使用离线数据对Dynalang进行预训练，而无需操作或奖励标签，此功能为Dynalang提供了一种可以从大规模离线数据集中受益的方法，所有这些都在单个模型架构中使用

<img src="https://img.saraba1st.com/forum/202308/05/021527zp0ce5m2kps9at9c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-021046.jpg</strong> (82.57 KB, 下载次数: 0)

下载附件

2023-8-5 02:15 上传

通过使用纯文本数据对Dynalang进行预训练，从头开始学习标记嵌入，在一般文本数据集(TinyStories、2M数量的短篇故事)上预训练模型可提高Mess上的下游RL任务表现

尽管工作重点是在世界中行动的语言理解，但依然可以像纯文本语言模型一样从世界模型中生成文本，从潜在空间中预训练的TinyStories模型中进行采样，并从每个时间步的表示中解码Token观察，模型生成的一致性令人惊讶，尽管仍低于现代语言模型的质量，但统一使用语言生成并在单一代理架构中执行是未来工作的一个令人兴奋的途径

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 768#       发表于 2023-8-5 02:59

 本帖最后由 Machinery 于 2023-8-5 03:01 编辑 

ScRel

使用大型语言模型学习数学推理的缩放关系(Scaling Relationship)

相关论文:https://arxiv.org/abs/2308.01825

github项目地址:https://github.com/OFA-Sys/gsm8k-ScRel

对于大型语言模型(LLM)来说，数学推理是一项具有挑战性的任务，其中任务与LLM能力之间的缩放关系尚未得到充分探索

在本文中，研究了预训练损失、监督数据量和增强数据量如何影响监督LLM的推理性能，发现预训练损失比模型的参数量更能反映模型的性能

通过对不同数量的监督数据应用监督微调(SFT/supervised fine-tuning)，并凭经验发现数据量和模型性能之间的对数线性关系(log-linear relation)，同时发现更好的模型随着监督数据集的扩大而改进较少，为了在无需任何人力的情况下增加更多数据样本以提高模型性能，建议应当使用拒绝采样微调(RFT/Rejection sampling Fine-Tuning)

RFT使用监督模型来生成和收集正确的推理路径(correct reasoning paths)作为增强的微调数据集，通过包含更多不同推理路径的增强样本，RFT可以更好地提高LLM的数学推理性能

还发现RFT可以为表现较差的LLM带来更多改进，此外，研究组结合了多个模型的拒绝样本，将LLaMA-7B的准确率提升至49.3%，显著优于监督微调(SFT)的35.9%准确率

<img src="https://img.saraba1st.com/forum/202308/05/025811htqddkqmkemmggm5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-024612.jpg</strong> (109.36 KB, 下载次数: 0)

下载附件

2023-8-5 02:58 上传

对于LLM学习数学推理能力的比例关系的关键发现

<img src="https://img.saraba1st.com/forum/202308/05/025818x0w9jc0jrljlww5r.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-024622.jpg</strong> (127.89 KB, 下载次数: 0)

下载附件

2023-8-5 02:58 上传

GSM8K上SFT(监督微调/蓝线)和ICL(上下文学习/红线)设置的性能表现情况，其中GPT-4表示他们在预训练中使用了GSM8K数据的一部分，建议将其认为其表现处于SFT与ICL之间

<img src="https://img.saraba1st.com/forum/202308/05/025826l8qp9lne7hcx6exp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-024631.jpg</strong> (143.09 KB, 下载次数: 0)

下载附件

2023-8-5 02:58 上传

GSM8K上不同监督数据量的SFT性能

<img src="https://img.saraba1st.com/forum/202308/05/025832si04ma63b9us6lxl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-024647.jpg</strong> (63.25 KB, 下载次数: 0)

下载附件

2023-8-5 02:58 上传

k=100情况下的RFT微调在GSM8K上的性能与SFT和ICL的比较，这里不同的路径数量是指不同的方程列表数量

<img src="https://img.saraba1st.com/forum/202308/05/025846a09590c0940990at.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-024726.jpg</strong> (129.12 KB, 下载次数: 0)

下载附件

2023-8-5 02:58 上传

不同采样数k的RFT在GSM8K上的性能

<img src="https://img.saraba1st.com/forum/202308/05/025854fmkciopnknkddll5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-024732.jpg</strong> (49.54 KB, 下载次数: 0)

下载附件

2023-8-5 02:58 上传

由具有不同k的不同SFT模型生成的每个问题的不同推理路径

<img src="https://img.saraba1st.com/forum/202308/05/025858h8mtcjl675mxlx6j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-024854.jpg</strong> (80.5 KB, 下载次数: 0)

下载附件

2023-8-5 02:58 上传

拒绝从多个模型采样样本的RFT性能

<img src="https://img.saraba1st.com/forum/202308/05/025902cplpm18cmmmmpmco.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-024950.jpg</strong> (82.56 KB, 下载次数: 0)

下载附件

2023-8-5 02:59 上传

各模型向D'U33B提供的推理计算路径比例的文氏图(Venn diagram)，例如D'U33B中15.5%(黄色部分)的推理计算路径只能在LLaMA2-13B-SFT的拒绝采样结果中找到

<img src="https://img.saraba1st.com/forum/202308/05/025907fp91f3cilvokop59.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-025210.jpg</strong> (183.75 KB, 下载次数: 0)

下载附件

2023-8-5 02:59 上传

将GSM8K结果与其他基线进行比较，RFT-U13B表示在D′U13B上微调的模型，FCS和PCS分别代表完全正确的解和部分正确的解，*如果没有指定情况，默认K=100

<img src="https://img.saraba1st.com/forum/202308/05/025913v1y19424e07skkck.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-025342.jpg</strong> (140.03 KB, 下载次数: 0)

下载附件

2023-8-5 02:59 上传

使用不同数量的唯一推理计算路径解决的问题编号的直方图，展示了在唯一推理计算路径的数量为1或大于10的两种情况下SFT和RFT U13B之间的问题数差异

<img src="https://img.saraba1st.com/forum/202308/05/025920aa0000tgxcusjuug.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230805-025513.jpg</strong> (97.96 KB, 下载次数: 0)

下载附件

2023-8-5 02:59 上传

预训练、SFT、RFT推理、RFT所需的FLOPs和GPU小时数统计，RFT推理的GPU时间针对7473个训练集问题和每个问题100个样本进行计算，为了充分利用GPU并使模型正确地适应GPU显存，调整了推理批量大小，对于 33B、65B 和 70B 模型，使用DeepSpeed ZeRO3进行分布式训练，所有GPU时间均基于NVIDIA A100 80GB GPU，请注意，在本实验中使用了非嵌入参数来计算FLOP

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 769#       发表于 2023-8-6 17:57

StarGLM

天文大语言模型StarGLM(ChatGLM for Variable Star)

github项目地址:https://github.com/Yu-Yang-Li/StarGLM

github项目简介:

<img src="https://img.saraba1st.com/forum/202308/06/175727aqxf0000gw0m5dzq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-175639__01.jpg</strong> (536.98 KB, 下载次数: 0)

下载附件

2023-8-6 17:57 上传

<img src="https://img.saraba1st.com/forum/202308/06/175727zz1xb9ql1vi9znip.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-175639__02.jpg</strong> (774.28 KB, 下载次数: 0)

下载附件

2023-8-6 17:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 770#       发表于 2023-8-6 18:35

ClassEval

用于评估编程中类(Class)级别的代码生成能力的人工制作基准

github项目主页:https://github.com/FudanSELab/ClassEval

在本文中，研究组首次尝试在更具挑战性的代码生成场景(类级代码生成/class-level code generation)中评估LLM

通过手工构建了第一个类级代码生成评估基准ClassEval，包含100个类级Python代码生成任务，总共耗时约500小时

在此基础上，对11个SOTA级代码生成LLM进行了首次研究，根据评估结果，发现与HumanEval等独立的方法级代码生成(standalone method-level code generation)基准相比，所有现有的LLM在类级代码生成方面的性能要差得多，LLM之间方法级的编码能力不能等价反映到类级的编码能力

还发现GPT-4和GPT-3.5在类级代码生成方面仍然表现出了优于其他LLM的优势，第二档的模型包括性能非常相似的Instruct-Starcoder、Instruct-Codegen和Wizardcoder

继而发现一次生成整个类(即整体生成策略)是仅适用于GPT-4和GPT-3.5的最佳生成策略，而逐方法生成(即增量和组合)对于理解长指令和利用中间信息的能力有限的其他模型来说是更好的策略

现存的常见代码评估基准:

<img src="https://img.saraba1st.com/forum/202308/06/183038lhhhhuwfiqhti2aw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-181814__01.jpg</strong> (438.7 KB, 下载次数: 0)

下载附件

2023-8-6 18:30 上传

常见的(HumanaEval)基准评估方法样例:

<img src="https://img.saraba1st.com/forum/202308/06/183128bysqqqpmx9ppm5zx.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-181824__01.jpg</strong> (157.76 KB, 下载次数: 0)

下载附件

2023-8-6 18:31 上传

ClassEval中的细粒度评估样例:

<img src="https://img.saraba1st.com/forum/202308/06/183158owy7ivkyy77hxcff.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-181833__01.jpg</strong> (295.53 KB, 下载次数: 0)

下载附件

2023-8-6 18:31 上传

ClassEval构建流程图:

<img src="https://img.saraba1st.com/forum/202308/06/183231luugg9mzgzx8s9z1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182001.jpg</strong> (109.12 KB, 下载次数: 0)

下载附件

2023-8-6 18:32 上传

类级评估构架下的元素定义与主题类定义:

<img src="https://img.saraba1st.com/forum/202308/06/183305oj6u463ytvrudrcl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182051__01.jpg</strong> (540.27 KB, 下载次数: 0)

下载附件

2023-8-6 18:33 上传

参与评估的LLM们:

<img src="https://img.saraba1st.com/forum/202308/06/183433evvtrnusu5uf5llf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182217__01__01__01.jpg</strong> (100.72 KB, 下载次数: 0)

下载附件

2023-8-6 18:34 上传

评估结果:

<img src="https://img.saraba1st.com/forum/202308/06/183516kva68h9pxslpp43l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182217__01__01__02.jpg</strong> (217.75 KB, 下载次数: 0)

下载附件

2023-8-6 18:35 上传

<img src="https://img.saraba1st.com/forum/202308/06/183516eazx9mcmsc9c9nds.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182547.jpg</strong> (202.43 KB, 下载次数: 0)

下载附件

2023-8-6 18:35 上传

<img src="https://img.saraba1st.com/forum/202308/06/183516km5tkvm7fk6dm8n0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182559.jpg</strong> (82.5 KB, 下载次数: 0)

下载附件

2023-8-6 18:35 上传

<img src="https://img.saraba1st.com/forum/202308/06/183516ai3r8klknrtpiupl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182621.jpg</strong> (127.44 KB, 下载次数: 0)

下载附件

2023-8-6 18:35 上传

<img src="https://img.saraba1st.com/forum/202308/06/183516wbhp9dcfadhv3fss.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182658.jpg</strong> (82.65 KB, 下载次数: 0)

下载附件

2023-8-6 18:35 上传

<img src="https://img.saraba1st.com/forum/202308/06/183517r8n8n3wxclmn8v5z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230806-182730.jpg</strong> (71.92 KB, 下载次数: 0)

下载附件

2023-8-6 18:35 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 771#       发表于 2023-8-8 14:39

MM-Vet

评估大型多模态模型的集成能力

github项目主页:https://github.com/yuweihao/MM-Vet

本文提出了MM-Vet，一种基准评估方法，用于检查复杂的多模态任务上的大型多模态模型(LMM/large multimodal models )的能力

最近的LMM表现出了各种有趣的能力，例如解决黑板上写的数学问题、推理新闻图像中的事件和名人，解释视觉笑话等，模型的快速进步给评估基准的开发带来了多种挑战，包括以下问题:

1.如何系统性构建和评估复杂的多模态任务
2.如何设计适用于多种问答类型的评估指标
3.如何在简单的性能排名之外为模型提供某种见解

为此本文提出了MM-Vet，其设计基于这样的见解，解决复杂任务的有趣能力，通常是通过集成了不同核心视觉语言(VL)功能的通用模型实现的

MM-Vet定义了6个核心VL功能，并测试了从功能组合中得到的16个有趣的集成效果，对于评估指标，提出了一个基于LLM的开放式输出评估器，评估器可以对不同的问题类型和答案风格进行评估，从而产生统一的评分指标。

在MM-Vet上评估了具有代表性的LMM，提供了对不同LMM系统范式和模型功能的见解

<img src="https://img.saraba1st.com/forum/202308/08/143727myz7nuw9j9y7qiw7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-142226.jpg</strong> (272.21 KB, 下载次数: 0)

下载附件

2023-8-8 14:37 上传

不同基准测试所需的功能，与传统的VL基准测试中一般只需要评估一两个能力不同，MM-Vet专注于不同核心的VL能力的集成，包括识别、OCR、知识、语言生成、空间感知和数学

<img src="https://img.saraba1st.com/forum/202308/08/143732czsszui4d66grcsi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-142354.jpg</strong> (127.28 KB, 下载次数: 0)

下载附件

2023-8-8 14:37 上传

MM-Vet中的能力比例，A图为每种能力的比例，由于大多数样本中都具有不止一种能力的评估，因此该比例之和大于100%，B图为整合后的比例，比例之和相当于100%

<img src="https://img.saraba1st.com/forum/202308/08/143755ubc5uxxngfknn15x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-142705.jpg</strong> (374.86 KB, 下载次数: 0)

下载附件

2023-8-8 14:37 上传

使用GPT-4评估模型Few-shot提示的输出结果，其中Q是样本的问题，G是基准事实(ground truth)，P是样本的模型输出，提示中附有长短开放式答案示例，可实现多种答案风格的评估，采取充满Q、G和P的提示，GPT-4会生成从0到1的软评分

<img src="https://img.saraba1st.com/forum/202308/08/143801nawhyulpguspgbou.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143030.jpg</strong> (194.87 KB, 下载次数: 0)

下载附件

2023-8-8 14:38 上传

本报告中评估的LMM的总结，考虑了两种不同的端到端调整模型(OpenFlamingo、BLIP-2、LLaVA、MiniGPT-4、LLaMA-Adapter v2、Otter、InstructBLIP)和LLM作为工具使用系统的模型(MM-ReAct和Transformers Agent)

<img src="https://img.saraba1st.com/forum/202308/08/143808pyouu0dwdul077l8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143253.jpg</strong> (228.55 KB, 下载次数: 0)

下载附件

2023-8-8 14:38 上传

MM-Vet对各种LMM的核心VL功能的评估结果，对于每一列，最高、第二和第三高的数字以绿色、橙色和蓝色突出显示，所有数字均以%表示，满分为100%

<img src="https://img.saraba1st.com/forum/202308/08/143816y94vxitzgvvutg9g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143416.jpg</strong> (226.57 KB, 下载次数: 0)

下载附件

2023-8-8 14:38 上传

<img src="https://img.saraba1st.com/forum/202308/08/143906trzucpc9jdz5dttr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143514.jpg</strong> (306.79 KB, 下载次数: 0)

下载附件

2023-8-8 14:39 上传

MM-Vet对各种LMM的能力集成的评估结果

评估样例:

<img src="https://img.saraba1st.com/forum/202308/08/143917dxly0fff2elckafr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143548.jpg</strong> (292.88 KB, 下载次数: 0)

下载附件

2023-8-8 14:39 上传

<img src="https://img.saraba1st.com/forum/202308/08/143917xg0vvhmvdzhzjud7.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143602.jpg</strong> (203.27 KB, 下载次数: 0)

下载附件

2023-8-8 14:39 上传

<img src="https://img.saraba1st.com/forum/202308/08/143917vfmqav7ag9gqhgkg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143611.jpg</strong> (368.45 KB, 下载次数: 0)

下载附件

2023-8-8 14:39 上传

<img src="https://img.saraba1st.com/forum/202308/08/143917nzttttswctzcj429.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143626.jpg</strong> (194.62 KB, 下载次数: 0)

下载附件

2023-8-8 14:39 上传

<img src="https://img.saraba1st.com/forum/202308/08/143918ay991zxy4dcsx292.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-143637.jpg</strong> (351.66 KB, 下载次数: 0)

下载附件

2023-8-8 14:39 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 772#       发表于 2023-8-8 15:15

fc-clip

卷积死忠派，使用单独的冻结卷积CLIP进行开放词汇分割

项目主页:https://github.com/bytedance/fc-clip

开放词汇分割(Open-vocabulary segmentation)是一项具有挑战性的任务，需要从一组开放类别中分割和识别对象，解决这一挑战的一种方法是利用多模态模型(例如CLIP)在共享嵌入空间(shared embedding space)中提供图像和文本特征，从而衔接封闭词汇表和开放词汇表识别之间的差距

现有方法通常采用两阶段框架来解决该问题，其中输入首先需要通过掩码生成器，然后使用CLIP模型预测输入的掩码，这个过程涉及多次从图像中提取特征，这可能是低效且无效的

相比之下，在本文中使用了共享的冻结卷积CLIP(shared Frozen Convolutional CLIP)主干将所有内容构建到单步框架中，这不仅显著简化了当前的两级阶段方法，而且还显著地产生了更好的准确性与成本权衡

所提出的FC-CLIP受益于以下观察结果，冻结的CLIP主干保持了开放词汇分类的能力，并且还可以成为强大的掩码生成器，并且卷积CLIP可以很好地推广到与对比图像文本预训练期间使用的输入分辨率更大的输入分辨率

仅在COCO全景数据上进行训练并以零样本方式进行测试时，FC-CLIP在ADE20K，Mapillary Vistas，Cityscapes上都实现了更好的结果

此外，FC-CLIP的训练和测试时间比相同的现有技术明显快7.5倍和6.6倍，使用的参数减少了5.9倍，FC-CLIP还在各种开放词汇语义分割数据集上达成了SOTA结果

<img src="https://img.saraba1st.com/forum/202308/08/151424rch4t95s5m395ppt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-150054.jpg</strong> (179.23 KB, 下载次数: 0)

下载附件

2023-8-8 15:14 上传

基于冻结CLIP主干特征的k均值可视化，包含不同的输入分辨率。 基于ViT和基于CNN的CLIP都会产生具有语义意义的特征，然而，当扩大输入分辨率时，注意到基于ViT的CLIP特征变得更加嘈杂，而基于CNN的特征更加平滑并且泛化能力更好，更平滑的特征图更适合设计中需要的掩码池模块

<img src="https://img.saraba1st.com/forum/202308/08/151430y2tda9eogswb1ceg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-150338.jpg</strong> (76.16 KB, 下载次数: 0)

下载附件

2023-8-8 15:14 上传

开放词汇表全景分割方法的工作流程间的比较

<img src="https://img.saraba1st.com/forum/202308/08/151435q28ww82z9g0z7aix.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-150521.jpg</strong> (63.74 KB, 下载次数: 0)

下载附件

2023-8-8 15:14 上传

FC-CLIP概览图，包含三个主要组件：掩码生成器、词汇内(in-vocab)分类器和词汇外(out-vocab)分类器

所有组件都构建在共享的冻结协作CLIP主干之上，像素解码器和掩码解码器建模遵循Mask2Former设计，生成与类无关的掩码，词汇内分类器通过对来自像素解码器的最终像素特征进行掩码池化来产生类嵌入

在测试过程中，FC-CLIP另外通过对冻结的CLIP主干特征进行掩码池化来利用词汇外分类器，并通过集成两个分类器来获得最终的类别预测

需要注意的是文本嵌入是通过将类别名称输入CLIP文本编码器来获得的，这是预先完成的并缓存在显存中的，因此不会产生额外的成本，此外，与类无关的掩码建议被馈送到了掩码池模块(为简化因此在图中未显示)

<img src="https://img.saraba1st.com/forum/202308/08/151528t8d2zv82vvggzkdq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-151340.jpg</strong> (389.04 KB, 下载次数: 0)

下载附件

2023-8-8 15:15 上传

FC-CLIP在ADE20K 验证集上的可视化示例

评估结果:

<img src="https://img.saraba1st.com/forum/202308/08/151445xyvrymizkjjmjr7y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-151232.jpg</strong> (373.9 KB, 下载次数: 0)

下载附件

2023-8-8 15:14 上传

<img src="https://img.saraba1st.com/forum/202308/08/151445cskj0001k0vfox2x.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-151242.jpg</strong> (137.51 KB, 下载次数: 0)

下载附件

2023-8-8 15:14 上传

<img src="https://img.saraba1st.com/forum/202308/08/151445iiqp1q4kfd4d4fz9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-151251.jpg</strong> (118.22 KB, 下载次数: 0)

下载附件

2023-8-8 15:14 上传

<img src="https://img.saraba1st.com/forum/202308/08/151445gqffv4xvs95rb5jf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-151313.jpg</strong> (233.22 KB, 下载次数: 0)

下载附件

2023-8-8 15:14 上传

<img src="https://img.saraba1st.com/forum/202308/08/151445rgwxugfggq61ve6i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-151334.jpg</strong> (223.51 KB, 下载次数: 0)

下载附件

2023-8-8 15:14 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 773#       发表于 2023-8-8 15:50

AvatarVerse

从文本和姿态创建高质量&amp;稳定的3D化身(Avatar)

项目主页:https://avatarverse3d.github.io/

从高度定制的文本描述和姿态指导中创建富有表现力、多样化和高质量的3D化身是一项具有挑战性的任务，3D模型结构非常复杂，同时还需要确保模型的纹理符合各种细节和风格(现实或虚构等)

本文推出了AvatarVerse，这是一个稳定的工作流程，用于仅根据文本描述和姿态指导生成富有表现力的高质量3D化身，具体来说，引入了一种以稠密姿态(DensePose)信号为条件的2D扩散模型，通过2D图像建立化身的3D姿态控制，从而增强部分观察场景的视图一致性，它解决了知名的多面问题(Janus Problem)并显着稳定了生成过程

此外还提出了一种渐进式的高分辨率3D合成策略，该策略显著提高了创建的3D化身的质量，因此AvatarVerse实现了3D化身的零样本3D建模，不仅更具表现力，而且比以前的作品具有更高的质量和保真度

严格的定性评估和用户研究展示了AvatarVerse在合成高保真3D化身方面的优越性，引领了高质量和稳定的3D化身的新标准

<img src="https://img.saraba1st.com/forum/202308/08/154923n11151lu15jj75jc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-153222.jpg</strong> (445.95 KB, 下载次数: 0)

下载附件

2023-8-8 15:49 上传

AvatarVerse基于简单的文本描述生成的高质量3D化身

<img src="https://img.saraba1st.com/forum/202308/08/154930kunf7icjynrdhnf4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-153300.jpg</strong> (124.74 KB, 下载次数: 0)

下载附件

2023-8-8 15:49 上传

AvatarVerse的概览图，通过采用文本提示和稠密姿态信号作为输入，使用DensePose-COCO预训练过的ControlNet模型显式地优化NeRF，并使用渐进网格、渐进半径和聚焦模式等策略来生成高分辨率和高质量的3D化身

<img src="https://img.saraba1st.com/forum/202308/08/154940xyryvv96iiiyo4ol.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-153723.jpg</strong> (112.55 KB, 下载次数: 0)

下载附件

2023-8-8 15:49 上传

稠密姿态条件Controlnet的定性评估结果，A图为使用稠密控制生成的10张具有不同视点和身体部位的图像，B图则由人体姿势(Openpose)信号控制的具有相同视点的10个对应图像，可以看到它经常无法生成化身的背面，并且难以生成模型的局部零件部分，C图的不紧贴皮肤生成会产生现实和虚构的化身

<img src="https://img.saraba1st.com/forum/202308/08/154946wco1nz6z55nqd9qq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-154129.jpg</strong> (159.88 KB, 下载次数: 0)

下载附件

2023-8-8 15:49 上传

与四种SOTA方法的定性比较，展示了AvatarVerse生成的几个非精选(non-cherry-picked)结果，与其他方法相比，本方法可以生成更高分辨率的细节并保持细粒度的几何形状

<img src="https://img.saraba1st.com/forum/202308/08/154954updgew32ee0zmax3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-154309.jpg</strong> (136.96 KB, 下载次数: 0)

下载附件

2023-8-8 15:49 上传

灵活的化身生成，A图为部分生成，所有结果均使用相同的文本提示“Stormtrooper”和“Batman”生成，B图为任意姿势生成

<img src="https://img.saraba1st.com/forum/202308/08/155004savdvlqz43viae38.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-154653.jpg</strong> (243.45 KB, 下载次数: 0)

下载附件

2023-8-8 15:50 上传

渐进策略的影响，A图为没有渐进策略，B图添加了渐进网格，C图在B图的基础上添加渐进半径，而D图又在C图上添加焦点模式，E图添加了网格细化，为最终的完整方法

<img src="https://img.saraba1st.com/forum/202308/08/155008jvqhrkgxzr26d6r0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-154701.jpg</strong> (128.93 KB, 下载次数: 0)

下载附件

2023-8-8 15:50 上传

控制信号的影响， A图没有额外的控制，B图具有骨架控制，C图使用了设计的稠密姿态控件，对于每种类型，都显示 RGB、法线、深度和相应的控制信号

<img src="https://img.saraba1st.com/forum/202308/08/155011mld3ppv6v6f3llp6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-154810.jpg</strong> (198.89 KB, 下载次数: 0)

下载附件

2023-8-8 15:50 上传

表面平滑策略的影响，A图没有进行表面平滑处理，B图进行表面平滑处理，使用相同的文本提示生成的结果

人类评估结果:

<img src="https://img.saraba1st.com/forum/202308/08/155028mw7u7ubbnjhjlhgw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-154434.jpg</strong> (60.27 KB, 下载次数: 0)

下载附件

2023-8-8 15:50 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 774#       发表于 2023-8-8 16:39

 本帖最后由 Machinery 于 2023-8-8 16:45 编辑 

ConceptLab

使用扩散先验约束(Diffusion Prior Constraints)的创意生成

项目主页:https://kfirgoldberg.github.io/ConceptLab/

github项目仓库:https://github.com/kfirgoldberg/ConceptLab

最近的文本到图像生成模型使我们能够将文字转化为充满活力、迷人的图像，随之而来的个性化(personalization)技术的激增也让我们能够在新场景中想象出独特的概念

然而，一个有趣的问题仍然存在：我们如何才能生成一个以前从未见过的新的、想象的概念？ 

在本文中提出了创意性文本到图像生成的任务(task of creative text-to-image generation)，通过寻求生成一个广泛类别的新成员(例如，生成与所有现有宠物不同的宠物)，利用尚未充分研究的扩散先验模型，并表明创意生成问题可以形式化为扩散先验输出空间的优化过程(optimization process)，从而产生一组“先验约束(prior constraints)”

为了防止生成的概念收敛到已存在的类中，使用了一个问答模型，该模型自适应地向优化问题添加新的约束，鼓励模型发现越来越多的独特创作

最后表明先验约束也可以充当强大的混合机制，使之能够在生成的概念之间创建混合，从而为创作过程引入更多的灵活性

<img src="https://img.saraba1st.com/forum/202308/08/164454t0la9b0650xcm5u9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-164214__01__01.jpg</strong> (590.97 KB, 下载次数: 0)

下载附件

2023-8-8 16:44 上传

在文本引导生成中，根据自由格式的文本提示创建图像，通过个性化方法可以学习代表特定概念或主题的新标记，创意生成方法学习代表属于给定类别(例如“宠物”或“水果”)的新概念的标记，所学到的概念经过优化，属于广泛的类别，但与该类别的现有成员不同

<img src="https://img.saraba1st.com/forum/202308/08/163916ixuzmli1vd9lu9ml.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-163441__01.jpg</strong> (112.3 KB, 下载次数: 0)

下载附件

2023-8-8 16:39 上传

<img src="https://img.saraba1st.com/forum/202308/08/163916ctg6iiicggfai8ks.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-163620.jpg</strong> (191.73 KB, 下载次数: 0)

下载附件

2023-8-8 16:39 上传

使用ConceptLab生成的新宠物，每对都描述了一个经过优化的新颖概念，与宠物类别的现有成员不匹配，使用不同的种子运行可以产生各种不同的全新概念

<img src="https://img.saraba1st.com/forum/202308/08/163921xjknkrk4a7c2jv7j.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-163632.jpg</strong> (84.96 KB, 下载次数: 0)

下载附件

2023-8-8 16:39 上传

优化代表希望生成的新概念的单个嵌入v*(例如，一种新型的“宠物”)

计算一组损失，鼓励学习的嵌入v*与目标类别的嵌入相似，同时不同于一组负子类(例如“狗”、“猫”)，这种相似性是在扩散先验模型的输出空间中计算的

为了逐渐生成更多独特的创作，在训练过程中查询预训练的BLIP-2 VQA模型，以根据当前生成的新概念逐步扩展负类集

使用实例:

<img src="https://img.saraba1st.com/forum/202308/08/163942qhoc6vxwdzzwwda4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-163813.jpg</strong> (752.94 KB, 下载次数: 0)

下载附件

2023-8-8 16:39 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 775#       发表于 2023-8-8 21:30

 本帖最后由 Machinery 于 2023-8-8 21:32 编辑 

UniversalNER

对大型语言模型进行针对性蒸馏以实现开放命名实体认知(Open Named Entity Recognition)

项目主页&amp;Demo演示:https://universal-ner.github.io/

github项目仓库:https://github.com/universal-ner/universal-ner

数据集&amp;模型下载:https://huggingface.co/Universal-NER

<img src="https://img.saraba1st.com/forum/202308/08/213220ozqkzkzs0ztfbbbb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-212838.jpg</strong> (80.11 KB, 下载次数: 0)

下载附件

2023-8-8 21:32 上传

<img src="https://img.saraba1st.com/forum/202308/08/213220inmrwws4murqwgwp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-212824.jpg</strong> (89.96 KB, 下载次数: 0)

下载附件

2023-8-8 21:32 上传

大型语言模型(LLM)已经表现出了卓越的通用性，例如理解任意实体和对应的关系，而事实证明，指令调整对于将LLM提炼成更具效益的模型(例如Alpaca和Vicuna)是卓有成效的

然而，这些学生模型(student models)在下游应用中仍然远远落后于最初的LLM，在本文中，通过以任务为中心的指令调整来探索有针对性的蒸馏，以训练能够在开放信息提取等广泛应用类别中表现出色的学生模型

使用命名实体认知(NER/named entity recognition)进行案例研究，展示了如何将ChatGPT提炼成更小的UniversalNER模型以用于开放 的NER任务

为了进行评估，汇集了迄今为止最大的NER基准，包括生物医学、编程、社交媒体、法律、金融等9个不同领域的总共43个数据集，在不使用任何直接监督的情况下，UniversalNER在数以万计的实体类型中获得了卓越的NER准确率，比Alpaca和Vicuna等一般指令调整模型平均高出30多个绝对F1 点

通过极小的参数，UniversalNER不仅获得了ChatGPT识别任意实体类型的能力，而且比其NER平均准确率高出7-9个绝对F1点

值得关注的是，UniversalNER的性能甚至远远超过了最先进的多任务指令调整系统，例如InstructUIE(使用有监督的NER示例)，本文还未进行彻底的消融研究，以评估蒸馏方法中各种成分的影响

将发布蒸馏配方、数据和UniversalNER模型，以方便未来的相关针对性蒸馏的研究

<img src="https://img.saraba1st.com/forum/202308/08/212931jhdcspprd9cp2797.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-210739.jpg</strong> (100.62 KB, 下载次数: 0)

下载附件

2023-8-8 21:29 上传

NER的指令数据构造，通过提示ChatGPT为NER任务生成了一个遵循指令的数据集，该数据集包含45889个输入输出对，涵盖240725个实体和13020个不同的实体类型

该数据集包含来自各个领域的实体类型，范围从一般领域(例如人员)到临床领域(例如医疗状况)，此外还观察到实体类型之间粒度的变化，例如County是Location的子集，Input Device是Product的子集，这些数据特征广泛覆盖实体类型，使其适合从各个领域的LLM中提取能力，实体类型按频率划分如下表所示

<img src="https://img.saraba1st.com/forum/202308/08/213000x51l4lgxt1ofcolw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-211511.jpg</strong> (70.29 KB, 下载次数: 0)

下载附件

2023-8-8 21:30 上传

以任务为中心的指令调整，与调整模型以执行不同任务的现有工作不同，本文提出了针对特定任务的指令调整的一般方法，其中预训练模型针对广泛的应用程序类(例如开放NER任务)进行了调整

对话式风格的指令调优：采用对话式风格调整格式，其中语言模型(LM)以一段段落作为输入，然后对于输出中出现的每个实体类型，将其转换为自然语言问题，随后调整LM以生成JSON列表形式的结构化输出，其中包含段落中查询类型的所有实体，将参考实体(图中的高亮)视为黄金Token，并对这些Token应用语言建模目标

<img src="https://img.saraba1st.com/forum/202308/08/213020dqgpdh27g6dbdvdp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-212005.jpg</strong> (50.08 KB, 下载次数: 0)

下载附件

2023-8-8 21:30 上传

负采样：在调整过程中，从未被作为查询而出现在段落中的所有实体类型的集合中随机采样负实体类型，并将预期输出设置为空JSON列表，这些负实体类型的采样是按概率比例在整个数据集中采样完成的，这种方法极大地改善了指令调整结果

通用 NER 基准，迄今为止最大的NER基准，包含9个领域的43个NER数据集，包含普通、生物医学、临床、STEM、编程、社交媒体、法律、金融和交通领域，数据分布概览如上所示

<img src="https://img.saraba1st.com/forum/202308/08/213027xijjzz9xjw4qbl7l.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-212451.jpg</strong> (206.23 KB, 下载次数: 0)

下载附件

2023-8-8 21:30 上传

零样本性能，UniversalNER大幅超越了相同尺寸的现有指令调整模型，更重要的是，UniversalNER在平均F1方面优于 ChatGPT，这表明本文提出的从不同输入中进行有针对性的蒸馏所产生的模型在广泛的应用类别上具有卓越的性能，同时保持相对较小的模型大小，领域细分还显示了UniversalNER相对于ChatGPT的改进

<img src="https://img.saraba1st.com/forum/202308/08/213031v17dc4hlf11p7zcp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-212724.jpg</strong> (137.25 KB, 下载次数: 0)

下载附件

2023-8-8 21:30 上传

有监督的多任务微调，跨不同数据集的单一模型的新SoTA，为了公平比较，使用InstructUIE-11B中相同的训练数据来训练UniversalNER-7B，下表结果显示，UniversalNER-7B在20个数据集上的平均F1为84.78%，分别超过BERT-base和InstructUIE-11B 4.69%和3.62%，该实验证明了模型在监督环境中的有效性

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 776#       发表于 2023-8-8 22:11

 本帖最后由 Machinery 于 2023-8-8 22:12 编辑 

AgentBench/Tiny LVLM-eHub

AgentBench：评估作为自动代理(Agent)的LLM

相关论文:https://arxiv.org/abs/2308.03688

github项目主页:https://github.com/THUDM/AgentBench

大型语言模型(LLM)正变得越来越智能和自主，其目标是超越传统NLP任务的实用现实世界任务，因此迫切需要评估LLM作为交互式环境中具有挑战性任务的自动代理的能力

本文推出了AgentBench，一个多维度不断发展的基准，目前由8个不同的环境组成，用于评估作为智能代理的LLM(LLM-as-Agent)在多轮开放式生成环境中的推理和决策能力，对25个LLM(包括API和开源模型)进行的广泛测试表明，虽然顶级商业LLM表现出在复杂环境中充当代理的强大能力，但它们与开源竞争对手之间的表现存在显著差异，作为一个正在进行的项目的组成部分，覆盖范围更广，系统性的对LLM评估需要更深入的考虑

<img src="https://img.saraba1st.com/forum/202308/08/221246pzz2anp3ppa62zph.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-221029__01.jpg</strong> (578.64 KB, 下载次数: 0)

下载附件

2023-8-8 22:12 上传

<img src="https://img.saraba1st.com/forum/202308/08/221141g6lo9pixve6eoolv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-221003.jpg</strong> (713.27 KB, 下载次数: 0)

下载附件

2023-8-8 22:11 上传

Tiny LVLM-eHub

Bard 的早期多模态实验

项目主页:http://lvlm-ehub.opengvlab.com/

相关论文:https://arxiv.org/abs/2308.03729

github项目主页:https://github.com/OpenGVLab/Multi-Modality-Arena

大型视觉语言模型(LVLM)的最新进展表明在处理复杂的多模态任务方面取得了重大进展，在这些前沿发展中，谷歌的Bard以其卓越的多模态能力脱颖而出，促进了跨领域的全面理解和推理

本文通过提出LVLM-eHub的轻量级变体(名为Tiny LVLM-eHub)，对LVLM的多模态能力进行了早期全面评估，其中特别关注了Bard，与原始版本相比，Tiny LVLM-eHub拥有几个瞩目的特性

首先，通过对42个标准文本相关视觉基准的定量评估，对视觉感知、视觉知识获取、视觉推理、视觉常识、物体幻觉和具身智能等六大类多模态能力进行系统评估

其次，还使用了ChatGPT集成评估(CEE)对LVLM的预测进行深入分析，从而实现稳健且准确的评估，并且与单词匹配方法相比，与人类评估的一致性得到了改善

第三，它仅包含2.1K个图像文本对，方便从业者轻松评估自己的离线LVLM，通过广泛的实验分析表明，Bard虽然仍然容易受到物体幻觉影响之外，在大多数多模态功能上都优于以前的LVLM

微型 LVLM-eHub可作为各种LVLM的评估基线，并鼓励推进多模态技术的创新

<img src="https://img.saraba1st.com/forum/202308/08/221119a9dd96zauhdhu6b6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230808-220842.jpg</strong> (402.99 KB, 下载次数: 0)

下载附件

2023-8-8 22:11 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  akitox  
##### 777#       发表于 2023-8-8 22:18

 本帖最后由 akitox 于 2023-8-8 22:23 编辑 

插眼，然而早期的链接似乎挂了？<img src="https://static.saraba1st.com/image/smiley/face2017/037.png" referrerpolicy="no-referrer">

*****

####  Machinery  
##### 778#       发表于 2023-8-8 22:22

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61957110&amp;ptid=2126390" target="_blank">akitox 发表于 2023-8-8 22:18</a>
插眼，然而链接似乎挂了？</blockquote>
哪个？项目时间跨度比较大，失效还挺正常的，不过一般项目名直接谷歌都能找到的，还有一些单纯就是代码还没整理好或者公布

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  akitox  
##### 779#       发表于 2023-8-8 22:26

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=61957152&amp;ptid=2126390" target="_blank">Machinery 发表于 2023-8-8 22:22</a>

哪个？项目时间跨度比较大，失效还挺正常的，不过一般项目名直接谷歌都能找到的，还有一些单纯就是代码还 ...</blockquote>
整理辛苦了，感谢。

项目链接的时效性是个问题，兴如野火，逝如夏蝉。<img src="https://static.saraba1st.com/image/smiley/face2017/012.png" referrerpolicy="no-referrer">


*****

####  Machinery  
##### 780#       发表于 2023-8-11 03:00

Shepherd

语言模型生成的谏言者/评断者(A Critic)

github项目地址:https://github.com/facebookresearch/Shepherd

随着大型语言模型的改进，人们对利用这些模型的功能来改进其输出的技术越来越感兴趣，在这项工作中，引入了Shepherd，一种专门针对批判性响应(critique responses)和改进建议(suggest refinements)进行调整的语言模型，超越了未经调整的模型的能力，可以识别各种错误并提供纠正建议

本文方法的核心是高质量的反馈数据集，根据社区反馈和人工标注来整理该数据集，尽管Shepherd很小(仅7B参数)，但它生成的评断相比包括ChatGPT在内的已存在模型的评断水平相同或更受欢迎

使用GPT-4进行评估，与竞争对手相比，Shepherd的平均胜率达到 53-87%，在人类评估中，Shepherd严格上优于其他模型，胜率相比chatgpt时接**局

<img src="https://img.saraba1st.com/forum/202308/11/025856jnvwk514ou5nu959.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-024301.jpg</strong> (318.2 KB, 下载次数: 0)

下载附件

2023-8-11 02:58 上传

Shepherd的概览图，给予LLM产生的问题和相应答案，Shepherd可以提供批判性评断，示例问题来自Stack Exchange社区，答案由Alpaca模型生成。 Shepherd通过识别错误或者提供建设性反馈来纠正Alpaca

<img src="https://img.saraba1st.com/forum/202308/11/025901hhmraf9i7777hqfz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-024517__01.jpg</strong> (52.06 KB, 下载次数: 0)

下载附件

2023-8-11 02:59 上传

左图为使用GPT-4 作为评估器进行偏好评估，右图为人类偏好评估，均为将Shepherd与7个不同数据集的3个不同的竞争模型进行对比

<img src="https://img.saraba1st.com/forum/202308/11/025906fa57txcv5xvb2p5c.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-024517__02.jpg</strong> (155.48 KB, 下载次数: 0)

下载附件

2023-8-11 02:59 上传

从Stack Exchange(网站)和人类标注收集的训练数据示例

<img src="https://img.saraba1st.com/forum/202308/11/025911gk5tmnz50ob8tbo3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-025348.jpg</strong> (362.91 KB, 下载次数: 0)

下载附件

2023-8-11 02:59 上传

GPT-4和人工评估的说明，以1-7的Likert分数对每个反馈进行评分

<img src="https://img.saraba1st.com/forum/202308/11/025917vs9dw9y429l51z9a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-025405.jpg</strong> (100.47 KB, 下载次数: 0)

下载附件

2023-8-11 02:59 上传

GPT-4和人工评估的说明，以从两个评断中选出更好的结果

<img src="https://img.saraba1st.com/forum/202308/11/025921t8wfi4148h0z4w4f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-025455.jpg</strong> (315.98 KB, 下载次数: 0)

下载附件

2023-8-11 02:59 上传

人类评估的挑战，在某些情况下，需要深厚的领域知识来评估各种反馈

<img src="https://img.saraba1st.com/forum/202308/11/025926e30pguipzoiquior.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-025646.jpg</strong> (241.82 KB, 下载次数: 0)

下载附件

2023-8-11 02:59 上传

来自人类评估的分数分布，分数范围1-2意味着模型未能给出正确的判断， 7分意味着反馈不仅给出了对答案的正确判断，而且还为改进答案提供了非常有帮助的反馈

<img src="https://img.saraba1st.com/forum/202308/11/025946azoiyp3wlwolbbwt.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-025711.jpg</strong> (221.74 KB, 下载次数: 0)

下载附件

2023-8-11 02:59 上传

与chatgpt的对比结果

<img src="https://img.saraba1st.com/forum/202308/11/025958fnvzh53vao1ga1gv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-025723.jpg</strong> (352.84 KB, 下载次数: 0)

下载附件

2023-8-11 02:59 上传

其他模型生成的评断

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 781#       发表于 2023-8-11 03:28

 本帖最后由 Machinery 于 2023-8-11 03:29 编辑 

LayoutLLM-T2I

从LLM获取布局指导以进行文本到图像生成

项目主页:https://layoutllm-t2i.github.io/

github项目地址:https://github.com/LayoutLLM-T2I/LayoutLLM-T2I

在文本到图像生成领域，Stable Diffusion(SD)在生成方面取得的卓越进展使得生成丰富种类的新颖的真实感图像成为可能，然而当前的模型在复杂的自然场景中仍然经常面临着未对齐(misalignment)问题(例如有问题的空间关系理解和图片内容生成计数错误等)，这阻碍了高保真度的文本到图像的生成

尽管最近已经努力通过提供细粒度的指导(例如草图和涂鸦)来提高可控性，但由于用户必须手动提供此类指导信息，因此这个问题尚未得到根本解决

在这项工作中，努力保证了合成的高保真图像质量，并在语义上与给定的文本提示一致，无需任何指导，为此提出了一种从粗到细的范式来实现布局规划和图像生成

具体来说，首先通过基于大型语言模型的上下文学习来生成以给定文本提示为条件的粗粒度布局，之后使用一种细粒度的对象交互扩散方法，以根据提示和自动生成的布局合成高保真度图像

大量的实验表明，本文提案的方法在布局和图像生成方面达成了SOTA结果

<img src="https://img.saraba1st.com/forum/202308/11/032713hd8kwybnh8p8uew8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-031626.jpg</strong> (210.55 KB, 下载次数: 0)

下载附件

2023-8-11 03:27 上传

T2I任务图示，提供一个文本提示，SD会遇到某些问题，例如空间混乱、动作模糊和计数失败等，本文提出模型能够通过利用自动生成的布局来合成高保真度图像，提示中的数学和关系术语用红色标记

<img src="https://img.saraba1st.com/forum/202308/11/032726ouubz3u0u0owomb0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-031647.jpg</strong> (187.13 KB, 下载次数: 0)

下载附件

2023-8-11 03:27 上传

框架概览图，所提出的布局引导扩散模型利用La-UNet，首先利用ChatGPT从给定的文本提示中引导出布局，然后提示编码器对文本提示进行建模，从文本中提取关系三元组(主题、关系、对象)，将文本诱导布局分开，最后，为了有效地集成布局信息，还引入了基于UNet的布局感知空间Transformer

<img src="https://img.saraba1st.com/forum/202308/11/032731cp57qbqpeqetq0lq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-032006.jpg</strong> (157.59 KB, 下载次数: 0)

下载附件

2023-8-11 03:27 上传

布局生成的相关示意图

<img src="https://img.saraba1st.com/forum/202308/11/032735c1uyn41tus8sna6m.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-032045.jpg</strong> (61.55 KB, 下载次数: 0)

下载附件

2023-8-11 03:27 上传

布局图像反馈模块由策略网络𝜋𝜓(𝑦𝑖，𝐶)和两个奖励𝑅和𝑅组成，在这两个奖励的指导下，该策略学习对信息丰富的训练数据实例进行采样，作为输入LLM的上下文，以激活布局规划能力

<img src="https://img.saraba1st.com/forum/202308/11/032740jp1lzyj814hzvcwe.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-032226.jpg</strong> (105.83 KB, 下载次数: 0)

下载附件

2023-8-11 03:27 上传

COCO 2014构建的文本到布局生成和文本到图像生成测试集的总体性能比较，其中最佳的结果以粗体突出显示

<img src="https://img.saraba1st.com/forum/202308/11/032744epjrkxkrxnkjb3gh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-032312.jpg</strong> (78.81 KB, 下载次数: 0)

下载附件

2023-8-11 03:27 上传

A图为上下文示例采样策略，B图为布局性能的少量比较，随机和NN采样分别表示随机采样和最近邻采样

相关评估结果，实例，与消融实验:

<img src="https://img.saraba1st.com/forum/202308/11/032801m6gjmen5ze165mt3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-032604.jpg</strong> (241.94 KB, 下载次数: 0)

下载附件

2023-8-11 03:28 上传

<img src="https://img.saraba1st.com/forum/202308/11/032801gllme7goq8tegp9o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230811-032632.jpg</strong> (386.06 KB, 下载次数: 0)

下载附件

2023-8-11 03:28 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  AKira730  
##### 782#       发表于 2023-8-11 15:04

插眼


*****

####  AKira730  
##### 783#       发表于 2023-8-11 15:04

插眼


*****

####  Machinery  
##### 784#       发表于 2023-8-13 00:58

AudioLDM 2

通过自监督预训练学习整体音频生成

项目主页:https://audioldm.github.io/audioldm2

github项目主页:https://github.com/haoheliu/audioldm2

Demo演示:https://huggingface.co/spaces/haoheliu/audioldm2-text2audio-text2music

尽管音频生成在不同类型的音频(例如语音、音乐、音效等)之间具有共通点，但为每种类型的音声设计模型时需要仔细考虑可能与其他类型所不同的特定目标和偏差

为了更接近统一的音频生成，本文提出了一个框架，该框架利用相同的学习方法来生成语音、音乐和音效，框架内引入了音频的通用表征方法，称为音频语言(LOA/language of audio)

任何音频都可以基于AudioMAE(一种自监督的预训练表征学习模型)转换为LOA，在生成过程中，使用GPT-2模型将模态转换为LOA，之后使用以LOA为条件的潜在扩散模型进行自监督音频生成学习

因此自然而然的带来了诸如上下文学习能力，可重用的自监督预训练AudioMAE，以及潜在扩散模型等优势

对文本到音频、文本到音乐和文本到语音的主要的基准实验证明了与以前的方法相比新的SOTA级或具有竞争力的性能

<img src="https://img.saraba1st.com/forum/202308/13/005802fukesu9n4ozj5kjv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-004727.jpg</strong> (172.59 KB, 下载次数: 0)

下载附件

2023-8-13 00:58 上传

AudioLDM 2架构概览图，AudioMAE的特点是可以作为连接音频语义语言模型阶段(GPT-2)和语义重建阶段(潜在扩散模型)的代理，概率切换控制器使用基准真实(ground truth)AudioMAE(Pgt)和GPT-2生成的AudioMAE特征(Ppred)作为条件来控制潜在扩散模型的概率，AudioMAE和潜在扩散模型都是使用音频数据进行自监督预训练的

<img src="https://img.saraba1st.com/forum/202308/13/005806rh55al3cian1slpl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-004839.jpg</strong> (76.76 KB, 下载次数: 0)

下载附件

2023-8-13 00:58 上传

可视化的基于TSNE和ESC50数据集中十个随机选择的类的潜在空间，图中的每个散点代表一个音频片段，AudioMAE特征空间倾向于将相似的音频分组在一起，这表明比起VAE特征来说有更多的语义结构

<img src="https://img.saraba1st.com/forum/202308/13/005810ei7zrdmibabdzwuu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-005047.jpg</strong> (96.94 KB, 下载次数: 0)

下载附件

2023-8-13 00:58 上传

AudioCaps评估集的性能测试对比，AudioLDM 2在主观和客观评估方面都远远优于以前的方法

<img src="https://img.saraba1st.com/forum/202308/13/005817i70rm9s87sdr9gfr.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-005200.jpg</strong> (66.75 KB, 下载次数: 0)

下载附件

2023-8-13 00:58 上传

AudioCaps评估集在不同的训练数据规模上的性能测试对比，其中具有完整训练数据的模型并未在AudioCaps上进行微调过

<img src="https://img.saraba1st.com/forum/202308/13/005821elojg97la7uaziuu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-005313.jpg</strong> (73.34 KB, 下载次数: 0)

下载附件

2023-8-13 00:58 上传

MusicCaps评估集上的性能测试对比，其中上标†表示使用其公开可用的实现复制的结果，MusicGen-Medium的开源版本不包含人声，导致性能比最初报告的结果稍差，在评估之前，所有生成的音频片段均被重新采样至16kHz

<img src="https://img.saraba1st.com/forum/202308/13/005824ht9tnn289zx7zu8s.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-005441.jpg</strong> (37.95 KB, 下载次数: 0)

下载附件

2023-8-13 00:58 上传

在LJSpeech测试集上评估文本转录语音性能

<img src="https://img.saraba1st.com/forum/202308/13/005829eoao3s3d69ngsdrg.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-005527.jpg</strong> (178.11 KB, 下载次数: 0)

下载附件

2023-8-13 00:58 上传

AudioLDM 2的上下文学习能力，左列显示了基准真实的音频，其中前导的2.5秒用作音频生成的上下文，音频上下文的延续显示在右列中，为了更好的演示，在继续之前手动插入0.15秒的蜂鸣声

<img src="https://img.saraba1st.com/forum/202308/13/005833ll5lrbofhlh62fm9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-005720.jpg</strong> (64.1 KB, 下载次数: 0)

下载附件

2023-8-13 00:58 上传

消融实验

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 785#       发表于 2023-8-13 01:15

Follow Anything

实时的开放集检测、轨迹跟踪与追随

github项目主页:https://github.com/alaamaalouf/FollowAnything

轨迹追踪与跟随感兴趣的对象对于多种机器人的用例至关重要，从工业自动化到物流和仓储，再到医疗保健和安全等领域

在本文中提出了一种实时检测、轨迹跟踪和追随任何物体的方法，称之为Follow Anything(FAn)，一种开放词汇和多模态的模型，它并不局限于只在训练时看到的概念，而且可以在推理时使用文本、图像或点击查询

通过利用来自大规模预训练模型(基础模型)的丰富视觉描述符，FAn可以通过将多模态查询输入(文本、图像、点击)与输入图像序列进行匹配来检测和分割对象，这些检测到和分割的对象在图像帧中进行跟踪，同时考虑了遮挡和对象重新出现

在现实世界的机器人系统(微型飞行器)上演示了FAn，并报告了其在实时控制循环中无缝跟踪感兴趣对象的能力，FAn可以部署在具有轻量级(6-8GB显存)显卡的笔记本电脑上，实现每秒6-20帧的吞吐量

为了实现快速采用、部署和可扩展性，研究组在项目主页上的https URL上开源了所有代码，同时也鼓励有兴趣的人可以在此(视频链接:https://www.youtube.com/watch?v=6Mgt3EPytrw)观看5分钟的相关讲解视频

github页面:

<img src="https://img.saraba1st.com/forum/202308/13/011539oq5emqdtqd05sml0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-011503.jpg</strong> (180.7 KB, 下载次数: 0)

下载附件

2023-8-13 01:15 上传

<img src="https://img.saraba1st.com/forum/202308/13/011539harzbay6mu6mrr9b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-011421.jpg</strong> (540.69 KB, 下载次数: 0)

下载附件

2023-8-13 01:15 上传

<img src="https://img.saraba1st.com/forum/202308/13/011539mlnllobi7m7n7vmy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230813-011445.jpg</strong> (259.16 KB, 下载次数: 0)

下载附件

2023-8-13 01:15 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 786#       发表于 2023-8-15 02:15

 本帖最后由 Machinery 于 2023-8-15 02:16 编辑 

PIPPA

部分合成的对话数据集

hugface数据集地址:https://huggingface.co/datasets/PygmalionAI/PIPPA

随着越来越强大的大型语言模型的出现，人们越来越有兴趣利用这些模型进行休闲对话和角色扮演应用，然而，现有的对话和角色扮演数据集通常无法捕捉现实世界中的用户们表现出的多样化而又微妙的交互

为了解决这一限制并为快速发展的领域做出贡献，本文引入了一个名为PIPPA(人与人工智能之间的个性化交互文本对/Personal Interaction Pairs between People and AI)的部分合成的数据集

PIPPA是社区驱动的众包努力的成果，来自一群角色扮演爱好者，该数据集包含26000个对话会话中的超过100万条语句，为研究人员和AI开发人员在角色扮演场景中探索和完善对话 AI 系统提供了丰富的资源

<img src="https://img.saraba1st.com/forum/202308/15/021455pr86rqoo0qqaxax0.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-020923.jpg</strong> (193.44 KB, 下载次数: 0)

下载附件

2023-8-15 02:14 上传

CharacterAI聊天界面的截图，滑动(Swipes)指的是丢弃当前的机器生成结果并重新提示生成新的结果

<img src="https://img.saraba1st.com/forum/202308/15/021500aahtp2hdtwavahmm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-021151.jpg</strong> (73.8 KB, 下载次数: 0)

下载附件

2023-8-15 02:15 上传

对话长度的分布(定义为对话中的“轮数”)，为了增强可读性，将显示范围限制为0-250轮

<img src="https://img.saraba1st.com/forum/202308/15/021505ptj82dkoja3jkjks.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-021252.jpg</strong> (90.48 KB, 下载次数: 0)

下载附件

2023-8-15 02:15 上传

PIPPA数据集中人类输入和机器人响应的消息长度分布

<img src="https://img.saraba1st.com/forum/202308/15/021509clldkvsknzyvkk9y.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-021339.jpg</strong> (77.07 KB, 下载次数: 0)

下载附件

2023-8-15 02:15 上传

PIPPA数据集中字符类别的分布，请注意，每个会话中的结果都可能被分配多个类别或根本没有分配

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 787#       发表于 2023-8-15 02:51

MeChat

中文心理健康支持对话大模型与数据集

github项目地址:https://github.com/qiuhuachuan/smile

<img src="https://img.saraba1st.com/forum/202308/15/025236lw4rqwniywnq3nsy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-025019__01.jpg</strong> (394.78 KB, 下载次数: 0)

下载附件

2023-8-15 02:52 上传

<img src="https://img.saraba1st.com/forum/202308/15/025126l8xiamzvxmaqffa2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-025049.jpg</strong> (768.53 KB, 下载次数: 0)

下载附件

2023-8-15 02:51 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 788#       发表于 2023-8-15 02:58

DoctorGPT

DoctorGPT是一个大型语言模型，可以通过美国医生执照考试(US Medical Licensing Exam)，作为一个开源项目，其使命是为每个人提供自己的私人医生

github项目地址:https://github.com/llSourcell/DoctorGPT

DoctorGPT使用Meta的Llama2 70亿参数大型语言模型作为基础模型在医疗对话数据集上进行了微调，然后使用强化学习和宪法人工智能(Constitutional AI)进一步改进

由于该模型的大小只有3GB，因此它适合任何本地设备，无需支付任何API费用，可以直接使用，完全免费，专为离线使用而设计，可以保护患者的隐私，并且还可以在iOS、Android和Web上使用

<img src="https://img.saraba1st.com/forum/202308/15/025756fbl8nprubn367orn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-025644.jpg</strong> (840.4 KB, 下载次数: 0)

下载附件

2023-8-15 02:57 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 789#       发表于 2023-8-15 03:10

FaceChain

FaceChain是一个可以用来打造个人数字形象的深度学习模型工具，用户仅需要提供最低三张照片即可获得独属于自己的个人形象数字替身

github项目主页:https://github.com/modelscope/facechain

<img src="https://img.saraba1st.com/forum/202308/15/031026t7cn9c17992cmbvv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-030828.jpg</strong> (444.86 KB, 下载次数: 0)

下载附件

2023-8-15 03:10 上传

<img src="https://img.saraba1st.com/forum/202308/15/031027cjc41z7k8kvfeeec.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-030913.jpg</strong> (750.61 KB, 下载次数: 0)

下载附件

2023-8-15 03:10 上传

<img src="https://img.saraba1st.com/forum/202308/15/031121nv5ddiw0dgzwg076.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-031107.jpg</strong> (752.1 KB, 下载次数: 0)

下载附件

2023-8-15 03:11 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 790#       发表于 2023-8-15 03:18

FlagEmbedding

将任意文本映射为低维稠密向量，以用于检索、分类、聚类或语义匹配等任务，并可支持为大模型调用外部知识

github项目主页:https://github.com/FlagOpen/FlagEmbedding

<img src="https://img.saraba1st.com/forum/202308/15/031426utt7ws3t07xx7wuu.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-031258.jpg</strong> (170.06 KB, 下载次数: 0)

下载附件

2023-8-15 03:14 上传

<img src="https://img.saraba1st.com/forum/202308/15/031756dqqiyqiy3zyccu3i.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-031648.jpg</strong> (305.05 KB, 下载次数: 0)

下载附件

2023-8-15 03:17 上传

<img src="https://img.saraba1st.com/forum/202308/15/031756f28o27rs4sqzkxxw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-031707.jpg</strong> (375.88 KB, 下载次数: 0)

下载附件

2023-8-15 03:17 上传

<img src="https://img.saraba1st.com/forum/202308/15/031756v097g4uc058805i4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-031737.jpg</strong> (509.81 KB, 下载次数: 0)

下载附件

2023-8-15 03:17 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 791#       发表于 2023-8-15 03:40

 本帖最后由 Machinery 于 2023-8-15 03:41 编辑 

Japanese StableLM

Japanese StableLM，Stability AI发布的第一个日语(Japanese)语言模型(LM/language model)，即Japanese StableLM Alpha，这是为日语使用者创建的性能最佳的开源可用LM

Japanese StableLM Base Alpha 7B版本权重下载:https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b

Japanese StableLM Instruct Alpha 7B版本权重下载:https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b

<img src="https://img.saraba1st.com/forum/202308/15/033934y7jo2ksfg2foo8bb.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-032555.jpg</strong> (238.61 KB, 下载次数: 0)

下载附件

2023-8-15 03:39 上传

Japanese StableLM 是一个拥有70亿参数的通用语言模型，根据四组基于日语的LM测试的基准组件评估结果，它是目前表现最好的开源日语模型

StableLM Base Alpha 7B将在可商用的Apache License 2.0许可下发布，而Japanese StableLM Instruct Alpha 7B是一个专为研究目的而创建的模型，仅供研究使用，详情请参阅Hugging Face Hub页面

Japanese StableLM Base Alpha 7B经过主要来自网络的大规模数据训练学习生成文本，训练数据主要由日语和英语文本组成，其余2%的语料为开源代码形式(source code)

除了开放数据集外，训练数据还包括Stability AI Japan创建的数据集以及EleutherAI Polyglot项目日本团与Stability AI Japan社区成员合作创建的数据集

在训练中使用了EleutherAI GPT-NeoX的扩展应用，例如模型架构融合了SwiGLU和xPos等新技术，在所有epoch中训练累积使用了约7500亿Token

<img src="https://img.saraba1st.com/forum/202308/15/033958nzal8q0f0z0gkfqy.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-033727.jpg</strong> (204.04 KB, 下载次数: 0)

下载附件

2023-8-15 03:39 上传

Japanese StableLM Instruct Alpha 7B模型则是一种经过额外调整以遵循用户指令的语言模型，采用了监督微调(Supervised Fine-tuning)进行额外训练，并使用多个开放数据集，还通过使用语言模型评估工具(lm-evaluation-harness项目链接:https://github.com/EleutherAI/lm-evaluation-harness)显著提高了性能评估分数

<img src="https://img.saraba1st.com/forum/202308/15/034007vdkk0hdcl3g0j3c5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-033805.jpg</strong> (144.41 KB, 下载次数: 0)

下载附件

2023-8-15 03:40 上传

为了评估性能，在句子分类、句子对分类、问答和句子摘要等任务上测试了模型，使用EleutherAI的lm-evaluation-harness基准测试来测量性能

与Open LLM Leaderboard中的惯例类似，计算了八个任务中得分的平均值，并将其用于每个模型的总体分数评估，Japanese StableLM Instruct Alpha 7B得分为54.71，遥遥领先于其他日本型号，同时Stability AI Japan也在改进测试这些模型的评估方法

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 792#       发表于 2023-8-15 03:56

 本帖最后由 Machinery 于 2023-8-15 03:57 编辑 

StableCode

Stability AI发布的StableCode，这是其第一个用于编码的LLM生成人工智能产品，该产品旨在帮助程序员完成日常工作，同时也为准备提升编码技能水平的新手开发人员提供了一个出色的学习工具

项目博客说明:https://stability.ai/blog/stablecode-llm-generative-ai-coding

基础模型权重下载:https://huggingface.co/stabilityai/stablecode-completion-alpha-3b-4k

指令微调模型权重下载:https://huggingface.co/stabilityai/stablecode-instruct-alpha-3b

具有长上下文窗口长度的模型:https://huggingface.co/stabilityai/stablecode-completion-alpha-3b

<img src="https://img.saraba1st.com/forum/202308/15/035627v47243537qg332zk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-034804.jpg</strong> (130.15 KB, 下载次数: 0)

下载附件

2023-8-15 03:56 上传

StableCode为开发人员提供了一种独特的方式，通过使用三种不同的模型来帮助进行编码，从而提高效率

基础模型首先使用来自BigCode的stack-dataset(v1.2)的多种编程语言进行训练，然后使用Python、Go、Java、Javascript、C、markdown和C++等流行语言进行进一步训练，总体来说，在HPC集群上使用5600亿代码Token训练了本模型

<img src="https://img.saraba1st.com/forum/202308/15/035635plcr1g6hjbb0go6o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-035335.jpg</strong> (172.52 KB, 下载次数: 0)

下载附件

2023-8-15 03:56 上传

建立基础模型后针对特定用例调整指令模型，以帮助解决复杂的编程任务，为了实现这一结果，在基础模型上训练了约120000个Alpaca格式的代码指令/响应对

对于想要了解更多编码知识的人来说，StableCode是理想的构建砖块，而长上下文窗口模型(long-context window model)是确保用户可以使用单行和多行自动完成建议代码的完美助手，该模型旨在一次处理更多代码(比之前发布的具有16000个Token上下文窗口的开放模型多约2-4倍)，允许用户查看或编辑相当于最多5个平均大小的Python文件同时，对于想要迎接更大挑战的初学者来说，也是理想的学习工具

<img src="https://img.saraba1st.com/forum/202308/15/035641hd747ordxokezdre.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230815-035424.jpg</strong> (126.36 KB, 下载次数: 0)

下载附件

2023-8-15 03:56 上传

与具有相似参数数量和训练Token数量的其他模型的比较，使用流行的HumanEval基准的标准pass@1和pass@10指标

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 793#       发表于 2023-8-16 13:38

OctoPack

指令调整代码大型语言模型

github项目主页:https://github.com/bigcode-project/octopack

<img src="https://img.saraba1st.com/forum/202308/16/133835mnvjkjjr37s2v5r8.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-132452.jpg</strong> (233.56 KB, 下载次数: 0)

下载附件

2023-8-16 13:38 上传

指令微调大型语言模型(LLM)可以有效提升自然语言任务的性能，本文通过应用使用代码进行指令调整，利用Git提交的自然结构，将代码更改与人类指令配对，编译了CommitPack，一个跨350种编程语言的4TB的Git提交数据集

在16B参数的StarCoder模型上对CommitPack与其他自然或合成代码指令数据集(如xP3x、Self-Instruct、OASST等)进行了基准测试，并在HumanEval Python基准测试上，在未经过OpenAI输出训练的模型中实现了46.2%的pass@1指标的SOTA性能

还引入了HumanEvalPack，将HumanEval基准扩展为跨6种语言(Python、JavaScript、Java、Go、C++、Rust)的总共3种类编码任务(代码修复、代码解释、代码合成)

其中本文的OctoCoder和OctoGeeX模型在所有许可模型测试中在HumanEvalPack上实现了最佳性能，还展示了CommitPack在泛化到更广泛的语言和自然编码任务方面的优势

<img src="https://img.saraba1st.com/forum/202308/16/133735r5mtlsgxgixgin58.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-132523.jpg</strong> (154.22 KB, 下载次数: 0)

下载附件

2023-8-16 13:37 上传

OCTOPACK概览图，图一为4TB数据集COMMITPACK的样本，图2为OCTOCODER、OCTOGEEX和其他代码模型在HUMANEVAL PACK上的性能，涵盖3种编码任务和6种编程语言

<img src="https://img.saraba1st.com/forum/202308/16/133740jbnf61tts3lnzf0b.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-132746.jpg</strong> (225.82 KB, 下载次数: 0)

下载附件

2023-8-16 13:37 上传

COMMITPACK和COMMITPACKFT概览图，上图为完整提交数据(COMMITPACK)的语言分布以及针对高质量指令过滤的变体(COMMITPACKFT)，下图为根据GPT-4判断的在COMMITPACKFT的Python子集(59K样本)上提交的任务分布

<img src="https://img.saraba1st.com/forum/202308/16/133745tn1qzgk1fpok3tz4.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-133124.jpg</strong> (64.54 KB, 下载次数: 0)

下载附件

2023-8-16 13:37 上传

代码指令数据的统计，编程语言的数量、总样本以及包含许可指令数据集代码的样本比例，为了对这些数据集进行微调，使用了每个约包含5000个样本的小子集数据集

<img src="https://img.saraba1st.com/forum/202308/16/133749d8ll2bxlhmwlvqvq.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-133134.jpg</strong> (240.7 KB, 下载次数: 0)

下载附件

2023-8-16 13:37 上传

HUMANEVAL PACK概览图，第一个HumanEval问题通过Python的三个场景进行描述，其中HUMANEVAL FIX的错误包括缺少“abs”语句

<img src="https://img.saraba1st.com/forum/202308/16/133755hzhzthzbff88hhhm.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-133318.jpg</strong> (57.91 KB, 下载次数: 0)

下载附件

2023-8-16 13:37 上传

通过使用指令调整的StarCoder模型比较许可的指令数据集，模型在HUMANEVALPACK的Python子集上进行评估

<img src="https://img.saraba1st.com/forum/202308/16/133810li66qjyj59tqvuqp.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-133454.jpg</strong> (362.58 KB, 下载次数: 0)

下载附件

2023-8-16 13:38 上传

HUMANEVALPACK中的零样本pass@1的百分比性能，InstructCodeT5+、WizardCoder、StarChat-β、StarCoder和OCTOCODER有16B参数，CodeGeeX2和OCTOGEEX有6B参数，BLOOMZ有176B个参数

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 794#       发表于 2023-8-16 14:13

 本帖最后由 Machinery 于 2023-8-16 14:15 编辑 

Platypus

快速、便宜又强大的LLM改进

项目主页:https://platypus-llm.github.io/

Platypus模型权重下载:https://huggingface.co/garage-bAInd

Open-Platypus数据集下载:https://huggingface.co/datasets/garage-bAInd/Open-Platypus

HuggingFace's Open LLM Leaderboard:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

<img src="https://img.saraba1st.com/forum/202308/16/141204e3jz53uu3he3u5v3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-141154.jpg</strong> (92.25 KB, 下载次数: 0)

下载附件

2023-8-16 14:12 上传

Platypus是一个经过微调和合并的大型语言模型(LLM)系列之一，它实现了目前最强的性能，截至该作品发布之日，在HuggingFace's Open LLM Leaderboard上排名第一

在本文中，描述了如何构造精心策划的数据集Open-Platypus，它是其他开放数据集的子集，同时向公众发布了微调和合并LoRA模块的过程，用以从预训练的LLM中保存强大的先验信息，同时让模型能够将特定领域的知识表现出来，本文在检查训练数据中的测试数据泄漏和污染方面所做的努力，可以为未来的研究提供信息

具体来说，Platypus系列在跨模型大小的定量LLM指标方面实现了强劲的性能，在全球开放LLM排行榜上名列前茅，同时仅使用其他最先进的微调所需的一小部分微调数据和算力调整LLM

13B Platypus模型可以在5小时内使用25k个问题样本在单个A100 GPU上进行训练，这证明了Open-Platypus数据集的质量，并为该领域的更多改进提供了机会

<img src="https://img.saraba1st.com/forum/202308/16/141253qfhdtozi9miwf9fv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-140341.jpg</strong> (153.2 KB, 下载次数: 0)

下载附件

2023-8-16 14:12 上传

数据集、许可证和泄露问题的数量，*号标示代表数据集未添加到Open-Platypus中，但依然将其包含在内，因为在考虑合并哪些模型时进行了污染检查

<img src="https://img.saraba1st.com/forum/202308/16/141300yv2tzq299jxve9z9.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-140720.jpg</strong> (204.96 KB, 下载次数: 0)

下载附件

2023-8-16 14:13 上传

优先考虑防止基准测试问题泄漏到训练集中，以避免模型仅仅通过记忆而产生偏差结果，在力求准确性的同时，考虑到提出问题的方式多种多样以及一般领域知识的影响，从而意识到将问题标记为重复项时需要灵活的标注

为了管理潜在的泄漏，精心设计了启发式方法，用于手动过滤来自Open-Platypus的问题，其与基准问题的余弦嵌入相似度超过80%，将潜在泄漏分为三类:重复、灰色区域、相似但不同，为了谨慎起见，将所有组都排除训练集外

完善数据集后，集中研究了两种方法：低秩近似(LoRA)训练和参数高效微调(PEFT)库，与完全微调不同，LoRA保留预先训练的模型权重，在Transformer层中集成秩分解矩阵，这减少了可训练参数，节省了训练时间和成本

最初微调针对的是v_proj、q_proj、k_proj和 o_proj等注意力模块，后来根据He等人的见解，过渡到了gate_proj、down_proj和up_proj模块，将其统一应用于13B和70B模型，从而得到了0.27%和0.2%的可训练参数，其中唯一的差异是这些模型之间的初始学习率，相关细节请参阅论文

评估结果:

<img src="https://img.saraba1st.com/forum/202308/16/141321ytd8rt8atltl3t3o.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-141132.jpg</strong> (126.22 KB, 下载次数: 0)

下载附件

2023-8-16 14:13 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 795#       发表于 2023-8-16 14:34

 本帖最后由 Machinery 于 2023-8-16 14:36 编辑 

IP-Adapter

用于文本到图像扩散模型的文本兼容的图像提示自动适配器(adapter)

项目主页:https://ip-adapter.github.io/

github项目仓库:https://github.com/tencent-ailab/IP-Adapter

近年来，人们见证了大型文本到图像扩散模型的强大能力，它具有令人印象深刻的生成高保真图像的能力，然而仅使用文本提示来生成所需的图像是非常棘手的，因为它通常涉及复杂的提示工程

文字提示的替代方案是图像提示，俗话说：“一图胜千言”，尽管现有的从预训练模型直接微调的方法是有效的，但它们需要大量的计算资源，并且与其他基础模型、文本提示和结构控制不兼容

在本文中，提出了IP-Adapter，一种有效且轻量级的适配器，可以为预训练的文本到图像扩散模型实现图像提示功能，IP-Adapter的核心设计是解耦的交叉注意力机制，它将文本特征和图像特征从交叉注意力层中分离开来

尽管方法很简单，但仅具有22M参数的IP-Adapter就可以实现与完全微调的图像提示模型相当甚至更好的性能，当冻结预训练的扩散模型时，所提出的IP-Adapter不仅可以推广到从同一基础模型微调的其他自定义模型，还可以推广到使用现有可控工具的可控生成，借助解耦的交叉注意力策略，图像提示也可以与文本提示很好地配合，实现多模态图像生成

<img src="https://img.saraba1st.com/forum/202308/16/143246e739xtr3ttwyky7z.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-143026.jpg</strong> (117.22 KB, 下载次数: 0)

下载附件

2023-8-16 14:32 上传

通过使用提出的IP-Adapter应用于预训练的文本到图像扩散模型和附加结构控制器进行各种图像合成

<img src="https://img.saraba1st.com/forum/202308/16/143251p7nipbnlfn5h4hnz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-143018.jpg</strong> (90.94 KB, 下载次数: 0)

下载附件

2023-8-16 14:32 上传

图像提示适配器(image prompt adapter)被设计用于预训练的文本到图像扩散模型能够生成带有图像提示的图像，所提出的IP-Adapter由两部分组成：一个图像编码器，用于从图像提示中提取图像特征；以及具有解耦交叉注意力的自适应模块，用于将图像特征嵌入到预训练的文本到图像扩散模型中

对比评估与生成样本，结构控制与多模态提示生成:

<img src="https://img.saraba1st.com/forum/202308/16/143440af1cvc3vyc3651kw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-143111__01__01__01.jpg</strong> (740.33 KB, 下载次数: 0)

下载附件

2023-8-16 14:34 上传

<img src="https://img.saraba1st.com/forum/202308/16/143441nq111gzez1jzpgy1.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-143111__01__01__02.jpg</strong> (934.78 KB, 下载次数: 0)

下载附件

2023-8-16 14:34 上传

<img src="https://img.saraba1st.com/forum/202308/16/143441gzgdgpu26cim25pe.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-143111__02__01.jpg</strong> (806.3 KB, 下载次数: 0)

下载附件

2023-8-16 14:34 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 796#       发表于 2023-8-16 15:47

VisIT-Bench

受现实世界使用作为灵感启发的视觉语言指令跟随基准

项目主页:http://visit-bench.github.io/

数据集:https://huggingface.co/datasets/mlfoundations/VisIT-Bench

github项目仓库:https://github.com/mlfoundations/VisIT-Bench/

Visual InsTruction Benchmark排行榜:https://huggingface.co/spaces/mlfoundations/VisIT-Bench-Leaderboard

<img src="https://img.saraba1st.com/forum/202308/16/154658n4ijp2bmpj6fs4i2.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154038.jpg</strong> (254.63 KB, 下载次数: 0)

下载附件

2023-8-16 15:46 上传

VisIT-Bench(视觉指令基准/Visual InsTruction Benchmark)，一个用于评估现实世界中实际使用情况下的指令跟随视觉语言模型能力的基准

项目策划了70个“指令群(instruction families)”，在预想中指令调整的视觉语言模型应该能够解决这些问题，评估范围超越了经典的VQAv2和COCO等，包括从基本认知到进行游戏和创意生成等任务

经过整理后的数据集包含592个测试查询，每个查询都有一个人工编写的指令条件标题，这些描述体现了特定于指令的因素，例如对于询问轮椅使用者到达店面门前的指令，指令条件标题描述了坡道与潜在障碍，这些描述使之能够收集每个实例的经人工验证的参考输出，使用纯文本LLM自动评估候选的多模态生成，并与人类判断保持一致

使用人工和自动评估来量化模型和参考之间的质量差距，例如，性能最佳的指令跟随模型仅在27%的比较中胜过GPT-4参考模型，VisIT-Bench作为开放排行榜，评估者只需在项目网站上提交模型响应即可

<img src="https://img.saraba1st.com/forum/202308/16/154712btethhx59754t422.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154433.jpg</strong> (212.28 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

<img src="https://img.saraba1st.com/forum/202308/16/154712hzu34kcl7ct4i3ci.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154440.jpg</strong> (197.96 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

<img src="https://img.saraba1st.com/forum/202308/16/154712v44qneg6zdnqtn48.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154457.jpg</strong> (227.05 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

<img src="https://img.saraba1st.com/forum/202308/16/154718tgkgmgwkkmqmmmmi.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154511.jpg</strong> (134.75 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

<img src="https://img.saraba1st.com/forum/202308/16/154718ndrjr18bb8p68wgj.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154521.jpg</strong> (173.06 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

<img src="https://img.saraba1st.com/forum/202308/16/154718d09ox6wxz9y6oo39.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154533.jpg</strong> (206.86 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

<img src="https://img.saraba1st.com/forum/202308/16/154718ugfdxfchzlh5fdd3.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154546.jpg</strong> (160.13 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

<img src="https://img.saraba1st.com/forum/202308/16/154718nbaq05exach05icc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154557.jpg</strong> (194.62 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

相关评估榜单:

<img src="https://img.saraba1st.com/forum/202308/16/154734g4xo5nm5nwodzbox.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230816-154612.jpg</strong> (134.39 KB, 下载次数: 0)

下载附件

2023-8-16 15:47 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  两袖清风和中堂  
##### 797#       发表于 2023-8-16 22:52

唉，论文看不懂，要是有傻瓜包就好了

刚刚凑了一条256G内存的垃圾服务器


*****

####  ZetaGo  
##### 798#       发表于 2023-8-16 23:34

请教下各位潭友，手里有1万来张标注好质量信息(云量，条纹，过曝，偏色...)的卫星影像，想炼一个能识别遥感影像质量的工具来保护视力，用哪种技术简单方便呢？要是有必要微调个多模态又用哪家的模型合适呢？
现在机器装了2*12G显卡，还有几张T4没地方插


*****

####  西敏寺君  
##### 799#       发表于 2023-8-17 06:11

马克马克


*****

####  Machinery  
##### 800#       发表于 2023-8-17 15:20

 本帖最后由 Machinery 于 2023-8-17 15:22 编辑 

CoDeF

用于时间一致性视频处理的内容变形场(Content Deformation Fields)

项目主页:https://qiuyu96.github.io/CoDeF/

github项目仓库:https://github.com/qiuyu96/CoDeF

<img src="https://img.saraba1st.com/forum/202308/17/151822eu5wrk6hu88r3ym6.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-145806.jpg</strong> (300.2 KB, 下载次数: 0)

下载附件

2023-8-17 15:18 上传

本文提出了内容变形场(CoDeF/content deformation)作为一种新的视频表征，它由聚合整个视频中的静态内容(static contents)的标准内容场(canonical content field)和记录沿时间轴行进的每个单独的帧的标准图像的变换(从标准内容渲染而来)的时间变形场(temporal deformation field)组成

<img src="https://img.saraba1st.com/forum/202308/17/151831agh4doo4gdso8i1a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-145900.jpg</strong> (318.4 KB, 下载次数: 0)

下载附件

2023-8-17 15:18 上传

给定一个目标视频，这两个场被联合优化，使用精心定制的渲染工作流程重建，通过在优化过程中引入一些正则化，可以促使标准内容场能够从视频继承语义(例如对象的形状)

通过这样的设计，CoDeF可以自然的支持视频处理中的图像提升算法，从某种意义上说，人们可以将图像算法应用于标准图像(canonical image)，借助时间变形场，可以轻松地将结果传播到整个视频

通过实验证明，CoDeF无需任何训练即可将图像到图像的转换提升为视频到视频的转换，同时也可以无需训练的将关键点检测提升为关键点跟踪

更重要的是，得益于提升策略，因为仅在一张图像上部署算法，与现有的视频到视频转换方法相比，本方法在处理的视频中实现了卓越的跨帧一致性，甚至可以跟踪水和烟雾等非刚性物体

CoDeF的多功能应用包括文本引导的视频到视频转换、视频对象跟踪、视频关键点跟踪，值得注意的是，通过所提出的视频表征类型，可以直接在无需任何视频处理的情况下使用提升图像算法进行视频处理

<img src="https://img.saraba1st.com/forum/202308/17/151851xkm0f0m0uwfnzonf.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-145923.jpg</strong> (114.14 KB, 下载次数: 0)

下载附件

2023-8-17 15:18 上传

所提出的视频表征CoDeF的图示，它将任意视频分解为2D内容标准场和3D时间变形场，每个场均通过使用高效MLP的多分辨率2D或 3D哈希表来实现，这种新型的表征可以自然支持视频处理的提升图像算法

<img src="https://img.saraba1st.com/forum/202308/17/151904bhzdo48h41yoll19.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-150312__01.jpg</strong> (475.58 KB, 下载次数: 0)

下载附件

2023-8-17 15:19 上传

layered neural atlas和CoDeF在视频重建方面的定性比较，这反映了捕捉视频表征的能力，并且在视频处理中也发挥着基础作用

<img src="https://img.saraba1st.com/forum/202308/17/151908gv88bc86csgwg5cv.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-150851.jpg</strong> (316.7 KB, 下载次数: 0)

下载附件

2023-8-17 15:19 上传

对不同方法的文本引导视频到视频转换任务的定性比较，包括 Text2Live、Tune-A-Video、FateZero，以及通过CoDeF直接提升ControlNet

<img src="https://img.saraba1st.com/forum/202308/17/152044tek8n2ae42vcfyyc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-152035.jpg</strong> (53.29 KB, 下载次数: 0)

下载附件

2023-8-17 15:20 上传

可视化的跨帧点对应，在使用CoDeF重建视频后直接从时间变形场中提取

<img src="https://img.saraba1st.com/forum/202308/17/151947cfmef4u4mbsom41f.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-151108.jpg</strong> (86.05 KB, 下载次数: 0)

下载附件

2023-8-17 15:19 上传

通过CoDeF提升图像分割算法来实现视频对象跟踪

<img src="https://img.saraba1st.com/forum/202308/17/151952bktk6pipdzd6t9sd.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-151201.jpg</strong> (144.62 KB, 下载次数: 0)

下载附件

2023-8-17 15:19 上传

通过CoDeF提升图像超分辨率算法来实现视频超分辨率

<img src="https://img.saraba1st.com/forum/202308/17/151932xruylyyddyzpzmru.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-151256.jpg</strong> (251.15 KB, 下载次数: 0)

下载附件

2023-8-17 15:19 上传

用户交互式视频编辑，通过仅编辑一张图像并使用CoDeF沿时间轴传播结果来实现

<img src="https://img.saraba1st.com/forum/202308/17/152013gvahh77v6hsqv21a.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-152000.jpg</strong> (204.6 KB, 下载次数: 0)

下载附件

2023-8-17 15:20 上传

消融实验中关于退火哈希的有效性验证，标准图像中的不自然会损害下游任务的性能

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 801#       发表于 2023-8-17 15:30

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=62054197&amp;ptid=2126390" target="_blank">ZetaGo 发表于 2023-8-16 23:34</a>
请教下各位潭友，手里有1万来张标注好质量信息(云量，条纹，过曝，偏色...)的卫星影像，想炼一个能识别遥感 ...</blockquote>
这个是非常经典的图像分类任务，类似AI图像鉴别，ModelScope和百度飞桨有大把现成的应用

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 802#       发表于 2023-8-17 15:53

Link-Context-Learning

多模态LLM的链接上下文学习(Link-Context Learning)

github项目主页:https://github.com/isekai-portal/Link-Context-Learning

从上下文中学习新概念并提供适当响应的能力在人类对话中至关重要，尽管当前的多模态大型语言模型(MLLM/Multimodal Large Language Models)和大型语言模型(LLM)在大规模数据集上进行训练，但以免训练的方式识别未见图像或理解新概念仍然是一个挑战

上下文学习(ICL)探索免训练的小样本学习，鼓励模型从有限的任务中“学会学习(learn to learn)”并泛化到未见过的任务

在这项工作中，提出了链接上下文学习(LCL/link-context learning)， 它强调“因果推理(reasoning from cause and effect)”来增强MLLM的学习能力

LCL超越了传统的ICL，明确强化了支持集和查询集之间的因果关系，通过提供因果关系的演示，LCL引导模型辨别，而不仅仅是类比，还包括数据点之间的潜在因果关系，这使MLLM能够更有效地识别未见图像并理解新概念

为了方便评估这种新方法，引入了ISEKAI数据集，该数据集专门包含为链接上下文学习而设计的未见的生成图像的标签对

大量实验表明，LCL-MLLM对新颖概念的链接上下文学习能力优于普通MLLM

<img src="https://img.saraba1st.com/forum/202308/17/155211w8naeeaez8nzfaaw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-154322.jpg</strong> (159.1 KB, 下载次数: 0)

下载附件

2023-8-17 15:52 上传

本文提出的链接上下文学习的演示对话，在向模型呈现一对未见的图像和新颖的概念后，改进的模型获得了在整个对话过程中学习和保留所获得知识的能力，而普通MLLM无法提供准确的答案

<img src="https://img.saraba1st.com/forum/202308/17/155219hr14umjmdjrmymvk.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-154435.jpg</strong> (138.77 KB, 下载次数: 0)

下载附件

2023-8-17 15:52 上传

链接上下文学习与普通上下文学习之间的区别，上下文学习涉及为演示提供不相关的任务，而链接上下文学习的演示和推理阶段之间存在直接的因果关系

<img src="https://img.saraba1st.com/forum/202308/17/155229qtw4sw2mfa4zdn3k.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-154544.jpg</strong> (78.05 KB, 下载次数: 0)

下载附件

2023-8-17 15:52 上传

ISEKAI数据集几个类别的结果概览图，本文模型在几乎所有类别上都优于OpenFlamingo和Otter，在涉及完全未见的图像的场景中展示了卓越的性能

<img src="https://img.saraba1st.com/forum/202308/17/155234khppph4a8pa75zfl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-154703.jpg</strong> (385.65 KB, 下载次数: 0)

下载附件

2023-8-17 15:52 上传

ISEKAI数据集概述，该数据集完全由生成的图像组成，其中“ISEKAI World”中的图像在现实生活中不存在，而“Real World”中的图像来自现实

<img src="https://img.saraba1st.com/forum/202308/17/155238lnnzs6w7d9ygv7gn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-154850.jpg</strong> (246.58 KB, 下载次数: 0)

下载附件

2023-8-17 15:52 上传

对ImageNet-100从0-shot到16-shot的定量评估，以准确率衡量，与 Otter和OpenFlamingo相比，取得了最好的结果

<img src="https://img.saraba1st.com/forum/202308/17/155243zf0xfxf376stqmae.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-155011.jpg</strong> (309.14 KB, 下载次数: 0)

下载附件

2023-8-17 15:52 上传

与 OpenFlamingo、Otter之间关于全新图像理解结果的定性比较，“Cactihog”这个名字是“cactus”和“hedgehog”的融合，结合了这两种生物的主要特征，“MushroomHaven”这个名字暗示了一个以巨型蘑菇为特征的居所

消融实验与评估结果:

<img src="https://img.saraba1st.com/forum/202308/17/155258y6kspjdzb4aps1js.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230817-155125.jpg</strong> (292.63 KB, 下载次数: 0)

下载附件

2023-8-17 15:52 上传

—— 来自 [S1Fun](https://s1fun.koalcat.com)


*****

####  Machinery  
##### 803#       发表于 2023-8-18 19:45

 本帖最后由 Machinery 于 2023-8-18 19:46 编辑 

TeCH

文本引导的拟真人类着装重建

项目主页(待整理):https://huangyangyi.github.io/tech

尽管最近从单个图像重建着装人类任务的解决方法取得了进展，但使用高标准的细节准确地恢复“未见区域(unseen regions)”仍然是一个尚未解决且缺乏关注的挑战

现有方法通常会生成过于光滑且纹理模糊的后视图(back-side)的表面纹理，如何从单个图像中有效地捕获个体的所有视觉属性，从而足以重建未见区域？

受到基础模型力量的推动，TeCH通过利用描述性文本提示(例如服装、颜色、发型)来重建3D人体，这些文本提示是通过服装解析模型和视觉问答VQA自动生成的

而个性化微调的文本到图像扩散模型(T2I/Text-to-Image diffusion model)，则可以学习“难以描述”的外观，为了以可承受的成本表现出高分辨率的3D着装人类，提出了一种基于DMTet的混合3D表征，它由显式的身体形状网格(explicit body shape grid)和隐式的距离场(implicit distance field)组成

在描述性提示+个性化T2I扩散模型的指导下，通过多视图分数蒸馏采样(SDS/multi-view Score Distillation Sampling)和基于原始观察(original observation)的重建损失来优化3D着装人体的几何和纹理，TeCH可以生产出高保真的3D着装人体，具有一致且精美的纹理和详细的全身几何形状

定量和定性实验表明，TeCH在重建精度和渲染质量方面都优于前SOTA方法

<img src="https://img.saraba1st.com/forum/202308/18/194206vyvl5yi4llvpvliw.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-190759__01.jpg</strong> (271.8 KB, 下载次数: 0)

下载附件

2023-8-18 19:42 上传

给定单个图像，TeCH可以重建一个栩栩如生的3D着装人类

“栩栩如生”包含详细的全身几何形状，正面和看不见的区域的面部特征和衣服皱纹，以及具有一致颜色和复杂图案的高质量纹理，其中的关键在于使用个性化的文本到图像扩散模型、视觉问答VQA导出的文本信息来指导重建、分数蒸馏采样(SDS)建立多视图监督等

<img src="https://img.saraba1st.com/forum/202308/18/194216o5gkkxwz1xkfx5zz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-191128__01.jpg</strong> (287.61 KB, 下载次数: 0)

下载附件

2023-8-18 19:42 上传

方法概览图，TeCH首先将人类图像I作为输入

文本引导是通过以下方式构建的：第一步使用服装解析模型(SegFormer)和VQA模型(BLIP)解析具有预定义问题Q的人类属性A，第二步则将特定于主题的外观作为唯一标记嵌入[V]送入DreamBooth D'，接下来，TeCH使用SMPL-X初始化的混合DMTet表征3D着装人类，并使用提示P=[V]+PVQA(A)引导的LSDS优化几何结构和纹理

在优化过程中，引入Lrecon来确保输入视图的一致性，LCD用于强制不同视图之间的颜色一致性，Lnormal用作服装表面的正则化，最后提取的高质量纹理网格可用于各种下游应用程序

<img src="https://img.saraba1st.com/forum/202308/18/194224kcvvimxsmrsbkikc.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-192020.jpg</strong> (111.72 KB, 下载次数: 0)

下载附件

2023-8-18 19:42 上传

提示构筑(P=PVQA+[V])，使用关于个人外观的预定义问题查询VQA模型，以构建可描述的提示PVQA，以及使用背景增强图像对DreamBooth进行微调，将难以描述的特定于主题的细节嵌入到唯一标识符[V]中

<img src="https://img.saraba1st.com/forum/202308/18/194241r3qfzazlqab1hxwh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-192201.jpg</strong> (141.22 KB, 下载次数: 0)

下载附件

2023-8-18 19:42 上传

文本引导的效果，比较了仅使用VQA描述(TeCH vqa)、仅使用DreamBooth身份Token (TeCHdb)以及两者都使用(TeCH)的有效性

<img src="https://img.saraba1st.com/forum/202308/18/194317gky6ykdjy8vqqqpl.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-192347__01.jpg</strong> (300.04 KB, 下载次数: 0)

下载附件

2023-8-18 19:43 上传

上方描述了文本指导中特定元素的影响，例如服装款式和颜色、发型、面部特征以及“[V]”的放置和包含，下方展示了TeCH可以进行文本引导的服装颜色编辑

<img src="https://img.saraba1st.com/forum/202308/18/194326e79sw8jgvlr68l8w.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-192652.jpg</strong> (123.24 KB, 下载次数: 0)

下载附件

2023-8-18 19:43 上传

正常正则化的效果，Lnorm使用预测的法线图像Nˆfront、Nˆback对表面进行正则化

<img src="https://img.saraba1st.com/forum/202308/18/194342wf2cbxffvcpx2z9g.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-192754.jpg</strong> (109.37 KB, 下载次数: 0)

下载附件

2023-8-18 19:43 上传

颜色一致性损失LCD和多姿态训练(MA/multipose training)对纹理优化的影响，LCD可以纠正SDS生成的过饱和的后视图颜色，而MA可以提高自遮挡或极端姿势下的纹理质量

<img src="https://img.saraba1st.com/forum/202308/18/194416a45c1mdmxsn4cxm5.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-193205__01.jpg</strong> (494.97 KB, 下载次数: 0)

下载附件

2023-8-18 19:44 上传

SHHQ图像的定性比较，TeCH很好地概括了具有不同服装风格和纹理的真实自然图像，通过文本引导成功恢复了衣服身体的整体结构，并生成与衣服的颜色图案和材质一致的逼真的全身纹理

<img src="https://img.saraba1st.com/forum/202308/18/194421tf2xbkxbui4oxnzn.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-193443.jpg</strong> (53.65 KB, 下载次数: 0)

下载附件

2023-8-18 19:44 上传

主观用户研究，报告了与其他基线相比用户对TeCH的偏好的百分比，大多数参与者更喜欢几何和彩色渲染纹理的TeCH

<img src="https://img.saraba1st.com/forum/202308/18/194427ev2ynia2ua4znu64.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-193219.jpg</strong> (97.13 KB, 下载次数: 0)

下载附件

2023-8-18 19:44 上传

针对SOTA的定量评估，TeCH的3D指标和2D图像质量指标方面均超过了SOTA基线，这证明了其在准确重建具有复杂细节的穿着人体几何形状以及生成具有一致外观的高质量纹理方面的卓越性能

<img src="https://img.saraba1st.com/forum/202308/18/194432y24plg1a3p3v2kan.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-193641.jpg</strong> (90.04 KB, 下载次数: 0)

下载附件

2023-8-18 19:44 上传

消融实验，定量评估每个组件的有效性，前两个结果的颜色为第一第二。 所有因素均按w.r.t分组，它们的影响：A.几何+纹理，B.仅几何，C.仅纹理

<img src="https://img.saraba1st.com/forum/202308/18/194516ll7o43g5e5gowgrh.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-194502.jpg</strong> (80.82 KB, 下载次数: 0)

下载附件

2023-8-18 19:45 上传

动画化结果，TeCH创建的化身可以使用SMPL-X的运动序列进行动画处理

<img src="https://img.saraba1st.com/forum/202308/18/194520i4w1lndsnro7bnsz.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-193854.jpg</strong> (130.07 KB, 下载次数: 0)

下载附件

2023-8-18 19:45 上传

文本引导的风格化

<img src="https://img.saraba1st.com/forum/202308/18/194524woriql0yz0id4eov.jpg" referrerpolicy="no-referrer">

<strong>Screenshot_20230818-193921.jpg</strong> (116.16 KB, 下载次数: 0)

下载附件

2023-8-18 19:45 上传

所提出的方法依然可能会表现出极其宽松的衣服的嘈杂表面或不匹配的图案，PIXEL预测错误的初始姿势，会将错误传播到TeCH

—— 来自 [S1Fun](https://s1fun.koalcat.com)

